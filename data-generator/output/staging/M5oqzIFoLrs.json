{
  "video_id": "M5oqzIFoLrs",
  "title": "DS320.22 Tuning Partitioning: Understanding Partitioning | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.22 Tuning Partitioning: Understanding Partitioning\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:29:25Z",
  "thumbnail": "https://i.ytimg.com/vi/M5oqzIFoLrs/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=M5oqzIFoLrs",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] the way data is partitioned in a spark application can have a big impact on performance we're going to take a deeper look at partitioning what it is and how we can affect it if you recall an rdd is a big collection of data items records of some kind and we're assuming that there are too many of them to fit on one computer so we have to partition them this diagram shows our list of names and ages those key value pairs being partitioned among a three node cluster you'll see alice and tom are on that top node emma and john are on that bottom node and sarah and bob and ron and other bob are on the middle node now spark automatically does this but we're going to look at what automatically means and what some of the rules regarding partitioning are rdds actually expose an api that let you get at how they've been partitioned there's a property called partitions which returns an array of partition objects you can of course count the size of that array and get the number of partitions that the rdd is distributed among we also might want to understand the algorithm used to do the partitioning spark's going to be looking at each record in an rdd and doing something to it to figure out which worker node it should be stored on the partitioner property tells us which partitioner the rdd is using there's the hash partitioner that's the most common strategy for key based operations there's a range partitioner we'll use that when sorting by key is required and partitioner can also be none which simply means that the partitioning strategy isn't based on the characteristics of the data but it is guaranteed to be random and uniform there are a number of factors that can affect partitioning the way partitioning actually happens and the way partitioning should happen for example the resources available to an application the number of cores that its tasks can run on there are external data sources such as a local collection that you might parallelize or a cassandra table or even potentially hdfs files the size of those data sources can affect the way they get partitioned transformations used to derive a new rdd from an existing rdd there are a number of rules that determine the number of partitions that by default will be used for the new rdd as a function of the partitioning of the source rdd and we'll eventually dive into each one of those in more detail when you think about an rdd in the abstract we've encouraged you to think of it as a collection of records and you might think of those individual records like maybe rows from a cassandra table as the fundamental unit inside those rdds well as rdds come to life on worker nodes the fundamental unit of representation is the partition the rdd is partitioned broken up into pieces stored on workers and that partition or that chunk of the rdd is the thing that lives in that worker node now the smallest unit of computation that spark thinks about is the task so we'll always find we're looking at the status of a running application that the number of tasks is equal to the number of partitions one task gets allocated to operate on one partition in one executor you'll never have more tasks than you have partitions now important to realize that's true on a per stage basis you could potentially have multiple stages in an application that are operating on the same partition but given a stage we have one task per partition now spark needs to have a sensible default of a number of partitions to create for a new rdd and that's represented in the default parallelism property of the spark context object that is defined by default as the number of cores available to the application now you can override that if you'd like but that's a good place to start we often find when we're measuring performance and actually optimizing that we want our number of partitions to be greater than the default parallelism which is why controlling partitioning in spark is so important in fact it's probably the most important optimization step that we're going to learn we're going to learn that there are actually special transformations for repartitioning data sets with the partitioning properties that we'd like to see given the structure of our application we'll find hidden in the api on most transformations we have the ability to specify how many partitions we'd like the resulting rdd to have and we'll see other application settings that affect the way partitioning works [Music] you",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.879,
        "duration": 3.041,
        "text": "the way data is partitioned in a spark"
      },
      {
        "start": 8.639,
        "duration": 3.12,
        "text": "application can have a big"
      },
      {
        "start": 9.92,
        "duration": 3.36,
        "text": "impact on performance we're going to"
      },
      {
        "start": 11.759,
        "duration": 1.92,
        "text": "take a deeper look at partitioning what"
      },
      {
        "start": 13.28,
        "duration": 2.319,
        "text": "it is"
      },
      {
        "start": 13.679,
        "duration": 4.481,
        "text": "and how we can affect it if you recall"
      },
      {
        "start": 15.599,
        "duration": 4.641,
        "text": "an rdd is a big collection of data items"
      },
      {
        "start": 18.16,
        "duration": 3.52,
        "text": "records of some kind and we're assuming"
      },
      {
        "start": 20.24,
        "duration": 2.4,
        "text": "that there are too many of them to fit"
      },
      {
        "start": 21.68,
        "duration": 2.56,
        "text": "on one computer"
      },
      {
        "start": 22.64,
        "duration": 3.12,
        "text": "so we have to partition them this"
      },
      {
        "start": 24.24,
        "duration": 4.64,
        "text": "diagram shows our"
      },
      {
        "start": 25.76,
        "duration": 3.599,
        "text": "list of names and ages those key value"
      },
      {
        "start": 28.88,
        "duration": 2.479,
        "text": "pairs"
      },
      {
        "start": 29.359,
        "duration": 4.241,
        "text": "being partitioned among a three node"
      },
      {
        "start": 31.359,
        "duration": 3.36,
        "text": "cluster you'll see alice and tom are on"
      },
      {
        "start": 33.6,
        "duration": 3.44,
        "text": "that top node"
      },
      {
        "start": 34.719,
        "duration": 3.761,
        "text": "emma and john are on that bottom node"
      },
      {
        "start": 37.04,
        "duration": 4.32,
        "text": "and sarah and bob"
      },
      {
        "start": 38.48,
        "duration": 4.64,
        "text": "and ron and other bob are on the middle"
      },
      {
        "start": 41.36,
        "duration": 3.199,
        "text": "node now spark automatically"
      },
      {
        "start": 43.12,
        "duration": 3.2,
        "text": "does this but we're going to look at"
      },
      {
        "start": 44.559,
        "duration": 3.601,
        "text": "what automatically means and what some"
      },
      {
        "start": 46.32,
        "duration": 4.32,
        "text": "of the rules regarding partitioning are"
      },
      {
        "start": 48.16,
        "duration": 3.68,
        "text": "rdds actually expose an api that let you"
      },
      {
        "start": 50.64,
        "duration": 2.559,
        "text": "get at how they've been"
      },
      {
        "start": 51.84,
        "duration": 4.08,
        "text": "partitioned there's a property called"
      },
      {
        "start": 53.199,
        "duration": 4.721,
        "text": "partitions which returns an array"
      },
      {
        "start": 55.92,
        "duration": 3.84,
        "text": "of partition objects you can of course"
      },
      {
        "start": 57.92,
        "duration": 3.119,
        "text": "count the size of that array and get the"
      },
      {
        "start": 59.76,
        "duration": 3.68,
        "text": "number of partitions"
      },
      {
        "start": 61.039,
        "duration": 3.52,
        "text": "that the rdd is distributed among we"
      },
      {
        "start": 63.44,
        "duration": 1.84,
        "text": "also might want to understand the"
      },
      {
        "start": 64.559,
        "duration": 2.88,
        "text": "algorithm"
      },
      {
        "start": 65.28,
        "duration": 3.68,
        "text": "used to do the partitioning spark's"
      },
      {
        "start": 67.439,
        "duration": 2.241,
        "text": "going to be looking at each record in an"
      },
      {
        "start": 68.96,
        "duration": 3.04,
        "text": "rdd"
      },
      {
        "start": 69.68,
        "duration": 3.759,
        "text": "and doing something to it to figure out"
      },
      {
        "start": 72.0,
        "duration": 3.6,
        "text": "which worker node"
      },
      {
        "start": 73.439,
        "duration": 4.401,
        "text": "it should be stored on the partitioner"
      },
      {
        "start": 75.6,
        "duration": 4.48,
        "text": "property tells us which partitioner"
      },
      {
        "start": 77.84,
        "duration": 3.52,
        "text": "the rdd is using there's the hash"
      },
      {
        "start": 80.08,
        "duration": 3.359,
        "text": "partitioner that's the most common"
      },
      {
        "start": 81.36,
        "duration": 4.32,
        "text": "strategy for key based operations"
      },
      {
        "start": 83.439,
        "duration": 3.68,
        "text": "there's a range partitioner we'll use"
      },
      {
        "start": 85.68,
        "duration": 4.079,
        "text": "that when sorting by"
      },
      {
        "start": 87.119,
        "duration": 3.04,
        "text": "key is required and partitioner can also"
      },
      {
        "start": 89.759,
        "duration": 1.601,
        "text": "be"
      },
      {
        "start": 90.159,
        "duration": 3.121,
        "text": "none which simply means that the"
      },
      {
        "start": 91.36,
        "duration": 4.16,
        "text": "partitioning strategy isn't based on the"
      },
      {
        "start": 93.28,
        "duration": 4.879,
        "text": "characteristics of the data but it is"
      },
      {
        "start": 95.52,
        "duration": 4.16,
        "text": "guaranteed to be random and uniform"
      },
      {
        "start": 98.159,
        "duration": 1.841,
        "text": "there are a number of factors that can"
      },
      {
        "start": 99.68,
        "duration": 2.079,
        "text": "affect"
      },
      {
        "start": 100.0,
        "duration": 3.28,
        "text": "partitioning the way partitioning"
      },
      {
        "start": 101.759,
        "duration": 2.32,
        "text": "actually happens and the way"
      },
      {
        "start": 103.28,
        "duration": 2.799,
        "text": "partitioning"
      },
      {
        "start": 104.079,
        "duration": 3.921,
        "text": "should happen for example the resources"
      },
      {
        "start": 106.079,
        "duration": 2.881,
        "text": "available to an application the number"
      },
      {
        "start": 108.0,
        "duration": 3.6,
        "text": "of cores"
      },
      {
        "start": 108.96,
        "duration": 4.0,
        "text": "that its tasks can run on there are"
      },
      {
        "start": 111.6,
        "duration": 2.879,
        "text": "external data sources"
      },
      {
        "start": 112.96,
        "duration": 3.92,
        "text": "such as a local collection that you"
      },
      {
        "start": 114.479,
        "duration": 4.0,
        "text": "might parallelize or a cassandra table"
      },
      {
        "start": 116.88,
        "duration": 4.0,
        "text": "or even potentially hdfs"
      },
      {
        "start": 118.479,
        "duration": 4.24,
        "text": "files the size of those data sources can"
      },
      {
        "start": 120.88,
        "duration": 4.239,
        "text": "affect the way they get partitioned"
      },
      {
        "start": 122.719,
        "duration": 4.4,
        "text": "transformations used to derive a new rdd"
      },
      {
        "start": 125.119,
        "duration": 3.681,
        "text": "from an existing rdd there are a number"
      },
      {
        "start": 127.119,
        "duration": 2.56,
        "text": "of rules that determine the number of"
      },
      {
        "start": 128.8,
        "duration": 3.76,
        "text": "partitions that"
      },
      {
        "start": 129.679,
        "duration": 3.2,
        "text": "by default will be used for the new rdd"
      },
      {
        "start": 132.56,
        "duration": 2.64,
        "text": "as"
      },
      {
        "start": 132.879,
        "duration": 3.281,
        "text": "a function of the partitioning of the"
      },
      {
        "start": 135.2,
        "duration": 2.72,
        "text": "source rdd"
      },
      {
        "start": 136.16,
        "duration": 3.52,
        "text": "and we'll eventually dive into each one"
      },
      {
        "start": 137.92,
        "duration": 3.28,
        "text": "of those in more detail when you think"
      },
      {
        "start": 139.68,
        "duration": 2.72,
        "text": "about an rdd in the abstract we've"
      },
      {
        "start": 141.2,
        "duration": 3.759,
        "text": "encouraged you to think of it"
      },
      {
        "start": 142.4,
        "duration": 3.199,
        "text": "as a collection of records and you might"
      },
      {
        "start": 144.959,
        "duration": 2.321,
        "text": "think of those"
      },
      {
        "start": 145.599,
        "duration": 3.921,
        "text": "individual records like maybe rows from"
      },
      {
        "start": 147.28,
        "duration": 4.0,
        "text": "a cassandra table as the fundamental"
      },
      {
        "start": 149.52,
        "duration": 4.32,
        "text": "unit inside those rdds"
      },
      {
        "start": 151.28,
        "duration": 3.039,
        "text": "well as rdds come to life on worker"
      },
      {
        "start": 153.84,
        "duration": 2.96,
        "text": "nodes"
      },
      {
        "start": 154.319,
        "duration": 3.92,
        "text": "the fundamental unit of representation"
      },
      {
        "start": 156.8,
        "duration": 3.76,
        "text": "is the partition"
      },
      {
        "start": 158.239,
        "duration": 4.161,
        "text": "the rdd is partitioned broken up into"
      },
      {
        "start": 160.56,
        "duration": 3.6,
        "text": "pieces stored on workers"
      },
      {
        "start": 162.4,
        "duration": 3.36,
        "text": "and that partition or that chunk of the"
      },
      {
        "start": 164.16,
        "duration": 3.439,
        "text": "rdd is the thing"
      },
      {
        "start": 165.76,
        "duration": 3.839,
        "text": "that lives in that worker node now the"
      },
      {
        "start": 167.599,
        "duration": 2.881,
        "text": "smallest unit of computation that spark"
      },
      {
        "start": 169.599,
        "duration": 3.36,
        "text": "thinks about"
      },
      {
        "start": 170.48,
        "duration": 3.839,
        "text": "is the task so we'll always find we're"
      },
      {
        "start": 172.959,
        "duration": 3.681,
        "text": "looking at the status of a running"
      },
      {
        "start": 174.319,
        "duration": 4.801,
        "text": "application that the number of tasks is"
      },
      {
        "start": 176.64,
        "duration": 4.8,
        "text": "equal to the number of partitions one"
      },
      {
        "start": 179.12,
        "duration": 3.119,
        "text": "task gets allocated to operate on one"
      },
      {
        "start": 181.44,
        "duration": 3.04,
        "text": "partition"
      },
      {
        "start": 182.239,
        "duration": 4.241,
        "text": "in one executor you'll never have more"
      },
      {
        "start": 184.48,
        "duration": 4.0,
        "text": "tasks than you have partitions"
      },
      {
        "start": 186.48,
        "duration": 4.08,
        "text": "now important to realize that's true on"
      },
      {
        "start": 188.48,
        "duration": 2.56,
        "text": "a per stage basis you could potentially"
      },
      {
        "start": 190.56,
        "duration": 3.039,
        "text": "have"
      },
      {
        "start": 191.04,
        "duration": 4.559,
        "text": "multiple stages in an application that"
      },
      {
        "start": 193.599,
        "duration": 4.321,
        "text": "are operating on the same partition but"
      },
      {
        "start": 195.599,
        "duration": 3.841,
        "text": "given a stage we have one task per"
      },
      {
        "start": 197.92,
        "duration": 2.56,
        "text": "partition now spark needs to have a"
      },
      {
        "start": 199.44,
        "duration": 3.92,
        "text": "sensible default"
      },
      {
        "start": 200.48,
        "duration": 4.08,
        "text": "of a number of partitions to create for"
      },
      {
        "start": 203.36,
        "duration": 2.959,
        "text": "a new rdd"
      },
      {
        "start": 204.56,
        "duration": 3.039,
        "text": "and that's represented in the default"
      },
      {
        "start": 206.319,
        "duration": 3.28,
        "text": "parallelism property"
      },
      {
        "start": 207.599,
        "duration": 4.0,
        "text": "of the spark context object that is"
      },
      {
        "start": 209.599,
        "duration": 4.081,
        "text": "defined by default as the number of"
      },
      {
        "start": 211.599,
        "duration": 3.681,
        "text": "cores available to the application"
      },
      {
        "start": 213.68,
        "duration": 3.68,
        "text": "now you can override that if you'd like"
      },
      {
        "start": 215.28,
        "duration": 3.36,
        "text": "but that's a good place to start"
      },
      {
        "start": 217.36,
        "duration": 2.959,
        "text": "we often find when we're measuring"
      },
      {
        "start": 218.64,
        "duration": 3.44,
        "text": "performance and actually optimizing that"
      },
      {
        "start": 220.319,
        "duration": 3.441,
        "text": "we want our number of partitions to be"
      },
      {
        "start": 222.08,
        "duration": 3.519,
        "text": "greater than the default parallelism"
      },
      {
        "start": 223.76,
        "duration": 4.0,
        "text": "which is why controlling partitioning in"
      },
      {
        "start": 225.599,
        "duration": 4.0,
        "text": "spark is so important in fact it's"
      },
      {
        "start": 227.76,
        "duration": 2.24,
        "text": "probably the most important optimization"
      },
      {
        "start": 229.599,
        "duration": 1.681,
        "text": "step"
      },
      {
        "start": 230.0,
        "duration": 2.56,
        "text": "that we're going to learn we're going to"
      },
      {
        "start": 231.28,
        "duration": 3.2,
        "text": "learn that there are actually special"
      },
      {
        "start": 232.56,
        "duration": 2.56,
        "text": "transformations for repartitioning data"
      },
      {
        "start": 234.48,
        "duration": 2.0,
        "text": "sets"
      },
      {
        "start": 235.12,
        "duration": 3.28,
        "text": "with the partitioning properties that"
      },
      {
        "start": 236.48,
        "duration": 3.52,
        "text": "we'd like to see given"
      },
      {
        "start": 238.4,
        "duration": 3.36,
        "text": "the structure of our application we'll"
      },
      {
        "start": 240.0,
        "duration": 3.439,
        "text": "find hidden in the api on most"
      },
      {
        "start": 241.76,
        "duration": 2.64,
        "text": "transformations we have the ability to"
      },
      {
        "start": 243.439,
        "duration": 2.401,
        "text": "specify"
      },
      {
        "start": 244.4,
        "duration": 3.44,
        "text": "how many partitions we'd like the"
      },
      {
        "start": 245.84,
        "duration": 3.84,
        "text": "resulting rdd to have"
      },
      {
        "start": 247.84,
        "duration": 8.0,
        "text": "and we'll see other application settings"
      },
      {
        "start": 249.68,
        "duration": 6.16,
        "text": "that affect the way partitioning works"
      },
      {
        "start": 256.5,
        "duration": 5.259,
        "text": "[Music]"
      },
      {
        "start": 259.68,
        "duration": 2.079,
        "text": "you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:34:29.959072+00:00"
}