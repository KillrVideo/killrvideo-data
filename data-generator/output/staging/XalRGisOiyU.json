{
  "video_id": "XalRGisOiyU",
  "title": "DS320.43 Spark SQL: RDD Operations on DataFrames | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.43 Spark SQL: RDD Operations on DataFrames\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:33:53Z",
  "thumbnail": "https://i.ytimg.com/vi/XalRGisOiyU/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=XalRGisOiyU",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's take a look at a few operations you can perform on data frames that are either rdd-like or result in creating an rdd now the methods you see here are methods that look very familiar they're from the rdd api they're re-partition and coalesce which change the number of partitions that a data frame is split into and persist cache and unpersist which are really if you will three sides of the same coin they determine the way a data frame is going to be persistent these are just like their equivalents in the rdd world so if you want to go back and take a look at rdd persistence the semantics of these methods are going to look exactly the same shorthand really here is that cache will persist the data frame to memory it'll keep it around in memory and unpersist will undo the operation of the cache method if you have a data frame and you want to convert it to an rdd for any reason because there's some operations using the rdd api that you'd rather use you can do that the map method on the data frame object produces an rdd every row in the data frame is going to result in a single element being produced in the resulting rdd flatmap just like its cousin in the rdd api proper is going to take a single row from the data frame and produce potentially many elements one or more elements in the resulting rdd the rdd method is going to convert the source data frame one to one into an rdd of rows the two json method is going to do that same thing except instead of row objects in the resulting rdd it's going to create json strings so each element in the resulting rdd will be represented in json some simple data frame actions there's collect which is going to gather up all of the rows in the data frame and return them to the application or the spark shell as an array so if you've got a lot of results maybe you don't want to call collect but if you know it's a small result set that's perfectly fine to do count just going to count them first or head is super handy for grabbing the first element great for hacking and sometimes it's the thing you want to do you want to grab the first row in the collection take is sort of the big brother to first that's going to allow you to get the first n rows from the data frame show is kind of the human readable version of that it's going to let you grab the first n rows out of the data frame and display them in tabular form it actually returns a string so if you're using this from the spark shell it's a very very nice hacking or debugging kind of method for each is just like the equivalent in the rdd we pass it a function and it's going to execute that function on each row in the data frame now the return value of for each is not the important thing here it is an action and it is going to trigger computation but we're expecting that function that we pass in to cause some kind of side effect outside of the function like updating an accumulator variable or doing something to interact with an external system and just one quick example here another simple query using the sql method we'll pass in the string literal cql query and then with the results of that which is a data frame we're going to call the coalesce method which is we know this is a pretty small result set we only want it to be in one partition we want it to be on one node the overhead of trying to split up work on more than one node isn't worth it let's just put it all in one place then we'll cache it so that will be persisted in memory at that point i can do two actions on that coalesced cached data frame which is what i do i call count i get the total number of elements in it and then i show the first four see if i hadn't cached it then both of those actions would trigger all the same computations twice to produce the data frame that they're called on so since it's cached we're in better shape this is more efficient trivial spark application and we see the results that we get a total count of 54 and the first four rows of the data so that's just a quick idea of a few actions that we can perform on data frames a little bit of means of converting data frames back to rdds should we ever need to you can see again here the api is not super complex we can really begin to do some meaningful things with just a little bit of knowledge you",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 7.12,
        "duration": 2.559,
        "text": "let's take a look at a few"
      },
      {
        "start": 8.32,
        "duration": 3.84,
        "text": "operations you can perform on data"
      },
      {
        "start": 9.679,
        "duration": 5.281,
        "text": "frames that are either rdd-like"
      },
      {
        "start": 12.16,
        "duration": 4.48,
        "text": "or result in creating an rdd now the"
      },
      {
        "start": 14.96,
        "duration": 2.0,
        "text": "methods you see here are methods that"
      },
      {
        "start": 16.64,
        "duration": 2.479,
        "text": "look"
      },
      {
        "start": 16.96,
        "duration": 4.319,
        "text": "very familiar they're from the rdd api"
      },
      {
        "start": 19.119,
        "duration": 4.0,
        "text": "they're re-partition and coalesce which"
      },
      {
        "start": 21.279,
        "duration": 4.721,
        "text": "change the number of partitions"
      },
      {
        "start": 23.119,
        "duration": 4.561,
        "text": "that a data frame is split into and"
      },
      {
        "start": 26.0,
        "duration": 3.359,
        "text": "persist cache and unpersist"
      },
      {
        "start": 27.68,
        "duration": 3.919,
        "text": "which are really if you will three sides"
      },
      {
        "start": 29.359,
        "duration": 3.841,
        "text": "of the same coin they determine the way"
      },
      {
        "start": 31.599,
        "duration": 3.441,
        "text": "a data frame is going to be persistent"
      },
      {
        "start": 33.2,
        "duration": 3.76,
        "text": "these are just like their equivalents"
      },
      {
        "start": 35.04,
        "duration": 3.839,
        "text": "in the rdd world so if you want to go"
      },
      {
        "start": 36.96,
        "duration": 3.279,
        "text": "back and take a look at rdd persistence"
      },
      {
        "start": 38.879,
        "duration": 1.761,
        "text": "the semantics of these methods are going"
      },
      {
        "start": 40.239,
        "duration": 3.121,
        "text": "to look"
      },
      {
        "start": 40.64,
        "duration": 3.439,
        "text": "exactly the same shorthand really here"
      },
      {
        "start": 43.36,
        "duration": 3.28,
        "text": "is that"
      },
      {
        "start": 44.079,
        "duration": 4.561,
        "text": "cache will persist the data frame to"
      },
      {
        "start": 46.64,
        "duration": 4.399,
        "text": "memory it'll keep it around in memory"
      },
      {
        "start": 48.64,
        "duration": 3.28,
        "text": "and unpersist will undo the operation of"
      },
      {
        "start": 51.039,
        "duration": 2.081,
        "text": "the cache method"
      },
      {
        "start": 51.92,
        "duration": 3.2,
        "text": "if you have a data frame and you want to"
      },
      {
        "start": 53.12,
        "duration": 2.88,
        "text": "convert it to an rdd for any reason"
      },
      {
        "start": 55.12,
        "duration": 2.88,
        "text": "because there's some"
      },
      {
        "start": 56.0,
        "duration": 3.6,
        "text": "operations using the rdd api that you'd"
      },
      {
        "start": 58.0,
        "duration": 4.64,
        "text": "rather use you can do that"
      },
      {
        "start": 59.6,
        "duration": 6.16,
        "text": "the map method on the data frame object"
      },
      {
        "start": 62.64,
        "duration": 5.28,
        "text": "produces an rdd every row"
      },
      {
        "start": 65.76,
        "duration": 3.92,
        "text": "in the data frame is going to result in"
      },
      {
        "start": 67.92,
        "duration": 4.32,
        "text": "a single element being produced"
      },
      {
        "start": 69.68,
        "duration": 4.799,
        "text": "in the resulting rdd flatmap just like"
      },
      {
        "start": 72.24,
        "duration": 4.08,
        "text": "its cousin in the rdd api proper"
      },
      {
        "start": 74.479,
        "duration": 4.0,
        "text": "is going to take a single row from the"
      },
      {
        "start": 76.32,
        "duration": 4.799,
        "text": "data frame and produce potentially"
      },
      {
        "start": 78.479,
        "duration": 3.68,
        "text": "many elements one or more elements in"
      },
      {
        "start": 81.119,
        "duration": 3.441,
        "text": "the resulting"
      },
      {
        "start": 82.159,
        "duration": 3.6,
        "text": "rdd the rdd method is going to convert"
      },
      {
        "start": 84.56,
        "duration": 4.879,
        "text": "the source data frame"
      },
      {
        "start": 85.759,
        "duration": 5.521,
        "text": "one to one into an rdd of rows the two"
      },
      {
        "start": 89.439,
        "duration": 4.161,
        "text": "json method is going to do that same"
      },
      {
        "start": 91.28,
        "duration": 4.24,
        "text": "thing except instead of row objects"
      },
      {
        "start": 93.6,
        "duration": 4.159,
        "text": "in the resulting rdd it's going to"
      },
      {
        "start": 95.52,
        "duration": 5.04,
        "text": "create json strings so each element"
      },
      {
        "start": 97.759,
        "duration": 3.68,
        "text": "in the resulting rdd will be represented"
      },
      {
        "start": 100.56,
        "duration": 3.36,
        "text": "in json"
      },
      {
        "start": 101.439,
        "duration": 4.481,
        "text": "some simple data frame actions there's"
      },
      {
        "start": 103.92,
        "duration": 4.559,
        "text": "collect which is going to gather up"
      },
      {
        "start": 105.92,
        "duration": 3.839,
        "text": "all of the rows in the data frame and"
      },
      {
        "start": 108.479,
        "duration": 3.521,
        "text": "return them to"
      },
      {
        "start": 109.759,
        "duration": 4.081,
        "text": "the application or the spark shell as an"
      },
      {
        "start": 112.0,
        "duration": 3.119,
        "text": "array so if you've got a lot of results"
      },
      {
        "start": 113.84,
        "duration": 2.879,
        "text": "maybe you don't want to call collect but"
      },
      {
        "start": 115.119,
        "duration": 3.921,
        "text": "if you know it's a small"
      },
      {
        "start": 116.719,
        "duration": 4.0,
        "text": "result set that's perfectly fine to do"
      },
      {
        "start": 119.04,
        "duration": 4.32,
        "text": "count just going to count them"
      },
      {
        "start": 120.719,
        "duration": 4.0,
        "text": "first or head is super handy for"
      },
      {
        "start": 123.36,
        "duration": 2.96,
        "text": "grabbing the first element"
      },
      {
        "start": 124.719,
        "duration": 3.52,
        "text": "great for hacking and sometimes it's the"
      },
      {
        "start": 126.32,
        "duration": 3.359,
        "text": "thing you want to do you want to grab"
      },
      {
        "start": 128.239,
        "duration": 3.761,
        "text": "the first row"
      },
      {
        "start": 129.679,
        "duration": 3.121,
        "text": "in the collection take is sort of the"
      },
      {
        "start": 132.0,
        "duration": 2.64,
        "text": "big brother"
      },
      {
        "start": 132.8,
        "duration": 3.04,
        "text": "to first that's going to allow you to"
      },
      {
        "start": 134.64,
        "duration": 4.319,
        "text": "get the first"
      },
      {
        "start": 135.84,
        "duration": 4.64,
        "text": "n rows from the data frame show is kind"
      },
      {
        "start": 138.959,
        "duration": 3.041,
        "text": "of the human readable version of that"
      },
      {
        "start": 140.48,
        "duration": 4.32,
        "text": "it's going to let you grab the first"
      },
      {
        "start": 142.0,
        "duration": 5.28,
        "text": "n rows out of the data frame and display"
      },
      {
        "start": 144.8,
        "duration": 4.56,
        "text": "them in tabular form it actually returns"
      },
      {
        "start": 147.28,
        "duration": 3.679,
        "text": "a string so if you're using this from"
      },
      {
        "start": 149.36,
        "duration": 3.28,
        "text": "the spark shell it's a very very nice"
      },
      {
        "start": 150.959,
        "duration": 4.321,
        "text": "hacking or debugging kind of method"
      },
      {
        "start": 152.64,
        "duration": 3.52,
        "text": "for each is just like the equivalent in"
      },
      {
        "start": 155.28,
        "duration": 2.959,
        "text": "the rdd"
      },
      {
        "start": 156.16,
        "duration": 3.28,
        "text": "we pass it a function and it's going to"
      },
      {
        "start": 158.239,
        "duration": 4.481,
        "text": "execute that function"
      },
      {
        "start": 159.44,
        "duration": 5.2,
        "text": "on each row in the data frame"
      },
      {
        "start": 162.72,
        "duration": 3.04,
        "text": "now the return value of for each is not"
      },
      {
        "start": 164.64,
        "duration": 2.72,
        "text": "the important thing here"
      },
      {
        "start": 165.76,
        "duration": 3.36,
        "text": "it is an action and it is going to"
      },
      {
        "start": 167.36,
        "duration": 3.28,
        "text": "trigger computation but we're expecting"
      },
      {
        "start": 169.12,
        "duration": 3.44,
        "text": "that function that we pass in"
      },
      {
        "start": 170.64,
        "duration": 3.04,
        "text": "to cause some kind of side effect"
      },
      {
        "start": 172.56,
        "duration": 3.44,
        "text": "outside of the function"
      },
      {
        "start": 173.68,
        "duration": 3.919,
        "text": "like updating an accumulator variable or"
      },
      {
        "start": 176.0,
        "duration": 3.519,
        "text": "doing something to interact with"
      },
      {
        "start": 177.599,
        "duration": 4.161,
        "text": "an external system and just one quick"
      },
      {
        "start": 179.519,
        "duration": 4.0,
        "text": "example here another simple query using"
      },
      {
        "start": 181.76,
        "duration": 3.28,
        "text": "the sql method we'll pass in the string"
      },
      {
        "start": 183.519,
        "duration": 3.36,
        "text": "literal cql query"
      },
      {
        "start": 185.04,
        "duration": 3.839,
        "text": "and then with the results of that which"
      },
      {
        "start": 186.879,
        "duration": 3.041,
        "text": "is a data frame we're going to call the"
      },
      {
        "start": 188.879,
        "duration": 2.64,
        "text": "coalesce method"
      },
      {
        "start": 189.92,
        "duration": 3.599,
        "text": "which is we know this is a pretty small"
      },
      {
        "start": 191.519,
        "duration": 3.681,
        "text": "result set we only want it to be in one"
      },
      {
        "start": 193.519,
        "duration": 3.201,
        "text": "partition we want it to be on one node"
      },
      {
        "start": 195.2,
        "duration": 3.2,
        "text": "the overhead of trying to split up work"
      },
      {
        "start": 196.72,
        "duration": 3.36,
        "text": "on more than one node isn't worth it"
      },
      {
        "start": 198.4,
        "duration": 4.0,
        "text": "let's just put it all in one place then"
      },
      {
        "start": 200.08,
        "duration": 5.2,
        "text": "we'll cache it so that will be persisted"
      },
      {
        "start": 202.4,
        "duration": 3.919,
        "text": "in memory at that point i can do two"
      },
      {
        "start": 205.28,
        "duration": 4.319,
        "text": "actions"
      },
      {
        "start": 206.319,
        "duration": 4.241,
        "text": "on that coalesced cached data frame"
      },
      {
        "start": 209.599,
        "duration": 2.64,
        "text": "which is what i do"
      },
      {
        "start": 210.56,
        "duration": 4.0,
        "text": "i call count i get the total number of"
      },
      {
        "start": 212.239,
        "duration": 4.72,
        "text": "elements in it and then i show the first"
      },
      {
        "start": 214.56,
        "duration": 3.679,
        "text": "four see if i hadn't cached it then both"
      },
      {
        "start": 216.959,
        "duration": 3.441,
        "text": "of those actions would trigger"
      },
      {
        "start": 218.239,
        "duration": 3.441,
        "text": "all the same computations twice to"
      },
      {
        "start": 220.4,
        "duration": 3.199,
        "text": "produce the data frame that they're"
      },
      {
        "start": 221.68,
        "duration": 3.279,
        "text": "called on so since it's cached"
      },
      {
        "start": 223.599,
        "duration": 3.521,
        "text": "we're in better shape this is more"
      },
      {
        "start": 224.959,
        "duration": 3.681,
        "text": "efficient trivial spark application"
      },
      {
        "start": 227.12,
        "duration": 3.679,
        "text": "and we see the results that we get a"
      },
      {
        "start": 228.64,
        "duration": 2.64,
        "text": "total count of 54 and the first four"
      },
      {
        "start": 230.799,
        "duration": 2.961,
        "text": "rows"
      },
      {
        "start": 231.28,
        "duration": 3.76,
        "text": "of the data so that's just a quick idea"
      },
      {
        "start": 233.76,
        "duration": 3.199,
        "text": "of a few actions"
      },
      {
        "start": 235.04,
        "duration": 3.759,
        "text": "that we can perform on data frames a"
      },
      {
        "start": 236.959,
        "duration": 3.28,
        "text": "little bit of means of converting data"
      },
      {
        "start": 238.799,
        "duration": 3.601,
        "text": "frames back to rdds"
      },
      {
        "start": 240.239,
        "duration": 3.121,
        "text": "should we ever need to you can see again"
      },
      {
        "start": 242.4,
        "duration": 2.96,
        "text": "here the api"
      },
      {
        "start": 243.36,
        "duration": 3.519,
        "text": "is not super complex we can really begin"
      },
      {
        "start": 245.36,
        "duration": 10.32,
        "text": "to do some meaningful things"
      },
      {
        "start": 246.879,
        "duration": 10.881,
        "text": "with just a little bit of knowledge"
      },
      {
        "start": 255.68,
        "duration": 2.08,
        "text": "you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:11:20.941351+00:00"
}