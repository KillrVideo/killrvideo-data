{
  "video_id": "-xJfUGCS7Ko",
  "title": "DS320.23 Tuning Partitioning: Partitioning Rules | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.23 Tuning Partitioning: Partitioning Rules\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:29:36Z",
  "thumbnail": "https://i.ytimg.com/vi/-xJfUGCS7Ko/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=-xJfUGCS7Ko",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] spark uses different defaults to partition different kinds of data depending on how we produce it when creating an rdd from an external data source it depends on where that data is coming from it could be coming from a local scholar collection that we've statically initialized and we're parallelizing could be coming from a cassandra table could potentially be coming from hdfs in the general case we could be transforming an rdd into a new rdd using either a generic transformation or a key based transformation all of these things have different rules let's walk through them and look at the defaults in each case it's important to know the defaults by the way so we know whether they're serving our purposes and whether we need to override them to make things run a little faster or even much faster when parallelizing a statically initialized scala collection we're going to get a number of partitions equal to default parallelism and we're going to get a partitioner of none if you look at the example here you see we've made a list containing some number of items it's really kind of alighted there on the slide and when we parallelize that and then inquire the number of partitions you see we get three which happens to be equal to default parallelism in this case and the partitioner reports as none just the way it's supposed to when retrieving data from the spark cassandra connector the situation is a little bit different the connector will estimate how much data is going to come back in that rdd from the cassandra query divide that by 64 megabytes and come up with some number say it was thinking it would get 256 megabytes back divided by 64 megabytes it'll come up with four in that case well if that's greater than default parallelism which in this example it is it's going to use the bigger number if default parallelism is bigger than that estimated data size calculation it'll use default parallelism so that calculation or default parallelism whichever is greater that's the number of partitions we'll get and again we'll have a partitioner of none the situation is similar when we're reading a text file from the cassandra file system or even hdfs it's going to do the same calculation estimate the size of the input data divide by 64 megabytes and come up with a number if that's bigger than default parallelism it'll use that otherwise it'll go with default parallelism and as you can see in this example here that is happening also again the partitioner returns as none now when using a generic transformation to create a new rdd out of a source rdd the number of partitions can change and potentially the partitioner can change let's walk through these rules so you have some idea how these work you probably want to keep this as a reference refer back to it when you're optimizing your own spark applications so by default transformations like filter map flat map and distinct those generic transformations will produce the same number of partitions as in the parent rdd they'll also produce a partitioner of none except in the case of filter which preserves its parent rdd's partitioner if it had one when creating a union of one rdd with another the result rdd will have a number of partitions equal to the number of partitions in both of the input rdds added together when creating an intersection of one rdd with another the resulting rdd will have a number of partitions equal to the greater of the partition counts of the two source rdds when doing a set subtraction of one rdd from another the resulting rdd will have a number of partitions equal to the number of partitions in the source rdd not considering the number of partitions in the other rdd when doing a cartesian product of two rdds the result rdd has a number of partitions equal to the product of the partition counts of the two input rdds clearly you could see how that could cause trouble here we see some examples of some of these generic transformations let's walk through some of the defaults of the key based transformations these work a little differently you see that top group including reduce by key fold by key combined by key group by key those all produce the same number of partitions as in the parent rdd and in fact those four transformations also always result in the output rdd having the hash partitioner sort by key always produces the range partitioner because that always has a sorted output map values and flat map values again don't change the count of the number of partitions but they're always going to use the parent rdds partitioner they have to because they haven't changed the keys you use those transformations when you explicitly don't want to touch the keys and so you don't want to cause any sort of re-partitioning to happen co-group join left out or join write out or join those are a little bit more complicated and we'll get to those in a little bit but here are some examples you can type in if you've got a spark cluster running and you've got access to the test data and you can actually try out some of these and watch the partitioning change or not change as it's supposed to now for the binary rdd operations i mentioned a moment ago like join and co-group and left and right outer join things are a little more complex take a look at this table to see how this works out all of this depends on whether the partitioner of the input rdd can be reused to give you a rough idea of that if it's the hash partitioner then it can be reused across the transformation certain kinds of custom partitioners may also have that property that'll depend on how they're implemented but if you look at the table you'll see if for example neither of the input rdds have a reusable partitioner then the number of partitions of the resulting rdd is simply going to be the maximum of the partition counts of the two input rdds that's also the case at the bottom of the table if both rdds have a partitioner that can be reused however if just one of them has a partitioner that can be reused then that rdd's partition size is going to dominate in the output for example the second row says if uh rdd has a reusable partitioner and other rdd doesn't then it's rdd whose partition count determines the partition count of the resulting rdd and here's an example of a join so you can see this happening note we've done two cassandra table queries and we've converted both of those into pair rdds with that key by transformation then we're able to join those two on user id which is the last line of executable code in the example the second rdd called interactions has a reusable partitioner you'll see it has the hash partitioner and four partitions because it gets that final group by key action called on it the first rdd we create users does not have a reusable partition and has a partition count of three so you see in the output the reusable rdds partition count dominates and we get four in the final result [Music] you",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.399,
        "duration": 2.721,
        "text": "spark uses"
      },
      {
        "start": 7.44,
        "duration": 3.279,
        "text": "different defaults to partition"
      },
      {
        "start": 9.12,
        "duration": 3.36,
        "text": "different kinds of data depending on how"
      },
      {
        "start": 10.719,
        "duration": 3.201,
        "text": "we produce it when creating an rdd from"
      },
      {
        "start": 12.48,
        "duration": 2.48,
        "text": "an external data source it depends on"
      },
      {
        "start": 13.92,
        "duration": 2.72,
        "text": "where that data"
      },
      {
        "start": 14.96,
        "duration": 2.96,
        "text": "is coming from it could be coming from a"
      },
      {
        "start": 16.64,
        "duration": 2.479,
        "text": "local scholar collection that we've"
      },
      {
        "start": 17.92,
        "duration": 2.24,
        "text": "statically initialized and we're"
      },
      {
        "start": 19.119,
        "duration": 2.641,
        "text": "parallelizing"
      },
      {
        "start": 20.16,
        "duration": 3.44,
        "text": "could be coming from a cassandra table"
      },
      {
        "start": 21.76,
        "duration": 3.2,
        "text": "could potentially be coming from hdfs"
      },
      {
        "start": 23.6,
        "duration": 3.36,
        "text": "in the general case we could be"
      },
      {
        "start": 24.96,
        "duration": 3.52,
        "text": "transforming an rdd into"
      },
      {
        "start": 26.96,
        "duration": 2.88,
        "text": "a new rdd using either a generic"
      },
      {
        "start": 28.48,
        "duration": 2.24,
        "text": "transformation or a key based"
      },
      {
        "start": 29.84,
        "duration": 2.239,
        "text": "transformation"
      },
      {
        "start": 30.72,
        "duration": 3.2,
        "text": "all of these things have different rules"
      },
      {
        "start": 32.079,
        "duration": 3.121,
        "text": "let's walk through them and look at the"
      },
      {
        "start": 33.92,
        "duration": 2.72,
        "text": "defaults in each case"
      },
      {
        "start": 35.2,
        "duration": 2.879,
        "text": "it's important to know the defaults by"
      },
      {
        "start": 36.64,
        "duration": 2.96,
        "text": "the way so we know whether they're"
      },
      {
        "start": 38.079,
        "duration": 2.561,
        "text": "serving our purposes and whether we need"
      },
      {
        "start": 39.6,
        "duration": 2.88,
        "text": "to override them"
      },
      {
        "start": 40.64,
        "duration": 3.759,
        "text": "to make things run a little faster or"
      },
      {
        "start": 42.48,
        "duration": 3.68,
        "text": "even much faster when parallelizing a"
      },
      {
        "start": 44.399,
        "duration": 3.121,
        "text": "statically initialized scala collection"
      },
      {
        "start": 46.16,
        "duration": 3.84,
        "text": "we're going to get a number of"
      },
      {
        "start": 47.52,
        "duration": 4.32,
        "text": "partitions equal to default parallelism"
      },
      {
        "start": 50.0,
        "duration": 3.52,
        "text": "and we're going to get a partitioner of"
      },
      {
        "start": 51.84,
        "duration": 4.0,
        "text": "none if you look at the example here you"
      },
      {
        "start": 53.52,
        "duration": 4.0,
        "text": "see we've made a list containing"
      },
      {
        "start": 55.84,
        "duration": 3.039,
        "text": "some number of items it's really kind of"
      },
      {
        "start": 57.52,
        "duration": 3.039,
        "text": "alighted there on the slide"
      },
      {
        "start": 58.879,
        "duration": 3.441,
        "text": "and when we parallelize that and then"
      },
      {
        "start": 60.559,
        "duration": 3.761,
        "text": "inquire the number of partitions"
      },
      {
        "start": 62.32,
        "duration": 3.119,
        "text": "you see we get three which happens to be"
      },
      {
        "start": 64.32,
        "duration": 2.799,
        "text": "equal to default"
      },
      {
        "start": 65.439,
        "duration": 3.521,
        "text": "parallelism in this case and the"
      },
      {
        "start": 67.119,
        "duration": 3.281,
        "text": "partitioner reports as none just the way"
      },
      {
        "start": 68.96,
        "duration": 2.64,
        "text": "it's supposed to when retrieving data"
      },
      {
        "start": 70.4,
        "duration": 1.759,
        "text": "from the spark cassandra connector the"
      },
      {
        "start": 71.6,
        "duration": 2.559,
        "text": "situation"
      },
      {
        "start": 72.159,
        "duration": 4.081,
        "text": "is a little bit different the connector"
      },
      {
        "start": 74.159,
        "duration": 4.081,
        "text": "will estimate how much data is going to"
      },
      {
        "start": 76.24,
        "duration": 2.4,
        "text": "come back in that rdd from the cassandra"
      },
      {
        "start": 78.24,
        "duration": 3.04,
        "text": "query"
      },
      {
        "start": 78.64,
        "duration": 3.68,
        "text": "divide that by 64 megabytes and come up"
      },
      {
        "start": 81.28,
        "duration": 3.28,
        "text": "with some number"
      },
      {
        "start": 82.32,
        "duration": 4.479,
        "text": "say it was thinking it would get 256"
      },
      {
        "start": 84.56,
        "duration": 3.12,
        "text": "megabytes back divided by 64 megabytes"
      },
      {
        "start": 86.799,
        "duration": 3.041,
        "text": "it'll come up with"
      },
      {
        "start": 87.68,
        "duration": 3.84,
        "text": "four in that case well if that's greater"
      },
      {
        "start": 89.84,
        "duration": 2.72,
        "text": "than default parallelism which in this"
      },
      {
        "start": 91.52,
        "duration": 3.12,
        "text": "example it is"
      },
      {
        "start": 92.56,
        "duration": 4.0,
        "text": "it's going to use the bigger number if"
      },
      {
        "start": 94.64,
        "duration": 3.92,
        "text": "default parallelism is bigger"
      },
      {
        "start": 96.56,
        "duration": 3.76,
        "text": "than that estimated data size"
      },
      {
        "start": 98.56,
        "duration": 3.04,
        "text": "calculation it'll use default"
      },
      {
        "start": 100.32,
        "duration": 3.52,
        "text": "parallelism so"
      },
      {
        "start": 101.6,
        "duration": 3.68,
        "text": "that calculation or default parallelism"
      },
      {
        "start": 103.84,
        "duration": 3.12,
        "text": "whichever is greater"
      },
      {
        "start": 105.28,
        "duration": 3.36,
        "text": "that's the number of partitions we'll"
      },
      {
        "start": 106.96,
        "duration": 4.4,
        "text": "get and again we'll have"
      },
      {
        "start": 108.64,
        "duration": 4.88,
        "text": "a partitioner of none the situation is"
      },
      {
        "start": 111.36,
        "duration": 4.24,
        "text": "similar when we're reading a text file"
      },
      {
        "start": 113.52,
        "duration": 3.599,
        "text": "from the cassandra file system or even"
      },
      {
        "start": 115.6,
        "duration": 2.32,
        "text": "hdfs it's going to do the same"
      },
      {
        "start": 117.119,
        "duration": 3.201,
        "text": "calculation"
      },
      {
        "start": 117.92,
        "duration": 3.04,
        "text": "estimate the size of the input data"
      },
      {
        "start": 120.32,
        "duration": 2.479,
        "text": "divide"
      },
      {
        "start": 120.96,
        "duration": 3.6,
        "text": "by 64 megabytes and come up with a"
      },
      {
        "start": 122.799,
        "duration": 3.6,
        "text": "number if that's bigger than default"
      },
      {
        "start": 124.56,
        "duration": 3.28,
        "text": "parallelism it'll use that"
      },
      {
        "start": 126.399,
        "duration": 3.2,
        "text": "otherwise it'll go with default"
      },
      {
        "start": 127.84,
        "duration": 3.119,
        "text": "parallelism and as you can see in this"
      },
      {
        "start": 129.599,
        "duration": 4.161,
        "text": "example here"
      },
      {
        "start": 130.959,
        "duration": 4.481,
        "text": "that is happening also again the"
      },
      {
        "start": 133.76,
        "duration": 3.6,
        "text": "partitioner returns as none"
      },
      {
        "start": 135.44,
        "duration": 3.519,
        "text": "now when using a generic transformation"
      },
      {
        "start": 137.36,
        "duration": 3.36,
        "text": "to create a new rdd"
      },
      {
        "start": 138.959,
        "duration": 3.041,
        "text": "out of a source rdd the number of"
      },
      {
        "start": 140.72,
        "duration": 3.44,
        "text": "partitions can change"
      },
      {
        "start": 142.0,
        "duration": 4.16,
        "text": "and potentially the partitioner can"
      },
      {
        "start": 144.16,
        "duration": 3.76,
        "text": "change let's walk through these rules"
      },
      {
        "start": 146.16,
        "duration": 2.799,
        "text": "so you have some idea how these work you"
      },
      {
        "start": 147.92,
        "duration": 2.56,
        "text": "probably want to keep this as a"
      },
      {
        "start": 148.959,
        "duration": 3.521,
        "text": "reference refer back to it when you're"
      },
      {
        "start": 150.48,
        "duration": 4.64,
        "text": "optimizing your own spark applications"
      },
      {
        "start": 152.48,
        "duration": 3.6,
        "text": "so by default transformations like"
      },
      {
        "start": 155.12,
        "duration": 2.8,
        "text": "filter map"
      },
      {
        "start": 156.08,
        "duration": 3.84,
        "text": "flat map and distinct those generic"
      },
      {
        "start": 157.92,
        "duration": 3.12,
        "text": "transformations will produce the same"
      },
      {
        "start": 159.92,
        "duration": 3.679,
        "text": "number of partitions"
      },
      {
        "start": 161.04,
        "duration": 4.479,
        "text": "as in the parent rdd they'll also"
      },
      {
        "start": 163.599,
        "duration": 4.241,
        "text": "produce a partitioner of"
      },
      {
        "start": 165.519,
        "duration": 5.041,
        "text": "none except in the case of filter which"
      },
      {
        "start": 167.84,
        "duration": 4.56,
        "text": "preserves its parent rdd's partitioner"
      },
      {
        "start": 170.56,
        "duration": 4.0,
        "text": "if it had one when creating a union of"
      },
      {
        "start": 172.4,
        "duration": 2.72,
        "text": "one rdd with another the result rdd will"
      },
      {
        "start": 174.56,
        "duration": 2.8,
        "text": "have"
      },
      {
        "start": 175.12,
        "duration": 3.28,
        "text": "a number of partitions equal to the"
      },
      {
        "start": 177.36,
        "duration": 4.239,
        "text": "number of partitions"
      },
      {
        "start": 178.4,
        "duration": 5.28,
        "text": "in both of the input rdds added together"
      },
      {
        "start": 181.599,
        "duration": 4.321,
        "text": "when creating an intersection of one rdd"
      },
      {
        "start": 183.68,
        "duration": 4.32,
        "text": "with another the resulting rdd"
      },
      {
        "start": 185.92,
        "duration": 3.36,
        "text": "will have a number of partitions equal"
      },
      {
        "start": 188.0,
        "duration": 3.76,
        "text": "to the greater"
      },
      {
        "start": 189.28,
        "duration": 3.599,
        "text": "of the partition counts of the two"
      },
      {
        "start": 191.76,
        "duration": 3.28,
        "text": "source rdds"
      },
      {
        "start": 192.879,
        "duration": 4.0,
        "text": "when doing a set subtraction of one rdd"
      },
      {
        "start": 195.04,
        "duration": 3.44,
        "text": "from another the resulting rdd"
      },
      {
        "start": 196.879,
        "duration": 3.121,
        "text": "will have a number of partitions equal"
      },
      {
        "start": 198.48,
        "duration": 2.64,
        "text": "to the number of partitions in the"
      },
      {
        "start": 200.0,
        "duration": 2.8,
        "text": "source rdd"
      },
      {
        "start": 201.12,
        "duration": 4.08,
        "text": "not considering the number of partitions"
      },
      {
        "start": 202.8,
        "duration": 3.6,
        "text": "in the other rdd when doing a cartesian"
      },
      {
        "start": 205.2,
        "duration": 2.8,
        "text": "product of two rdds"
      },
      {
        "start": 206.4,
        "duration": 3.68,
        "text": "the result rdd has a number of"
      },
      {
        "start": 208.0,
        "duration": 3.12,
        "text": "partitions equal to the product of the"
      },
      {
        "start": 210.08,
        "duration": 3.04,
        "text": "partition counts"
      },
      {
        "start": 211.12,
        "duration": 3.52,
        "text": "of the two input rdds clearly you could"
      },
      {
        "start": 213.12,
        "duration": 3.28,
        "text": "see how that could cause trouble"
      },
      {
        "start": 214.64,
        "duration": 5.2,
        "text": "here we see some examples of some of"
      },
      {
        "start": 216.4,
        "duration": 3.44,
        "text": "these generic transformations"
      },
      {
        "start": 221.36,
        "duration": 3.04,
        "text": "let's walk through some of the defaults"
      },
      {
        "start": 222.64,
        "duration": 2.879,
        "text": "of the key based transformations these"
      },
      {
        "start": 224.4,
        "duration": 2.72,
        "text": "work a little differently"
      },
      {
        "start": 225.519,
        "duration": 3.841,
        "text": "you see that top group including reduce"
      },
      {
        "start": 227.12,
        "duration": 3.039,
        "text": "by key fold by key combined by key group"
      },
      {
        "start": 229.36,
        "duration": 2.64,
        "text": "by key"
      },
      {
        "start": 230.159,
        "duration": 4.08,
        "text": "those all produce the same number of"
      },
      {
        "start": 232.0,
        "duration": 4.48,
        "text": "partitions as in the parent"
      },
      {
        "start": 234.239,
        "duration": 3.2,
        "text": "rdd and in fact those four"
      },
      {
        "start": 236.48,
        "duration": 4.56,
        "text": "transformations"
      },
      {
        "start": 237.439,
        "duration": 4.481,
        "text": "also always result in the output rdd"
      },
      {
        "start": 241.04,
        "duration": 3.36,
        "text": "having the hash"
      },
      {
        "start": 241.92,
        "duration": 3.44,
        "text": "partitioner sort by key always produces"
      },
      {
        "start": 244.4,
        "duration": 3.039,
        "text": "the range"
      },
      {
        "start": 245.36,
        "duration": 3.12,
        "text": "partitioner because that always has a"
      },
      {
        "start": 247.439,
        "duration": 3.36,
        "text": "sorted output"
      },
      {
        "start": 248.48,
        "duration": 3.839,
        "text": "map values and flat map values again"
      },
      {
        "start": 250.799,
        "duration": 2.321,
        "text": "don't change the count of the number of"
      },
      {
        "start": 252.319,
        "duration": 2.721,
        "text": "partitions"
      },
      {
        "start": 253.12,
        "duration": 3.519,
        "text": "but they're always going to use the"
      },
      {
        "start": 255.04,
        "duration": 2.879,
        "text": "parent rdds partitioner"
      },
      {
        "start": 256.639,
        "duration": 3.041,
        "text": "they have to because they haven't"
      },
      {
        "start": 257.919,
        "duration": 2.641,
        "text": "changed the keys you use those"
      },
      {
        "start": 259.68,
        "duration": 2.799,
        "text": "transformations"
      },
      {
        "start": 260.56,
        "duration": 4.16,
        "text": "when you explicitly don't want to touch"
      },
      {
        "start": 262.479,
        "duration": 3.921,
        "text": "the keys and so you don't want to cause"
      },
      {
        "start": 264.72,
        "duration": 3.84,
        "text": "any sort of re-partitioning to happen"
      },
      {
        "start": 266.4,
        "duration": 3.44,
        "text": "co-group join left out or join write out"
      },
      {
        "start": 268.56,
        "duration": 2.0,
        "text": "or join those are a little bit more"
      },
      {
        "start": 269.84,
        "duration": 2.24,
        "text": "complicated"
      },
      {
        "start": 270.56,
        "duration": 2.88,
        "text": "and we'll get to those in a little bit"
      },
      {
        "start": 272.08,
        "duration": 3.04,
        "text": "but here are some examples you can type"
      },
      {
        "start": 273.44,
        "duration": 3.52,
        "text": "in if you've got a spark cluster running"
      },
      {
        "start": 275.12,
        "duration": 3.359,
        "text": "and you've got access to the test data"
      },
      {
        "start": 276.96,
        "duration": 3.28,
        "text": "and you can actually try out some of"
      },
      {
        "start": 278.479,
        "duration": 2.881,
        "text": "these and watch the partitioning change"
      },
      {
        "start": 280.24,
        "duration": 4.08,
        "text": "or not change"
      },
      {
        "start": 281.36,
        "duration": 2.96,
        "text": "as it's supposed to"
      },
      {
        "start": 284.639,
        "duration": 3.921,
        "text": "now for the binary rdd operations i"
      },
      {
        "start": 286.88,
        "duration": 2.48,
        "text": "mentioned a moment ago like join and"
      },
      {
        "start": 288.56,
        "duration": 2.48,
        "text": "co-group and"
      },
      {
        "start": 289.36,
        "duration": 3.44,
        "text": "left and right outer join things are a"
      },
      {
        "start": 291.04,
        "duration": 2.24,
        "text": "little more complex take a look at this"
      },
      {
        "start": 292.8,
        "duration": 2.08,
        "text": "table"
      },
      {
        "start": 293.28,
        "duration": 3.84,
        "text": "to see how this works out all of this"
      },
      {
        "start": 294.88,
        "duration": 5.44,
        "text": "depends on whether the partitioner"
      },
      {
        "start": 297.12,
        "duration": 4.639,
        "text": "of the input rdd can be reused to give"
      },
      {
        "start": 300.32,
        "duration": 3.36,
        "text": "you a rough idea of that"
      },
      {
        "start": 301.759,
        "duration": 4.081,
        "text": "if it's the hash partitioner then it can"
      },
      {
        "start": 303.68,
        "duration": 4.079,
        "text": "be reused across the transformation"
      },
      {
        "start": 305.84,
        "duration": 3.199,
        "text": "certain kinds of custom partitioners may"
      },
      {
        "start": 307.759,
        "duration": 2.321,
        "text": "also have that property"
      },
      {
        "start": 309.039,
        "duration": 2.961,
        "text": "that'll depend on how they're"
      },
      {
        "start": 310.08,
        "duration": 2.8,
        "text": "implemented but if you look at the table"
      },
      {
        "start": 312.0,
        "duration": 4.72,
        "text": "you'll see"
      },
      {
        "start": 312.88,
        "duration": 6.64,
        "text": "if for example neither of the input rdds"
      },
      {
        "start": 316.72,
        "duration": 4.4,
        "text": "have a reusable partitioner then the"
      },
      {
        "start": 319.52,
        "duration": 2.32,
        "text": "number of partitions of the resulting"
      },
      {
        "start": 321.12,
        "duration": 3.04,
        "text": "rdd"
      },
      {
        "start": 321.84,
        "duration": 4.4,
        "text": "is simply going to be the maximum of the"
      },
      {
        "start": 324.16,
        "duration": 3.84,
        "text": "partition counts of the two input rdds"
      },
      {
        "start": 326.24,
        "duration": 3.04,
        "text": "that's also the case at the bottom of"
      },
      {
        "start": 328.0,
        "duration": 3.6,
        "text": "the table if"
      },
      {
        "start": 329.28,
        "duration": 3.759,
        "text": "both rdds have a partitioner that can be"
      },
      {
        "start": 331.6,
        "duration": 3.36,
        "text": "reused however if"
      },
      {
        "start": 333.039,
        "duration": 3.361,
        "text": "just one of them has a partitioner that"
      },
      {
        "start": 334.96,
        "duration": 4.079,
        "text": "can be reused then"
      },
      {
        "start": 336.4,
        "duration": 3.6,
        "text": "that rdd's partition size is going to"
      },
      {
        "start": 339.039,
        "duration": 3.921,
        "text": "dominate"
      },
      {
        "start": 340.0,
        "duration": 3.36,
        "text": "in the output for example the second row"
      },
      {
        "start": 342.96,
        "duration": 3.519,
        "text": "says"
      },
      {
        "start": 343.36,
        "duration": 4.48,
        "text": "if uh rdd has a reusable partitioner and"
      },
      {
        "start": 346.479,
        "duration": 3.361,
        "text": "other rdd doesn't"
      },
      {
        "start": 347.84,
        "duration": 4.24,
        "text": "then it's rdd whose partition count"
      },
      {
        "start": 349.84,
        "duration": 3.6,
        "text": "determines the partition count of the"
      },
      {
        "start": 352.08,
        "duration": 3.36,
        "text": "resulting rdd"
      },
      {
        "start": 353.44,
        "duration": 3.599,
        "text": "and here's an example of a join so you"
      },
      {
        "start": 355.44,
        "duration": 3.199,
        "text": "can see this happening note we've done"
      },
      {
        "start": 357.039,
        "duration": 3.121,
        "text": "two cassandra table queries and we've"
      },
      {
        "start": 358.639,
        "duration": 3.84,
        "text": "converted both of those"
      },
      {
        "start": 360.16,
        "duration": 3.2,
        "text": "into pair rdds with that key by"
      },
      {
        "start": 362.479,
        "duration": 2.56,
        "text": "transformation"
      },
      {
        "start": 363.36,
        "duration": 3.119,
        "text": "then we're able to join those two on"
      },
      {
        "start": 365.039,
        "duration": 2.88,
        "text": "user id which is the last line of"
      },
      {
        "start": 366.479,
        "duration": 4.081,
        "text": "executable code in the example"
      },
      {
        "start": 367.919,
        "duration": 4.641,
        "text": "the second rdd called interactions has a"
      },
      {
        "start": 370.56,
        "duration": 4.479,
        "text": "reusable partitioner you'll see it has"
      },
      {
        "start": 372.56,
        "duration": 4.8,
        "text": "the hash partitioner and four partitions"
      },
      {
        "start": 375.039,
        "duration": 5.121,
        "text": "because it gets that final group by key"
      },
      {
        "start": 377.36,
        "duration": 3.279,
        "text": "action called on it the first rdd we"
      },
      {
        "start": 380.16,
        "duration": 3.36,
        "text": "create"
      },
      {
        "start": 380.639,
        "duration": 3.28,
        "text": "users does not have a reusable partition"
      },
      {
        "start": 383.52,
        "duration": 2.56,
        "text": "and"
      },
      {
        "start": 383.919,
        "duration": 3.041,
        "text": "has a partition count of three so you"
      },
      {
        "start": 386.08,
        "duration": 3.28,
        "text": "see in the output"
      },
      {
        "start": 386.96,
        "duration": 3.12,
        "text": "the reusable rdds partition count"
      },
      {
        "start": 389.36,
        "duration": 6.48,
        "text": "dominates"
      },
      {
        "start": 390.08,
        "duration": 5.76,
        "text": "and we get four in the final result"
      },
      {
        "start": 396.31,
        "duration": 5.29,
        "text": "[Music]"
      },
      {
        "start": 399.52,
        "duration": 2.08,
        "text": "you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:33:29.783520+00:00"
}