{
  "video_id": "wonPgjMC4BM",
  "title": "DS320.36 Spark Streaming: Output Operations | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.36 Spark Streaming: Output Operations\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:32:31Z",
  "thumbnail": "https://i.ytimg.com/vi/wonPgjMC4BM/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=wonPgjMC4BM",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] now let's talk about output operations on d streams now remember the transformations that we run on d streams are lazily evaluated they turn old d streams into new ones and really just record metadata at the time that we make those calls telling spark streaming that that's work we'd like to do when it becomes necessary once we call an output operation though that will actually trigger computation to happen all those transformations will be executed and we'll get a result let's take a look at how that works there are a few common output operations that we've used already let's take a look at those in just a little detail there's print which conveniently prints the first 10 elements of each batch's rdd to the screen then there's the save as files save as text files save as object files save as hadoop files each of those saves each rdd in the source d stream so that's each batch creates a new rdd in our examples every four seconds there's a new rdd those methods will save that rdd to that destination every single time so that can be potentially a lot of rdds they can save as text they can save as object files or they can save directly to hadoop for each rdd is extremely general and extremely powerful that's going to let us pass in a function where we'll get to handle each generated rdd in the d stream independently we can interact with the rdd api call transformations and actions and do whatever we want that rdd on a per rdd basis then there's save to cassandra that works a lot like the regular rdd saved to cassandra except for each rdd in the d stream it's going to do that save operation so the way we define the metadata and the saving that happens inside that call we're gonna have to make sure that it generates unique rows or that we're willing to tolerate upserts on each rdd in each successive batch let's look at a diagram of what it would look like to save that movie click stream data we've been looking at we have some stream that's continuously providing us with uuids movie identifiers that we're trying to process so on the top we see them coming in from the input stream coming in through the receiver we have movie uuids and we simply want to save them into cassandra now those movie uuids by themselves are not unique by definition there are only so many movies and we would expect them to get clicked on more than once so we're going to make them unique with a click id that's simply going to be a time uuid we're going to add a time stamp to each click to individuate it from the other clicks to make sure we don't upset and lose events let's walk through the few steps of this spark streaming application that would get this work done now here in the first step we're going to show you a new feature that we haven't looked at before which is the cassandra connector this is a part of the spark cassandra connector api but it allows us to perform arbitrary cql queries against the cassandra instance we pass in our configuration object that's already telling the sparca center connector where to go to look for cassandra all that stuff happens for us and with a session we can execute that cql query to create a table it's a convenience if you ever need to do that sort of housekeeping you've got this object available to you then in step 2 we'll create our streaming context open up our stream again to that text stream of those uuids and then map the stream now the stream consists of simply one uuid after another so each one of those we're going to convert into a java uuid object because that's what the save to cassandra method is going to expect we're also since remember in our schema we have the movie uuid and then a time stamp to make it unique and that timestamp is a time uuid we're going to use a method in the cassandra java driver to generate a time uuid and return that from the map function then the rdd generated by that mapping is saved to cassandra and it's columns mapped to movie id and click id we've then saved an input stream into cassandra and that table will continuously fill up with the click stream data that's coming in on the input stream let's look at how to aggregate some data coming in off a streaming data source stored into a cassandra table we've got the streaming data coming into the receiver becoming an input d stream which again is a list of uuids movie identifiers we then aggregate them somehow grouping them by key and counting up the number of times they occur and write those key value pairs into the cassandra table the key of each of those is going to be unique it's going to be a movie id we're going to want to aggregate those counts though and we're actually going to do that with a counter column in the cassandra table not by using a stateful transformation to count them up through spark streaming let's look at the code at first we'll use that handy cassandra connector object to create the table again a nice convenient way of doing some ddl within a spark streaming application then we'll create our streaming context open up the text stream and map the stream just like in the last example we're going to take that text uuid we get in from the stream each record of the stream and convert it to a java util uuid then we're going to pair it with the number one this is the map and reduce counting pattern we pair it with the number one because once we see that uuid we're saying i'm counting this one time i've seen it once we'll then pass it on to the reduce by key method and add up the counts that we get with that pair rdd that we create as a result we'll save it to cassandra notice there's nothing in the code here that says anything about updating a counter column that's because the saved to cassandra method knows if you're saving to a counter column it'll just do the right thing and increment the counter by the value that you're passing in we could do the same work with the for each rdd api this again is the most powerful most general thing you can do on the rdds of a d stream let's take a look at how to do the simple thing with that api we create our streaming context we open up our stream and then we call for each rdd now for each rdd counts as an output operation we're passing in an anonymous function to it that takes an rdd as a parameter and does some work on it that work is strictly in terms of the rdd api it's as if we're temporarily out of the world of spark streaming and just considering each individual rdd by itself and that's what we do sure enough we map that rdd again converting the string movie id into a java util uuid pairing it with a one to begin counting we call the reducer and then save those things into cassandra save those pair rdds into cassandra again counting on the safety cassandra method to magically do the right thing for us with that counter column and increment it pretty nice that we get to do that it avoids us having to do the stateful transformation and gives us a nice tight piece of code that does that aggregation on a streaming input",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.56,
        "duration": 3.44,
        "text": "now let's talk about"
      },
      {
        "start": 7.52,
        "duration": 3.999,
        "text": "output operations on d streams now"
      },
      {
        "start": 10.0,
        "duration": 2.559,
        "text": "remember the transformations that we run"
      },
      {
        "start": 11.519,
        "duration": 3.521,
        "text": "on d streams are"
      },
      {
        "start": 12.559,
        "duration": 3.521,
        "text": "lazily evaluated they turn old d streams"
      },
      {
        "start": 15.04,
        "duration": 2.88,
        "text": "into new ones"
      },
      {
        "start": 16.08,
        "duration": 3.359,
        "text": "and really just record metadata at the"
      },
      {
        "start": 17.92,
        "duration": 3.119,
        "text": "time that we make those calls"
      },
      {
        "start": 19.439,
        "duration": 3.6,
        "text": "telling spark streaming that that's work"
      },
      {
        "start": 21.039,
        "duration": 2.881,
        "text": "we'd like to do when it becomes"
      },
      {
        "start": 23.039,
        "duration": 2.641,
        "text": "necessary"
      },
      {
        "start": 23.92,
        "duration": 3.359,
        "text": "once we call an output operation though"
      },
      {
        "start": 25.68,
        "duration": 2.16,
        "text": "that will actually trigger computation"
      },
      {
        "start": 27.279,
        "duration": 1.76,
        "text": "to happen"
      },
      {
        "start": 27.84,
        "duration": 3.279,
        "text": "all those transformations will be"
      },
      {
        "start": 29.039,
        "duration": 3.52,
        "text": "executed and we'll get a result let's"
      },
      {
        "start": 31.119,
        "duration": 3.12,
        "text": "take a look at how that works"
      },
      {
        "start": 32.559,
        "duration": 3.201,
        "text": "there are a few common output operations"
      },
      {
        "start": 34.239,
        "duration": 2.801,
        "text": "that we've used already"
      },
      {
        "start": 35.76,
        "duration": 2.799,
        "text": "let's take a look at those in just a"
      },
      {
        "start": 37.04,
        "duration": 3.039,
        "text": "little detail there's print which"
      },
      {
        "start": 38.559,
        "duration": 4.881,
        "text": "conveniently prints the first"
      },
      {
        "start": 40.079,
        "duration": 6.0,
        "text": "10 elements of each batch's rdd"
      },
      {
        "start": 43.44,
        "duration": 3.279,
        "text": "to the screen then there's the save as"
      },
      {
        "start": 46.079,
        "duration": 2.8,
        "text": "files"
      },
      {
        "start": 46.719,
        "duration": 3.36,
        "text": "save as text files save as object files"
      },
      {
        "start": 48.879,
        "duration": 3.601,
        "text": "save as hadoop files"
      },
      {
        "start": 50.079,
        "duration": 4.401,
        "text": "each of those saves each rdd in the"
      },
      {
        "start": 52.48,
        "duration": 4.239,
        "text": "source d stream so that's each batch"
      },
      {
        "start": 54.48,
        "duration": 4.0,
        "text": "creates a new rdd in our examples every"
      },
      {
        "start": 56.719,
        "duration": 4.64,
        "text": "four seconds there's a new rdd"
      },
      {
        "start": 58.48,
        "duration": 3.919,
        "text": "those methods will save that rdd to that"
      },
      {
        "start": 61.359,
        "duration": 2.481,
        "text": "destination"
      },
      {
        "start": 62.399,
        "duration": 3.681,
        "text": "every single time so that can be"
      },
      {
        "start": 63.84,
        "duration": 4.16,
        "text": "potentially a lot of rdds they can save"
      },
      {
        "start": 66.08,
        "duration": 2.96,
        "text": "as text they can save as object files or"
      },
      {
        "start": 68.0,
        "duration": 4.56,
        "text": "they can save directly"
      },
      {
        "start": 69.04,
        "duration": 4.96,
        "text": "to hadoop for each rdd is extremely"
      },
      {
        "start": 72.56,
        "duration": 3.12,
        "text": "general and extremely"
      },
      {
        "start": 74.0,
        "duration": 3.759,
        "text": "powerful that's going to let us pass in"
      },
      {
        "start": 75.68,
        "duration": 4.479,
        "text": "a function where we'll get to handle"
      },
      {
        "start": 77.759,
        "duration": 3.201,
        "text": "each generated rdd in the d stream"
      },
      {
        "start": 80.159,
        "duration": 2.881,
        "text": "independently"
      },
      {
        "start": 80.96,
        "duration": 3.36,
        "text": "we can interact with the rdd api call"
      },
      {
        "start": 83.04,
        "duration": 3.439,
        "text": "transformations and actions"
      },
      {
        "start": 84.32,
        "duration": 3.439,
        "text": "and do whatever we want that rdd on a"
      },
      {
        "start": 86.479,
        "duration": 3.28,
        "text": "per rdd basis"
      },
      {
        "start": 87.759,
        "duration": 3.761,
        "text": "then there's save to cassandra that"
      },
      {
        "start": 89.759,
        "duration": 3.121,
        "text": "works a lot like the regular rdd saved"
      },
      {
        "start": 91.52,
        "duration": 3.919,
        "text": "to cassandra except"
      },
      {
        "start": 92.88,
        "duration": 4.16,
        "text": "for each rdd in the d stream it's going"
      },
      {
        "start": 95.439,
        "duration": 4.241,
        "text": "to do that save operation"
      },
      {
        "start": 97.04,
        "duration": 3.6,
        "text": "so the way we define the metadata and"
      },
      {
        "start": 99.68,
        "duration": 2.96,
        "text": "the saving"
      },
      {
        "start": 100.64,
        "duration": 2.799,
        "text": "that happens inside that call we're"
      },
      {
        "start": 102.64,
        "duration": 2.159,
        "text": "gonna have to make sure that it"
      },
      {
        "start": 103.439,
        "duration": 3.04,
        "text": "generates unique rows"
      },
      {
        "start": 104.799,
        "duration": 3.761,
        "text": "or that we're willing to tolerate"
      },
      {
        "start": 106.479,
        "duration": 3.761,
        "text": "upserts on each rdd"
      },
      {
        "start": 108.56,
        "duration": 3.36,
        "text": "in each successive batch let's look at a"
      },
      {
        "start": 110.24,
        "duration": 2.32,
        "text": "diagram of what it would look like to"
      },
      {
        "start": 111.92,
        "duration": 2.4,
        "text": "save"
      },
      {
        "start": 112.56,
        "duration": 3.28,
        "text": "that movie click stream data we've been"
      },
      {
        "start": 114.32,
        "duration": 3.2,
        "text": "looking at we have some stream"
      },
      {
        "start": 115.84,
        "duration": 3.68,
        "text": "that's continuously providing us with"
      },
      {
        "start": 117.52,
        "duration": 4.0,
        "text": "uuids movie identifiers"
      },
      {
        "start": 119.52,
        "duration": 3.68,
        "text": "that we're trying to process so on the"
      },
      {
        "start": 121.52,
        "duration": 2.559,
        "text": "top we see them coming in from the input"
      },
      {
        "start": 123.2,
        "duration": 3.599,
        "text": "stream coming in"
      },
      {
        "start": 124.079,
        "duration": 4.4,
        "text": "through the receiver we have movie uuids"
      },
      {
        "start": 126.799,
        "duration": 4.641,
        "text": "and we simply want to save them"
      },
      {
        "start": 128.479,
        "duration": 4.001,
        "text": "into cassandra now those movie uuids by"
      },
      {
        "start": 131.44,
        "duration": 3.04,
        "text": "themselves are"
      },
      {
        "start": 132.48,
        "duration": 3.6,
        "text": "not unique by definition there are only"
      },
      {
        "start": 134.48,
        "duration": 3.04,
        "text": "so many movies and we would expect them"
      },
      {
        "start": 136.08,
        "duration": 3.519,
        "text": "to get clicked on more than once"
      },
      {
        "start": 137.52,
        "duration": 3.04,
        "text": "so we're going to make them unique with"
      },
      {
        "start": 139.599,
        "duration": 2.881,
        "text": "a click id"
      },
      {
        "start": 140.56,
        "duration": 3.36,
        "text": "that's simply going to be a time uuid"
      },
      {
        "start": 142.48,
        "duration": 3.68,
        "text": "we're going to add a time stamp"
      },
      {
        "start": 143.92,
        "duration": 3.92,
        "text": "to each click to individuate it from the"
      },
      {
        "start": 146.16,
        "duration": 4.56,
        "text": "other clicks to make sure we don't"
      },
      {
        "start": 147.84,
        "duration": 4.399,
        "text": "upset and lose events let's walk through"
      },
      {
        "start": 150.72,
        "duration": 3.599,
        "text": "the few steps of"
      },
      {
        "start": 152.239,
        "duration": 3.201,
        "text": "this spark streaming application that"
      },
      {
        "start": 154.319,
        "duration": 2.321,
        "text": "would get this work done"
      },
      {
        "start": 155.44,
        "duration": 2.879,
        "text": "now here in the first step we're going"
      },
      {
        "start": 156.64,
        "duration": 2.879,
        "text": "to show you a new feature that we"
      },
      {
        "start": 158.319,
        "duration": 4.161,
        "text": "haven't looked at before"
      },
      {
        "start": 159.519,
        "duration": 4.72,
        "text": "which is the cassandra connector this is"
      },
      {
        "start": 162.48,
        "duration": 2.479,
        "text": "a part of the spark cassandra connector"
      },
      {
        "start": 164.239,
        "duration": 2.72,
        "text": "api"
      },
      {
        "start": 164.959,
        "duration": 3.441,
        "text": "but it allows us to perform arbitrary"
      },
      {
        "start": 166.959,
        "duration": 3.761,
        "text": "cql queries"
      },
      {
        "start": 168.4,
        "duration": 3.839,
        "text": "against the cassandra instance we pass"
      },
      {
        "start": 170.72,
        "duration": 2.56,
        "text": "in our configuration object that's"
      },
      {
        "start": 172.239,
        "duration": 2.881,
        "text": "already telling"
      },
      {
        "start": 173.28,
        "duration": 3.28,
        "text": "the sparca center connector where to go"
      },
      {
        "start": 175.12,
        "duration": 3.759,
        "text": "to look for cassandra"
      },
      {
        "start": 176.56,
        "duration": 4.72,
        "text": "all that stuff happens for us and with a"
      },
      {
        "start": 178.879,
        "duration": 3.28,
        "text": "session we can execute that cql query to"
      },
      {
        "start": 181.28,
        "duration": 2.239,
        "text": "create a table"
      },
      {
        "start": 182.159,
        "duration": 2.961,
        "text": "it's a convenience if you ever need to"
      },
      {
        "start": 183.519,
        "duration": 3.121,
        "text": "do that sort of housekeeping you've got"
      },
      {
        "start": 185.12,
        "duration": 3.36,
        "text": "this object available to you"
      },
      {
        "start": 186.64,
        "duration": 3.28,
        "text": "then in step 2 we'll create our"
      },
      {
        "start": 188.48,
        "duration": 3.679,
        "text": "streaming context"
      },
      {
        "start": 189.92,
        "duration": 4.08,
        "text": "open up our stream again to that text"
      },
      {
        "start": 192.159,
        "duration": 4.0,
        "text": "stream of those uuids"
      },
      {
        "start": 194.0,
        "duration": 3.12,
        "text": "and then map the stream now the stream"
      },
      {
        "start": 196.159,
        "duration": 4.0,
        "text": "consists of"
      },
      {
        "start": 197.12,
        "duration": 4.32,
        "text": "simply one uuid after another"
      },
      {
        "start": 200.159,
        "duration": 4.08,
        "text": "so each one of those we're going to"
      },
      {
        "start": 201.44,
        "duration": 3.84,
        "text": "convert into a java uuid object"
      },
      {
        "start": 204.239,
        "duration": 4.0,
        "text": "because that's what the save to"
      },
      {
        "start": 205.28,
        "duration": 5.84,
        "text": "cassandra method is going to expect"
      },
      {
        "start": 208.239,
        "duration": 3.28,
        "text": "we're also since remember in our schema"
      },
      {
        "start": 211.12,
        "duration": 2.72,
        "text": "we"
      },
      {
        "start": 211.519,
        "duration": 3.121,
        "text": "have the movie uuid and then a time"
      },
      {
        "start": 213.84,
        "duration": 2.479,
        "text": "stamp"
      },
      {
        "start": 214.64,
        "duration": 4.0,
        "text": "to make it unique and that timestamp is"
      },
      {
        "start": 216.319,
        "duration": 5.041,
        "text": "a time uuid we're going to use a method"
      },
      {
        "start": 218.64,
        "duration": 4.239,
        "text": "in the cassandra java driver to generate"
      },
      {
        "start": 221.36,
        "duration": 4.56,
        "text": "a time uuid"
      },
      {
        "start": 222.879,
        "duration": 5.36,
        "text": "and return that from the map function"
      },
      {
        "start": 225.92,
        "duration": 5.52,
        "text": "then the rdd generated by that mapping"
      },
      {
        "start": 228.239,
        "duration": 4.801,
        "text": "is saved to cassandra and it's columns"
      },
      {
        "start": 231.44,
        "duration": 4.48,
        "text": "mapped to movie id"
      },
      {
        "start": 233.04,
        "duration": 3.6,
        "text": "and click id we've then saved an input"
      },
      {
        "start": 235.92,
        "duration": 2.72,
        "text": "stream"
      },
      {
        "start": 236.64,
        "duration": 3.36,
        "text": "into cassandra and that table will"
      },
      {
        "start": 238.64,
        "duration": 3.04,
        "text": "continuously fill up"
      },
      {
        "start": 240.0,
        "duration": 3.28,
        "text": "with the click stream data that's coming"
      },
      {
        "start": 241.68,
        "duration": 2.559,
        "text": "in on the input stream let's look at how"
      },
      {
        "start": 243.28,
        "duration": 2.959,
        "text": "to aggregate"
      },
      {
        "start": 244.239,
        "duration": 3.2,
        "text": "some data coming in off a streaming data"
      },
      {
        "start": 246.239,
        "duration": 3.521,
        "text": "source stored"
      },
      {
        "start": 247.439,
        "duration": 4.0,
        "text": "into a cassandra table we've got the"
      },
      {
        "start": 249.76,
        "duration": 3.199,
        "text": "streaming data coming into the receiver"
      },
      {
        "start": 251.439,
        "duration": 4.8,
        "text": "becoming an input d stream"
      },
      {
        "start": 252.959,
        "duration": 4.161,
        "text": "which again is a list of uuids movie"
      },
      {
        "start": 256.239,
        "duration": 3.12,
        "text": "identifiers"
      },
      {
        "start": 257.12,
        "duration": 3.839,
        "text": "we then aggregate them somehow grouping"
      },
      {
        "start": 259.359,
        "duration": 2.801,
        "text": "them by key and counting up the number"
      },
      {
        "start": 260.959,
        "duration": 4.401,
        "text": "of times they occur"
      },
      {
        "start": 262.16,
        "duration": 4.4,
        "text": "and write those key value pairs into the"
      },
      {
        "start": 265.36,
        "duration": 2.8,
        "text": "cassandra table"
      },
      {
        "start": 266.56,
        "duration": 3.44,
        "text": "the key of each of those is going to be"
      },
      {
        "start": 268.16,
        "duration": 3.36,
        "text": "unique it's going to be a movie id"
      },
      {
        "start": 270.0,
        "duration": 2.72,
        "text": "we're going to want to aggregate those"
      },
      {
        "start": 271.52,
        "duration": 1.84,
        "text": "counts though and we're actually going"
      },
      {
        "start": 272.72,
        "duration": 2.72,
        "text": "to do that"
      },
      {
        "start": 273.36,
        "duration": 4.0,
        "text": "with a counter column in the cassandra"
      },
      {
        "start": 275.44,
        "duration": 2.88,
        "text": "table not by using a stateful"
      },
      {
        "start": 277.36,
        "duration": 2.96,
        "text": "transformation"
      },
      {
        "start": 278.32,
        "duration": 3.599,
        "text": "to count them up through spark streaming"
      },
      {
        "start": 280.32,
        "duration": 3.439,
        "text": "let's look at the code at first we'll"
      },
      {
        "start": 281.919,
        "duration": 2.481,
        "text": "use that handy cassandra connector"
      },
      {
        "start": 283.759,
        "duration": 2.961,
        "text": "object"
      },
      {
        "start": 284.4,
        "duration": 4.4,
        "text": "to create the table again a nice"
      },
      {
        "start": 286.72,
        "duration": 4.0,
        "text": "convenient way of doing some ddl"
      },
      {
        "start": 288.8,
        "duration": 3.76,
        "text": "within a spark streaming application"
      },
      {
        "start": 290.72,
        "duration": 4.8,
        "text": "then we'll create our streaming context"
      },
      {
        "start": 292.56,
        "duration": 3.919,
        "text": "open up the text stream and map the"
      },
      {
        "start": 295.52,
        "duration": 2.48,
        "text": "stream"
      },
      {
        "start": 296.479,
        "duration": 3.681,
        "text": "just like in the last example we're"
      },
      {
        "start": 298.0,
        "duration": 3.199,
        "text": "going to take that text uuid we get in"
      },
      {
        "start": 300.16,
        "duration": 2.96,
        "text": "from the stream"
      },
      {
        "start": 301.199,
        "duration": 3.28,
        "text": "each record of the stream and convert it"
      },
      {
        "start": 303.12,
        "duration": 4.24,
        "text": "to a java util"
      },
      {
        "start": 304.479,
        "duration": 4.0,
        "text": "uuid then we're going to pair it with"
      },
      {
        "start": 307.36,
        "duration": 2.8,
        "text": "the number one"
      },
      {
        "start": 308.479,
        "duration": 3.201,
        "text": "this is the map and reduce counting"
      },
      {
        "start": 310.16,
        "duration": 3.68,
        "text": "pattern we pair it with the number one"
      },
      {
        "start": 311.68,
        "duration": 3.92,
        "text": "because once we see that uuid"
      },
      {
        "start": 313.84,
        "duration": 3.359,
        "text": "we're saying i'm counting this one time"
      },
      {
        "start": 315.6,
        "duration": 3.52,
        "text": "i've seen it once"
      },
      {
        "start": 317.199,
        "duration": 3.84,
        "text": "we'll then pass it on to the reduce by"
      },
      {
        "start": 319.12,
        "duration": 4.4,
        "text": "key method and add up"
      },
      {
        "start": 321.039,
        "duration": 4.401,
        "text": "the counts that we get with that pair"
      },
      {
        "start": 323.52,
        "duration": 4.8,
        "text": "rdd that we create as a result"
      },
      {
        "start": 325.44,
        "duration": 4.16,
        "text": "we'll save it to cassandra notice"
      },
      {
        "start": 328.32,
        "duration": 2.96,
        "text": "there's nothing in the code here that"
      },
      {
        "start": 329.6,
        "duration": 2.319,
        "text": "says anything about updating a counter"
      },
      {
        "start": 331.28,
        "duration": 2.08,
        "text": "column"
      },
      {
        "start": 331.919,
        "duration": 3.201,
        "text": "that's because the saved to cassandra"
      },
      {
        "start": 333.36,
        "duration": 3.279,
        "text": "method knows if you're saving to a"
      },
      {
        "start": 335.12,
        "duration": 3.12,
        "text": "counter column it'll just do the right"
      },
      {
        "start": 336.639,
        "duration": 3.441,
        "text": "thing and increment the counter"
      },
      {
        "start": 338.24,
        "duration": 3.36,
        "text": "by the value that you're passing in we"
      },
      {
        "start": 340.08,
        "duration": 4.24,
        "text": "could do the same work with the for"
      },
      {
        "start": 341.6,
        "duration": 4.72,
        "text": "each rdd api this again is the most"
      },
      {
        "start": 344.32,
        "duration": 4.879,
        "text": "powerful most general thing you can do"
      },
      {
        "start": 346.32,
        "duration": 4.4,
        "text": "on the rdds of a d stream let's take a"
      },
      {
        "start": 349.199,
        "duration": 2.241,
        "text": "look at how to do the simple thing with"
      },
      {
        "start": 350.72,
        "duration": 2.56,
        "text": "that api"
      },
      {
        "start": 351.44,
        "duration": 4.08,
        "text": "we create our streaming context we open"
      },
      {
        "start": 353.28,
        "duration": 5.039,
        "text": "up our stream and then we call"
      },
      {
        "start": 355.52,
        "duration": 4.08,
        "text": "for each rdd now for each rdd counts as"
      },
      {
        "start": 358.319,
        "duration": 2.961,
        "text": "an output operation"
      },
      {
        "start": 359.6,
        "duration": 3.84,
        "text": "we're passing in an anonymous function"
      },
      {
        "start": 361.28,
        "duration": 3.68,
        "text": "to it that takes an rdd as a parameter"
      },
      {
        "start": 363.44,
        "duration": 4.16,
        "text": "and does some work on it"
      },
      {
        "start": 364.96,
        "duration": 3.84,
        "text": "that work is strictly in terms of the"
      },
      {
        "start": 367.6,
        "duration": 2.96,
        "text": "rdd api"
      },
      {
        "start": 368.8,
        "duration": 3.28,
        "text": "it's as if we're temporarily out of the"
      },
      {
        "start": 370.56,
        "duration": 4.96,
        "text": "world of spark streaming"
      },
      {
        "start": 372.08,
        "duration": 5.36,
        "text": "and just considering each individual rdd"
      },
      {
        "start": 375.52,
        "duration": 3.2,
        "text": "by itself and that's what we do sure"
      },
      {
        "start": 377.44,
        "duration": 4.319,
        "text": "enough we"
      },
      {
        "start": 378.72,
        "duration": 4.16,
        "text": "map that rdd again converting the string"
      },
      {
        "start": 381.759,
        "duration": 4.0,
        "text": "movie id"
      },
      {
        "start": 382.88,
        "duration": 3.36,
        "text": "into a java util uuid pairing it with a"
      },
      {
        "start": 385.759,
        "duration": 3.521,
        "text": "one"
      },
      {
        "start": 386.24,
        "duration": 4.64,
        "text": "to begin counting we call the reducer"
      },
      {
        "start": 389.28,
        "duration": 3.6,
        "text": "and then save those things into"
      },
      {
        "start": 390.88,
        "duration": 4.24,
        "text": "cassandra save those pair rdds into"
      },
      {
        "start": 392.88,
        "duration": 3.36,
        "text": "cassandra again counting on the safety"
      },
      {
        "start": 395.12,
        "duration": 2.799,
        "text": "cassandra method"
      },
      {
        "start": 396.24,
        "duration": 3.04,
        "text": "to magically do the right thing for us"
      },
      {
        "start": 397.919,
        "duration": 3.361,
        "text": "with that counter column"
      },
      {
        "start": 399.28,
        "duration": 3.52,
        "text": "and increment it pretty nice that we get"
      },
      {
        "start": 401.28,
        "duration": 3.44,
        "text": "to do that it avoids us"
      },
      {
        "start": 402.8,
        "duration": 3.36,
        "text": "having to do the stateful transformation"
      },
      {
        "start": 404.72,
        "duration": 3.52,
        "text": "and gives us a nice"
      },
      {
        "start": 406.16,
        "duration": 12.4,
        "text": "tight piece of code that does that"
      },
      {
        "start": 408.24,
        "duration": 10.32,
        "text": "aggregation on a streaming input"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:20:23.986121+00:00"
}