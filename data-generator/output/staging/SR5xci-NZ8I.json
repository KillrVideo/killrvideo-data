{
  "video_id": "SR5xci-NZ8I",
  "title": "DS320.13 Connecting Spark: Saving Data Back to Cassandra | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.13 Connecting Spark: Saving Data Back to Cassandra\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:27:39Z",
  "thumbnail": "https://i.ytimg.com/vi/SR5xci-NZ8I/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=SR5xci-NZ8I",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] so far we've been content to collect and for each and print line our data because it's a fairly small data set and we've been exploring and we'd really just like to be able to see the results that we're creating but in the real world it's very likely that you're going to do computation at spark and emit those results to another cassandra table that can then be very efficiently queried by some other transactional process that's a typical use case and typical architecture so that requires that we know how to save data back into cassandra we're going to look at three cases one where we've got cassandra row objects one where we've got user-defined case classes that we want to persist and one where we've got tuples so a lot like the examples we've been through so far with the cassandra api we're going to look at all three of those and ways to write them back into the database and as we persist this data we're going to be using a new table called favorite movies take a look at that schema we've got a title and a release here and those two participate in the primary key there's a rating there's a set of genres which are strings and there's user defined type called details details has country language and run time in it so we're adding a little bit of extra flair here in the schema but this will help keep the examples interesting as we go forward the key method that we need to write data back to cassandra from spark is save to cassandra this is called on an rdd and writes each record each element from the source rdd into the destination table in the destination key space optionally selecting just certain columns from the source rdd save as cassandra table automatically creates a new cassandra table there's sort of an assumption in the api that we're going to be creating data and persisting it in interesting ways and persisting maybe transitory collections of data so much so that it'll even offer to create the table for us so there are elements of that api that i'm not going to dive into in a whole lot of detail right now they're worth reading about in the documentation but elements of that api that govern the schema of the created table in particular defining the key is an important thing and i encourage you to read the documentation on that to get into it a little bit more we're going to keep things simple in our examples here but definitely worth digging into after you master the basics and if you really have an elaborate table definition you want to create a table and it has an interesting key definition that's entirely possible you have the save as cassandra table x that allows you to specify a data structure pass in a data structure that describes the partition key and clustering key definitions and any other additional columns and their data types of course you could also just create the table that you want and use the regular cassandra cql driver to issue that create table statement and then call save to cassandra so if you've got an elaborate schema you just prefer to do it that way completely fine it's not an anti-pattern at all to mix and match the cql and the spark calls let's take a look at a trivial example of safety cassandra now for convenience we're reconstituting that movie rdd we're selecting movies by actor we're actors johnny depp and we just want title release here in raiding and we're going to filter only on those members that rdd those elements in that rdd whose title happens to be alice in wonderland so not exactly big data at the end of that it's one element but now we can see in a very simple way we call on those final two lines save to cassandra we'll specify as always the key space and the table the table is now favorite movies that's where our output data our processed analytics data is going to that favorite movies table and we'll write the title release here and rating columns to that table wrapped in that sum columns object now details is going to be null that's that user defined type we don't have any details to write yet but that's fine now here's a very simple example of persisting case class data now suppose we had some cassandra rows that were created by instantiating case classes we looked at an example of that in the previous section in this code here i'm going to show you again just enough code for you to type in and something that you wrap your mind around fairly easily we'll see there's a new case class called genres info that has three fields title a string release here and int genres a set of strings and in that next line of code there we simply parallelize a sequence literal there of a single member of new genre's info so this is like an rdd created from a case class that just has one element in the rdd and we assign it to the val genres now that genres rdd we call save to cassandra killer video favorite movies the favorite movies table the output table and we specify some columns title release here in genres and that rdd which was instantiated through the the case class pathway is persisted just as neatly back to the new table finally with tuples we'll do the same thing here where i'll create sort of in a literal a new rdd from a tuple and then we'll persist it just so you can see how the api works with real code we parallelize a literal sequence that has a single entry it's alice in wonderland it's from 2010 and finally we have a value for our user defined type that's on that third line that says udt value from map and then a map mapping country to usa language to english runtime to 108. those are those three members from the udt that we defined back at the beginning of the section having created that new rdd we can simply save it to cassandra same api specify the three columns and it gets done and we have a value in the details column we have a value for our user defined type first time we've seen that be not null that's the basics of the spark cassandra connector we've looked at how to read data out of cassandra how to operate on cassandra row objects how to do more interesting things a couple of different ways to transform a cassandra row into a tuple or an instance of a class useful for for different kinds of use cases and of course we looked at how to save data back to cassandra regardless of which way we got the data in from cassandra and sort of what it became during the process of being transformed we could write it back out to tables and that's a very common use case bring the data in do the distributed computation on the data and write summary results out to a cassandra table that can then be accessed with very low latency with a typical cassandra query so we begin to have some pretty powerful tools under our belt now that we know something about what rdds are and how to interact with cassandra and just the basics of how spark works you're starting to get to the point where you could be dangerous in the following section we're going to build on those skills in some important ways stick with us you",
    "segments": [
      {
        "start": 0.09,
        "duration": 8.469,
        "text": "[Music]"
      },
      {
        "start": 7.12,
        "duration": 4.32,
        "text": "so far we've been content"
      },
      {
        "start": 8.559,
        "duration": 4.401,
        "text": "to collect and for each and print line"
      },
      {
        "start": 11.44,
        "duration": 2.159,
        "text": "our data because it's a fairly small"
      },
      {
        "start": 12.96,
        "duration": 2.08,
        "text": "data set"
      },
      {
        "start": 13.599,
        "duration": 3.52,
        "text": "and we've been exploring and we'd really"
      },
      {
        "start": 15.04,
        "duration": 4.399,
        "text": "just like to be able to see the results"
      },
      {
        "start": 17.119,
        "duration": 3.201,
        "text": "that we're creating but in the real"
      },
      {
        "start": 19.439,
        "duration": 2.481,
        "text": "world it's"
      },
      {
        "start": 20.32,
        "duration": 3.28,
        "text": "very likely that you're going to do"
      },
      {
        "start": 21.92,
        "duration": 4.08,
        "text": "computation at spark and"
      },
      {
        "start": 23.6,
        "duration": 3.04,
        "text": "emit those results to another cassandra"
      },
      {
        "start": 26.0,
        "duration": 2.32,
        "text": "table"
      },
      {
        "start": 26.64,
        "duration": 3.039,
        "text": "that can then be very efficiently"
      },
      {
        "start": 28.32,
        "duration": 3.279,
        "text": "queried by"
      },
      {
        "start": 29.679,
        "duration": 3.521,
        "text": "some other transactional process that's"
      },
      {
        "start": 31.599,
        "duration": 2.401,
        "text": "a typical use case and typical"
      },
      {
        "start": 33.2,
        "duration": 2.56,
        "text": "architecture"
      },
      {
        "start": 34.0,
        "duration": 3.92,
        "text": "so that requires that we know how to"
      },
      {
        "start": 35.76,
        "duration": 3.76,
        "text": "save data back into cassandra"
      },
      {
        "start": 37.92,
        "duration": 3.2,
        "text": "we're going to look at three cases one"
      },
      {
        "start": 39.52,
        "duration": 3.6,
        "text": "where we've got cassandra row"
      },
      {
        "start": 41.12,
        "duration": 3.84,
        "text": "objects one where we've got user-defined"
      },
      {
        "start": 43.12,
        "duration": 4.56,
        "text": "case classes that we want to persist"
      },
      {
        "start": 44.96,
        "duration": 4.16,
        "text": "and one where we've got tuples so a lot"
      },
      {
        "start": 47.68,
        "duration": 3.039,
        "text": "like the examples we've been through so"
      },
      {
        "start": 49.12,
        "duration": 2.8,
        "text": "far with the cassandra api"
      },
      {
        "start": 50.719,
        "duration": 3.121,
        "text": "we're going to look at all three of"
      },
      {
        "start": 51.92,
        "duration": 2.639,
        "text": "those and ways to write them back into"
      },
      {
        "start": 53.84,
        "duration": 2.08,
        "text": "the database"
      },
      {
        "start": 54.559,
        "duration": 3.441,
        "text": "and as we persist this data we're going"
      },
      {
        "start": 55.92,
        "duration": 4.08,
        "text": "to be using a new table called favorite"
      },
      {
        "start": 58.0,
        "duration": 4.079,
        "text": "movies take a look at that schema we've"
      },
      {
        "start": 60.0,
        "duration": 2.96,
        "text": "got a title and a release here and those"
      },
      {
        "start": 62.079,
        "duration": 3.04,
        "text": "two participate"
      },
      {
        "start": 62.96,
        "duration": 3.839,
        "text": "in the primary key there's a rating"
      },
      {
        "start": 65.119,
        "duration": 2.721,
        "text": "there's a set of genres which are"
      },
      {
        "start": 66.799,
        "duration": 2.881,
        "text": "strings"
      },
      {
        "start": 67.84,
        "duration": 4.319,
        "text": "and there's user defined type called"
      },
      {
        "start": 69.68,
        "duration": 5.52,
        "text": "details details has country"
      },
      {
        "start": 72.159,
        "duration": 4.721,
        "text": "language and run time in it so we're"
      },
      {
        "start": 75.2,
        "duration": 2.559,
        "text": "adding a little bit of extra flair here"
      },
      {
        "start": 76.88,
        "duration": 2.239,
        "text": "in the schema"
      },
      {
        "start": 77.759,
        "duration": 3.281,
        "text": "but this will help keep the examples"
      },
      {
        "start": 79.119,
        "duration": 3.441,
        "text": "interesting as we go forward the key"
      },
      {
        "start": 81.04,
        "duration": 4.079,
        "text": "method that we need"
      },
      {
        "start": 82.56,
        "duration": 3.12,
        "text": "to write data back to cassandra from"
      },
      {
        "start": 85.119,
        "duration": 3.36,
        "text": "spark"
      },
      {
        "start": 85.68,
        "duration": 3.84,
        "text": "is save to cassandra this is called on"
      },
      {
        "start": 88.479,
        "duration": 3.841,
        "text": "an rdd"
      },
      {
        "start": 89.52,
        "duration": 4.239,
        "text": "and writes each record each element from"
      },
      {
        "start": 92.32,
        "duration": 3.119,
        "text": "the source rdd"
      },
      {
        "start": 93.759,
        "duration": 3.441,
        "text": "into the destination table in the"
      },
      {
        "start": 95.439,
        "duration": 2.64,
        "text": "destination key space optionally"
      },
      {
        "start": 97.2,
        "duration": 4.559,
        "text": "selecting"
      },
      {
        "start": 98.079,
        "duration": 6.32,
        "text": "just certain columns from the source rdd"
      },
      {
        "start": 101.759,
        "duration": 4.32,
        "text": "save as cassandra table automatically"
      },
      {
        "start": 104.399,
        "duration": 3.36,
        "text": "creates a new cassandra table there's"
      },
      {
        "start": 106.079,
        "duration": 3.921,
        "text": "sort of an assumption in the api"
      },
      {
        "start": 107.759,
        "duration": 4.0,
        "text": "that we're going to be creating data and"
      },
      {
        "start": 110.0,
        "duration": 3.68,
        "text": "persisting it in interesting ways and"
      },
      {
        "start": 111.759,
        "duration": 2.561,
        "text": "persisting maybe transitory collections"
      },
      {
        "start": 113.68,
        "duration": 3.039,
        "text": "of data"
      },
      {
        "start": 114.32,
        "duration": 3.92,
        "text": "so much so that it'll even offer to"
      },
      {
        "start": 116.719,
        "duration": 3.04,
        "text": "create the table for us"
      },
      {
        "start": 118.24,
        "duration": 2.879,
        "text": "so there are elements of that api that"
      },
      {
        "start": 119.759,
        "duration": 2.561,
        "text": "i'm not going to dive into in a whole"
      },
      {
        "start": 121.119,
        "duration": 2.561,
        "text": "lot of detail right now"
      },
      {
        "start": 122.32,
        "duration": 3.6,
        "text": "they're worth reading about in the"
      },
      {
        "start": 123.68,
        "duration": 2.56,
        "text": "documentation but elements of that api"
      },
      {
        "start": 125.92,
        "duration": 2.8,
        "text": "that"
      },
      {
        "start": 126.24,
        "duration": 4.48,
        "text": "govern the schema of the created table"
      },
      {
        "start": 128.72,
        "duration": 4.159,
        "text": "in particular defining the key"
      },
      {
        "start": 130.72,
        "duration": 3.599,
        "text": "is an important thing and i encourage"
      },
      {
        "start": 132.879,
        "duration": 2.72,
        "text": "you to read the documentation on that to"
      },
      {
        "start": 134.319,
        "duration": 2.56,
        "text": "get into it a little bit more"
      },
      {
        "start": 135.599,
        "duration": 2.801,
        "text": "we're going to keep things simple in our"
      },
      {
        "start": 136.879,
        "duration": 2.401,
        "text": "examples here but definitely worth"
      },
      {
        "start": 138.4,
        "duration": 3.04,
        "text": "digging into"
      },
      {
        "start": 139.28,
        "duration": 3.44,
        "text": "after you master the basics and if you"
      },
      {
        "start": 141.44,
        "duration": 3.36,
        "text": "really have an elaborate table"
      },
      {
        "start": 142.72,
        "duration": 4.239,
        "text": "definition you want to create a table"
      },
      {
        "start": 144.8,
        "duration": 3.68,
        "text": "and it has an interesting key definition"
      },
      {
        "start": 146.959,
        "duration": 4.56,
        "text": "that's entirely possible"
      },
      {
        "start": 148.48,
        "duration": 4.72,
        "text": "you have the save as cassandra table x"
      },
      {
        "start": 151.519,
        "duration": 3.281,
        "text": "that allows you to specify a data"
      },
      {
        "start": 153.2,
        "duration": 3.84,
        "text": "structure pass in a data structure"
      },
      {
        "start": 154.8,
        "duration": 4.0,
        "text": "that describes the partition key and"
      },
      {
        "start": 157.04,
        "duration": 3.68,
        "text": "clustering key definitions"
      },
      {
        "start": 158.8,
        "duration": 3.92,
        "text": "and any other additional columns and"
      },
      {
        "start": 160.72,
        "duration": 4.239,
        "text": "their data types of course you could"
      },
      {
        "start": 162.72,
        "duration": 3.44,
        "text": "also just create the table that you want"
      },
      {
        "start": 164.959,
        "duration": 4.481,
        "text": "and use the"
      },
      {
        "start": 166.16,
        "duration": 3.84,
        "text": "regular cassandra cql driver to issue"
      },
      {
        "start": 169.44,
        "duration": 2.56,
        "text": "that create"
      },
      {
        "start": 170.0,
        "duration": 3.76,
        "text": "table statement and then call save to"
      },
      {
        "start": 172.0,
        "duration": 3.76,
        "text": "cassandra so if you've got an elaborate"
      },
      {
        "start": 173.76,
        "duration": 4.479,
        "text": "schema you just prefer to do it that way"
      },
      {
        "start": 175.76,
        "duration": 3.04,
        "text": "completely fine it's not an anti-pattern"
      },
      {
        "start": 178.239,
        "duration": 2.801,
        "text": "at all"
      },
      {
        "start": 178.8,
        "duration": 3.92,
        "text": "to mix and match the cql and the spark"
      },
      {
        "start": 181.04,
        "duration": 2.32,
        "text": "calls let's take a look at a trivial"
      },
      {
        "start": 182.72,
        "duration": 3.68,
        "text": "example"
      },
      {
        "start": 183.36,
        "duration": 4.879,
        "text": "of safety cassandra now for convenience"
      },
      {
        "start": 186.4,
        "duration": 4.64,
        "text": "we're reconstituting that movie"
      },
      {
        "start": 188.239,
        "duration": 4.321,
        "text": "rdd we're selecting movies by actor"
      },
      {
        "start": 191.04,
        "duration": 3.919,
        "text": "we're actors johnny depp and we just"
      },
      {
        "start": 192.56,
        "duration": 5.599,
        "text": "want title release here in raiding"
      },
      {
        "start": 194.959,
        "duration": 5.521,
        "text": "and we're going to filter only on"
      },
      {
        "start": 198.159,
        "duration": 4.321,
        "text": "those members that rdd those elements in"
      },
      {
        "start": 200.48,
        "duration": 4.479,
        "text": "that rdd whose title happens to be"
      },
      {
        "start": 202.48,
        "duration": 4.399,
        "text": "alice in wonderland so not exactly big"
      },
      {
        "start": 204.959,
        "duration": 4.0,
        "text": "data at the end of that it's one element"
      },
      {
        "start": 206.879,
        "duration": 3.841,
        "text": "but now we can see in a very simple way"
      },
      {
        "start": 208.959,
        "duration": 3.84,
        "text": "we call on those final two lines"
      },
      {
        "start": 210.72,
        "duration": 3.84,
        "text": "save to cassandra we'll specify as"
      },
      {
        "start": 212.799,
        "duration": 3.601,
        "text": "always the key space and the table the"
      },
      {
        "start": 214.56,
        "duration": 3.36,
        "text": "table is now favorite movies"
      },
      {
        "start": 216.4,
        "duration": 3.839,
        "text": "that's where our output data our"
      },
      {
        "start": 217.92,
        "duration": 3.84,
        "text": "processed analytics data is going to"
      },
      {
        "start": 220.239,
        "duration": 3.601,
        "text": "that favorite movies table and we'll"
      },
      {
        "start": 221.76,
        "duration": 2.8,
        "text": "write the title release here and rating"
      },
      {
        "start": 223.84,
        "duration": 2.959,
        "text": "columns"
      },
      {
        "start": 224.56,
        "duration": 3.599,
        "text": "to that table wrapped in that sum"
      },
      {
        "start": 226.799,
        "duration": 3.201,
        "text": "columns object"
      },
      {
        "start": 228.159,
        "duration": 3.521,
        "text": "now details is going to be null that's"
      },
      {
        "start": 230.0,
        "duration": 3.36,
        "text": "that user defined type we don't have any"
      },
      {
        "start": 231.68,
        "duration": 4.08,
        "text": "details to write yet but that's fine"
      },
      {
        "start": 233.36,
        "duration": 3.519,
        "text": "now here's a very simple example of"
      },
      {
        "start": 235.76,
        "duration": 3.6,
        "text": "persisting"
      },
      {
        "start": 236.879,
        "duration": 3.121,
        "text": "case class data now suppose we had some"
      },
      {
        "start": 239.36,
        "duration": 3.28,
        "text": "cassandra"
      },
      {
        "start": 240.0,
        "duration": 4.56,
        "text": "rows that were created by instantiating"
      },
      {
        "start": 242.64,
        "duration": 2.48,
        "text": "case classes we looked at an example of"
      },
      {
        "start": 244.56,
        "duration": 2.64,
        "text": "that"
      },
      {
        "start": 245.12,
        "duration": 3.28,
        "text": "in the previous section in this code"
      },
      {
        "start": 247.2,
        "duration": 2.88,
        "text": "here i'm going to show you"
      },
      {
        "start": 248.4,
        "duration": 3.919,
        "text": "again just enough code for you to type"
      },
      {
        "start": 250.08,
        "duration": 2.879,
        "text": "in and something that you wrap your mind"
      },
      {
        "start": 252.319,
        "duration": 2.721,
        "text": "around"
      },
      {
        "start": 252.959,
        "duration": 3.52,
        "text": "fairly easily we'll see there's a new"
      },
      {
        "start": 255.04,
        "duration": 4.56,
        "text": "case class called genres"
      },
      {
        "start": 256.479,
        "duration": 4.561,
        "text": "info that has three fields title a"
      },
      {
        "start": 259.6,
        "duration": 4.4,
        "text": "string release here and int"
      },
      {
        "start": 261.04,
        "duration": 5.04,
        "text": "genres a set of strings and in that next"
      },
      {
        "start": 264.0,
        "duration": 3.52,
        "text": "line of code there we simply parallelize"
      },
      {
        "start": 266.08,
        "duration": 4.559,
        "text": "a sequence literal there"
      },
      {
        "start": 267.52,
        "duration": 5.36,
        "text": "of a single member of new genre's info"
      },
      {
        "start": 270.639,
        "duration": 3.12,
        "text": "so this is like an rdd created from a"
      },
      {
        "start": 272.88,
        "duration": 3.92,
        "text": "case class"
      },
      {
        "start": 273.759,
        "duration": 4.321,
        "text": "that just has one element in the rdd and"
      },
      {
        "start": 276.8,
        "duration": 4.48,
        "text": "we assign it to the val"
      },
      {
        "start": 278.08,
        "duration": 4.0,
        "text": "genres now that genres rdd we call save"
      },
      {
        "start": 281.28,
        "duration": 2.96,
        "text": "to cassandra"
      },
      {
        "start": 282.08,
        "duration": 4.32,
        "text": "killer video favorite movies the"
      },
      {
        "start": 284.24,
        "duration": 4.16,
        "text": "favorite movies table the output table"
      },
      {
        "start": 286.4,
        "duration": 3.28,
        "text": "and we specify some columns title"
      },
      {
        "start": 288.4,
        "duration": 3.359,
        "text": "release here in genres"
      },
      {
        "start": 289.68,
        "duration": 3.92,
        "text": "and that rdd which was instantiated"
      },
      {
        "start": 291.759,
        "duration": 4.321,
        "text": "through the the case class"
      },
      {
        "start": 293.6,
        "duration": 3.28,
        "text": "pathway is persisted just as neatly back"
      },
      {
        "start": 296.08,
        "duration": 3.04,
        "text": "to the new"
      },
      {
        "start": 296.88,
        "duration": 3.2,
        "text": "table finally with tuples we'll do the"
      },
      {
        "start": 299.12,
        "duration": 3.76,
        "text": "same thing here"
      },
      {
        "start": 300.08,
        "duration": 4.72,
        "text": "where i'll create sort of in a literal a"
      },
      {
        "start": 302.88,
        "duration": 3.44,
        "text": "new rdd from a tuple and then we'll"
      },
      {
        "start": 304.8,
        "duration": 2.64,
        "text": "persist it just so you can see how the"
      },
      {
        "start": 306.32,
        "duration": 3.439,
        "text": "api works"
      },
      {
        "start": 307.44,
        "duration": 3.039,
        "text": "with real code we parallelize a literal"
      },
      {
        "start": 309.759,
        "duration": 2.481,
        "text": "sequence"
      },
      {
        "start": 310.479,
        "duration": 3.681,
        "text": "that has a single entry it's alice in"
      },
      {
        "start": 312.24,
        "duration": 5.28,
        "text": "wonderland it's from 2010"
      },
      {
        "start": 314.16,
        "duration": 5.12,
        "text": "and finally we have a value for our user"
      },
      {
        "start": 317.52,
        "duration": 2.32,
        "text": "defined type that's on that third line"
      },
      {
        "start": 319.28,
        "duration": 3.199,
        "text": "that says"
      },
      {
        "start": 319.84,
        "duration": 4.4,
        "text": "udt value from map and then a map"
      },
      {
        "start": 322.479,
        "duration": 3.601,
        "text": "mapping country to usa language to"
      },
      {
        "start": 324.24,
        "duration": 3.28,
        "text": "english runtime to 108. those are those"
      },
      {
        "start": 326.08,
        "duration": 2.8,
        "text": "three members from the udt that we"
      },
      {
        "start": 327.52,
        "duration": 1.84,
        "text": "defined back at the beginning of the"
      },
      {
        "start": 328.88,
        "duration": 2.4,
        "text": "section"
      },
      {
        "start": 329.36,
        "duration": 4.48,
        "text": "having created that new rdd we can"
      },
      {
        "start": 331.28,
        "duration": 4.96,
        "text": "simply save it to cassandra same api"
      },
      {
        "start": 333.84,
        "duration": 4.24,
        "text": "specify the three columns and it gets"
      },
      {
        "start": 336.24,
        "duration": 4.239,
        "text": "done and we have a value"
      },
      {
        "start": 338.08,
        "duration": 4.08,
        "text": "in the details column we have a value"
      },
      {
        "start": 340.479,
        "duration": 3.521,
        "text": "for our user defined type"
      },
      {
        "start": 342.16,
        "duration": 4.24,
        "text": "first time we've seen that be not null"
      },
      {
        "start": 344.0,
        "duration": 3.759,
        "text": "that's the basics of the spark cassandra"
      },
      {
        "start": 346.4,
        "duration": 3.6,
        "text": "connector we've looked at how to read"
      },
      {
        "start": 347.759,
        "duration": 3.841,
        "text": "data out of cassandra how to operate on"
      },
      {
        "start": 350.0,
        "duration": 3.68,
        "text": "cassandra row objects"
      },
      {
        "start": 351.6,
        "duration": 3.439,
        "text": "how to do more interesting things a"
      },
      {
        "start": 353.68,
        "duration": 3.84,
        "text": "couple of different ways"
      },
      {
        "start": 355.039,
        "duration": 3.121,
        "text": "to transform a cassandra row into a"
      },
      {
        "start": 357.52,
        "duration": 3.36,
        "text": "tuple"
      },
      {
        "start": 358.16,
        "duration": 4.08,
        "text": "or an instance of a class useful for for"
      },
      {
        "start": 360.88,
        "duration": 2.879,
        "text": "different kinds of use cases"
      },
      {
        "start": 362.24,
        "duration": 3.92,
        "text": "and of course we looked at how to save"
      },
      {
        "start": 363.759,
        "duration": 4.081,
        "text": "data back to cassandra regardless of"
      },
      {
        "start": 366.16,
        "duration": 3.68,
        "text": "which way we got the data in from"
      },
      {
        "start": 367.84,
        "duration": 3.44,
        "text": "cassandra and sort of what it became"
      },
      {
        "start": 369.84,
        "duration": 3.52,
        "text": "during the process of being"
      },
      {
        "start": 371.28,
        "duration": 3.84,
        "text": "transformed we could write it back out"
      },
      {
        "start": 373.36,
        "duration": 3.52,
        "text": "to tables and that's a very common use"
      },
      {
        "start": 375.12,
        "duration": 3.44,
        "text": "case bring the data in"
      },
      {
        "start": 376.88,
        "duration": 3.12,
        "text": "do the distributed computation on the"
      },
      {
        "start": 378.56,
        "duration": 3.6,
        "text": "data and write"
      },
      {
        "start": 380.0,
        "duration": 3.68,
        "text": "summary results out to a cassandra table"
      },
      {
        "start": 382.16,
        "duration": 4.08,
        "text": "that can then be accessed with"
      },
      {
        "start": 383.68,
        "duration": 3.68,
        "text": "very low latency with a typical"
      },
      {
        "start": 386.24,
        "duration": 2.64,
        "text": "cassandra query"
      },
      {
        "start": 387.36,
        "duration": 3.52,
        "text": "so we begin to have some pretty powerful"
      },
      {
        "start": 388.88,
        "duration": 4.0,
        "text": "tools under our belt now that we know"
      },
      {
        "start": 390.88,
        "duration": 3.439,
        "text": "something about what rdds are and how to"
      },
      {
        "start": 392.88,
        "duration": 3.12,
        "text": "interact with cassandra and just the"
      },
      {
        "start": 394.319,
        "duration": 2.88,
        "text": "basics of how spark works"
      },
      {
        "start": 396.0,
        "duration": 2.72,
        "text": "you're starting to get to the point"
      },
      {
        "start": 397.199,
        "duration": 2.401,
        "text": "where you could be dangerous in the"
      },
      {
        "start": 398.72,
        "duration": 2.64,
        "text": "following section"
      },
      {
        "start": 399.6,
        "duration": 9.84,
        "text": "we're going to build on those skills in"
      },
      {
        "start": 401.36,
        "duration": 10.16,
        "text": "some important ways stick with us"
      },
      {
        "start": 409.44,
        "duration": 2.08,
        "text": "you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:45:35.528247+00:00"
}