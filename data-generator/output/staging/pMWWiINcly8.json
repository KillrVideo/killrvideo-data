{
  "video_id": "pMWWiINcly8",
  "title": "DS320.32 Spark Streaming: First App | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.32 Spark Streaming: First App\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:31:43Z",
  "thumbnail": "https://i.ytimg.com/vi/pMWWiINcly8/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=pMWWiINcly8",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's look at a complete example of a standalone spark streaming application that takes place in the context of our killer video site and this application is going to be really the minimum subset spark streaming standalone program it's got six steps in it what we'll do is we'll import classes make a configuration object we'll initialize the streaming context and the spark context create the d stream object define our computation and then start it running and wait for it to terminate we'll begin by importing these classes for spark spark streaming and the spark cassandra connector and we'll create a configuration object we'll give our application a name tell it where its master is located and set just a couple of parameters and now that configuration takes a little bit of explanation the spark cassandra connection host is the address of at least one node in your cassandra cluster spark cleaner ttl set to 3600 seconds is the thing we normally haven't had to think about in regular spark applications but this is the amount of time spark will persist its computation metadata the the metadata that describes the directed acyclic graph this is how long that stuff can live since a streaming application can be very long-lived it's important to set this ttl to something finite otherwise you'll have effectively a memory leak by constantly allocating new metadata and never throwing it out finally setting the name of the jar or jars at the bottom there your app.jar would be the name of the jar that the build for this particular standalone program would create with the code that we're walking through here spark needs to know the name of that jar so it can distribute that code around the cluster there are two ways to create a spark streaming context the first one relies on the configuration object we created up above and simply directly instantiates a new streaming context the second creates a conventional spark context first and instantiates a streaming context on top of that spark context now you note both of them pass in a number of seconds that number of seconds is four that's the batch interval that's how long the receiver will wait and batch up data coming in off the input stream before creating a new rdd in the d stream four is great for our illustration purposes here and there really is no general purpose answer to what that number should be that depends entirely upon the requirements of your application if data is coming in at a very very high rate you're going to tend to want a small number to keep the data set manageable if you have a requirement for processing latency that's very aggressive like you need to make a decision within two seconds of some event on the stream then you'd want to set that to a small number like one in the absence of those considerations you might be able to tolerate a larger number the socket text stream method is creating the actual receiver this is an api that creates a text receiver and it opens a socket to that ip address and that port it's going to make a tcp connection to that port and ip and expect to receive text from it it'll create unique records in the dstream rdd by line feed delimiting the text that comes in since it's a text stream that's the protocol it implements other kinds of receivers of course will do their thing twitter xero mq kafka etc but text stream works that way finally we get to the actual computation we've got our context created we've got the stream coming in we need to define what we're going to do with it and it looks here like we are counting words we're going to flat map those records having delimited them on spaces then we'll count each word individually just really what we're doing with that map line we're mapping word to a tuple of word and the number one because every time we see a word we have seen it once and then we'll reduce those by key using the anonymous argument syntax in scala there that's going to give us effectively a pair rdd as output with the values of the key value pairs being the counts of words and that's going to update every four seconds we'll get a new one out of the computation every four seconds because we defined four as our batch interval final in our program we'll start the context and we will await termination a wait termination will effectively block until the job is terminated now here is the complete application of course you see in this code a little bit of naughtiness like the spark master and the cassandra node things are hard coded to the localhost ip address and all that stuff should be passed in as parameters to the main method in a production application but this program in its entirety could be compiled using sbt or gradle or something like that then the jar that we get out of that could be submitted to the dse cluster using the dse spark submit command and passing in the jar to it that'll send it to the spark cluster and allow us to run it and here's some sample output this is too long to fit on the screen so we'll scroll through it but every four seconds we're going to get the output of the job we'll see for example in that first output of batch data we'll see 22 occurrences of drama uh 12 of comedy 16 of action 9 of romance and so forth and if you look at the ending digits of that time stamp there 0 0 0 0 and down below we'll scroll down there that's 4 seconds later 4 0 0 0 is the ending of that time stamp but we have different word counts that happened during that stream interval but you see that we have a running standalone spark streaming application super easy to understand something that you can begin with and extend for your own applications",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.16,
        "duration": 3.84,
        "text": "let's look at a complete example"
      },
      {
        "start": 7.919,
        "duration": 3.76,
        "text": "of a standalone spark streaming"
      },
      {
        "start": 10.0,
        "duration": 3.759,
        "text": "application that takes place"
      },
      {
        "start": 11.679,
        "duration": 3.441,
        "text": "in the context of our killer video site"
      },
      {
        "start": 13.759,
        "duration": 2.961,
        "text": "and this application is going to be"
      },
      {
        "start": 15.12,
        "duration": 3.44,
        "text": "really the minimum subset spark"
      },
      {
        "start": 16.72,
        "duration": 2.8,
        "text": "streaming standalone program it's got"
      },
      {
        "start": 18.56,
        "duration": 3.2,
        "text": "six steps in it"
      },
      {
        "start": 19.52,
        "duration": 3.999,
        "text": "what we'll do is we'll import classes"
      },
      {
        "start": 21.76,
        "duration": 3.439,
        "text": "make a configuration object"
      },
      {
        "start": 23.519,
        "duration": 3.441,
        "text": "we'll initialize the streaming context"
      },
      {
        "start": 25.199,
        "duration": 2.561,
        "text": "and the spark context create the d"
      },
      {
        "start": 26.96,
        "duration": 4.0,
        "text": "stream object"
      },
      {
        "start": 27.76,
        "duration": 4.959,
        "text": "define our computation and then start it"
      },
      {
        "start": 30.96,
        "duration": 5.04,
        "text": "running and wait for it to terminate"
      },
      {
        "start": 32.719,
        "duration": 5.441,
        "text": "we'll begin by importing these classes"
      },
      {
        "start": 36.0,
        "duration": 3.28,
        "text": "for spark spark streaming and the spark"
      },
      {
        "start": 38.16,
        "duration": 3.28,
        "text": "cassandra connector"
      },
      {
        "start": 39.28,
        "duration": 3.92,
        "text": "and we'll create a configuration object"
      },
      {
        "start": 41.44,
        "duration": 4.0,
        "text": "we'll give our application a name"
      },
      {
        "start": 43.2,
        "duration": 4.0,
        "text": "tell it where its master is located and"
      },
      {
        "start": 45.44,
        "duration": 3.2,
        "text": "set just a couple of parameters"
      },
      {
        "start": 47.2,
        "duration": 3.12,
        "text": "and now that configuration takes a"
      },
      {
        "start": 48.64,
        "duration": 4.8,
        "text": "little bit of explanation"
      },
      {
        "start": 50.32,
        "duration": 5.6,
        "text": "the spark cassandra connection host is"
      },
      {
        "start": 53.44,
        "duration": 3.759,
        "text": "the address of at least one node in your"
      },
      {
        "start": 55.92,
        "duration": 5.279,
        "text": "cassandra cluster"
      },
      {
        "start": 57.199,
        "duration": 5.84,
        "text": "spark cleaner ttl set to 3600 seconds"
      },
      {
        "start": 61.199,
        "duration": 3.68,
        "text": "is the thing we normally haven't had to"
      },
      {
        "start": 63.039,
        "duration": 2.721,
        "text": "think about in regular spark"
      },
      {
        "start": 64.879,
        "duration": 3.6,
        "text": "applications"
      },
      {
        "start": 65.76,
        "duration": 3.84,
        "text": "but this is the amount of time spark"
      },
      {
        "start": 68.479,
        "duration": 3.921,
        "text": "will persist"
      },
      {
        "start": 69.6,
        "duration": 4.32,
        "text": "its computation metadata the the"
      },
      {
        "start": 72.4,
        "duration": 2.56,
        "text": "metadata that describes the directed"
      },
      {
        "start": 73.92,
        "duration": 3.44,
        "text": "acyclic graph"
      },
      {
        "start": 74.96,
        "duration": 4.32,
        "text": "this is how long that stuff can live"
      },
      {
        "start": 77.36,
        "duration": 3.2,
        "text": "since a streaming application can be"
      },
      {
        "start": 79.28,
        "duration": 4.4,
        "text": "very long-lived"
      },
      {
        "start": 80.56,
        "duration": 4.64,
        "text": "it's important to set this ttl to"
      },
      {
        "start": 83.68,
        "duration": 2.32,
        "text": "something finite otherwise you'll have"
      },
      {
        "start": 85.2,
        "duration": 3.2,
        "text": "effectively"
      },
      {
        "start": 86.0,
        "duration": 3.36,
        "text": "a memory leak by constantly allocating"
      },
      {
        "start": 88.4,
        "duration": 3.039,
        "text": "new metadata"
      },
      {
        "start": 89.36,
        "duration": 3.52,
        "text": "and never throwing it out finally"
      },
      {
        "start": 91.439,
        "duration": 4.241,
        "text": "setting the name of the"
      },
      {
        "start": 92.88,
        "duration": 3.76,
        "text": "jar or jars at the bottom there your"
      },
      {
        "start": 95.68,
        "duration": 2.799,
        "text": "app.jar"
      },
      {
        "start": 96.64,
        "duration": 3.6,
        "text": "would be the name of the jar that the"
      },
      {
        "start": 98.479,
        "duration": 2.401,
        "text": "build for this particular standalone"
      },
      {
        "start": 100.24,
        "duration": 2.879,
        "text": "program"
      },
      {
        "start": 100.88,
        "duration": 3.44,
        "text": "would create with the code that we're"
      },
      {
        "start": 103.119,
        "duration": 2.96,
        "text": "walking through here"
      },
      {
        "start": 104.32,
        "duration": 3.36,
        "text": "spark needs to know the name of that jar"
      },
      {
        "start": 106.079,
        "duration": 2.561,
        "text": "so it can distribute that code around"
      },
      {
        "start": 107.68,
        "duration": 3.119,
        "text": "the cluster"
      },
      {
        "start": 108.64,
        "duration": 3.28,
        "text": "there are two ways to create a spark"
      },
      {
        "start": 110.799,
        "duration": 2.32,
        "text": "streaming context"
      },
      {
        "start": 111.92,
        "duration": 3.12,
        "text": "the first one relies on the"
      },
      {
        "start": 113.119,
        "duration": 3.841,
        "text": "configuration object we created up above"
      },
      {
        "start": 115.04,
        "duration": 4.32,
        "text": "and simply directly instantiates"
      },
      {
        "start": 116.96,
        "duration": 4.479,
        "text": "a new streaming context the second"
      },
      {
        "start": 119.36,
        "duration": 2.799,
        "text": "creates a conventional spark context"
      },
      {
        "start": 121.439,
        "duration": 3.121,
        "text": "first"
      },
      {
        "start": 122.159,
        "duration": 3.28,
        "text": "and instantiates a streaming context on"
      },
      {
        "start": 124.56,
        "duration": 3.759,
        "text": "top of that"
      },
      {
        "start": 125.439,
        "duration": 4.96,
        "text": "spark context now you note both of them"
      },
      {
        "start": 128.319,
        "duration": 3.361,
        "text": "pass in a number of seconds that number"
      },
      {
        "start": 130.399,
        "duration": 3.281,
        "text": "of seconds is four"
      },
      {
        "start": 131.68,
        "duration": 4.32,
        "text": "that's the batch interval that's how"
      },
      {
        "start": 133.68,
        "duration": 2.88,
        "text": "long the receiver will wait and batch up"
      },
      {
        "start": 136.0,
        "duration": 2.72,
        "text": "data"
      },
      {
        "start": 136.56,
        "duration": 4.24,
        "text": "coming in off the input stream before"
      },
      {
        "start": 138.72,
        "duration": 4.239,
        "text": "creating a new rdd"
      },
      {
        "start": 140.8,
        "duration": 3.519,
        "text": "in the d stream four is great for our"
      },
      {
        "start": 142.959,
        "duration": 3.201,
        "text": "illustration purposes here"
      },
      {
        "start": 144.319,
        "duration": 3.441,
        "text": "and there really is no general purpose"
      },
      {
        "start": 146.16,
        "duration": 3.6,
        "text": "answer to what that number should be"
      },
      {
        "start": 147.76,
        "duration": 3.759,
        "text": "that depends entirely upon the"
      },
      {
        "start": 149.76,
        "duration": 3.839,
        "text": "requirements of your application"
      },
      {
        "start": 151.519,
        "duration": 3.601,
        "text": "if data is coming in at a very very high"
      },
      {
        "start": 153.599,
        "duration": 2.801,
        "text": "rate you're going to tend to want a"
      },
      {
        "start": 155.12,
        "duration": 3.92,
        "text": "small number to keep"
      },
      {
        "start": 156.4,
        "duration": 4.32,
        "text": "the data set manageable if you have a"
      },
      {
        "start": 159.04,
        "duration": 3.12,
        "text": "requirement for processing latency"
      },
      {
        "start": 160.72,
        "duration": 3.12,
        "text": "that's very aggressive"
      },
      {
        "start": 162.16,
        "duration": 4.48,
        "text": "like you need to make a decision within"
      },
      {
        "start": 163.84,
        "duration": 4.24,
        "text": "two seconds of some event on the stream"
      },
      {
        "start": 166.64,
        "duration": 3.44,
        "text": "then you'd want to set that to a small"
      },
      {
        "start": 168.08,
        "duration": 3.439,
        "text": "number like one in the absence of those"
      },
      {
        "start": 170.08,
        "duration": 2.159,
        "text": "considerations you might be able to"
      },
      {
        "start": 171.519,
        "duration": 4.0,
        "text": "tolerate"
      },
      {
        "start": 172.239,
        "duration": 6.321,
        "text": "a larger number the socket text stream"
      },
      {
        "start": 175.519,
        "duration": 4.961,
        "text": "method is creating the actual receiver"
      },
      {
        "start": 178.56,
        "duration": 4.319,
        "text": "this is an api that creates a text"
      },
      {
        "start": 180.48,
        "duration": 4.08,
        "text": "receiver and it opens a socket to that"
      },
      {
        "start": 182.879,
        "duration": 3.761,
        "text": "ip address and"
      },
      {
        "start": 184.56,
        "duration": 4.08,
        "text": "that port it's going to make a tcp"
      },
      {
        "start": 186.64,
        "duration": 5.36,
        "text": "connection to that port and ip"
      },
      {
        "start": 188.64,
        "duration": 5.519,
        "text": "and expect to receive text from it"
      },
      {
        "start": 192.0,
        "duration": 3.68,
        "text": "it'll create unique records in the"
      },
      {
        "start": 194.159,
        "duration": 4.16,
        "text": "dstream rdd"
      },
      {
        "start": 195.68,
        "duration": 3.279,
        "text": "by line feed delimiting the text that"
      },
      {
        "start": 198.319,
        "duration": 2.721,
        "text": "comes in"
      },
      {
        "start": 198.959,
        "duration": 3.441,
        "text": "since it's a text stream that's the"
      },
      {
        "start": 201.04,
        "duration": 2.88,
        "text": "protocol it implements"
      },
      {
        "start": 202.4,
        "duration": 3.28,
        "text": "other kinds of receivers of course will"
      },
      {
        "start": 203.92,
        "duration": 4.959,
        "text": "do their thing twitter"
      },
      {
        "start": 205.68,
        "duration": 3.919,
        "text": "xero mq kafka etc but text stream works"
      },
      {
        "start": 208.879,
        "duration": 2.64,
        "text": "that way"
      },
      {
        "start": 209.599,
        "duration": 3.841,
        "text": "finally we get to the actual computation"
      },
      {
        "start": 211.519,
        "duration": 2.961,
        "text": "we've got our context created we've got"
      },
      {
        "start": 213.44,
        "duration": 2.879,
        "text": "the stream coming in"
      },
      {
        "start": 214.48,
        "duration": 4.0,
        "text": "we need to define what we're going to do"
      },
      {
        "start": 216.319,
        "duration": 2.881,
        "text": "with it and it looks here like we are"
      },
      {
        "start": 218.48,
        "duration": 2.399,
        "text": "counting"
      },
      {
        "start": 219.2,
        "duration": 3.2,
        "text": "words we're going to flat map those"
      },
      {
        "start": 220.879,
        "duration": 4.72,
        "text": "records having delimited them"
      },
      {
        "start": 222.4,
        "duration": 4.72,
        "text": "on spaces then we'll count each word"
      },
      {
        "start": 225.599,
        "duration": 3.041,
        "text": "individually just really what we're"
      },
      {
        "start": 227.12,
        "duration": 4.08,
        "text": "doing with that map line"
      },
      {
        "start": 228.64,
        "duration": 3.599,
        "text": "we're mapping word to a tuple of word"
      },
      {
        "start": 231.2,
        "duration": 2.88,
        "text": "and the number one"
      },
      {
        "start": 232.239,
        "duration": 4.241,
        "text": "because every time we see a word we have"
      },
      {
        "start": 234.08,
        "duration": 4.32,
        "text": "seen it once and then we'll reduce those"
      },
      {
        "start": 236.48,
        "duration": 3.679,
        "text": "by key using the anonymous argument"
      },
      {
        "start": 238.4,
        "duration": 3.759,
        "text": "syntax in scala there"
      },
      {
        "start": 240.159,
        "duration": 3.201,
        "text": "that's going to give us effectively a"
      },
      {
        "start": 242.159,
        "duration": 4.481,
        "text": "pair rdd as"
      },
      {
        "start": 243.36,
        "duration": 5.12,
        "text": "output with the values of the key value"
      },
      {
        "start": 246.64,
        "duration": 4.4,
        "text": "pairs being the counts of"
      },
      {
        "start": 248.48,
        "duration": 3.039,
        "text": "words and that's going to update every"
      },
      {
        "start": 251.04,
        "duration": 2.08,
        "text": "four"
      },
      {
        "start": 251.519,
        "duration": 3.681,
        "text": "seconds we'll get a new one out of the"
      },
      {
        "start": 253.12,
        "duration": 2.64,
        "text": "computation every four seconds because"
      },
      {
        "start": 255.2,
        "duration": 2.8,
        "text": "we defined"
      },
      {
        "start": 255.76,
        "duration": 4.159,
        "text": "four as our batch interval final in our"
      },
      {
        "start": 258.0,
        "duration": 2.239,
        "text": "program we'll start the context and we"
      },
      {
        "start": 259.919,
        "duration": 2.801,
        "text": "will"
      },
      {
        "start": 260.239,
        "duration": 3.841,
        "text": "await termination a wait termination"
      },
      {
        "start": 262.72,
        "duration": 4.64,
        "text": "will effectively block"
      },
      {
        "start": 264.08,
        "duration": 3.92,
        "text": "until the job is terminated now here is"
      },
      {
        "start": 267.36,
        "duration": 2.08,
        "text": "the complete"
      },
      {
        "start": 268.0,
        "duration": 2.88,
        "text": "application of course you see in this"
      },
      {
        "start": 269.44,
        "duration": 3.36,
        "text": "code a little bit of naughtiness like"
      },
      {
        "start": 270.88,
        "duration": 2.56,
        "text": "the spark master and the cassandra node"
      },
      {
        "start": 272.8,
        "duration": 4.0,
        "text": "things are"
      },
      {
        "start": 273.44,
        "duration": 4.16,
        "text": "hard coded to the localhost ip address"
      },
      {
        "start": 276.8,
        "duration": 2.64,
        "text": "and all that stuff"
      },
      {
        "start": 277.6,
        "duration": 4.08,
        "text": "should be passed in as parameters to the"
      },
      {
        "start": 279.44,
        "duration": 4.319,
        "text": "main method in a production application"
      },
      {
        "start": 281.68,
        "duration": 3.2,
        "text": "but this program in its entirety could"
      },
      {
        "start": 283.759,
        "duration": 4.72,
        "text": "be compiled"
      },
      {
        "start": 284.88,
        "duration": 4.08,
        "text": "using sbt or gradle or something like"
      },
      {
        "start": 288.479,
        "duration": 2.321,
        "text": "that"
      },
      {
        "start": 288.96,
        "duration": 3.92,
        "text": "then the jar that we get out of that"
      },
      {
        "start": 290.8,
        "duration": 5.2,
        "text": "could be submitted to the dse cluster"
      },
      {
        "start": 292.88,
        "duration": 5.039,
        "text": "using the dse spark submit command and"
      },
      {
        "start": 296.0,
        "duration": 3.68,
        "text": "passing in the jar to it that'll send it"
      },
      {
        "start": 297.919,
        "duration": 2.0,
        "text": "to the spark cluster and allow us to run"
      },
      {
        "start": 299.68,
        "duration": 1.84,
        "text": "it"
      },
      {
        "start": 299.919,
        "duration": 2.881,
        "text": "and here's some sample output this is"
      },
      {
        "start": 301.52,
        "duration": 2.16,
        "text": "too long to fit on the screen so we'll"
      },
      {
        "start": 302.8,
        "duration": 3.119,
        "text": "scroll through it"
      },
      {
        "start": 303.68,
        "duration": 4.0,
        "text": "but every four seconds we're going to"
      },
      {
        "start": 305.919,
        "duration": 4.56,
        "text": "get the output of"
      },
      {
        "start": 307.68,
        "duration": 4.16,
        "text": "the job we'll see for example in that"
      },
      {
        "start": 310.479,
        "duration": 3.921,
        "text": "first output of"
      },
      {
        "start": 311.84,
        "duration": 3.28,
        "text": "batch data we'll see 22 occurrences of"
      },
      {
        "start": 314.4,
        "duration": 3.84,
        "text": "drama"
      },
      {
        "start": 315.12,
        "duration": 4.32,
        "text": "uh 12 of comedy 16 of action 9 of"
      },
      {
        "start": 318.24,
        "duration": 2.799,
        "text": "romance and so forth"
      },
      {
        "start": 319.44,
        "duration": 3.199,
        "text": "and if you look at the ending digits of"
      },
      {
        "start": 321.039,
        "duration": 4.641,
        "text": "that time stamp there 0"
      },
      {
        "start": 322.639,
        "duration": 5.201,
        "text": "0 0 0 and down below we'll scroll down"
      },
      {
        "start": 325.68,
        "duration": 5.28,
        "text": "there"
      },
      {
        "start": 327.84,
        "duration": 5.76,
        "text": "that's 4 seconds later 4 0"
      },
      {
        "start": 330.96,
        "duration": 4.0,
        "text": "0 0 is the ending of that time stamp but"
      },
      {
        "start": 333.6,
        "duration": 1.84,
        "text": "we have different word counts that"
      },
      {
        "start": 334.96,
        "duration": 2.56,
        "text": "happened"
      },
      {
        "start": 335.44,
        "duration": 3.44,
        "text": "during that stream interval but you see"
      },
      {
        "start": 337.52,
        "duration": 3.92,
        "text": "that we have a running"
      },
      {
        "start": 338.88,
        "duration": 4.0,
        "text": "standalone spark streaming application"
      },
      {
        "start": 341.44,
        "duration": 2.8,
        "text": "super easy to understand"
      },
      {
        "start": 342.88,
        "duration": 10.96,
        "text": "something that you can begin with and"
      },
      {
        "start": 344.24,
        "duration": 9.6,
        "text": "extend for your own applications"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:24:25.776245+00:00"
}