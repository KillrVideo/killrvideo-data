{
  "video_id": "HG5bNxCYA40",
  "title": "DSE Analytics: Workpools and File Ownership",
  "description": "How to use workpools in DataStax Enterprise Analytics. Also includes an explanation of how folder ownership works in Apache Spark applications.\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastaxdevs?sub_confirmation=1 \nTwitter: https://twitter.com/datastaxdevs\nTwitch: https://www.twitch.tv/datastaxdevs\n\nABOUT DATASTAX DEVELOPERS\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache CassandraÂ©.  Create an account on https://academy.datastax.com to watch our free online courses, tutorials, and more.",
  "published_at": "2020-07-22T20:50:16Z",
  "thumbnail": "https://i.ytimg.com/vi/HG5bNxCYA40/hqdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "datastax",
    "apache_cassandra",
    "dse",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=HG5bNxCYA40",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "hello my name is sarah bispass i am a data architect with data stacks today we are going to do a short demonstration which will show you how to use workflows with es analytics at the same time we'll also make sure that we understand the ownership of the folders and files that are created when you run a spark application on these analytics in this video we'll cover a few things we'll give you an overview of how we have set up our cluster then we'll tell you how to set up workforce and show you how it can restrict permission to those work pools using dac authentication and authorization will give you an overview of the sample java code that will simulate folder creation will execute the code and we'll verify the ownership of the folders that were created by the driver as well as the executors while they run the application and finally we're going to demonstrate how workforce can restrict usage of spark resources even when your users might try to run their jobs with a higher resource then that's what's allocated for them in their workflow so let's go ahead and start a demonstration let's take a look at the cluster i have set up for this particular demo i've used ec2 instances it's all on amazon cloud there are three nodes and all the nodes have search as well as analytics workloads enabled on them i'm going to use the dc tool ring utility to take a look at the server health the cluster has as well as the workloads uh their topology their back and dc so you see there are like three nodes all on the same rack on the same pc all of them have searched analytics and i've used a very simple setup just to demonstrate a particular viscose that we're going to target so you can see like the spark master is running on the node with ip ending in 244 this is the private ip and the corresponding public ip is 102 and on port 7080 of that particular ip we have the spark master ui running which gives you overview of the spark cluster in this cluster you will see as expected there are three workers running the total number of cores that my spark cluster has is 33 and the total memory is close to 160 gigabytes uh one interesting thing to note here is under the course of the memory essentially the resources of your spark cluster you will see there are three categories one is default one is high one is low high resources slow resources similarly for memory i will briefly explain where these particular subcategories are coming from these are essentially the work pools if you look at the jobs that we have run we assign a work pool to all our spark submissions when you submit a driver you mention a work pool and that work pool is also associated with the user the dac user who is submitting this particular job what that ensures is if i am a tenant who has certain resources allocated to me i can under no circumstances submit a job that consumes more resources so i can always go ahead and try to give a total number of course that's higher than my low resource allocation of three course right word pool and the whole setup of work pool ensures that you're never able to do that let me show you how you set it up and then while demonstrating uh the actual application or actual process uh we will show you how if you try to use something that you're not supposed to like a workflow that you're not supposed to uh you will not be allowed to submit your jobs going back to that particular node we were in let's take a look at the dsc.wire i have package installation so my default location is at cdhcp it's like ml for turbo you can go to that particular location of configurations now out here search for work pool you will see its sub category under resource manager options this is major options essentially controls how much resources are related to your workers and out here what i've defined is a higher resource and the low resource now if you remember you don't see always on sql in the spark master ui right because this is reserved for you always on server if you're not using that feature you can always go ahead and remove some of some course memories from here the high and the low resources are what we have defined and it can be number of workbooks you can have multiple workflows whatever is left from these so uh one minus the sum of code which workflow goes to the default workflow so the default workflow is used when no other pools are being used and you can actually ensure that your resources or your users can only use a particular workflow and we'll see how to grant those permissions documented but let me show you how it looks like so let me go to the simple shadow i'm going to do the worst practice of typing in the password on the screen but since it's just a demo environment i'm doing it so let me take a look at a user i have created right so list of permissions of right so this is a role i had created and if you look at it you'll see it has different permissions and stuff but the most important aspect of it is the first row in the rows where you see it has create permission on the low resources for pool in enterprise data center so as long as he's they are using the low resource as the workforce they can create submissions or they can successfully create applications and drivers in your spark cluster if they do not have access they want to use a workflow three supposedly they wouldn't be allowed to do it they'll encounter the error when they try to do it so this is a very good way of tenant resource control if a multi-tenant system as operations team you can address the concern of over utilization of resources by a particular team you can totally restrict them to some resources uh very useful tool that's why we wanted to demonstrate this feature also as part of it next i wanted to show you uh what my sample spark application looks like i've created a very simple class all it does is create a folder from the driver and then create folders from each of the executors or workers so how have i achieved it right so if you see this is the main class that's going to be provided as the main class for the spark submit job here i'm creating a simple directory and i'm appending the directory name with driver underscore sample everything is going to be created in the temp folder i'm using the local file system but the behavior would be same when you're using the nfs file system like you do the important thing to note here is i want to figure out which operating system user is becoming the owner of that particular file right so suppose i launch my spark job or issue my spark submit as operation system user one do these folders get created as os1 as the owner or are they created under the os user that runs the spark process which by default is the dac runner process which is uh cassandra user of course there's this whole concept of slot users and we're not going to discuss slot users we are just assuming we are using the default cassandra user for running all spark applications so that is one aspect of it our understanding our expectation is the owner after the full discretion should be cassandra user similarly to make sure that the workers are also writing some the executors the workers writing or creating folders as part of the tasks right i have created a sample code here where it's doing the same thing but the folder is appended with executable sample and even at the end of it again i'm going to print the owner as well as going to check the folders using a list command and make sure that our expectation matches with what is being done what is been actually produced on the servers and that expectation being both of them should be owned by the cassandra all right now i'm back to the node and i'm going to try to [Music] do a spark segment and see what happens uh before i start let's do one more task right let's make sure just on one node that have no driver underscore sam hello execute underscore sample folder i made sure that i have removed it from all the notes just for a fresh demo but as you can see it's not there right so it's not going to be an issue um i can also do this right let's do this thing what i'm gonna do is use a utility to do a ls on all the nodes on my cluster so it's like a parallel ssh tool and i'm going to use it to make sure that the three notes that i am on right doesn't have any folder that's fascinating driver on this was not executed on this example just making sure it's clean uh so that we have fresh test results when you write uh now let's try to run a spark submit i have used dacfs to put the jar file that i've compiled out of the class that i showed you earlier and the reason of for using dsfs is it's always the best practice to have a shared location where you put your jar files or executables it can be nfs mount too and also use a local but just have to make sure that your jar is present and all the local nodes and it's not always pretty when you do that because you can run into version mismatch stuff like that so let's assume that you'll always be using a centralized location which is accessible from all the nodes in our case it's uh dhcfs so let's go into the dacfa system and show you where i'm putting my chat or where i've already put major um execute a ls command on the acfs and if you see there is a jar called write 301 all right i'm going to use it in my spark submit so i'm going to go back and use one of the spark submit commands that we already used in the past so if we really look at it right what all parameters i'm passing through it i'm passing the name which is in dlcfs right so this is the journal name then i'm passing the main class which i showed you right uh i'm passing the master url and the reason for passing the mass url is this is how i can make sure i mentioned the workflows i'm mentioning low resource workflow for my master url indicates this spark submit should submit the job using low resource workflow then you have the diplomat cluster obviously we always use the cluster mode for uh all production deployments um and i have to pass a ds user uh who will help with submission of the job right so inside the code the ds user has to have access to your key spaces and your data but this particular user you're going to pass should have the bare minimum permissions which are listed up there if you see to be able to submit a job to a particular workflow right so let's try to run it and see what happens all right so you see that the driver has been created uh this is the work on which the driver is running and this is the id of the driver uh let's go to our spot master and see what it looks like so you see that driver ending in zero zero 11 is running the application id is running let's go into this driver worker right and see the driver is still running and if you remember i was printing the folder and permissions right so here we are we see it says that we have created this folder on this particular worker node and then the owner is cassandra another important part i want to show you when i go back to the code is at the very beginning i'm printing the operating system user who is executing this piece of code and that is to figure out how or who owns the process that is being spun off by the dc's person is the user who is actually issuing the ds spark submit or is it a default cassandra user who is running all the spark processes so this particular log statement makes share or confirms our understanding that cassandra or the default spark user should be running this code and not automate an user who is actually issuing the command right so that part is confirmed now let's go back to the master right and let's look i'll look at the application that was spun off as part of this whole job right the job we submitted uh if you look at the two workers that were involved right so if we really go to the worker hd out we should be able to see those statements that were being printed out by the executor code so as expected again you see there are this temp hyphen execute a sample folder being created this folder is being again owned by the cassandra so once again as expected the cassandra user is creating or running that piece of code so that being said we confirmed a few things right we confirmed that doesn't matter who is issuing the dc spark submit command your code is going to be executed as a spark user if you had used slot users maybe you have got slot users as the owners but we are again not going to the discussion of slot users we are going to stick to the default cassandra user and how do i make sure just to make sure that uh whatever is getting printed is actually what's happening i'm going to issue again that a parallelism meant to look at the actual lsla output from the same folder and if you see we have the driver sample that is included here owned by cassandra the executive sample on because there are two executive samples so two partitions were executed here one partition was within this worker but essentially proves her point let's take a look at another use case for example you're using a correct workflow right so you're a low resource user you're using the lower resource but what if you're trying to bypass the system and pass extra number of course right so totally executed force is 10 although i'm allowed a max of three as per my configuration uh let's see okay so now all three cores are available so what happens if i execute something like this the driver i'm expecting will get run and submit it and accept it because of course it's calendar it will look like just one core what happens to the application that the driver spawns it has got two cores and it's running right let me try to run it again while the original one is also right again the driver is instead of running and if i go there you see it's running but no matter what we have passed as a parameter the total number of cores that are assigned to the application plus the private core always results in the maximum number of course that your lower resource workflow so even if you try to bypass the system it only in this demonstration we have used data stacks enterprise edition 677 please note that this is not a official data tax direction but a knowledge share that may go stale as a result of new releases make sure to check the current documentation of the person you're going to use thanks for your time and hopefully this content was helpful for more information you can always go to datastax.com and find excellent articles on different subject matters",
    "segments": [
      {
        "start": 2.32,
        "duration": 2.959,
        "text": "hello"
      },
      {
        "start": 3.12,
        "duration": 4.159,
        "text": "my name is sarah bispass i am a data"
      },
      {
        "start": 5.279,
        "duration": 3.681,
        "text": "architect with data stacks"
      },
      {
        "start": 7.279,
        "duration": 3.041,
        "text": "today we are going to do a short"
      },
      {
        "start": 8.96,
        "duration": 3.759,
        "text": "demonstration"
      },
      {
        "start": 10.32,
        "duration": 4.96,
        "text": "which will show you how to use workflows"
      },
      {
        "start": 12.719,
        "duration": 5.601,
        "text": "with es analytics"
      },
      {
        "start": 15.28,
        "duration": 5.2,
        "text": "at the same time we'll also"
      },
      {
        "start": 18.32,
        "duration": 4.4,
        "text": "make sure that we understand the"
      },
      {
        "start": 20.48,
        "duration": 4.559,
        "text": "ownership of the folders and files"
      },
      {
        "start": 22.72,
        "duration": 3.52,
        "text": "that are created when you run a spark"
      },
      {
        "start": 25.039,
        "duration": 5.201,
        "text": "application"
      },
      {
        "start": 26.24,
        "duration": 7.04,
        "text": "on these analytics"
      },
      {
        "start": 30.24,
        "duration": 5.12,
        "text": "in this video we'll cover a few things"
      },
      {
        "start": 33.28,
        "duration": 4.88,
        "text": "we'll give you an overview of how we"
      },
      {
        "start": 35.36,
        "duration": 5.12,
        "text": "have set up our cluster"
      },
      {
        "start": 38.16,
        "duration": 4.16,
        "text": "then we'll tell you how to set up"
      },
      {
        "start": 40.48,
        "duration": 3.759,
        "text": "workforce"
      },
      {
        "start": 42.32,
        "duration": 3.759,
        "text": "and show you how it can restrict"
      },
      {
        "start": 44.239,
        "duration": 3.681,
        "text": "permission to those work pools"
      },
      {
        "start": 46.079,
        "duration": 3.281,
        "text": "using dac authentication and"
      },
      {
        "start": 47.92,
        "duration": 3.52,
        "text": "authorization"
      },
      {
        "start": 49.36,
        "duration": 3.199,
        "text": "will give you an overview of the sample"
      },
      {
        "start": 51.44,
        "duration": 4.799,
        "text": "java code"
      },
      {
        "start": 52.559,
        "duration": 5.84,
        "text": "that will simulate folder creation"
      },
      {
        "start": 56.239,
        "duration": 3.84,
        "text": "will execute the code and we'll verify"
      },
      {
        "start": 58.399,
        "duration": 2.96,
        "text": "the ownership of the folders that were"
      },
      {
        "start": 60.079,
        "duration": 3.441,
        "text": "created by the driver"
      },
      {
        "start": 61.359,
        "duration": 3.681,
        "text": "as well as the executors while they run"
      },
      {
        "start": 63.52,
        "duration": 3.68,
        "text": "the application"
      },
      {
        "start": 65.04,
        "duration": 3.52,
        "text": "and finally we're going to demonstrate"
      },
      {
        "start": 67.2,
        "duration": 5.04,
        "text": "how workforce"
      },
      {
        "start": 68.56,
        "duration": 6.8,
        "text": "can restrict usage of spark resources"
      },
      {
        "start": 72.24,
        "duration": 4.16,
        "text": "even when your users might try to run"
      },
      {
        "start": 75.36,
        "duration": 3.84,
        "text": "their jobs"
      },
      {
        "start": 76.4,
        "duration": 4.32,
        "text": "with a higher resource then that's"
      },
      {
        "start": 79.2,
        "duration": 2.64,
        "text": "what's allocated for them in their"
      },
      {
        "start": 80.72,
        "duration": 4.12,
        "text": "workflow"
      },
      {
        "start": 81.84,
        "duration": 6.0,
        "text": "so let's go ahead and start a"
      },
      {
        "start": 84.84,
        "duration": 3.0,
        "text": "demonstration"
      },
      {
        "start": 88.08,
        "duration": 5.12,
        "text": "let's take a look at the cluster i have"
      },
      {
        "start": 90.72,
        "duration": 5.84,
        "text": "set up for this particular demo"
      },
      {
        "start": 93.2,
        "duration": 6.16,
        "text": "i've used ec2 instances it's all on"
      },
      {
        "start": 96.56,
        "duration": 3.919,
        "text": "amazon cloud there are three nodes and"
      },
      {
        "start": 99.36,
        "duration": 3.68,
        "text": "all the nodes have"
      },
      {
        "start": 100.479,
        "duration": 4.081,
        "text": "search as well as analytics workloads"
      },
      {
        "start": 103.04,
        "duration": 3.28,
        "text": "enabled on them"
      },
      {
        "start": 104.56,
        "duration": 4.32,
        "text": "i'm going to use the dc tool ring"
      },
      {
        "start": 106.32,
        "duration": 4.72,
        "text": "utility to take a look at the server"
      },
      {
        "start": 108.88,
        "duration": 3.44,
        "text": "health the cluster has as well as the"
      },
      {
        "start": 111.04,
        "duration": 4.48,
        "text": "workloads"
      },
      {
        "start": 112.32,
        "duration": 5.04,
        "text": "uh their topology their back and dc"
      },
      {
        "start": 115.52,
        "duration": 3.44,
        "text": "so you see there are like three nodes"
      },
      {
        "start": 117.36,
        "duration": 4.88,
        "text": "all on the same rack"
      },
      {
        "start": 118.96,
        "duration": 6.479,
        "text": "on the same pc all of them have searched"
      },
      {
        "start": 122.24,
        "duration": 4.08,
        "text": "analytics and i've used a very simple"
      },
      {
        "start": 125.439,
        "duration": 4.32,
        "text": "setup"
      },
      {
        "start": 126.32,
        "duration": 6.16,
        "text": "just to demonstrate a particular viscose"
      },
      {
        "start": 129.759,
        "duration": 5.601,
        "text": "that we're going to target"
      },
      {
        "start": 132.48,
        "duration": 3.92,
        "text": "so you can see like the spark master is"
      },
      {
        "start": 135.36,
        "duration": 6.0,
        "text": "running on the node"
      },
      {
        "start": 136.4,
        "duration": 8.64,
        "text": "with ip ending in 244 this is the"
      },
      {
        "start": 141.36,
        "duration": 6.48,
        "text": "private ip and the corresponding public"
      },
      {
        "start": 145.04,
        "duration": 6.64,
        "text": "ip is 102"
      },
      {
        "start": 147.84,
        "duration": 6.8,
        "text": "and on port 7080 of that particular"
      },
      {
        "start": 151.68,
        "duration": 3.919,
        "text": "ip we have the spark master ui running"
      },
      {
        "start": 154.64,
        "duration": 4.239,
        "text": "which gives you"
      },
      {
        "start": 155.599,
        "duration": 5.681,
        "text": "overview of the spark cluster"
      },
      {
        "start": 158.879,
        "duration": 5.681,
        "text": "in this cluster you will see as expected"
      },
      {
        "start": 161.28,
        "duration": 5.92,
        "text": "there are three workers running"
      },
      {
        "start": 164.56,
        "duration": 4.08,
        "text": "the total number of cores that my spark"
      },
      {
        "start": 167.2,
        "duration": 4.399,
        "text": "cluster has is 33"
      },
      {
        "start": 168.64,
        "duration": 4.879,
        "text": "and the total memory is close to 160"
      },
      {
        "start": 171.599,
        "duration": 5.681,
        "text": "gigabytes"
      },
      {
        "start": 173.519,
        "duration": 5.921,
        "text": "uh one interesting thing to note here is"
      },
      {
        "start": 177.28,
        "duration": 4.72,
        "text": "under the course of the memory"
      },
      {
        "start": 179.44,
        "duration": 4.96,
        "text": "essentially the resources of your spark"
      },
      {
        "start": 182.0,
        "duration": 4.159,
        "text": "cluster you will see there are three"
      },
      {
        "start": 184.4,
        "duration": 5.44,
        "text": "categories one is default"
      },
      {
        "start": 186.159,
        "duration": 7.601,
        "text": "one is high one is low high resources"
      },
      {
        "start": 189.84,
        "duration": 7.52,
        "text": "slow resources similarly for memory"
      },
      {
        "start": 193.76,
        "duration": 7.759,
        "text": "i will briefly explain where these"
      },
      {
        "start": 197.36,
        "duration": 6.56,
        "text": "particular subcategories are coming from"
      },
      {
        "start": 201.519,
        "duration": 5.521,
        "text": "these are essentially the work pools"
      },
      {
        "start": 203.92,
        "duration": 6.92,
        "text": "if you look at the jobs that we have run"
      },
      {
        "start": 207.04,
        "duration": 5.839,
        "text": "we assign a work pool to all our spark"
      },
      {
        "start": 210.84,
        "duration": 5.56,
        "text": "submissions when you"
      },
      {
        "start": 212.879,
        "duration": 7.121,
        "text": "submit a driver you"
      },
      {
        "start": 216.4,
        "duration": 6.96,
        "text": "mention a work pool and that work"
      },
      {
        "start": 220.0,
        "duration": 6.08,
        "text": "pool is also associated with the user"
      },
      {
        "start": 223.36,
        "duration": 4.64,
        "text": "the dac user who is submitting this"
      },
      {
        "start": 226.08,
        "duration": 6.159,
        "text": "particular job"
      },
      {
        "start": 228.0,
        "duration": 7.36,
        "text": "what that ensures is if i am a tenant"
      },
      {
        "start": 232.239,
        "duration": 3.92,
        "text": "who has certain resources allocated to"
      },
      {
        "start": 235.36,
        "duration": 3.439,
        "text": "me"
      },
      {
        "start": 236.159,
        "duration": 3.121,
        "text": "i can under no circumstances submit a"
      },
      {
        "start": 238.799,
        "duration": 3.601,
        "text": "job"
      },
      {
        "start": 239.28,
        "duration": 6.319,
        "text": "that consumes more resources so i can"
      },
      {
        "start": 242.4,
        "duration": 4.16,
        "text": "always go ahead and try to give a total"
      },
      {
        "start": 245.599,
        "duration": 4.481,
        "text": "number of course"
      },
      {
        "start": 246.56,
        "duration": 6.72,
        "text": "that's higher than my low resource"
      },
      {
        "start": 250.08,
        "duration": 6.64,
        "text": "allocation of three course right"
      },
      {
        "start": 253.28,
        "duration": 3.84,
        "text": "word pool and the whole setup of work"
      },
      {
        "start": 256.72,
        "duration": 2.799,
        "text": "pool"
      },
      {
        "start": 257.12,
        "duration": 3.04,
        "text": "ensures that you're never able to do"
      },
      {
        "start": 259.519,
        "duration": 3.041,
        "text": "that"
      },
      {
        "start": 260.16,
        "duration": 5.36,
        "text": "let me show you how you set it up and"
      },
      {
        "start": 262.56,
        "duration": 6.079,
        "text": "then while demonstrating"
      },
      {
        "start": 265.52,
        "duration": 4.16,
        "text": "uh the actual application or actual"
      },
      {
        "start": 268.639,
        "duration": 4.56,
        "text": "process"
      },
      {
        "start": 269.68,
        "duration": 6.239,
        "text": "uh we will show you how if you"
      },
      {
        "start": 273.199,
        "duration": 4.081,
        "text": "try to use something that you're not"
      },
      {
        "start": 275.919,
        "duration": 2.881,
        "text": "supposed to"
      },
      {
        "start": 277.28,
        "duration": 4.0,
        "text": "like a workflow that you're not supposed"
      },
      {
        "start": 278.8,
        "duration": 6.32,
        "text": "to uh you will"
      },
      {
        "start": 281.28,
        "duration": 3.84,
        "text": "not be allowed to submit your jobs"
      },
      {
        "start": 291.28,
        "duration": 3.919,
        "text": "going back to that particular node we"
      },
      {
        "start": 293.36,
        "duration": 6.24,
        "text": "were in let's take a look"
      },
      {
        "start": 295.199,
        "duration": 4.401,
        "text": "at the dsc.wire"
      },
      {
        "start": 300.24,
        "duration": 4.64,
        "text": "i have package installation so my"
      },
      {
        "start": 302.639,
        "duration": 4.161,
        "text": "default location is at cdhcp it's like"
      },
      {
        "start": 304.88,
        "duration": 2.879,
        "text": "ml for turbo you can go to that"
      },
      {
        "start": 306.8,
        "duration": 4.08,
        "text": "particular"
      },
      {
        "start": 307.759,
        "duration": 6.561,
        "text": "location of configurations"
      },
      {
        "start": 310.88,
        "duration": 3.44,
        "text": "now out here"
      },
      {
        "start": 314.8,
        "duration": 4.56,
        "text": "search for work pool you will see its"
      },
      {
        "start": 317.44,
        "duration": 2.64,
        "text": "sub category under resource manager"
      },
      {
        "start": 319.36,
        "duration": 2.399,
        "text": "options"
      },
      {
        "start": 320.08,
        "duration": 3.04,
        "text": "this is major options essentially"
      },
      {
        "start": 321.759,
        "duration": 3.361,
        "text": "controls"
      },
      {
        "start": 323.12,
        "duration": 3.28,
        "text": "how much resources are related to your"
      },
      {
        "start": 325.12,
        "duration": 4.0,
        "text": "workers"
      },
      {
        "start": 326.4,
        "duration": 4.72,
        "text": "and out here what i've defined is a"
      },
      {
        "start": 329.12,
        "duration": 3.6,
        "text": "higher resource and the low resource"
      },
      {
        "start": 331.12,
        "duration": 3.84,
        "text": "now if you remember you don't see always"
      },
      {
        "start": 332.72,
        "duration": 4.96,
        "text": "on sql in the spark master"
      },
      {
        "start": 334.96,
        "duration": 4.48,
        "text": "ui right because this is reserved for"
      },
      {
        "start": 337.68,
        "duration": 3.519,
        "text": "you always on server"
      },
      {
        "start": 339.44,
        "duration": 4.72,
        "text": "if you're not using that feature you can"
      },
      {
        "start": 341.199,
        "duration": 5.84,
        "text": "always go ahead and remove"
      },
      {
        "start": 344.16,
        "duration": 4.72,
        "text": "some of some course memories from here"
      },
      {
        "start": 347.039,
        "duration": 4.081,
        "text": "the high and the low resources are what"
      },
      {
        "start": 348.88,
        "duration": 4.0,
        "text": "we have defined and it can be"
      },
      {
        "start": 351.12,
        "duration": 3.359,
        "text": "number of workbooks you can have"
      },
      {
        "start": 352.88,
        "duration": 4.64,
        "text": "multiple workflows"
      },
      {
        "start": 354.479,
        "duration": 5.681,
        "text": "whatever is left from these so"
      },
      {
        "start": 357.52,
        "duration": 5.04,
        "text": "uh one minus the sum of code which"
      },
      {
        "start": 360.16,
        "duration": 5.52,
        "text": "workflow goes to the default workflow"
      },
      {
        "start": 362.56,
        "duration": 3.68,
        "text": "so the default workflow is used when no"
      },
      {
        "start": 365.68,
        "duration": 3.359,
        "text": "other"
      },
      {
        "start": 366.24,
        "duration": 3.2,
        "text": "pools are being used and you can"
      },
      {
        "start": 369.039,
        "duration": 4.16,
        "text": "actually"
      },
      {
        "start": 369.44,
        "duration": 5.92,
        "text": "ensure that your resources or your users"
      },
      {
        "start": 373.199,
        "duration": 4.401,
        "text": "can only use a particular workflow"
      },
      {
        "start": 375.36,
        "duration": 3.119,
        "text": "and we'll see how to grant those"
      },
      {
        "start": 377.6,
        "duration": 3.12,
        "text": "permissions"
      },
      {
        "start": 378.479,
        "duration": 4.72,
        "text": "documented but let me show you how it"
      },
      {
        "start": 380.72,
        "duration": 2.479,
        "text": "looks like"
      },
      {
        "start": 383.44,
        "duration": 4.96,
        "text": "so let me go to the simple shadow"
      },
      {
        "start": 392.16,
        "duration": 6.08,
        "text": "i'm going to do the worst practice"
      },
      {
        "start": 395.28,
        "duration": 4.96,
        "text": "of typing in the password on the screen"
      },
      {
        "start": 398.24,
        "duration": 5.84,
        "text": "but since it's just a demo environment"
      },
      {
        "start": 400.24,
        "duration": 3.84,
        "text": "i'm doing it so"
      },
      {
        "start": 404.639,
        "duration": 3.68,
        "text": "let me take a look at a user i have"
      },
      {
        "start": 407.68,
        "duration": 2.959,
        "text": "created"
      },
      {
        "start": 408.319,
        "duration": 2.32,
        "text": "right"
      },
      {
        "start": 411.759,
        "duration": 7.84,
        "text": "so list of permissions"
      },
      {
        "start": 417.28,
        "duration": 2.319,
        "text": "of"
      },
      {
        "start": 420.319,
        "duration": 4.72,
        "text": "right so this is a role i had created"
      },
      {
        "start": 422.8,
        "duration": 4.239,
        "text": "and if you look at it"
      },
      {
        "start": 425.039,
        "duration": 4.321,
        "text": "you'll see it has different permissions"
      },
      {
        "start": 427.039,
        "duration": 5.28,
        "text": "and stuff but the most important"
      },
      {
        "start": 429.36,
        "duration": 4.72,
        "text": "aspect of it is the first row in the"
      },
      {
        "start": 432.319,
        "duration": 4.801,
        "text": "rows where you see"
      },
      {
        "start": 434.08,
        "duration": 5.76,
        "text": "it has create permission"
      },
      {
        "start": 437.12,
        "duration": 4.72,
        "text": "on the low resources for pool in"
      },
      {
        "start": 439.84,
        "duration": 5.359,
        "text": "enterprise data center"
      },
      {
        "start": 441.84,
        "duration": 6.4,
        "text": "so as long as he's"
      },
      {
        "start": 445.199,
        "duration": 4.321,
        "text": "they are using the low resource as the"
      },
      {
        "start": 448.24,
        "duration": 4.799,
        "text": "workforce"
      },
      {
        "start": 449.52,
        "duration": 7.36,
        "text": "they can create submissions or they can"
      },
      {
        "start": 453.039,
        "duration": 5.121,
        "text": "successfully create applications and"
      },
      {
        "start": 456.88,
        "duration": 3.2,
        "text": "drivers"
      },
      {
        "start": 458.16,
        "duration": 3.28,
        "text": "in your spark cluster if they do not"
      },
      {
        "start": 460.08,
        "duration": 4.559,
        "text": "have access"
      },
      {
        "start": 461.44,
        "duration": 6.72,
        "text": "they want to use a workflow three"
      },
      {
        "start": 464.639,
        "duration": 4.321,
        "text": "supposedly they wouldn't be allowed to"
      },
      {
        "start": 468.16,
        "duration": 2.479,
        "text": "do it they'll"
      },
      {
        "start": 468.96,
        "duration": 3.359,
        "text": "encounter the error when they try to do"
      },
      {
        "start": 470.639,
        "duration": 5.201,
        "text": "it so this is a very good"
      },
      {
        "start": 472.319,
        "duration": 5.041,
        "text": "way of tenant resource control if a"
      },
      {
        "start": 475.84,
        "duration": 5.12,
        "text": "multi-tenant system"
      },
      {
        "start": 477.36,
        "duration": 3.6,
        "text": "as operations team you can"
      },
      {
        "start": 481.599,
        "duration": 4.72,
        "text": "address the concern of over utilization"
      },
      {
        "start": 484.4,
        "duration": 3.199,
        "text": "of resources by a particular team you"
      },
      {
        "start": 486.319,
        "duration": 4.241,
        "text": "can totally restrict them"
      },
      {
        "start": 487.599,
        "duration": 5.6,
        "text": "to some resources uh very useful tool"
      },
      {
        "start": 490.56,
        "duration": 5.6,
        "text": "that's why we wanted to"
      },
      {
        "start": 493.199,
        "duration": 4.161,
        "text": "demonstrate this feature also as part of"
      },
      {
        "start": 496.16,
        "duration": 4.56,
        "text": "it"
      },
      {
        "start": 497.36,
        "duration": 6.959,
        "text": "next i wanted to show you uh what"
      },
      {
        "start": 500.72,
        "duration": 6.8,
        "text": "my sample spark application looks like"
      },
      {
        "start": 504.319,
        "duration": 4.481,
        "text": "i've created a very simple class all it"
      },
      {
        "start": 507.52,
        "duration": 4.72,
        "text": "does"
      },
      {
        "start": 508.8,
        "duration": 3.44,
        "text": "is create"
      },
      {
        "start": 512.479,
        "duration": 7.281,
        "text": "a folder from the driver and then create"
      },
      {
        "start": 516.159,
        "duration": 6.32,
        "text": "folders from each of the executors"
      },
      {
        "start": 519.76,
        "duration": 2.719,
        "text": "or workers"
      },
      {
        "start": 523.36,
        "duration": 4.72,
        "text": "so how have i achieved it right so if"
      },
      {
        "start": 526.24,
        "duration": 5.12,
        "text": "you see this is the main class"
      },
      {
        "start": 528.08,
        "duration": 5.92,
        "text": "that's going to be provided as"
      },
      {
        "start": 531.36,
        "duration": 5.2,
        "text": "the main class for the spark submit job"
      },
      {
        "start": 534.0,
        "duration": 4.48,
        "text": "here i'm creating a simple directory"
      },
      {
        "start": 536.56,
        "duration": 4.24,
        "text": "and i'm appending the directory name"
      },
      {
        "start": 538.48,
        "duration": 3.6,
        "text": "with driver underscore sample everything"
      },
      {
        "start": 540.8,
        "duration": 2.08,
        "text": "is going to be created in the temp"
      },
      {
        "start": 542.08,
        "duration": 3.92,
        "text": "folder"
      },
      {
        "start": 542.88,
        "duration": 5.12,
        "text": "i'm using the local file system but the"
      },
      {
        "start": 546.0,
        "duration": 5.76,
        "text": "behavior would be same when you're using"
      },
      {
        "start": 548.0,
        "duration": 8.08,
        "text": "the nfs file system"
      },
      {
        "start": 551.76,
        "duration": 4.8,
        "text": "like you do the important thing to note"
      },
      {
        "start": 556.08,
        "duration": 3.68,
        "text": "here"
      },
      {
        "start": 556.56,
        "duration": 4.64,
        "text": "is i want to figure out which operating"
      },
      {
        "start": 559.76,
        "duration": 3.759,
        "text": "system user"
      },
      {
        "start": 561.2,
        "duration": 4.4,
        "text": "is becoming the owner of that particular"
      },
      {
        "start": 563.519,
        "duration": 5.361,
        "text": "file right so"
      },
      {
        "start": 565.6,
        "duration": 6.56,
        "text": "suppose i launch my spark"
      },
      {
        "start": 568.88,
        "duration": 6.639,
        "text": "job or issue my spark submit"
      },
      {
        "start": 572.16,
        "duration": 6.72,
        "text": "as operation system"
      },
      {
        "start": 575.519,
        "duration": 6.561,
        "text": "user one do"
      },
      {
        "start": 578.88,
        "duration": 6.72,
        "text": "these folders get created as os1"
      },
      {
        "start": 582.08,
        "duration": 5.199,
        "text": "as the owner or are they created under"
      },
      {
        "start": 585.6,
        "duration": 4.239,
        "text": "the os user that runs"
      },
      {
        "start": 587.279,
        "duration": 3.841,
        "text": "the spark process which by default is"
      },
      {
        "start": 589.839,
        "duration": 4.801,
        "text": "the dac runner"
      },
      {
        "start": 591.12,
        "duration": 3.52,
        "text": "process which is"
      },
      {
        "start": 594.8,
        "duration": 5.039,
        "text": "uh cassandra user"
      },
      {
        "start": 598.32,
        "duration": 3.92,
        "text": "of course there's this whole concept of"
      },
      {
        "start": 599.839,
        "duration": 4.401,
        "text": "slot users and we're not going to"
      },
      {
        "start": 602.24,
        "duration": 3.36,
        "text": "discuss slot users we are just assuming"
      },
      {
        "start": 604.24,
        "duration": 3.279,
        "text": "we are using the default"
      },
      {
        "start": 605.6,
        "duration": 4.0,
        "text": "cassandra user for running all spark"
      },
      {
        "start": 607.519,
        "duration": 5.201,
        "text": "applications so that is"
      },
      {
        "start": 609.6,
        "duration": 4.56,
        "text": "one aspect of it our understanding our"
      },
      {
        "start": 612.72,
        "duration": 3.2,
        "text": "expectation is the owner"
      },
      {
        "start": 614.16,
        "duration": 3.52,
        "text": "after the full discretion should be"
      },
      {
        "start": 615.92,
        "duration": 4.88,
        "text": "cassandra user"
      },
      {
        "start": 617.68,
        "duration": 6.159,
        "text": "similarly to make sure that"
      },
      {
        "start": 620.8,
        "duration": 6.32,
        "text": "the workers are also writing"
      },
      {
        "start": 623.839,
        "duration": 7.041,
        "text": "some the executors the workers writing"
      },
      {
        "start": 627.12,
        "duration": 7.04,
        "text": "or creating folders as part of"
      },
      {
        "start": 630.88,
        "duration": 3.92,
        "text": "the tasks right i have created a sample"
      },
      {
        "start": 634.16,
        "duration": 2.4,
        "text": "code here"
      },
      {
        "start": 634.8,
        "duration": 3.84,
        "text": "where it's doing the same thing but the"
      },
      {
        "start": 636.56,
        "duration": 3.12,
        "text": "folder is appended with executable"
      },
      {
        "start": 638.64,
        "duration": 2.72,
        "text": "sample"
      },
      {
        "start": 639.68,
        "duration": 3.44,
        "text": "and even at the end of it again i'm"
      },
      {
        "start": 641.36,
        "duration": 2.88,
        "text": "going to print the owner as well as"
      },
      {
        "start": 643.12,
        "duration": 4.159,
        "text": "going to check"
      },
      {
        "start": 644.24,
        "duration": 5.68,
        "text": "the folders using a list command"
      },
      {
        "start": 647.279,
        "duration": 3.361,
        "text": "and make sure that our expectation"
      },
      {
        "start": 649.92,
        "duration": 3.12,
        "text": "matches"
      },
      {
        "start": 650.64,
        "duration": 3.92,
        "text": "with what is being done what is been"
      },
      {
        "start": 653.04,
        "duration": 5.52,
        "text": "actually produced"
      },
      {
        "start": 654.56,
        "duration": 4.0,
        "text": "on the servers"
      },
      {
        "start": 658.8,
        "duration": 4.479,
        "text": "and that expectation being both of them"
      },
      {
        "start": 660.88,
        "duration": 4.399,
        "text": "should be owned by the cassandra"
      },
      {
        "start": 663.279,
        "duration": 3.061,
        "text": "all right now i'm back to the node and"
      },
      {
        "start": 665.279,
        "duration": 2.56,
        "text": "i'm going to try to"
      },
      {
        "start": 666.34,
        "duration": 4.54,
        "text": "[Music]"
      },
      {
        "start": 667.839,
        "duration": 6.801,
        "text": "do a spark segment"
      },
      {
        "start": 670.88,
        "duration": 8.24,
        "text": "and see what happens"
      },
      {
        "start": 674.64,
        "duration": 7.84,
        "text": "uh before i start let's do"
      },
      {
        "start": 679.12,
        "duration": 3.36,
        "text": "one more task right"
      },
      {
        "start": 683.279,
        "duration": 8.321,
        "text": "let's make sure just on one node that"
      },
      {
        "start": 688.959,
        "duration": 2.641,
        "text": "have no"
      },
      {
        "start": 693.04,
        "duration": 6.08,
        "text": "driver underscore sam hello execute"
      },
      {
        "start": 696.88,
        "duration": 4.24,
        "text": "underscore sample folder"
      },
      {
        "start": 699.12,
        "duration": 3.36,
        "text": "i made sure that i have removed it from"
      },
      {
        "start": 701.12,
        "duration": 5.2,
        "text": "all the notes"
      },
      {
        "start": 702.48,
        "duration": 7.28,
        "text": "just for a fresh demo but as you can see"
      },
      {
        "start": 706.32,
        "duration": 4.079,
        "text": "it's not there right so it's not going"
      },
      {
        "start": 709.76,
        "duration": 3.36,
        "text": "to be"
      },
      {
        "start": 710.399,
        "duration": 3.44,
        "text": "an issue um i can also do this right"
      },
      {
        "start": 713.12,
        "duration": 5.12,
        "text": "let's"
      },
      {
        "start": 713.839,
        "duration": 7.201,
        "text": "do this thing what i'm gonna do is"
      },
      {
        "start": 718.24,
        "duration": 4.0,
        "text": "use a utility to do a ls on all the"
      },
      {
        "start": 721.04,
        "duration": 4.479,
        "text": "nodes on my cluster"
      },
      {
        "start": 722.24,
        "duration": 5.52,
        "text": "so it's like a parallel ssh tool and i'm"
      },
      {
        "start": 725.519,
        "duration": 6.161,
        "text": "going to use it to make sure"
      },
      {
        "start": 727.76,
        "duration": 7.12,
        "text": "that the three notes that i am on"
      },
      {
        "start": 731.68,
        "duration": 4.56,
        "text": "right doesn't have any folder that's"
      },
      {
        "start": 734.88,
        "duration": 3.199,
        "text": "fascinating driver on this was"
      },
      {
        "start": 736.24,
        "duration": 3.36,
        "text": "not executed on this example just making"
      },
      {
        "start": 738.079,
        "duration": 3.76,
        "text": "sure it's clean"
      },
      {
        "start": 739.6,
        "duration": 3.359,
        "text": "uh so that we have fresh test results"
      },
      {
        "start": 741.839,
        "duration": 4.961,
        "text": "when you write"
      },
      {
        "start": 742.959,
        "duration": 7.921,
        "text": "uh now let's try to run a spark submit"
      },
      {
        "start": 746.8,
        "duration": 5.76,
        "text": "i have used dacfs to put the jar file"
      },
      {
        "start": 750.88,
        "duration": 3.12,
        "text": "that i've compiled out of the class that"
      },
      {
        "start": 752.56,
        "duration": 4.8,
        "text": "i showed you earlier"
      },
      {
        "start": 754.0,
        "duration": 6.16,
        "text": "and the reason of for using dsfs is"
      },
      {
        "start": 757.36,
        "duration": 4.08,
        "text": "it's always the best practice to have a"
      },
      {
        "start": 760.16,
        "duration": 3.52,
        "text": "shared location where"
      },
      {
        "start": 761.44,
        "duration": 3.839,
        "text": "you put your jar files or executables it"
      },
      {
        "start": 763.68,
        "duration": 3.68,
        "text": "can be nfs mount too"
      },
      {
        "start": 765.279,
        "duration": 3.36,
        "text": "and also use a local but just have to"
      },
      {
        "start": 767.36,
        "duration": 5.279,
        "text": "make sure that"
      },
      {
        "start": 768.639,
        "duration": 8.161,
        "text": "your jar is present and all the"
      },
      {
        "start": 772.639,
        "duration": 7.921,
        "text": "local nodes"
      },
      {
        "start": 776.8,
        "duration": 4.32,
        "text": "and it's not always pretty when you do"
      },
      {
        "start": 780.56,
        "duration": 2.079,
        "text": "that"
      },
      {
        "start": 781.12,
        "duration": 3.279,
        "text": "because you can run into version"
      },
      {
        "start": 782.639,
        "duration": 4.081,
        "text": "mismatch stuff like that"
      },
      {
        "start": 784.399,
        "duration": 4.321,
        "text": "so let's assume that you'll always be"
      },
      {
        "start": 786.72,
        "duration": 3.44,
        "text": "using a centralized location which is"
      },
      {
        "start": 788.72,
        "duration": 2.4,
        "text": "accessible from all the nodes in our"
      },
      {
        "start": 790.16,
        "duration": 4.0,
        "text": "case it's"
      },
      {
        "start": 791.12,
        "duration": 6.959,
        "text": "uh dhcfs so"
      },
      {
        "start": 794.16,
        "duration": 3.919,
        "text": "let's go into"
      },
      {
        "start": 798.88,
        "duration": 3.68,
        "text": "the dacfa system and show you where i'm"
      },
      {
        "start": 801.68,
        "duration": 13.12,
        "text": "putting my"
      },
      {
        "start": 802.56,
        "duration": 15.04,
        "text": "chat or where i've already put major um"
      },
      {
        "start": 814.8,
        "duration": 4.8,
        "text": "execute a ls command on the acfs and if"
      },
      {
        "start": 817.6,
        "duration": 6.0,
        "text": "you see there is a jar called write"
      },
      {
        "start": 819.6,
        "duration": 5.84,
        "text": "301 all right i'm going to use it in my"
      },
      {
        "start": 823.6,
        "duration": 4.96,
        "text": "spark submit"
      },
      {
        "start": 825.44,
        "duration": 6.88,
        "text": "so i'm going to go back and use"
      },
      {
        "start": 828.56,
        "duration": 3.76,
        "text": "one of the spark submit"
      },
      {
        "start": 834.32,
        "duration": 4.879,
        "text": "commands that we already used in the"
      },
      {
        "start": 835.92,
        "duration": 5.52,
        "text": "past so if we really look at it right"
      },
      {
        "start": 839.199,
        "duration": 3.681,
        "text": "what all parameters i'm passing through"
      },
      {
        "start": 841.44,
        "duration": 5.519,
        "text": "it i'm passing the"
      },
      {
        "start": 842.88,
        "duration": 7.36,
        "text": "name which is in dlcfs right"
      },
      {
        "start": 846.959,
        "duration": 4.961,
        "text": "so this is the journal name then i'm"
      },
      {
        "start": 850.24,
        "duration": 2.48,
        "text": "passing the main class which i showed"
      },
      {
        "start": 851.92,
        "duration": 6.0,
        "text": "you"
      },
      {
        "start": 852.72,
        "duration": 7.2,
        "text": "right uh i'm passing the master url"
      },
      {
        "start": 857.92,
        "duration": 4.0,
        "text": "and the reason for passing the mass url"
      },
      {
        "start": 859.92,
        "duration": 4.24,
        "text": "is this is how i can"
      },
      {
        "start": 861.92,
        "duration": 4.56,
        "text": "make sure i mentioned the workflows i'm"
      },
      {
        "start": 864.16,
        "duration": 5.84,
        "text": "mentioning low resource workflow"
      },
      {
        "start": 866.48,
        "duration": 6.719,
        "text": "for my master url indicates"
      },
      {
        "start": 870.0,
        "duration": 5.12,
        "text": "this spark submit should submit the job"
      },
      {
        "start": 873.199,
        "duration": 3.041,
        "text": "using low resource workflow then you"
      },
      {
        "start": 875.12,
        "duration": 3.36,
        "text": "have the"
      },
      {
        "start": 876.24,
        "duration": 3.279,
        "text": "diplomat cluster obviously we always use"
      },
      {
        "start": 878.48,
        "duration": 4.24,
        "text": "the cluster mode"
      },
      {
        "start": 879.519,
        "duration": 6.721,
        "text": "for uh all"
      },
      {
        "start": 882.72,
        "duration": 4.64,
        "text": "production deployments um and i have to"
      },
      {
        "start": 886.24,
        "duration": 4.399,
        "text": "pass"
      },
      {
        "start": 887.36,
        "duration": 6.0,
        "text": "a ds user uh"
      },
      {
        "start": 890.639,
        "duration": 3.44,
        "text": "who will help with submission of the job"
      },
      {
        "start": 893.36,
        "duration": 3.839,
        "text": "right so"
      },
      {
        "start": 894.079,
        "duration": 4.641,
        "text": "inside the code the ds user has to have"
      },
      {
        "start": 897.199,
        "duration": 4.481,
        "text": "access to your"
      },
      {
        "start": 898.72,
        "duration": 4.799,
        "text": "key spaces and your data but this"
      },
      {
        "start": 901.68,
        "duration": 4.64,
        "text": "particular user you're going to pass"
      },
      {
        "start": 903.519,
        "duration": 4.88,
        "text": "should have the bare minimum permissions"
      },
      {
        "start": 906.32,
        "duration": 3.84,
        "text": "which are listed up there if you see"
      },
      {
        "start": 908.399,
        "duration": 3.041,
        "text": "to be able to submit a job to a"
      },
      {
        "start": 910.16,
        "duration": 4.72,
        "text": "particular workflow"
      },
      {
        "start": 911.44,
        "duration": 16.399,
        "text": "right so let's try to run it and see"
      },
      {
        "start": 914.88,
        "duration": 12.959,
        "text": "what happens"
      },
      {
        "start": 929.199,
        "duration": 4.241,
        "text": "all right so you see that the driver has"
      },
      {
        "start": 932.16,
        "duration": 2.799,
        "text": "been created"
      },
      {
        "start": 933.44,
        "duration": 3.6,
        "text": "uh this is the work on which the driver"
      },
      {
        "start": 934.959,
        "duration": 3.201,
        "text": "is running and this is the id of the"
      },
      {
        "start": 937.04,
        "duration": 4.719,
        "text": "driver"
      },
      {
        "start": 938.16,
        "duration": 5.52,
        "text": "uh let's go to our spot master and see"
      },
      {
        "start": 941.759,
        "duration": 3.841,
        "text": "what it looks like so you see that"
      },
      {
        "start": 943.68,
        "duration": 5.279,
        "text": "driver ending in zero zero 11"
      },
      {
        "start": 945.6,
        "duration": 7.599,
        "text": "is running the application id is running"
      },
      {
        "start": 948.959,
        "duration": 4.24,
        "text": "let's go into this driver"
      },
      {
        "start": 953.279,
        "duration": 3.201,
        "text": "worker right and see the driver is still"
      },
      {
        "start": 956.0,
        "duration": 2.16,
        "text": "running"
      },
      {
        "start": 956.48,
        "duration": 4.719,
        "text": "and if you remember i was printing the"
      },
      {
        "start": 958.16,
        "duration": 5.84,
        "text": "folder and permissions"
      },
      {
        "start": 961.199,
        "duration": 4.961,
        "text": "right so here we are we see it says that"
      },
      {
        "start": 964.0,
        "duration": 5.92,
        "text": "we have created this folder"
      },
      {
        "start": 966.16,
        "duration": 7.2,
        "text": "on this particular worker node"
      },
      {
        "start": 969.92,
        "duration": 5.12,
        "text": "and then the owner is cassandra another"
      },
      {
        "start": 973.36,
        "duration": 3.599,
        "text": "important part"
      },
      {
        "start": 975.04,
        "duration": 3.919,
        "text": "i want to show you when i go back to the"
      },
      {
        "start": 976.959,
        "duration": 6.161,
        "text": "code is at the very beginning"
      },
      {
        "start": 978.959,
        "duration": 7.521,
        "text": "i'm printing the operating system user"
      },
      {
        "start": 983.12,
        "duration": 6.639,
        "text": "who is executing this piece of code"
      },
      {
        "start": 986.48,
        "duration": 5.84,
        "text": "and that is to figure out how"
      },
      {
        "start": 989.759,
        "duration": 4.64,
        "text": "or who owns the process that is being"
      },
      {
        "start": 992.32,
        "duration": 5.12,
        "text": "spun off by the dc's person"
      },
      {
        "start": 994.399,
        "duration": 5.12,
        "text": "is the user who is actually issuing the"
      },
      {
        "start": 997.44,
        "duration": 4.319,
        "text": "ds spark submit or is it"
      },
      {
        "start": 999.519,
        "duration": 3.76,
        "text": "a default cassandra user who is running"
      },
      {
        "start": 1001.759,
        "duration": 5.121,
        "text": "all the spark processes"
      },
      {
        "start": 1003.279,
        "duration": 6.881,
        "text": "so this particular log statement makes"
      },
      {
        "start": 1006.88,
        "duration": 4.24,
        "text": "share or confirms our understanding that"
      },
      {
        "start": 1010.16,
        "duration": 3.84,
        "text": "cassandra"
      },
      {
        "start": 1011.12,
        "duration": 5.839,
        "text": "or the default spark user should be"
      },
      {
        "start": 1014.0,
        "duration": 5.6,
        "text": "running this code and not"
      },
      {
        "start": 1016.959,
        "duration": 3.761,
        "text": "automate an user who is actually issuing"
      },
      {
        "start": 1019.6,
        "duration": 4.159,
        "text": "the command"
      },
      {
        "start": 1020.72,
        "duration": 5.04,
        "text": "right so"
      },
      {
        "start": 1023.759,
        "duration": 3.601,
        "text": "that part is confirmed now let's go back"
      },
      {
        "start": 1025.76,
        "duration": 3.199,
        "text": "to the master right"
      },
      {
        "start": 1027.36,
        "duration": 4.8,
        "text": "and let's look i'll look at the"
      },
      {
        "start": 1028.959,
        "duration": 6.401,
        "text": "application that was spun off as part of"
      },
      {
        "start": 1032.16,
        "duration": 6.32,
        "text": "this whole"
      },
      {
        "start": 1035.36,
        "duration": 4.0,
        "text": "job right the job we submitted uh if you"
      },
      {
        "start": 1038.48,
        "duration": 3.599,
        "text": "look at"
      },
      {
        "start": 1039.36,
        "duration": 3.28,
        "text": "the two workers that were involved right"
      },
      {
        "start": 1042.079,
        "duration": 2.801,
        "text": "so"
      },
      {
        "start": 1042.64,
        "duration": 3.36,
        "text": "if we really go to the worker hd out we"
      },
      {
        "start": 1044.88,
        "duration": 3.76,
        "text": "should be able to see"
      },
      {
        "start": 1046.0,
        "duration": 3.44,
        "text": "those statements that were being printed"
      },
      {
        "start": 1048.64,
        "duration": 4.159,
        "text": "out"
      },
      {
        "start": 1049.44,
        "duration": 6.0,
        "text": "by the executor code"
      },
      {
        "start": 1052.799,
        "duration": 4.24,
        "text": "so as expected again you see there are"
      },
      {
        "start": 1055.44,
        "duration": 6.8,
        "text": "this"
      },
      {
        "start": 1057.039,
        "duration": 5.201,
        "text": "temp hyphen"
      },
      {
        "start": 1062.64,
        "duration": 7.039,
        "text": "execute a sample folder being created"
      },
      {
        "start": 1066.72,
        "duration": 4.319,
        "text": "this folder is being again owned by the"
      },
      {
        "start": 1069.679,
        "duration": 4.401,
        "text": "cassandra"
      },
      {
        "start": 1071.039,
        "duration": 3.601,
        "text": "so once again as expected the cassandra"
      },
      {
        "start": 1074.08,
        "duration": 4.76,
        "text": "user"
      },
      {
        "start": 1074.64,
        "duration": 5.6,
        "text": "is creating or running that piece of"
      },
      {
        "start": 1078.84,
        "duration": 6.92,
        "text": "code"
      },
      {
        "start": 1080.24,
        "duration": 8.559,
        "text": "so that being said we confirmed"
      },
      {
        "start": 1085.76,
        "duration": 5.6,
        "text": "a few things right we confirmed that"
      },
      {
        "start": 1088.799,
        "duration": 4.481,
        "text": "doesn't matter who is issuing the dc"
      },
      {
        "start": 1091.36,
        "duration": 5.52,
        "text": "spark submit command"
      },
      {
        "start": 1093.28,
        "duration": 4.96,
        "text": "your code is going to be executed as a"
      },
      {
        "start": 1096.88,
        "duration": 4.159,
        "text": "spark user"
      },
      {
        "start": 1098.24,
        "duration": 4.16,
        "text": "if you had used slot users maybe you"
      },
      {
        "start": 1101.039,
        "duration": 3.121,
        "text": "have got"
      },
      {
        "start": 1102.4,
        "duration": 3.519,
        "text": "slot users as the owners but we are"
      },
      {
        "start": 1104.16,
        "duration": 3.6,
        "text": "again not going to the discussion of"
      },
      {
        "start": 1105.919,
        "duration": 4.241,
        "text": "slot users we are going to stick to the"
      },
      {
        "start": 1107.76,
        "duration": 5.2,
        "text": "default cassandra user"
      },
      {
        "start": 1110.16,
        "duration": 3.759,
        "text": "and how do i make sure just to make sure"
      },
      {
        "start": 1112.96,
        "duration": 3.44,
        "text": "that"
      },
      {
        "start": 1113.919,
        "duration": 3.601,
        "text": "uh whatever is getting printed is"
      },
      {
        "start": 1116.4,
        "duration": 2.88,
        "text": "actually what's happening"
      },
      {
        "start": 1117.52,
        "duration": 3.12,
        "text": "i'm going to issue again that a"
      },
      {
        "start": 1119.28,
        "duration": 4.72,
        "text": "parallelism meant"
      },
      {
        "start": 1120.64,
        "duration": 5.76,
        "text": "to look at the actual lsla"
      },
      {
        "start": 1124.0,
        "duration": 3.84,
        "text": "output from the same folder and if you"
      },
      {
        "start": 1126.4,
        "duration": 3.76,
        "text": "see"
      },
      {
        "start": 1127.84,
        "duration": 5.199,
        "text": "we have the driver sample that is"
      },
      {
        "start": 1130.16,
        "duration": 5.759,
        "text": "included here owned by cassandra"
      },
      {
        "start": 1133.039,
        "duration": 5.281,
        "text": "the executive sample on because there"
      },
      {
        "start": 1135.919,
        "duration": 4.401,
        "text": "are two executive samples"
      },
      {
        "start": 1138.32,
        "duration": 4.08,
        "text": "so two partitions were executed here one"
      },
      {
        "start": 1140.32,
        "duration": 6.4,
        "text": "partition was within this worker"
      },
      {
        "start": 1142.4,
        "duration": 4.32,
        "text": "but essentially proves her point"
      },
      {
        "start": 1148.24,
        "duration": 4.24,
        "text": "let's take a look at another use case"
      },
      {
        "start": 1150.32,
        "duration": 3.84,
        "text": "for example you're using a correct"
      },
      {
        "start": 1152.48,
        "duration": 3.439,
        "text": "workflow"
      },
      {
        "start": 1154.16,
        "duration": 3.36,
        "text": "right so you're a low resource user"
      },
      {
        "start": 1155.919,
        "duration": 2.961,
        "text": "you're using the lower resource but what"
      },
      {
        "start": 1157.52,
        "duration": 3.84,
        "text": "if you're trying"
      },
      {
        "start": 1158.88,
        "duration": 4.08,
        "text": "to bypass the system and pass extra"
      },
      {
        "start": 1161.36,
        "duration": 2.72,
        "text": "number of course right so totally"
      },
      {
        "start": 1162.96,
        "duration": 4.0,
        "text": "executed force is"
      },
      {
        "start": 1164.08,
        "duration": 3.599,
        "text": "10 although i'm allowed a max of three"
      },
      {
        "start": 1166.96,
        "duration": 4.24,
        "text": "as per"
      },
      {
        "start": 1167.679,
        "duration": 7.281,
        "text": "my configuration uh"
      },
      {
        "start": 1171.2,
        "duration": 7.2,
        "text": "let's see okay so now all three cores"
      },
      {
        "start": 1174.96,
        "duration": 4.079,
        "text": "are available so what happens if i"
      },
      {
        "start": 1178.4,
        "duration": 3.519,
        "text": "execute"
      },
      {
        "start": 1179.039,
        "duration": 2.88,
        "text": "something like this"
      },
      {
        "start": 1186.08,
        "duration": 6.64,
        "text": "the driver i'm expecting will get"
      },
      {
        "start": 1189.679,
        "duration": 5.681,
        "text": "run and submit it and accept it because"
      },
      {
        "start": 1192.72,
        "duration": 5.52,
        "text": "of course it's calendar it will look"
      },
      {
        "start": 1195.36,
        "duration": 2.88,
        "text": "like just one core"
      },
      {
        "start": 1198.32,
        "duration": 5.359,
        "text": "what happens to the application that the"
      },
      {
        "start": 1200.799,
        "duration": 2.88,
        "text": "driver spawns"
      },
      {
        "start": 1203.919,
        "duration": 7.441,
        "text": "it has got two cores and it's running"
      },
      {
        "start": 1208.08,
        "duration": 5.52,
        "text": "right let me try"
      },
      {
        "start": 1211.36,
        "duration": 2.24,
        "text": "to"
      },
      {
        "start": 1215.039,
        "duration": 3.441,
        "text": "run it again while the original one is"
      },
      {
        "start": 1217.52,
        "duration": 3.519,
        "text": "also right"
      },
      {
        "start": 1218.48,
        "duration": 5.52,
        "text": "again the driver is instead of running"
      },
      {
        "start": 1221.039,
        "duration": 2.961,
        "text": "and if i go there"
      },
      {
        "start": 1226.64,
        "duration": 5.44,
        "text": "you see it's running but no matter what"
      },
      {
        "start": 1229.76,
        "duration": 3.919,
        "text": "we have passed as a parameter"
      },
      {
        "start": 1232.08,
        "duration": 3.04,
        "text": "the total number of cores that are"
      },
      {
        "start": 1233.679,
        "duration": 3.681,
        "text": "assigned to the application"
      },
      {
        "start": 1235.12,
        "duration": 4.32,
        "text": "plus the private core always results in"
      },
      {
        "start": 1237.36,
        "duration": 3.92,
        "text": "the maximum number of course that your"
      },
      {
        "start": 1239.44,
        "duration": 4.32,
        "text": "lower resource workflow"
      },
      {
        "start": 1241.28,
        "duration": 4.24,
        "text": "so even if you try to bypass the system"
      },
      {
        "start": 1243.76,
        "duration": 3.919,
        "text": "it only"
      },
      {
        "start": 1245.52,
        "duration": 5.76,
        "text": "in this demonstration we have used data"
      },
      {
        "start": 1247.679,
        "duration": 6.561,
        "text": "stacks enterprise edition 677"
      },
      {
        "start": 1251.28,
        "duration": 4.399,
        "text": "please note that this is not a official"
      },
      {
        "start": 1254.24,
        "duration": 4.0,
        "text": "data tax direction"
      },
      {
        "start": 1255.679,
        "duration": 4.641,
        "text": "but a knowledge share that may go stale"
      },
      {
        "start": 1258.24,
        "duration": 3.6,
        "text": "as a result of new releases"
      },
      {
        "start": 1260.32,
        "duration": 3.44,
        "text": "make sure to check the current"
      },
      {
        "start": 1261.84,
        "duration": 4.4,
        "text": "documentation of the person you're going"
      },
      {
        "start": 1263.76,
        "duration": 2.48,
        "text": "to use"
      },
      {
        "start": 1268.48,
        "duration": 5.76,
        "text": "thanks for your time and hopefully"
      },
      {
        "start": 1271.76,
        "duration": 4.76,
        "text": "this content was helpful for more"
      },
      {
        "start": 1274.24,
        "duration": 3.6,
        "text": "information you can always go to"
      },
      {
        "start": 1276.52,
        "duration": 4.519,
        "text": "datastax.com"
      },
      {
        "start": 1277.84,
        "duration": 6.88,
        "text": "and find excellent articles on"
      },
      {
        "start": 1281.039,
        "duration": 3.681,
        "text": "different subject matters"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T02:16:51.392642+00:00"
}