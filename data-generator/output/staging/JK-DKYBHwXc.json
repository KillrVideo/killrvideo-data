{
  "video_id": "JK-DKYBHwXc",
  "title": "DS320.34 Spark Streaming: Stateful Transformations | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.34 Spark Streaming: Stateful Transformations\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:32:06Z",
  "thumbnail": "https://i.ytimg.com/vi/JK-DKYBHwXc/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=JK-DKYBHwXc",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] we've looked at stateless transformations on d streams which are fairly simple to understand given our previous understanding of rdd transformations now let's look at state full transformations which are just a little bit more complex stateful transformations combine data across multiple batches in a d stream in some cases which we'll see in these examples by maintaining state that gets added to the computation between batches that are processed let's take a look at the primary api that we have for this we've got one new method called update state by key now this expects to be working on key value pair d streams we pass a function to it that's a little bit more complex than most of the functions that we've been passing into our transformations so far this function takes two parameters one is a sequence of values which are all the values in that batch for a given key so this function is going to get called by spark for each key in the batch the second parameter that the function takes is the current state the function will then return a new state value based on whatever computation it does internally now for update state by key to work we have an important qualification here checkpointing has to be enabled what checkpointing does is on a periodic basis it's going to persist all the spark metadata to a file system so you'll configure the directory where that stuff gets written and in dse that's going to be a cfs directory and spark will persist all that metadata there periodically this is important since this streaming job could run for a long period of time we want to persist the state metadata on some regular interval so if there is a failure we don't have too much recomputation to work without checkpointing spark would have to recompute the entire history of the computation from when the job started to the present time and of course that's not practical let's have a look at an example of a streaming job where we might use state again we're going to start with a diagram very abstract then we'll move to the code looking at what's going on here we have a receiver bringing in data these are again ids probably movie ids from our movie database the first thing we'll do is we're going to run a transformation called count by value that's going to group up like keys and give us a key value pair of each key and the number of times we see that key occur in that batch then we want to aggregate those counts so far that stream is just going to show us a count of how many times each key occurred during the last batch but we'd like a total running count so for that we'll use update state by key which we show on the bottom notice how the two batches come in with counts of ten and eight and four and two you can go ahead and pause the video and check that those ids are identical they are we want a running total of all of those counts which we see in the box at lower left which is 14 and 10. that's the count across the whole stream not just one batch in the stream let's walk through the code that gets the job done that we just saw in the diagram now the code block at the top is the actual function we're going to pass in to update state by key up till now all the functions we've passed into spark methods that take functions have been anonymous because they've been pretty short kind of trivial things in this case though it seems like it's a complex enough thing that we want to give it a life of its own and a name and that name is update movie count it takes two parameters a sequence of values and the old count or the state that we've maintained from previous invocations of this function on previous batches previous rdds in the d stream it performs a little bit of logic there essentially adding the new values to the existing state and returns the new state the new count as essentially a scalar value then the code below is the spark application itself we create a new streaming context we open up a text stream to our stream of movie ids and then we count by value on that stream turning it into a pair rdd which is what update state by key needs then we call update state by key passing in the identifier update movie count which is the name of the function we wrote up above finally we print the result and we're done the sample output looks pretty clean you may need to pause the video and kind of sort through this make sure it all makes sense i encourage you to pay attention to the timestamp and you'll see that it changes by 4000 milliseconds because our time is 4 seconds and see that the counts are actually incrementing four unique ids we'll see at the top there 467 in the next stream it appears to be 469 470 becomes 471 and so forth so there is a very simple spark application using stateful transformations",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.399,
        "duration": 2.16,
        "text": "we've looked at stateless"
      },
      {
        "start": 7.359,
        "duration": 3.121,
        "text": "transformations on d"
      },
      {
        "start": 8.559,
        "duration": 3.281,
        "text": "streams which are fairly simple to"
      },
      {
        "start": 10.48,
        "duration": 2.48,
        "text": "understand given our previous"
      },
      {
        "start": 11.84,
        "duration": 3.52,
        "text": "understanding of"
      },
      {
        "start": 12.96,
        "duration": 4.72,
        "text": "rdd transformations now let's look at"
      },
      {
        "start": 15.36,
        "duration": 4.48,
        "text": "state full transformations which are"
      },
      {
        "start": 17.68,
        "duration": 3.519,
        "text": "just a little bit more complex stateful"
      },
      {
        "start": 19.84,
        "duration": 3.679,
        "text": "transformations combine"
      },
      {
        "start": 21.199,
        "duration": 4.24,
        "text": "data across multiple batches in a d"
      },
      {
        "start": 23.519,
        "duration": 2.881,
        "text": "stream in some cases which we'll see in"
      },
      {
        "start": 25.439,
        "duration": 3.521,
        "text": "these examples"
      },
      {
        "start": 26.4,
        "duration": 3.68,
        "text": "by maintaining state that gets added to"
      },
      {
        "start": 28.96,
        "duration": 3.68,
        "text": "the computation"
      },
      {
        "start": 30.08,
        "duration": 4.56,
        "text": "between batches that are processed let's"
      },
      {
        "start": 32.64,
        "duration": 3.919,
        "text": "take a look at the primary api"
      },
      {
        "start": 34.64,
        "duration": 3.04,
        "text": "that we have for this we've got one new"
      },
      {
        "start": 36.559,
        "duration": 4.16,
        "text": "method called"
      },
      {
        "start": 37.68,
        "duration": 3.84,
        "text": "update state by key now this expects to"
      },
      {
        "start": 40.719,
        "duration": 4.081,
        "text": "be working"
      },
      {
        "start": 41.52,
        "duration": 4.879,
        "text": "on key value pair d streams we pass a"
      },
      {
        "start": 44.8,
        "duration": 2.4,
        "text": "function to it that's a little bit more"
      },
      {
        "start": 46.399,
        "duration": 2.48,
        "text": "complex"
      },
      {
        "start": 47.2,
        "duration": 3.999,
        "text": "than most of the functions that we've"
      },
      {
        "start": 48.879,
        "duration": 4.081,
        "text": "been passing into our transformations"
      },
      {
        "start": 51.199,
        "duration": 3.04,
        "text": "so far this function takes two"
      },
      {
        "start": 52.96,
        "duration": 3.52,
        "text": "parameters one"
      },
      {
        "start": 54.239,
        "duration": 3.121,
        "text": "is a sequence of values which are all"
      },
      {
        "start": 56.48,
        "duration": 3.919,
        "text": "the values"
      },
      {
        "start": 57.36,
        "duration": 4.4,
        "text": "in that batch for a given key"
      },
      {
        "start": 60.399,
        "duration": 4.001,
        "text": "so this function is going to get called"
      },
      {
        "start": 61.76,
        "duration": 4.64,
        "text": "by spark for each key in the batch"
      },
      {
        "start": 64.4,
        "duration": 3.28,
        "text": "the second parameter that the function"
      },
      {
        "start": 66.4,
        "duration": 3.6,
        "text": "takes is"
      },
      {
        "start": 67.68,
        "duration": 3.119,
        "text": "the current state the function will then"
      },
      {
        "start": 70.0,
        "duration": 2.72,
        "text": "return"
      },
      {
        "start": 70.799,
        "duration": 3.121,
        "text": "a new state value based on whatever"
      },
      {
        "start": 72.72,
        "duration": 3.6,
        "text": "computation it does"
      },
      {
        "start": 73.92,
        "duration": 3.519,
        "text": "internally now for update state by key"
      },
      {
        "start": 76.32,
        "duration": 2.56,
        "text": "to work we have an important"
      },
      {
        "start": 77.439,
        "duration": 3.841,
        "text": "qualification here"
      },
      {
        "start": 78.88,
        "duration": 4.16,
        "text": "checkpointing has to be enabled what"
      },
      {
        "start": 81.28,
        "duration": 2.32,
        "text": "checkpointing does is on a periodic"
      },
      {
        "start": 83.04,
        "duration": 2.24,
        "text": "basis"
      },
      {
        "start": 83.6,
        "duration": 3.92,
        "text": "it's going to persist all the spark"
      },
      {
        "start": 85.28,
        "duration": 3.76,
        "text": "metadata to a file system"
      },
      {
        "start": 87.52,
        "duration": 3.04,
        "text": "so you'll configure the directory where"
      },
      {
        "start": 89.04,
        "duration": 3.759,
        "text": "that stuff gets written and"
      },
      {
        "start": 90.56,
        "duration": 3.12,
        "text": "in dse that's going to be a cfs"
      },
      {
        "start": 92.799,
        "duration": 3.36,
        "text": "directory"
      },
      {
        "start": 93.68,
        "duration": 3.759,
        "text": "and spark will persist all that metadata"
      },
      {
        "start": 96.159,
        "duration": 3.521,
        "text": "there periodically"
      },
      {
        "start": 97.439,
        "duration": 4.561,
        "text": "this is important since this streaming"
      },
      {
        "start": 99.68,
        "duration": 5.119,
        "text": "job could run for a long period of time"
      },
      {
        "start": 102.0,
        "duration": 4.0,
        "text": "we want to persist the state metadata on"
      },
      {
        "start": 104.799,
        "duration": 2.801,
        "text": "some regular interval"
      },
      {
        "start": 106.0,
        "duration": 3.36,
        "text": "so if there is a failure we don't have"
      },
      {
        "start": 107.6,
        "duration": 3.6,
        "text": "too much recomputation to work"
      },
      {
        "start": 109.36,
        "duration": 3.84,
        "text": "without checkpointing spark would have"
      },
      {
        "start": 111.2,
        "duration": 2.879,
        "text": "to recompute the entire history of the"
      },
      {
        "start": 113.2,
        "duration": 2.48,
        "text": "computation"
      },
      {
        "start": 114.079,
        "duration": 3.281,
        "text": "from when the job started to the present"
      },
      {
        "start": 115.68,
        "duration": 3.68,
        "text": "time and of course that's not practical"
      },
      {
        "start": 117.36,
        "duration": 3.52,
        "text": "let's have a look at an example of a"
      },
      {
        "start": 119.36,
        "duration": 3.2,
        "text": "streaming job where we might"
      },
      {
        "start": 120.88,
        "duration": 3.519,
        "text": "use state again we're going to start"
      },
      {
        "start": 122.56,
        "duration": 4.239,
        "text": "with a diagram very abstract"
      },
      {
        "start": 124.399,
        "duration": 3.601,
        "text": "then we'll move to the code looking at"
      },
      {
        "start": 126.799,
        "duration": 3.6,
        "text": "what's going on here"
      },
      {
        "start": 128.0,
        "duration": 3.2,
        "text": "we have a receiver bringing in data"
      },
      {
        "start": 130.399,
        "duration": 2.961,
        "text": "these are again"
      },
      {
        "start": 131.2,
        "duration": 3.6,
        "text": "ids probably movie ids from our movie"
      },
      {
        "start": 133.36,
        "duration": 2.56,
        "text": "database the first thing we'll do is"
      },
      {
        "start": 134.8,
        "duration": 1.84,
        "text": "we're going to run a transformation"
      },
      {
        "start": 135.92,
        "duration": 3.2,
        "text": "called count"
      },
      {
        "start": 136.64,
        "duration": 3.2,
        "text": "by value that's going to group up like"
      },
      {
        "start": 139.12,
        "duration": 3.68,
        "text": "keys"
      },
      {
        "start": 139.84,
        "duration": 4.32,
        "text": "and give us a key value pair of each key"
      },
      {
        "start": 142.8,
        "duration": 4.4,
        "text": "and the number of times"
      },
      {
        "start": 144.16,
        "duration": 4.88,
        "text": "we see that key occur in that batch then"
      },
      {
        "start": 147.2,
        "duration": 3.52,
        "text": "we want to aggregate those counts so far"
      },
      {
        "start": 149.04,
        "duration": 3.44,
        "text": "that stream is just going to show us"
      },
      {
        "start": 150.72,
        "duration": 4.08,
        "text": "a count of how many times each key"
      },
      {
        "start": 152.48,
        "duration": 3.52,
        "text": "occurred during the last batch but we'd"
      },
      {
        "start": 154.8,
        "duration": 3.92,
        "text": "like a total running"
      },
      {
        "start": 156.0,
        "duration": 4.64,
        "text": "count so for that we'll use update state"
      },
      {
        "start": 158.72,
        "duration": 3.92,
        "text": "by key which we show on the bottom"
      },
      {
        "start": 160.64,
        "duration": 3.04,
        "text": "notice how the two batches come in with"
      },
      {
        "start": 162.64,
        "duration": 3.92,
        "text": "counts of ten"
      },
      {
        "start": 163.68,
        "duration": 4.24,
        "text": "and eight and four and two you can go"
      },
      {
        "start": 166.56,
        "duration": 3.679,
        "text": "ahead and pause the video and check that"
      },
      {
        "start": 167.92,
        "duration": 4.56,
        "text": "those ids are identical they are"
      },
      {
        "start": 170.239,
        "duration": 3.921,
        "text": "we want a running total of all of those"
      },
      {
        "start": 172.48,
        "duration": 5.44,
        "text": "counts which we see"
      },
      {
        "start": 174.16,
        "duration": 6.079,
        "text": "in the box at lower left which is 14"
      },
      {
        "start": 177.92,
        "duration": 3.36,
        "text": "and 10. that's the count across the"
      },
      {
        "start": 180.239,
        "duration": 3.441,
        "text": "whole stream"
      },
      {
        "start": 181.28,
        "duration": 3.679,
        "text": "not just one batch in the stream let's"
      },
      {
        "start": 183.68,
        "duration": 2.479,
        "text": "walk through the code that gets the job"
      },
      {
        "start": 184.959,
        "duration": 2.801,
        "text": "done that we just saw"
      },
      {
        "start": 186.159,
        "duration": 3.121,
        "text": "in the diagram now the code block at the"
      },
      {
        "start": 187.76,
        "duration": 3.52,
        "text": "top is the actual function we're going"
      },
      {
        "start": 189.28,
        "duration": 3.679,
        "text": "to pass in to update state by key"
      },
      {
        "start": 191.28,
        "duration": 3.44,
        "text": "up till now all the functions we've"
      },
      {
        "start": 192.959,
        "duration": 2.401,
        "text": "passed into spark methods that take"
      },
      {
        "start": 194.72,
        "duration": 2.08,
        "text": "functions"
      },
      {
        "start": 195.36,
        "duration": 3.2,
        "text": "have been anonymous because they've been"
      },
      {
        "start": 196.8,
        "duration": 3.439,
        "text": "pretty short kind of trivial things"
      },
      {
        "start": 198.56,
        "duration": 2.88,
        "text": "in this case though it seems like it's a"
      },
      {
        "start": 200.239,
        "duration": 1.681,
        "text": "complex enough thing that we want to"
      },
      {
        "start": 201.44,
        "duration": 2.32,
        "text": "give it"
      },
      {
        "start": 201.92,
        "duration": 4.0,
        "text": "a life of its own and a name and that"
      },
      {
        "start": 203.76,
        "duration": 2.96,
        "text": "name is update movie count it takes two"
      },
      {
        "start": 205.92,
        "duration": 4.08,
        "text": "parameters"
      },
      {
        "start": 206.72,
        "duration": 5.439,
        "text": "a sequence of values and the"
      },
      {
        "start": 210.0,
        "duration": 3.92,
        "text": "old count or the state that we've"
      },
      {
        "start": 212.159,
        "duration": 2.72,
        "text": "maintained from previous invocations of"
      },
      {
        "start": 213.92,
        "duration": 3.84,
        "text": "this function"
      },
      {
        "start": 214.879,
        "duration": 3.521,
        "text": "on previous batches previous rdds in the"
      },
      {
        "start": 217.76,
        "duration": 2.479,
        "text": "d stream"
      },
      {
        "start": 218.4,
        "duration": 4.0,
        "text": "it performs a little bit of logic there"
      },
      {
        "start": 220.239,
        "duration": 6.0,
        "text": "essentially adding the new values"
      },
      {
        "start": 222.4,
        "duration": 6.24,
        "text": "to the existing state and returns"
      },
      {
        "start": 226.239,
        "duration": 4.161,
        "text": "the new state the new count as"
      },
      {
        "start": 228.64,
        "duration": 3.76,
        "text": "essentially a scalar value"
      },
      {
        "start": 230.4,
        "duration": 3.039,
        "text": "then the code below is the spark"
      },
      {
        "start": 232.4,
        "duration": 2.8,
        "text": "application itself"
      },
      {
        "start": 233.439,
        "duration": 3.201,
        "text": "we create a new streaming context we"
      },
      {
        "start": 235.2,
        "duration": 3.84,
        "text": "open up a text stream"
      },
      {
        "start": 236.64,
        "duration": 3.519,
        "text": "to our stream of movie ids and then we"
      },
      {
        "start": 239.04,
        "duration": 2.88,
        "text": "count by value"
      },
      {
        "start": 240.159,
        "duration": 3.121,
        "text": "on that stream turning it into a pair"
      },
      {
        "start": 241.92,
        "duration": 4.16,
        "text": "rdd which is"
      },
      {
        "start": 243.28,
        "duration": 3.12,
        "text": "what update state by key needs then we"
      },
      {
        "start": 246.08,
        "duration": 2.32,
        "text": "call"
      },
      {
        "start": 246.4,
        "duration": 3.759,
        "text": "update state by key passing in the"
      },
      {
        "start": 248.4,
        "duration": 2.88,
        "text": "identifier update movie count which is"
      },
      {
        "start": 250.159,
        "duration": 3.36,
        "text": "the name of the function"
      },
      {
        "start": 251.28,
        "duration": 3.599,
        "text": "we wrote up above finally we print the"
      },
      {
        "start": 253.519,
        "duration": 3.361,
        "text": "result and we're done"
      },
      {
        "start": 254.879,
        "duration": 3.36,
        "text": "the sample output looks pretty clean you"
      },
      {
        "start": 256.88,
        "duration": 2.639,
        "text": "may need to pause the video"
      },
      {
        "start": 258.239,
        "duration": 2.72,
        "text": "and kind of sort through this make sure"
      },
      {
        "start": 259.519,
        "duration": 2.641,
        "text": "it all makes sense i encourage you to"
      },
      {
        "start": 260.959,
        "duration": 3.201,
        "text": "pay attention to"
      },
      {
        "start": 262.16,
        "duration": 4.0,
        "text": "the timestamp and you'll see that it"
      },
      {
        "start": 264.16,
        "duration": 2.64,
        "text": "changes by 4000 milliseconds because our"
      },
      {
        "start": 266.16,
        "duration": 3.44,
        "text": "time is"
      },
      {
        "start": 266.8,
        "duration": 4.24,
        "text": "4 seconds and see that the counts are"
      },
      {
        "start": 269.6,
        "duration": 3.92,
        "text": "actually incrementing"
      },
      {
        "start": 271.04,
        "duration": 3.84,
        "text": "four unique ids we'll see at the top"
      },
      {
        "start": 273.52,
        "duration": 4.08,
        "text": "there 467"
      },
      {
        "start": 274.88,
        "duration": 5.12,
        "text": "in the next stream it appears to be 469"
      },
      {
        "start": 277.6,
        "duration": 5.2,
        "text": "470 becomes 471"
      },
      {
        "start": 280.0,
        "duration": 3.919,
        "text": "and so forth so there is a very simple"
      },
      {
        "start": 282.8,
        "duration": 12.08,
        "text": "spark application"
      },
      {
        "start": 283.919,
        "duration": 10.961,
        "text": "using stateful transformations"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:22:24.895905+00:00"
}