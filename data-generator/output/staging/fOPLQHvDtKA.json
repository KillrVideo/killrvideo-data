{
  "video_id": "fOPLQHvDtKA",
  "title": "Agent X: The AI Agent Experience Digital Summit",
  "description": "Build the Generative AI Experience: Agent Architecture is a virtual summit for architects and practitioners where we will unpack and demonstrate AI agent architecture for Generative AI applications. We'll show you how to use in-context data to create memory for LLM agents, and demo Retrieval Augmented Generation (RAG) with vector search, embeddings, LangChain, and real-world prompt engineering along the way.\n\nAbout DataStax:\nDataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra™. DataStax gives developers and enterprises the freedom to run data in any cloud, Kubernetes, hybrid or bare metal at global scale with zero downtime and zero lock-in. More than 450 of the world’s leading enterprises including Capital One, Cisco, Comcast, Delta Airlines, Macy’s, McDonald’s, Safeway, Sony, and Walmart use DataStax to build transformational data architectures for real-world outcomes. For more, visit DataStax.com and @DataStax.\n\nAbout DataStax Developer:\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache Cassandra©.  Visit https://datastax.com/dev for more free learning resources.",
  "published_at": "2023-07-11T19:15:02Z",
  "thumbnail": "https://i.ytimg.com/vi/fOPLQHvDtKA/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "scalable",
    "workshop",
    "cassandra",
    "search",
    "tutorial",
    "apache_cassandra",
    "vector",
    "nosql",
    "demo",
    "architecture",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=fOPLQHvDtKA",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "hi I'm Dr sharnaparky real-time AI product and strategy leader at data Stacks welcome to Agent X we wanted to bring you together today because we think generative AI is the biggest platform shift since the original internet in the mid 90s may be bigger we're inspired by the incredible speed and excitement around what we're seeing today with emerging applications and architectures so today we will pose the question if you were to build the next Salesforce for Shopify how will you build it we think that you choose to build it with generative Ai and we think that you soon might choose to build it as an AI agent so what is the Asian experience look like what's the architecture how do we use llms with our own real-time and historical data how are the lens changing the stock and how are they a power tool both during the development time and run time in the next few hours we are going to unpack the emerging generative Ai and LM architecture where you can create highly personalized and compelling agent experiences with your own real-time and historical data in our discussions just about every architect and practitioner is thinking about how to build genitive AI apps it's happening now so this is not going to be just Theory we'll give you real demos of real production AI agents and before we get started here's a quick summary of our two hour agenda and we'll build a couple of short breaks in there so don't worry first we'll talk about the AI stock and emerging architectures with our chief product officer Ed enough then Alan ho and Alex leventer are AI strategy and development leaders will take you on a journey and how we built our own production generative AI agent along the way we'll Deep dive into Lessons Learned on data engineering with Technologies like vector search embeddings real world prompt engineering and testing I'd send prompt retrieval and inference then Jonathan Ellis founder of data sacks will do a deep dive into how you can maximize performance on Vector search and really get under the hood on how it works finally myself and Brian kirschner our VP of strategy will lead a discussion on what you should think about before they before putting AI agents into production want to think about hallucinations privacy security and other emerging best practices so again welcome to Agent X the agent AI experience and now let's get started and let me introduce you to our chief product officer at enough take it away Ed foreign [Music] the chief product officer of data stacks today I'm going to talk to you about how generative AI has completely redefined the software stack for building applications in many ways the disruption that we're seeing is the most pronounced we've seen since 1994. when every company needed to get online needed to build websites needed to build e-commerce experiences now every company is trying to figure out how to build generative AI into every customer every partner every employee experience that they're delivering we can see this with the growth of chat GPT plugins we've all used chat GPT and recently they've made it available for developers to extend building plugins that hook into the chat gbt experience and allow you to go and couple it with external services and hundreds of developers have gone and built plugins and we can see here the growth of the chat gbt plug-in ecosystem now in many ways what we've seen with chat GPT is the emergence of generative AI as the latest step in a number of advances that we've seen within AI we saw deep learning machine learning become a very big deal a few years ago many of us started to research and learn this technology many of us have taken the lessons from Coursera and read all of the great O'Reilly books and all of that predictive AI was what we used this technology for how to figure out what a next best action would be or how to identify from an image whether it's a picture of a cat for example but along those same lines we saw generative AI emerge starting from text completion and text prediction to suddenly with the emergence of GPT gpt3 and now most recently gpt4 where we're able to create conversational experiences that are really remarkable and we're seeing the next Generations of that we're seeing things like Lang chain being used to build more complicated language interfaces towards the creation of autonomous agents at the same time we're seeing a lot of optimization happening where models are actually running on your device so a lot of exciting stuff has happened a lot of development that has happened to get here in fact if we were to look at the entire history of AI this stretches Back 40 or more years but certainly in the last decade we've started to see this accelerate and I think anybody who's following social media feeds or reading the latest developments have seen just how fast it's moving now so what's exciting to us is how we use this to build applications because the technology itself is very interesting but what's really cool is putting this to work in a way that users can actually experience and can benefit from and so as we look at that let's take a look at the application stack and see how it's being reinvented how every stage of it is being changed by the emergence and application of generative AI so let's take a look at how generative AI has redefined the application here if we look at every level of the stack everything from the user experience through business logic through the integration tier through the data tier we can see that generative AI is transforming every piece of the stack at the ux tier generative AI is not just powering the textual interaction a lot of people look at generative Ai and they go and they say look I don't like chat interfaces well the reality is you're actually going to see graphical user interfaces you see image recognition in addition to actual voice as well as chat all able to be used as inputs and outputs at the ux tier so this isn't going to just be chat Bots or chat agents although many of the early applications do take that form at the business logic level we see the ability to actually go and Define the business actions in more of a do what I mean format versus actually having to go and code it explicit uh programmatic rules for how the application works at the integration tier we've seen examples of this with the way that chatgpt plugins are built where they're able to couple directly to API definitions so the integration tier being able to pull in data and connect to other services is actually going to be more powerful and easier to do than it ever has been in previous generations of software development and then finally in order for this stall to be possible you need a very robust high performance data tier that's actually able to go and speak the native language of llms which is vectors so that you can have your large language models retrieve the data that they need as they need it so from an end standpoint we see that the stack is has been pretty significantly impacted so let's take a look at how that's typically used from an architectural standpoint so the typical way that an application that uses llms is built is using something called retrieval augmented generation and what this means is that when I'm building the context that I'm sending to the llm I actually am using existing data sources many of them are going to be within databases although they can be also retrieved from apis and other sources that context is what provides memory to the llm and also is what makes it personalized it's what allows it to provide real-time relevant responses llms are stateless they don't learn except at training time and that training time may have happened days weeks months even years ago so how is it possible that an llm is able to give you a response that might have the latest weather as of this morning or stock prices from 15 minutes ago retrieval augmented generation is what makes it possible that data the data that the llm needs is retrieved real time and passed into the context and then the llm takes that and uses it as a crafts the response so part of how we build a generative AI application is typically to use one or more models you may use a small data set which I just talked about that data set in fact might be real-time data that comes from your databases and you might have a smaller model which pre-processes that data and determines what additional data is required that then goes into the larger foundational model which perhaps you've tuned for your use cases but many times that's not even necessary but you're providing a robust set of context that comes from your data sources that's processed by one or more models that gets augmented as necessary using Vector retrieval from your database and then goes into these foundational models you often hear about Technologies like Lang chain that are used to make this possible as the name would imply you typically have two or more models that go into the processing and generation of a response before it goes to the users so these models are chained together they build and augment context and then deliver that personalized response one of the things that I that I did just touch upon was that llms are stateless I often hear about people going and saying oh you know gpt4 it's going to steal your information be careful what you give it well the model itself isn't going to steal anything the model itself has no memory as I said it is frozen in time from the point that it's been trained but clearly we've all experienced using chat GPT and other you know agents that they do have a memory well the memory comes from the database so it's the database that remembers what you input and as we're using databases with llms one of the things that makes them more effective is if the database is able to query on vectors if we think about what a vector is a vector is a mathematical representation of a concept it basically goes and takes whatever the topic is whatever the content is and Maps it in multiple dimensions and the important thing about exploding this data out or embedding it in a dimensional space is that you can compare the distance from any two positions in space and those positions determine the distance goes into determining the the similarity so if I am looking conceptually for something if I am shopping and I am looking for for you know one type of product and another product is very similar to it they're going to be close similar in dimensional space and llms typically represent their knowledge particularly when they're inputting and outputting uh either the taking in context or outputting a response in the form of these vectors so you can take those vectors and use them to look something up if you have a product catalog for example that's in a vector database you can now look up your products by concept is it summertime outdoor furniture is it indoor living room furniture obviously there are different concepts associated with those and so if I'm going and saying recommend to me furniture for my living room it's going to be able to go and find a set of furniture that's appropriate for that versus what perhaps I want to go and put out in my backyard by the pool so this allows me to take the data sets that I already have that I've Vector encoded and use them with my llms it's an important capability that makes it possible to build these types of applications so typically when I build this out I do this in an architecture that leverages Vector databases leverages an embedding API that allows me to convert unstructured content perhaps a question from the user into a vector that I can be used for looking things up and I'm delivering that at runtime you know microservice based application that can be used to deliver application experiences wherever my users are interacting whether it's mobile apps whether it's web apps however they are interacting with my systems because llms typically are powering not exclusively but typically are powering interactive applications that's where you're probably going to put these agents agents is on the front lines of where customers are interacting with your business and as we look at the technologies that go into this architecture we can see that Vector databases like Cassandra that is part of our Astra database as well as Predictive Technologies all play A Part within this you'll also Leverage online inference Services things like vertex AI from Google or sagemaker to generate embeddings and also to go and refine your data and actually help build better context that goes into these llms and that's part of slide talks about which is that when you're building these real-time apps you're actually combining generative Ai and predictive AI predictive AI allows me to go and look at historical data sets and perhaps generate Dynamic pricing offering up a discount for example it might be able to go and look at a set of data points coming from an iot application to determine maintenance is necessary or it might be able to go and look at inventory stock levels and figure out do I need to reorder or do I have a surplus that perhaps would generate a flash sale for an e-commerce site but all of those inputs need to be delivered to the user in a form that makes sense to them and that's where generative AI fits into the picture general of AI is fantastic at the places where the user is interacting whether it's conversationally whether it's for example many of us experience with GitHub co-pilot where code Auto completion happens as you're typing and fills out sections of your code base allowing you to develop in a much more rapid fashion obviously generative AI is and predictive AI are used within advertising predictive AI figures out which ad to show to which person but generative AI can actually create an advertisement and offer a promotion an email that's tailored to an individual it's a very powerful conversation combination or going and taking a set of content and going and condensing it down and delivering the right message at the right time so we've actually practiced what we preached we've used these capabilities to build something that we call Astra assistant which is designed to make it possible for any developer that is using Cassandra that's using Astra DB to be able to go and build queries to be able to go and Define their database schemas or generate sample code that they can use in their applications and this is something that's live today we're talking about it in some of the other sessions today on how we built it it's a very cool use case that will give you a tangible perspective of how you can put one of these applications to use for you so stick around a lot more great stuff to show you thank you [Music] the architecture is for llms and agents things are changing quickly and it's an exciting time to be an architect or designer next we're going to see an agent architecture in action by diving into our own generative AI application and case study in our next session two of our AI leaders Alan ho and Alex levantera will deep that into the code of an AI agent and Lessons Learned in data and prompt engineering Alan ho is an AI strategy lead at datastacks and Alex leventer is one of our developers building AI applications here at data Stacks take it away Alan foreign [Music] so we're going to be sharing with you how to actually build Hands-On a generative AI agent and so what we've done today is that we decided to show you our journey in building our own generative AI agent for our Astra assistant and I'll be going over some of the lessons learned and how to do it and then Alex going to be helping us get dived right into the code and showing you exactly where you need to code things up to build your own all right so I'm going to first give you an overview of the assistant of what we're trying to do then I'm going to talk a little bit about architecture um how the generative AI workflow and the various components needed then I'm going to go step by step on how to build a retrieval augmented generation workflow we'll talk about the data pre-processing that's needed how to generate highly contextual prompts and how to execute the prompts and what to do afterwards to give you some context of what we're doing at data Stacks is that we're using generative AI to improve the results of our self-service business so when customers use our website and they want to use our app we've already used AI to increase our subscription business by 5X so now what we're doing with uh with a generative AI which is different from predictive AI is require the reduce the time it takes for people to actually deploy their apps for production and so today because databases are very complicated pieces of software most of the time we use actual real agents real people to help them out and this is our journey in replacing those people or augmenting those people uh with a generative AI agents so our key metric to get to production is our key metric that we care about is shorten the data time to production because the typical thing that happens for a software service like ours is that once they start using the software and they finish their first use case then they tend to spend more money for their second use case and we get them to the cycle through this uh a lot of times people just use the chat app to help them get through this cycle so our as Astra assistant that we built it's primarily focused on answering some of the uh technical questions it leverages our chat history and the documentation Etc to answer these questions and um we also use it as a mechanism to um tell people about how much it costs as well as uh get help and there's also the ability within it to refer or escalate to an actual person so we don't have uh we don't have 24x7 support for our our this chat bot so we're primarily thinking about using it especially during the off hours all right with that I'm going to hand it over to Alex who's going to show you a demo of how it works awesome thank you very much cool um so what you're seeing here is the dashboard of our product Astra um if you haven't created an account you can create a free account at ashrae.datorsex.com register and on the bottom right of the screen you'll see this chat icon um so this chat icon for the past couple years has been managed by a team of sport engineers and just recently um it's now powered by the assisting column suspensions um so I'm first gonna start by asking a simple query hello generate token and I'm going to add this new context text and what no context does is it tells the assistant that we don't want to inject any additional information in the prompts and you'll see um because we haven't injected any additional information in the prompt um we get an answer from the bot that has nothing to do with data Stacks to actually work Cassandra um so you can see we get some information on how to generate um an API token from Google documentation um has nothing to do with Ashraf is super um you know unhelpful for our users so I'm going to ask that same question with information from our documentation in our users in the problems and you'll see um with the improved prompts we get and enter the super relevant to Astro um so here you can see exactly how to generate a token some of this information was pulled directly from the docs I'm in in later in the session we'll we'll teach you exactly how to do this and the importance of prompt engineering um and how you could potentially build your own ball like this um we can also ask more advanced questions like tell me about my databases and because in the prompt we also include information on the user all their activity um we get this super awesome answer back on you know exactly the databases the user has created and we can also ask the bot to generate sample code so here you can see um we have the bot generated just a simple sample app on exactly how um you know you connect to asteroid using python um and I'll hand it back over to Alan to talk through um exactly how you can build this on your end all right thank you so there was a lot of magic being behind the scene that was going on so uh stick around and we'll be able to show you that magic before we get started to get to that magic let's talk a little bit about the workflow you need to build these what we call retrieval augmented generation workflows these are workflow General retrieval augmented generation basically means pairing up the large language model with your data set and by putting the two together you generate these highly relevant prompts that allow the the llm to give you good responses like what you just saw so at a high level the first thing you need to do is you want to find what your agent does um is it a chat bot is an Enterprise Search application is it a GitHub co-pilot you want to do some definition of what it actually is doing um then you want to provide um you want to select which llm you use then you want to figure out what is the data that you're going to be using to augment the large language model with and then figure out how to take that that the that data and properly put it into what we call vectors these allow for the llm to semantically figure out what is relevant information for it to process and build that prompt then we use then we leverage that information to build the prompt and you code it up and then we have to figure out how when we actually launch in production how you're going to maintain this uh regenerative AI agent and how you're going to do it how you're going to deal with your operations so with that there's a lot of various different uh tools and there's a lot of different decisions that you have to ask yourself when going through all these decisions so this is kind of the architectural work that you need to do up front all right let's take a look at the the uh the the um the overall um architecture so at another high level we showed you is an application which was the chat application it's talking to your agent this is the agent that you code up in our case we cut it up in Python and it's leveraged a combination of proprietary data this is a proprietary structured data just like any kind of data you'd store on Cassandra day and then it also leverages uh proprietary unstructured data this is the data that's retrieved via Vector search and this is a brand new capability that we've added to astrodb Cassandra very recently now in order to deal with your unstructured data you need to leverage something what we call an embeddings API so this embeddings API takes unstructured data and turns them into the vectors as needed so you can do the semantic search we'll get into more details on it later and then last but not least when the agent is being executing it calls these data sources and the lln in order to return it to the application now there's a lot of different Technologies you could use and so today what we're going to demonstrate is obviously leveraging data Stacks technology for proprieted structured data we're leveraging Astro DB for our Vector data we're also leveraging astrodb leveraging the new Vector database capabilities then betting apis there's a lot of different Technologies you could use there's Technologies from vertex AI That's from Google open AI Sage maker hugging faces Etc there's a lot of different embedding models today and then today for llms there's also a lot of various different applications available today open AI Google Cloud Google vertex AI hugging faces Etc and we want to make sure that when we're doing this execution we want to actually actually have it execute in one of the clouds because latency becomes very very important we want to make sure that all these services are running in one cloud in a secure location so the technology that we're using on needs ahead for this one is the llms from Google we're using the vector database of that we have we're leveraging the embeddings API from Google vertex AI we're leveraging Lang chain and then we've also open sourced a new project called cast IO which basically allows you to create these highly scalable agent memory which plugs into all the very common Frameworks such as line chain and hopefully very soon Lama index so why don't I go into talk a little bit about the first step the data pre-processing so I wanted to give you a little context of what Vector search is so Vector search is a way that you can um you can interact with unstructured data and the reason why this is so important is because the llms need to have context it needs to have information proprietary information especially unstructured information in order to generate very relevant responses these can be information from your chat's history system from actual people conversations this can be documentation this can be policy information Etc and the reason why this is so important is because if the llm doesn't have context if it doesn't have information then a lot of times that's the reason why it actually creates a lot of hallucinations so let's talk a little bit about how embeddings work so the first thing is that you what you would do is that you would map your database items into an embank space so we're leveraging an off-the-shelf model machine a neural network to generate these embeddings and you can see for every single piece of data it's mapping into this space now the time of inference when you actually uh doing the vector simulator search what it does is that when somebody types in a question it figures out what are the documents that are near it so in this case this in this scenario what it's doing is that it's mapping different um pieces of text to this embedding space and then somebody's asking a question what is like a good Shakespeare tragedy and it finds out the very nearest neighbors like Romeo Juliet et cetera so for our use case we have two sources of unstructured data that we care about one is our product documentation this tells people how to use the product this is probably the primary source of it and the second is chat history and what the chat history does is that it gives the uh it gives the AI agent example questions and answers that people ask and this is important for not necessarily for finding the answer but especially in the scenario because that information is in the product documentation but it gives kind of an example this is also what we call fuchsia learning of how to answer these questions so we take this information we generate embeddings and then we write the raw the embeddings and the raw text directly into the database so what does a raw text look like so for example this information here is um from right from our documentation and it generates about a thousand five hundred vectors sorry 5000 1 500 um uh scalar values to create a vector and this is the data that allows you to do a similarity search so with that I'm going to head it over to um uh um I'm going to hand it over to Alex to show a little bit about how the pre-processing works yeah let me share my screen again um so what you're looking at here is a collab notebook um where you where we demonstrate how you can potentially um you know process all your data like Alan just mentioned um so I'm just going to walk through all the cells and how you can potentially build an app to do this um so at the top um really basic setup right we install our necessary dependencies um like Alan said we're using uh vertex AI for this demo right so we need to install Google dependencies along with the python driver our own python driver that supports vectors um we do some basic environment variables set up right we need the intercom API we need Cassandra creds we need um uh protects AI credits um and then the process is pretty straightforward um so first of all what we do is we call the intercom API to pull all our conversation history um so this is just kind of a simple call to intercom to um get all the conversations and then iterate through each of the pages of conversations um down here we do some Google vertex AI setup um we were at the library we set our project ID we load our credentials um and it really is just a couple API calls to um create the embeddings right so here we can see here's just kind of example of how you create embeddings Google has a very simple get embeddings API where you can feed in the Raw unstructured text and you get back into bedding um so if you wanted to kind of create embeddings for all of the intercom conversations it's pretty straightforward right so here we're doing some basic setup of the tables I'm in the vector store right we're creating the chat table we're creating an Sai index um and then all we do is we iterate through each we iterate through a list of conversations um for each conversation we generate the embedding and then we're loading it all into Cassandra um so pretty straightforward um and down here we just have kind of an example of how you potentially make uh a vector search once you've load all this data um so in order to actually query the vector store you would need to convert your search query into an embedding right we're doing that right here so let's say the question is where do I find a token we'd first generate the embedding um and then it's a really simple query right get the raw text where the embedding is the embedding which is generated um and I'll hand it back to Alan to walk through the next step of the process so there's a few tips that you that you should remember when you're doing your data pre-processing number one is that clean data matters so if you garbage in garbage out if you put irrelevant if you put data that's um that's uh it's actually irrelevant data is not that important that's not not that terrible it's actually data that's wrong so if you put in answers that are semantically wrong your llm is going to give you actually wrong answers the second thing is to actually properly split your data and this is kind of a 200 level type of a a tip here so when you're processing a very large document you don't want to store the entire document into the vector database what you actually want to do is you want to Chunk Up the document into pieces that are small enough that you could put in the large language model and when you split when you chunk it up you don't chunk it up let's say take a document and evenly split into 10 chunks what you would do is that you would take the document and you would split it into 10 overlapping chunks and this is necessary because the semantic search capability needs to have um a little bit for for a particular paragraph of text it only makes sense if it has a little bit of information about the previous paragraph of text so you have to be very smart and this is something that you need to there's no out of the magic answer for every piece of the text you got to be really smart about it uh the third thing that you want to do is you want to be careful about loading the information at speed so one of the nice things about Cassandra is that it can it can do a lot of parallel inserts at the order of thousands of Records per second for just even like a three note a small three node cluster so you want to leverage parallelization we have some tools called DS bulk that helps you load the data faster now another part that we have to be very careful is keeping the data safe especially if you're trying to leverage your chat history for a few shot learning you have to make sure you strip out all the pii data so you know customers often do things like put in their tokens in the chat history things of that sort their their proprietary secrets you want to make sure that's outside kept on your data set and last but not least the large language model is only as good as data it has so you need to make sure that the information you have is relevant uh and updated on a regular basis okay with that let's kind of get into the details of prompt engineering so the purpose of prompt engineering is to make sure that the data that's being the prompt that's being sent to the llm is as contextually relevant as possible and this is not easy because LMS are inherently they're kind of like human beings there's only a certain amount of working memory that it has so it has it has something called a token limit you can't throw your entire knowledge base into the llm because there's simply not enough memory for the working memory for the llm to take it to do so so we have prompt templates and testing framework that you can use and it's really important to kind of compare all the various prompts now there's a there's a bunch of challenges so the first challenge is how do we make sure that the results are qualitatively good so how do you actually go do around that testing um sometimes there's a balance of detail you want to give enough information to the llm but you don't want to give too much llm information to the llm otherwise it gives a very vague responses and then last but not least um there's relevancy and safety so how do we prevent the llm from saying harmful things irrelevant things or very inappropriate things how do we do all that so Kyle let's go into the data flow so let's say a customer uses the application calls the nosql assistant to with a question the first thing that happens is that the nosql assistant calls the vertex AI to get the embedding uh so what for that question semantic what are vectors that are close to it um and then this this way is I just just a reminder of how this is done is that um you know there's there's the the database already has all these vectors in it and the question that's being asked is related to some of those uh those uh pieces of data in this case product information and Returns the correct product information so for example how do I generate a token this turns back into the 1500 floatware float vector and it finds the closest ones and so when you're doing the vector when you're calling the vector store not only are you calling the vector content but you're also calling the vector store uh the the Cassandra database for information that's relevant about the user so what are what is the elements of a good prompt so a good prompt has the directive so what what the kind of what kind of Bot that person is it usually has information about the user the user itself like who's this particular person whether that person particularly done in the past and then also the previous interactions with the user so this is what we often call the chat history so when somebody's typing in and having a conversation just like a regular human being the llm needs to know what are the previous chats it had in the past in the context for that then it would have then also the prompt would have relevant information like sorry the relevant documents that give the answer and then finally of course the prompt has a user's question so let's take a look at this is kind of a subset of all this information but let's take a look at uh if the prompt asks if the user asks how do I generate a token inside the prompt it would have a directive so in this case we're telling using the using the person's profile information we determine that this is an advanced user trying to perform some actions in the database and this is very helpful because if you tell that llm that this is an advanced User it's it ends up not providing it providing the user more advanced answers so like more sustained code and things of that sort it assumes that the user knows how to use Cassandra already um now we also provide uh information about the user and some of the databases that the person has created these are just regular key value pair lookups um in Cassandra nothing fancy here and then we have a bunch of unstructured uh data in this case we're showing similar chat history conversations with this user and you can see over here there's like three different conversations with various folks like Melissa who's one of our chat support reps who gives answers on how to connect to the how to connect to uh to a particular API for generating a token a talk a little bit about prompt selection so what you do is that for prompt selection you would use the information about the user's profile the recent history and then select a particular prompt to use and this prompt is basic template you would select a prompt template to use so in this scenario we have two types of prompts one is a learner prompt template another is a qualified user prompt template and this is what a qualified user prompt template would look like it would say oh you know you should answer um okay sorry this is a little bit cut off but say you should answer customer questions it tells that this person is a qualified user and then the learner template what it says that you are giving answers to someone who's new to data stacks and Cassandra so be especially helpful and and be a little bit more verbose on how you answer the questions and if you look inside the prompt template it has both a little bit as um information about uh um you know helpful responses from intercom so this is used for prompt templating sorry for a few shop uh learning it has the user information it has the user's questions so we're going to give a demo of this so we'll Alex will show um the Astro assistant how to construct The Prompt The Prompt templates and how to test with some of History okay headed over to you awesome um so what you're seeing here is just my ID icon and to manage prompts we use just the line chain prompt templates um so both of the templates that Alan just mentioned are within a prompts directory they're both yaml files and these templates are pretty straightforward essentially we Define that you know the XML file is a prompt um we Define input variables right here there's only a single input variable which is the output of the vector search and that's pretty much about it um the more interesting part is how the prompt template is actually selected um So within kind of our main bot code um we have a very simple um you know method to get the Persona right we check um the user's information on user right when it when a user registers for a product we basically ask them a questionnaire on if the use consider before what their primary programming which is um and and basically based on these responses um we determine if we want to give them the learner template or um the qualified template that on this mentions um so here's where we actually select the template and then um you know you've seen the single products now here the actual templates um and we really struggled with kind of how do you actually test these templates how do you actually make sure that the responses are high quality um so to basically debug this process and consistently improve our templates um we built a pretty cool slack bot um so let me set that up real quick okay awesome um so on the left we have just the desperate of our product in the chat chatbot and on the right we have a slack Channel we set up to automatically log all the prompts along with the responses um so we can keep reviewing and improving the quality um so I'm just going to ask a simple question the chatbot and on the right um we can see the entire prompt along with the response I'm just going to scroll up a bit um so here we can see we have the directives basically over here is the full prompt that I'll just talked about um so first we have the directive um we then have the output of the vector search right through here you can see a couple pieces of documentation we have information on the user so here's the username their email um their primary programming language and we have information on all of our databases what's really interesting here is you can be the LM raw um Json and table two interpret all that data and then finally we have the response that was actually fed back to the user um so really our process of testing and improving the prompt has been I'm reviewing all the prompts reviewing all the responses um you know let's say we get um you know bad documentation left but sorry bad documentation back maybe we'd update the data in the vector store let's say you know maybe there isn't enough information on a topic we insert new documentation in the vector store but really it's just been kind of this iterative process of um you know we review uh The Prompt the response and we kind of keep improving um and now I'll hand it back to you just before you go there um actually if you bring up your screen for a second yes go back to there you can see if you look in the document in the prompt template in the in the prompt there's a lot of information in here um and if you go up into the into the the responses The Prompt template even to the human eye doesn't look super understandable so for example there's this in the middle of the screen that says Ah the token will no longer exist the length of time to persist the token is configurable that's unrelated to how to generate a token and so one of the things that we notice is that the semantic search that you're doing from the vector store it's more of a gross level search it doesn't it doesn't exactly understand the the the the context of the question it's able to get things that are related to it um and the llm is extremely good at ignoring irrelevant information so if we kind of go back to I'm going to kind of Select so talking a little bit about um if you can stop your share screen I'm going to show you some particular tips that you can use to help improve your prompts so the first thing is uh Vector search so what are the things that you really care about the vector search number one is that if there's no l if there's no data that's coming back relevant from your knowledge base there's a very high probability that you're going to have a hallucination um and so and the other thing too is that because lens are so good at rejecting irrelevant information it's really good idea to get as many nearest neighbors as possible at least whatever would fit within the prompt um and then actually there's another Advanced algorithm called uh maximal marginal relevance that what it does is that it takes the let's say a hundred nearest neighbors and it can isolate it down to the 70 or sorry the the 10 most nearest neighbors that's uh most relevant for for this particular prompt and the reason why that's very important this is a kind of a 200 level type of um of use case is that approximate nearest neighbor algorithm which is the underlying algorithm used for nearest neighbor is very very susceptible for a duplicate or near duplicate vectors in your prompt so for example if you're if you're if you're using your scripting or web page uh your web pages and you have five web pages with almost identical information what ends up happening is that it it fills up the database with those those vectors if it so happens that the vector infra those duplicates were also irrelevant what happens is we use search for let's say uh five nearest neighbors then end up what happens at all five nearest neighbors end up being very uh data becomes irrelevant whereas if you use a wider search let's say 50 um and then you for 50 nearest neighbors and then ask for the five most different um uh documents within that set of 50 all the duplicates automatically get removed because of this maximal marginal relevance it basically tries to find out the most distinct pieces of information so this is kind of an advanced thing that you should remember to use especially if you're concerned of your of your uh your system having duplicate data uh there's also another bunch of other things that you that we've learned to prompt engineering so number one is because this is a real-time application um we you want to respond to the user as quickly as possible so you want to fetch as much context from your data your your database in parallel so one of the optimizations that you didn't see underneath the hood is that the data is being fetched in parallel like calling the vector search for getting the user information getting uh um the the user activity uh getting the chat history and as well as getting the documents that are relevant that's all being happening in parallel against Cassandra which is great because Cassandra itself is built as a paralyzable database um you got to be very careful about managing the token size so this is important not just for making sure that you um it's it's not just important for Speed but it's also important to use a cost and then you want to have all the tools you need to simplify your development so prompt templates and in particular is a very very easy way to simplify the development and in our case because our our company is a slack so much instead of if we want to democratize more people testing out these LMS we use slack as a way that our customer support reps can look at all the data that's coming back and forth from that particular bot and last but not least you have to for hallucinations you really got to kind of test it out and then figure out what's the problem iterate and constantly improve the uh reduce the the the um that'll improve your problems to reduce the hallucinations all right so with that the next portion is actually and so we've talked about generating The Prompt now let's talk about execution of the llm and so what the way it would happen is that we're going to start from after the prompt is generated the first thing that we're going to do is that instead of calling the LM directly we do actually a cache lookup so we're calling the vector store to get see if that particular question has been asked before and if it's not in the cache that's at the time when we actually invoke the LM so calling this text bison llm model and getting the answer and at that point after we get the answer we store the chat history back in the vector store we also store the cached value into the vector store as well for further use and then we respond return the the response to the to the user so just to give you some context to hear some other Lessons Learned uh when we're doing caching you got to be really careful about uh privacy so especially when you're doing a chat bot usually people are having one-on-one conversations that are private and so this doesn't lend itself very well like our Astro agent unfortunately doesn't lend itself very well to caching because usually the conversations are in the context of that particular person um and then to also reduce the hallucinations there's a lot of settings in the llm the temperature the K value p-value Etc that you can play with in order to um to uh to generate to see whether or not the answers are correct and last but not least one of the things you need to do before you roll out is you need to capture qualitatively these conversations are good conversations or bad conversations so what we do is that we we actually capture a thumbs up thumbs down of these conversations or good conversations and bad conversations and then we use that to understand um to we use that as feedback to improve our prompt templates and improve our prompts down the road one of the other things that we've made a lot of usage of is land chain so Lang chain has these key abstractions for accelerating development of your application the first key abstraction that we use this thing called indexes this is the data like for retrieving the um the the the the chat history as well as the um the the uh the DB um uh sorry retrieving the product documentation we use the CH The Prompt template capability to contextually generate contextually relevant prompts in fact we have this feature in Cass i o that you know you can write the prompt template and then the prompt template uh and in the prompt template itself you can kind of feed in uh cql type functions such as Auto populate the prop template uh we use this application a algorithm called the Maxwell marginal relevance this is kind of an advanced algorithm on top of vector search to improve the relevancy of the data coming back from our database we leverage the memory abstraction uh to record all the chat history um and uh underneath the hood there's a lever you can leverage something called a semantic memory so instead of just pulling all like let's say the 10 last 100 chat interactions it's able to semantically figure out what are the previous interactions are relevant to the con the question the person is using and then last we use the cache for improving the performance and the cost all right so with that I recommend you guys to get started um we can do a little demo why don't Alex can you show exactly how you would provision the vector database so if you haven't done so I'll link it on set you can create an account and ask for account at Azure daystax.com register um once you've done that you'll see the dashboard that looks like this and you really can create a vector store in just a couple minutes um so you're just going to click this create database button at the top of the screen you're going to select serverless with Vector um enter some basic details like your database name your key space name um finally choose a region and a couple minutes later you will have um your vector database in the in the region you select um so really I encourage everyone to give this a try um and if you need any additional help feel free to ask the uh no SQL assistant at the bottom of the screen all right so with that um I want to tell you a little bit what we're going to be doing in the future uh so what we've done is that we've connected our chat bot to our internal data warehouse which is um uh bigquery and we're going to leverage looker which is a reporting system on top of bigquery to generate a lot of cement reports on how people are doing and using the chatbot uh what we also hope to do is that right now the chatbot is integrated into the nosql into the into the page but there's no reason why that this is the only location where you can integrate the chat bot so what we plan to actually do is integrate that into the shell the cql shell on the website or potentially integrated directly into chat GPT as a chat GPT plugin and last but not least we plan to leverage the nosql assistant to also do email follow-ups so not only can your agent be reactive you know you ask me a question it gives an answer but you can also use the agent to predict when the person needs to would would prefer would want to interact with you and generate automatically relevant emails to help that user along their Journey so this is just kind of the beginning uh there's a lot of other things that we want to talk about in this in the next while so we're going to talk a lot about autonomous AI agents you may have heard this thing called Auto GPT so this is the idea of leveraging multiple llm calls in in sequence or in parallel to get to a particular uh to to enable a particular outcome whether it's fixing your database things of that sort so we're going to be experimenting a lot with that and talking about it we're also going to be talking about Advanced retrieval augmentation generation algorithms so we talked a little bit about the mark uh the the Maxwell marginal relevance algorithm uh next time around I really want to get into the details of the math behind it why is approximate nearest neighbor and uh K nearest neighbor algorithm useful but also what are the limitations so we want to talk a little bit about the limitations of vector search and then we're going to talk about things such as the forward-looking augmented retrieval generation algorithm also known as flare these are out these are Advanced algorithms that instead of doing just one API call to your vector store getting the content and then uh getting content and putting into the llm and to summarize a response this is whereby the uh the the agent can actually make multiple calls to the the vector store and multiple calls to the llms to actually summarize and extract information which is very similar to what us humans do so for example if we're reading a book and if we want to get an answer we would go reap different portions of the book we'd jump around with a book summarize go back to the places we forgot about and then try to come up with the answer by going to iteratively calling uh looking at different sections of the book processing Etc and then last but not least we want to talk about what this agent memory is so the idea behind how do we create a human-like memory for uh for autonomous AI agents and so we're going to get into the details of how data stacks and the Cassandra Community are trying to push this technology forward so we got a lot of resources that we've uh that we've put together that you can do more reading and um you know in terms of The Next Step uh we love to see what you build uh so if you're going to build something uh let's uh let's schedule a live end-to-end demonstration uh we can demonstrate what we have and we'd love to see yours uh let's schedule a collaborative Workshop so in the future what I hope to do is build a community of uh AI agent Builders and we're going to show each other what we've been building over the last month and so we want to have a regular touch point with the community to do so and last but not least uh try Astra DB with Vector search and build some awesome llms and awesome AI agents thank you thank you [Music] thank you Alan and Alex I think the takeaway from the session that we just saw is that you can dive in and start to build some pretty amazing agent experiences with your own data right now next I'm delighted to introduce Jonathan Ellis co-founder of data Stacks Jonathan is going to do a deep dive on after a vector search especially how you can get the most out of it in performance and scalability and now here's Jonathan foreign [Music] I'd like to explain a little bit about what's going on under the hood with our implementation of vector search and how you can use that information to make better decisions as you build applications on top of it the first thing to understand is that Vector search is built on top of Sai or rather it's built as a part of Sai Sai stands for storage attached indexes and it's a Next Generation Cassandra index coming in Cassandra 5.0 and it's available now in datastax Astra Sai gives you a couple things in this graph we're showing the effects of indexing a billion rows with 10 indexes defined across the rows in that table on the left is the space used by the raw row data itself so that's about 160 gigabytes of data and then we have in red an experimental kind of index called sassy in the green we have uh traditional Cassandra secondary indexes so that's what you get if you download Cassandra fortado and you say create index whatever the green bar is what you get and then the blue is the Sai index so you can see that Sai is more efficient by a factor of about five compared to the next closest index implementation the other thing maybe even more important is that with Sai you get much more powerful queries the traditional indexes the green ones in the last graph could only index exact match queries and they can only combine predicates using and so you could select column one equals X and column two equals y but you cannot select column one equals X or column two equals y you couldn't do that before you can do that now with Sai and you can also do inequalities with Sai and you can also do nested predicates with Sai so here is an example of doing nested uh predicates with an or uh using Sai that's completely legal you can do that today again uh in Astra so we're going to apply this to indexing vectors and to do that I'm going to back up just a little bit and talk about what we mean when we're comparing vectors in the first place uh so one of the the our goal is to given a bunch of vectors that we've inserted in the database find the closest match to a given query and you you'll see discussed a couple different abbreviations one of them is k n k n stands for the K nearest neighbors usually when people are talking about this they're talking about the exact nearest neighbors so if I've got 10 million vectors I'm going to find the 10 out of those 10 million that are closest to the query the thing is that to do this you're you have to compare the query with each Vector in the source um there's there's more clever there's some clever tricks we can play if we only have a vector of Dimension two or three or ten however those those algorithms don't work uh when you start to get to vectors of dimensionality 300 700 a thousand and when you're dealing with machine learning vectors which is kind of the most interesting Vector to Index right now uh you start at several hundred and so when you're doing vectors of several hundred Dimensions Brute Force is is the best we've been able to come up with to get exact matches um and so naturally when you're when you start to get 10 million 100 million a billion vectors inserted uh doing that Brute Force gets less and less attractive and you can actually get down to logarithmic queries uh if you are willing to say I'm I can live with approximate results instead of exact results so this is uh usually abbreviated a n approximate nearest neighbors and we can do logarithmic uh query time with that so uh when we're going there there's a bunch of different uh implementation options for getting logarithmic query results uh but when we're applying this to Cassandra we've got some specific requirements that we need to meet that narrows down our uh our options uh the first is that we need to have a reasonable performance from disk uh if the index only really works when it's 100 in memory then it's not a great fit because in Cassandra we're typically dealing with billions of rows uh and so we don't want to have to spend enough money to fit all of that in memory the second requirement is that the index needs to be incrementally buildable and immediately queryable what I mean by that is uh as I insert rows into my table uh I should be able to do that one row at a time and as soon as I've inserted that one row I should be able to use it in a query uh I shouldn't have to do batch inserts of thousands of rows uh I shouldn't have to wait for some kind of indexing process to happen in the background I want it to be available as soon as I've inserted it and then finally uh we don't know the data set in advance and some of these index implementations for vectors they only work if you do know that in advance and so where those get ruled out as well so um this is a graph of some of the index implementations in Facebook ai's approximate similarity search Library uh there's literally dozens of combinations you can put together here to to test and what you're seeing on the axes by the way is that the x-axis is recall that's how accurate are my results and that's measured as a percentage or a fraction from zero to one where one is I got a hundred percent perfect results and then the y-axis here is how many uh queries per second I can do and the y-axis is logarithmic I'll point that out as well um and so all of these index types you can trade off between how accurate do I need my results to be and how fast do I want it to be but there's clearly some algorithms are better than others uh even given the fact that that we can make different trade-offs um and so applying the uh criteria that Cassandra has we end up with basically one appropriate candidate for building the uh indexes and that's called hnsw which stands for hierarchical navigable small world and the I the idea here is similar to a classic skip list where I've got I'm going to create multiple levels in my index and at level zero the bottom all of my vectors live in in level zero but every level I go up from there I only promote 10 of the vectors to that next level so I you know I've got a hundred percent of my vectors in level zero I've got ten percent in level one I've got one percent in level two point one percent and level three and so forth and what we do is when we search for a query vector and the the page here is uh giving an example of a query Vector in red what we do is we go to the top level in our our index and we compare it to uh the the nodes the vectors in that top level and then the one that is closest to we look for The Neighbors of that node in the next level down and then we take the one that it's closest to in that level go to the next level down and so forth until we get down to the bottom level where all of our vectors live and we've narrowed down the vicinity of where we're going to look by following that chain down from the top and so we get it's not an exact search again because each we're only remembering a small fraction of the potential neighbors for each Vector first of all and then second of all we're bounding how far we search on each level but you can see how we get the logarithmic behavior from uh the graph construction of one-tenth of the nodes on each level uh the details of how we build that uh I would if you're interested I definitely encourage you to read the paper that this comes from uh it's super readable uh the core of the algorithm is just about one page uh so at its core it's it's very concise and simple and elegant um I won't go into it in more detail here except to say that you know this this is something that we can do concurrently and give you those logarithmic query results so some more details of our implementation is that this is actually based on a h s w index from leucine but we've made a few changes because leucine does not qualify for one of those requirements that I gave of when I add a value to the index it needs to be immediately queryable leucine is built around more of a bulk ingest kind of design and it has what's called a commit interval and it and that says we're going to go through and serialize all of the data that we've had in the last interval and make that available for queries but until we do that until we do that commit it is not available for queries so there's a a Time window where you know I've inserted my new record but I can't see it yet so that's not a good fit for Cassandra and so what we did is we modified uh the leucine index structure to be able to index that and query that without having a commit delay uh and as part of that we were able to handle not just doing reads instantly but doing reads while rights are happening at the same time and even multiple rights happening at the same time uh the last thing is that I mentioned that dis you know serving this index from disk is also very important to Cassandra and so we took a look at how uh the you know we cache parts of the index in memory uh and we adjusted that as well and so what what our strategy is is that uh since the top levels of the hnsw index are accessed more frequently we're going to uh preferentially store those levels in cash so that we're not having to read from disk as often so putting those together uh we have a vector index that's part of our Sai implementation uh and so you can do a simple uh give me the closest vectors anywhere in the table that's this first uh code fragment here where I'm just going to say you know I'm not specifying any other predicates I'm just saying give me the closest vectors so here's the Syntax for that I just say order by my Vector column name the column name here is embedding and then A and N of my query Vector so the question mark would be the bind variable that my query Vector gets attached to and then I can say you know how many results do I want that's the the limit 10. so that's the the simplest uh version the the next one is that uh you know the most maybe the most common kind of predicate in Cassandra is I want to limit this to a single partition of data so here I've added where partition ID equals some other Vine variable uh and so that you know that composes with the vector ordering that we get from our index and then we can also include other index predicates uh with or without a partition ID and that's what this third example shows is you know I've got that nested Boolean uh query and then I'm saying I want to order that by Vector similarity so we can do any of these all of these uh and it and it just works so our philosophy is that you know Vector search is a feature it's an important feature but it's not the only feature you want you want Vector search to play nice and be part of uh the rest of your database and compose with what the rest of your database does your app needs to do you know normal crud operations uh you know create retrieve update delete and also Vector search typically Vector embeddings are derived from the rest of your app data so the most straightforward way to deal with that is to add a vector column to your existing table and then put the embedding uh into that column instead of doing it in a separate system if you do it in a separate system you know now you have to deal with all the pain of keeping those in sync uh two different systems to administer it it's just you know this is why you know the the history of of databases has been I want to use the smallest number of systems as possible that let me accomplish my goal uh and then it helps that the ordering by Vector similarity is a natural extension of cql the Cassandra query language um and then you know uh as as part of our implementation we designed it that uh such that you know it's not going to surprise you you know reads and writes can happen concurrently that is is uh immediately queryable on insert it composes the way you'd expect with other predicates we we want we want this to just work as much as possible so a few implications for uh building your application so the first is that as as far as performance is concerned the most important variable is how large your vectors are um so I put three examples on this page uh probably the most well-known at this point I guess it's kind of the default for a lot of people now is open AI is Ada uh embeddings model and that's going to give you vectors of 1576 dimensionality uh Google's uh gecko embeddings uh which are based on their Palm architecture are going to give you half that size 768 uh and these are competitive in terms of precision like in terms of distinguishing uh actually similar pieces of text uh Gekko is competitive with Ada and so other things being equal uh I would prefer to use the gecko embeddings because every time that I'm doing a comparison as I'm navigating down that search index I'm Computing uh cosine or a DOT product of my query Vector with the vector in the index and so it's literally twice as slow to do that with 1576 Dimensions as it is with 768. I've got one more model on this page and that's the mini ilium L6 model from hugging face and that's half the size yet smaller of uh the gecko model here is where we do start to see uh worse performance on the dimension of how good is it at distinguishing these pieces of text in my opinion you should use a model like mini LM L6 carefully because if it does give you enough uh distinguishability across the vectors then great like you're you're faster and you're less expensive uh so if it solves your problem definitely use it however uh it I would not put it in the same category of you can probably just use it and have it just work like the other two larger models uh the next thing is that I you should use normalized vectors and Dot products instead of unnormalized vectors and cosines and all that means is that if you're if you're embedding vectors have been normalized to length one then we can compute the similarity as a simple dot product between the components of those vectors which is mathematically more efficient than Computing the cosine between two large vectors all of the models on the previous page they give you normalized vectors so you should use dot product and here's the syntax that you use to tell Astra that you want to create a index that uses dot product so that will give you a performance difference about 40 percent it's about 40 faster if you use dot product than cosine so that's material the reason that we don't make that the default is that if your vectors aren't normalized and we have no way of knowing ahead of time uh whether they are unless you tell us by using this syntax if your vectors aren't normalized then dot product will give you effectively nonsense and so that's why we default to cosine but in general uh you know check to make sure that your model is giving you normalized vectors and then opt into the dot product uh another thing is that you should avoid deletes if possible so if it's not possible then no it's not and uh you know Cassandra will handle it just fine however uh it does become less efficient and that's because of how the uh index Works remember that the index is basically a graph of nodes representing each vector and the neighbors of that node and so if I delete a vector uh there's no way to pull that out of the graph without causing collateral damage and so what we do is we uh leave the vector in the graph but we mark it as hey this has been deleted it's not used anymore so if you so deletes work but it does make things slow down because now I have to go and check and say is this Vector actually still valid but if you don't do deletes then we don't need to do that check and it's faster um another piece of advice is this applies to all of Cassandra not just vectors is that Cassandra's designed for concurrency uh and especially when you're adding data to the system uh I I'd say 90 of the time our inclination as programmers is to write code that looks like this where you know I'll prepare my statement because I know that that's more efficient than making Cassandra Parson uh fresh every time but then I'll go through uh every item in my collection and execute that prepared insert with that item that will work but what that means is that I'm sending a request to Cassandra I'm waiting for it to get back and then I'm sending another request waiting for it to get back and so you know I've got you know this this server of you know dozens of cores on the other side of that request and I'm just doing one at a time uh with all the latency of the network and the way of doing that so it's much more uh it's much faster if I just do those those inserts concurrently and the Cassandra drivers uh provide uh this functionality out of the box so this is an example with the python driver so I'm importing at the top I'm importing execute concurrent with args and what that means is I want to execute a single statement against all the arguments that I give it in that uh function call so I'm calling that with my session object with my prepared request and then I just give it my collection of tuples of data to apply to the bind variables in my statement and now the driver can go and do that uh you know it can it will parallelize that and issue multiple requests uh concurrently so this is kind of the tutorial version of this code in the next slide I'm going to show you uh a more realistic version because in the real world sometimes requests fail because uh the server was overloaded or because there was a network hiccup uh and so we we want to be able to to deal with that uh without starting over and so this is actually from some code I wrote uh over the weekend just playing around um and you can see a couple differences from kind of the tutorial version here and I've bolded a couple of those so the first is that when we're running that function execute concurrent with args I want to I want to specify how how many requests in parallel do you want to make that's the concurrency argument and then the second one says don't just give up if you get an error default is as soon as I get an error I'm gonna throw an exception back to you to you but that's not what we want we want you to keep going and then when you're done give me for each request did it succeed or fail so then that that next line there where I've got the list comprehension I'm saying uh go through all of those uh items that I sent the database the chunks that I sent in the database um and then uh associate it with its successor failure result then pull out the ones that failed into a new list and then we'll loop again with only the failed ones we'll retry those that's what's going on there oh another another easy uh way to improve your performance is to do your reads at consistency level of local one so the default consistency level is Quorum which means we're going to compare results from two out of three replicas but uh this is not the this is not the best thing to do in the case of vector data where we usually depends on your application but almost always in the case of vector data it's not something that you change frequently uh typically you uh you know your your vector embedding is computed based on the data in your row once and that tends to let that tends to live a long time uh and so rather than wasting time checking two replicas that are pretty much guaranteed to have the same data on them uh we'll just request that Cassandra check a single replica so that's half the work it's twice as fast uh it's just a good idea the the last tip I'm going to leave you with also depends on a little bit of understanding of how Cassandra works and I I referred to this earlier when I talked about how often when we're querying indexes uh we'll want to narrow down what the index has to search by giving it a partition ID now in Cassandra you can Partition by user you can Partition by time you can Partition by lots of different strategies but uh the bottom line is by specifying that partition uh the index doesn't have to do an exhaustive search it can limit its search to that one partition uh and so not only does that matter on an individual replica but it also means that Cassandra has to check fewer uh nodes in the Cassandra cluster because each partition is uniquely assigned to a a triple of replicas and we we know that we only need to touch those specific replicas so if if your data model is appropriate then uh giving this hint to the query is going to make your workloads more scalable as well as giving you lower latency for each of those queries and so with that uh thank you and I'll turn it back over to the team thank you [Music] thank you Jonathan that was awesome in our final session for today I got the chance to catch up with Brian kirschner VPO strategy at data Stacks the other day for a discussion on things Architects and practitioners should consider as you put your first AI agents in production we got the chance to talk about many of the issues you will no doubt be debating yourselves in your journey from security and privacy to relevance and hallucinations to the ethics of AI thank you hi I'm Sharna Parky um I'm the real-time AI product and strategy leader at data stacks and here with me I have Brian as the VPO strategy and data Stacks with a long history of working with business and technology leaders on digital transformation and AI strategy it's really good to have you here I'm very excited about this conversation glad to be here uh let's start with you um so I know that you've been thinking deeply lately about challenges and risks in this space the agent space the AI space the generative AI space and um we were talking to our CEO the other day and we know that he's talked about how we believe we reach our best future of AI for all fastest if the right people who want to do the right things in the right way can learn by doing so how can we mitigate those risks and manage challenges when so many people are working on AI yeah I think that that's a great question and I think you know it's worth thinking about this in the context of the opportunity the risks and the challenges and the precedent we have right so if you think about smartphones about 85 penetration worldwide with smartphones right yeah there's no reason why AI won't be a hundred percent worldwide and certainly in everybody's organization right so if you didn't start to think about that you know you kind of back into look AI will redefine Industries where your company sits will depend on what you do with AI we know it's going to be part of everybody's job both doing their own work and serving their internal external customers so the sooner you Embrace that and say yes it's a hundred percent of everybody's job um then you start to think about how do you do that safely you know how do you do that right and I think one pillar we think about is openness right being explicit this is the journey we're on it's all part of our jobs and we're all learning how to do it um and I think there's you know there's a couple angles on that one is we all know the pain of technical debt if we've been around in in technology you've got the chance not just to avoid technical debt as you start off on this journey by doing it right but you know use AI to document every meeting create the body of knowledge and text and data about all the questions people ask you know what harm could we do how would we know what level of trust are we trying to achieve what would have to be true to achieve that you're actually creating a body of knowledge you can point AI at right the faster you proceed by learning by doing carefully cautiously you actually build this possibility of an AI agent to help people do AI organization with your context with your customers you know in the geographies you operate that's bespoke to you um and then I think you know so many people working on it then gets back again to openness ensure everybody feels like they can do their work in the open right including surfacing a question of we're not sure how to achieve this level of trust right we don't know how we would detect this type of harm um and then it shouldn't be going alone in your organization and your organization shouldn't go it alone right like whatever vertical you're in you probably lean into understanding you know Trends in consumer marketing and participate maybe in an academic body or you know Trends in financial regulation and you go to those those bodies and you've got experts do the same for AI right in each uh company but also each function each vertical uh or each line of business lean into picking up the phone or engaging with folks um I think you know we all see and I mean we meaning business government Academia we all see the potential if you reach out to talk about what you want to do with AI as an organization and people will probably be very interested in talking to you yeah that is definitely I mean in my experience recently been the case anytime I've reached out to want to talk about it people are engaging it we're all learning this together and so but the topics like some of the words you just use safety and trustworthiness it seems like those are key where you know we have to there's there are feelings we're trying to evoke right feelings of safety feelings of trust so how do we sort of Ensure we're building for safety and for trust versus kind of doing it after the fact yes absolutely I think you know one two two other things I think about a lot are the sense of being Mission driven and context aware um and so you know every time you're going to deploy an AI you are entailing some risk right the return will be higher for those things that really deliver on your brand promise that connect to what customers really want from you right so so first off is thinking about what's going to give us the highest return for the level of risk we take right and sort of lean into what what do folks really value from us right yeah and and the second is you're going to do best probably at what you know well right so stay aware of the context are you reaching outside of the context where you've got good customer knowledge and intimacy and domain expertise um probably not the first AIS you want to deploy right figure out how you can enrich or extend what you know well um which will then Pro you know if you learn how to deliver safe trustworthy experiences in a space that's defined and scoped the tools you learn the techniques you learn right the the ways to measure and monitor you learn will probably build your skills on you know okay maybe we're now going to extend into more foreign territory but the foreign territory shouldn't be your first foray right so if we're if we're putting this in con today or listeners or architects what are some maybe more concrete actions they can take that can help yeah that's a great question I think you know the the bottom line is as a technical practitioner and architect your organization can't do this without you can't succeed at safety and Trust without you your your skills are critical but also cross-functional teams is already an established pattern for success and I think becomes even more important in when you think about generative AI where there's probabilistic interaction you know with the end user right lots of things start to matter you know everything from you know their brand perception of you to their age to their reading level to you know cultural nuances and so as a technologist you know a service you can do not only to your organization but to yourself is to really emphasize that the path to success for this type of application is a diverse team a cross-functional team is you know making wrestling with or chasing down the qualitative dimensions of things just a first order you know task and not something you've you know we figured out how to deploy a capability let's sort of shoehorn it into what we think will work like no build build from the customer build from that emotion you want to create and of course at the end of the day the prize for this is you know obviously losing trust is bad but the prize is if you build trust people will be willing to share that much more data with you over time which you know is is the route to competitive Advantage right right exactly and I mean as practitioners we all want more data for all of those answers we're trying to come up with and getting a more complete picture of you know our users and things like that right uh on that on that note uh you know I know there's actually a body of knowledge under the cognitive handle of responsible AI that you are extremely well versed in and do a lot of thinking thinking about um and I think it's important to surface that you don't have to reinvent the wheel here um you should stand on the shoulders of giants um so you tell me a little about you know how would you express your point of view on responsible Ai and what that means to folks who are listening yeah I mean we may have heard about this in in several different ways over the last couple of years right it may be responsibility or ethics or transparency accountability they're all kind of interconnected and they can't really exist in a vacuum um basically anything that we do with AI is going to impact people in process and Technology I tend to think about this instead of about you know AI like just responsibility in general because what we're trying to do is drive Behavior change some change in some outcomes so it means that we have to consider the people the behaviors trying to drive and the ethics of that in the products that we're making intended to create change so for me part of ensuring something is ethical um since there are no real like objective values is this consumable communication to the reader so what you were talking about earlier where it's not just that you can willy-nilly make some someone go do something that you want them to do that that is not ethical if it's not aligned with you know their intention or their goal or some of their values in some way and so like you said we can stand on the shoulders and Giants here and look into things like um behavioral psychology like um products that create change there are a lot of books out there that we can read about this Behavior change I mean I think most of us well I'm dating myself most of us remember um like when like using cigarettes and different things in movies were banned yes unless right we made it look cool but the the impact of the sort of promoting this was that people were like oh I'm cool that they're not thinking about like it caused me death that sort of a thing so we have to be careful with AI in that regard as well are there a couple examples that you would cite about you know what it means to to build responsibly yeah I mean I like to think kind of of responsible and irresponsible and so irresponsible examples are a little easier for me to kind of pull out so um and maybe I can just negate them as we're talking so I I would find it irresponsible especially in AI if you didn't provide some sort of feedback loop right AI is learning from the data and learning from different outcomes and so if you're not providing any mechanisms for closing that feedback loop like you're keeping um your idea of someone or the state of the world constant there's no change that would be irresponsible we know I mean just look at how things are so rapidly changing we know language changes over time we know that who a person is doesn't equate to the order of like credit rights or things like that we don't know how things are going to change and we don't know what kind of Concepts we're using today that should not be codified so it would be very irresponsible to keep sort of that feedback loop broken yeah and I love I love that example because you know if you think about you know however let's call it a manipulative system to stay in the irresponsible Zone however effective it is today um if it's not a learning system it may become less effective and now obviously the win-win is you're not manipulative but you are a learning system and you kind of miss the boat if you're not a learning system yeah yeah and I think part of this openness that you were talking about before also applies to telling the end user about how you're updating this data how you're updating the model so you you have to communicate the assumptions that you took when you built the model uh the motivations that you had in order to like change the behavior this this idea of trust um it doesn't come with secrecy right I don't know like we can think about just like teaching our children how to trust other people like it comes from a hundred thousand little interactions so that we can kind of build this up over time and that's going to be with communication that's just yes yeah and I think in the moment we're in it's actually pretty fascinating I think you're hitting on something which people should lean into because you know in the past lots of folks weren't really aware of how AI was working in the background but now because of generative AI because AI will be in every you know every Microsoft Office or every Google app your customers will understand that they're interacting with AI and you can actually disclose this information and they'll get it right and that that's uh that's an opportunity really um so to bring this back to the final question you asked me um you know what advice do you have for the practitioners and architects in our audience yeah I think in general when I think of responsibility it's a it's a shared framework we all have some responsibility in this our users our Builders the providers of the technology collectors and providers of data there's some level of responsibility and similar to what you were saying I would encourage the practitioners to understand where do they sit in terms of what they're accountable for and who they're accountable to if you're building something that's going to impact an end user are you building in mechanisms for communicating the risks to the user that's associated with using the content that's being generated with your app or maybe that what your application May prevent them from doing you know what what are you communicating and how are you letting that sort of come out in your end user experience as you're building but also part of what you're doing by exposing this information and building in a more transparent and open way is you're enabling your end users to take part in their own responsibility as part of this shared responsibility model right so you know there are ways to do this whether it's you know terms of services not the way that you're going to communicate this to your end user you need these simple experiences that are understandable that allow a user to know like hey I just saw this friend recommendation maybe you have a little pop-up that said where did this recommendation come from or how did it get here letting them opt in and out of different components of your experience and applications so yeah I would yeah mostly encourage like individualize put yourself in that ecosystem of shared trust and responsibility and I love the scoping and the contextualization because you know we've got these models now that we can interact with and have a conversation about anything that's not why your customers come to you right right don't come for some generic all-encompassing thing they come for something specific where you can hone in on oh a great feedback mechanism for this specific interaction is this because they're not here to chat about all things under the sun right I mean that brings us to the Practical example that we were using earlier in our earlier assessments we saw a demo of an AI agent that datastex built and so maybe we should go into some of the considerations that they had and learnings from that experience to kind of help contextualize this to you know the agent we did sounds great so yeah so um I think the first thing um when I spoke with a team that they were considering is like you said what's the customer experience what what part of our brand and what part of our business requirements are actually going into this agent because we knew that our agents had to be as effective as our current support and sales teams right our current human chat is critical in go to market like we are a developer Community like most of our all of our products are open source and so everything we do it's critical that we are having that that communication experience and so when we were thinking about like what the requirements are that was going to be a very important part of the brand experience as sort of that primary interaction um you know from a business perspective it's going to impact that promoter score and other things like that right but also you know as as we were thinking about that we were thinking like okay well what are the mitigation steps we need to take and to your point earlier we were we had to like limit the audience first we needed to run tests we only deployed the first assistant to like a small percentage of our audience and we put in some checks there right we had human review um so we're we're still working on this and we're going to experiment with the agent um but the the human part of that loop we're keeping intact because we need to kind of like as we expand outside of chat to writing emails and other multimodal agents we need this sort of expert review to make sure that you know the audience that we're speaking to and the brand as it's being represented is still within you know what's acceptable to us yes absolutely and I think you know there's a there's a there's a human side of cross-functional teams as a pattern for success but there's a there's another side which is you know no one goes it alone that includes the AIS yes right there's just no AI that doesn't have a steward a shepherd on an ongoing basis exactly yeah exactly okay so you know when you were thinking about that with this application what kind of things did we hit there yeah I mean so I think this the support is very interesting because as a as a SAS company a cloud company we've of course got all kinds of operational data um we've got data about the customers the customers are communicating data about their situation in the support chat and so you really have to think about you know isolating users um and and even also you know putting boundaries around around what type of user information winds up getting into the system so no Pai you know anonymizing names um you know you don't want to create traces of individual users that can be identified in the data set you're building right yeah um and you have to you know and and you know one of the things I love about the the system we're building is of course you know uh uh Apache Cassandra data Stacks asteroid can be used to give an LM memory essentially but you need to scope that context that you're using that user's history um and exclusively that user's history to provide context um and so I think the team did a pretty thoughtful job about building the the boundaries and boundary conditions around all that yeah yeah for sure and I know that there was quite a bit of work around the sort of feature engineering side of that right because you're trying to summarize you know this user's experience in history but also contextualize it into what have people at this stage of their journey journey done yeah and so that's kind of a you know like you're saying anonymization but also engineering the right features to provide context was incredibly important yes right yes absolutely um but for all the engineering we did as you mentioned humans are still in the loop in a couple different ways and we've got I think both some automated triggers um and some human interventions uh in terms of sentiments and escalation can you talk about that a little bit yeah yeah so um part of what we knew like I said before is that that Human Experience is very important and we needed to be able to know when was the right moment to escalate to a human if a user has like said something negative to the chatbot no you know certainly sometimes users are going to just play with a bot and see like what's going on right um but the difference between I think a bot and an agent is an agent knows when to escalate they've you know looked at the sentiment of what's Happening they've sort of seen a journey in there and they're like I can't handle this anymore right it's sort of this idea we talked about where it's like an agent can act on your behalf a lot but they can't they can't do everything right they they kind of know when to give it back to you um yes and we also had a little bit of um uh intense detection in there as well so if there was started to be questions around not just like how do I use this product how do I get unblocked but around like pricing and how would I actually put this into other things then we would bring them to an expert in that area because that's not what this agent was trained on right they were trained on how to assist not how to sell so yeah other kinds of triggers right so that and that that's actually a really wonderful example about respecting user intent which is we're not actually trying to use this interaction to persuade someone who's not here to buy to buy but if they wind up coming through this Channel and that's what they're trying to do right we will quickly connect them to the right resource which I think is a great example of you know the opposite of manipulative it's respecting and detecting if they're in the wrong place and helping them out right it you know in any business if you're trying to manipulate someone to buy some essentially something they need like you're going to lose that business you don't want that kind of negative experience so consider consider not doing that um yes so so hallucinations we hear a lot about them everyone's afraid of them it's a really hard problem I know we're always working on it um how have we been handling it today um I think there's you know a lot of you know prompt engineering and prompt testing um and I think there's also the humans come into play right because we are actually devoting human time and effort to reviewing and tagging hallucinations right yeah yeah I think there's a whole team in the beginning who was doing this and now we can sort of escalate the more likely hallucinations but yeah it's such a hard problem and I'm sure you know the industry is going to find better and better ways to do this but the the point of generative AI is to create new content and so you have to be pretty careful about what is real what's actually you know coming from your support docs versus something that's like it'd be cool if we had this but we don't actually have that feature today please don't tell in users about it right right right you know this this thing ties back to you know yes hallucinations are at risk of learning by doing but by learning by doing responsibly and diligently you start to understand the interaction of you know your particular data and their domain and what types of hallucinations get produced and how you can detect and prevent them and so you know as generic tools you know general purpose tools evolve in the ecosystem you'll also have a leg up on kind of understanding you know how do hallucinations surface in our context and that's better to have done that by learning by doing and including investing in some human you know oversight time than to start out from from Square zero you know down the road yeah yeah well um I know that we're about points we were going to talk about response format and relevancy and testing and metrics maybe we do that in shorter than what we wanted to because we got a little deep in some of these things I'll just like give a few points um so you don't just need to get an out of the box right we knew that we had to tune various things like temperature like the degree of Randomness versus a varied um an unexpected response and mistakes and nonsense Etc like you can tune these different settings between zero and one there were other things that we had to like EK values P values different prompt settings that controlled the dispersion in the content and the creativity of the answers um and so when you tell a prompt like you should only answer a customer questions about Cassandra data stacks and its products that means that the user isn't going to be able to ask the irrelevant questions that are not pertaining to what this agent's expertise is and so we definitely consider putting some of those guardrails on um and I know um it this is going to just continue as we start to make these specialized agents across our product experience yeah without any last remarks Brian before we let everyone go uh no it's just been a pleasure uh talking to you and I hope folks take away that you know there's work to be done there's caution that you need to be take uh but it is doable right and you can start to think about use cases and agents um and again learn by doing documenting um and the doing being the shepherding and management after the fact as well well thanks so much we'll see you guys next time Take Care thank you [Music] and that's around thanks for joining us for these sessions we hope that you've seen something here today that speeds up your journey into generative AI architecture and development I think the big takeaway from today is the next wave of generative AI applications are being built today and the time to start exploring agent architecture is now it's an incredibly exciting time and you can expect us to share more Deep dive content demos and code with you over the following months and you can start building agents affect your search on asker DB now just go to askra.datastacks.com it's free we want to hear from you find us on social email us let us know what you're building",
    "segments": [
      {
        "start": 0.42,
        "duration": 5.1,
        "text": "hi I'm Dr sharnaparky real-time AI"
      },
      {
        "start": 3.54,
        "duration": 4.5,
        "text": "product and strategy leader at data"
      },
      {
        "start": 5.52,
        "duration": 4.02,
        "text": "Stacks welcome to Agent X we wanted to"
      },
      {
        "start": 8.04,
        "duration": 3.599,
        "text": "bring you together today because we"
      },
      {
        "start": 9.54,
        "duration": 3.9,
        "text": "think generative AI is the biggest"
      },
      {
        "start": 11.639,
        "duration": 5.101,
        "text": "platform shift since the original"
      },
      {
        "start": 13.44,
        "duration": 5.58,
        "text": "internet in the mid 90s may be bigger"
      },
      {
        "start": 16.74,
        "duration": 3.78,
        "text": "we're inspired by the incredible speed"
      },
      {
        "start": 19.02,
        "duration": 3.36,
        "text": "and excitement around what we're seeing"
      },
      {
        "start": 20.52,
        "duration": 4.679,
        "text": "today with emerging applications and"
      },
      {
        "start": 22.38,
        "duration": 4.979,
        "text": "architectures so today we will pose the"
      },
      {
        "start": 25.199,
        "duration": 4.381,
        "text": "question if you were to build the next"
      },
      {
        "start": 27.359,
        "duration": 3.121,
        "text": "Salesforce for Shopify how will you"
      },
      {
        "start": 29.58,
        "duration": 2.64,
        "text": "build it"
      },
      {
        "start": 30.48,
        "duration": 4.44,
        "text": "we think that you choose to build it"
      },
      {
        "start": 32.22,
        "duration": 4.56,
        "text": "with generative Ai and we think that you"
      },
      {
        "start": 34.92,
        "duration": 3.18,
        "text": "soon might choose to build it as an AI"
      },
      {
        "start": 36.78,
        "duration": 3.24,
        "text": "agent"
      },
      {
        "start": 38.1,
        "duration": 4.44,
        "text": "so what is the Asian experience look"
      },
      {
        "start": 40.02,
        "duration": 4.859,
        "text": "like what's the architecture how do we"
      },
      {
        "start": 42.54,
        "duration": 3.839,
        "text": "use llms with our own real-time and"
      },
      {
        "start": 44.879,
        "duration": 4.5,
        "text": "historical data"
      },
      {
        "start": 46.379,
        "duration": 5.941,
        "text": "how are the lens changing the stock and"
      },
      {
        "start": 49.379,
        "duration": 5.761,
        "text": "how are they a power tool both during"
      },
      {
        "start": 52.32,
        "duration": 4.739,
        "text": "the development time and run time"
      },
      {
        "start": 55.14,
        "duration": 5.04,
        "text": "in the next few hours we are going to"
      },
      {
        "start": 57.059,
        "duration": 5.401,
        "text": "unpack the emerging generative Ai and LM"
      },
      {
        "start": 60.18,
        "duration": 4.08,
        "text": "architecture where you can create highly"
      },
      {
        "start": 62.46,
        "duration": 3.839,
        "text": "personalized and compelling agent"
      },
      {
        "start": 64.26,
        "duration": 4.859,
        "text": "experiences with your own real-time and"
      },
      {
        "start": 66.299,
        "duration": 5.101,
        "text": "historical data in our discussions just"
      },
      {
        "start": 69.119,
        "duration": 4.32,
        "text": "about every architect and practitioner"
      },
      {
        "start": 71.4,
        "duration": 5.7,
        "text": "is thinking about how to build genitive"
      },
      {
        "start": 73.439,
        "duration": 6.481,
        "text": "AI apps it's happening now"
      },
      {
        "start": 77.1,
        "duration": 5.159,
        "text": "so this is not going to be just Theory"
      },
      {
        "start": 79.92,
        "duration": 4.86,
        "text": "we'll give you real demos of real"
      },
      {
        "start": 82.259,
        "duration": 4.321,
        "text": "production AI agents and before we get"
      },
      {
        "start": 84.78,
        "duration": 4.14,
        "text": "started here's a quick summary of our"
      },
      {
        "start": 86.58,
        "duration": 3.84,
        "text": "two hour agenda and we'll build a couple"
      },
      {
        "start": 88.92,
        "duration": 4.199,
        "text": "of short breaks in there so don't worry"
      },
      {
        "start": 90.42,
        "duration": 4.26,
        "text": "first we'll talk about the AI stock and"
      },
      {
        "start": 93.119,
        "duration": 4.621,
        "text": "emerging architectures with our chief"
      },
      {
        "start": 94.68,
        "duration": 5.52,
        "text": "product officer Ed enough then Alan ho"
      },
      {
        "start": 97.74,
        "duration": 4.559,
        "text": "and Alex leventer are AI strategy and"
      },
      {
        "start": 100.2,
        "duration": 3.959,
        "text": "development leaders will take you on a"
      },
      {
        "start": 102.299,
        "duration": 4.86,
        "text": "journey and how we built our own"
      },
      {
        "start": 104.159,
        "duration": 5.161,
        "text": "production generative AI agent along the"
      },
      {
        "start": 107.159,
        "duration": 4.92,
        "text": "way we'll Deep dive into Lessons Learned"
      },
      {
        "start": 109.32,
        "duration": 5.759,
        "text": "on data engineering with Technologies"
      },
      {
        "start": 112.079,
        "duration": 6.36,
        "text": "like vector search embeddings real world"
      },
      {
        "start": 115.079,
        "duration": 5.701,
        "text": "prompt engineering and testing I'd send"
      },
      {
        "start": 118.439,
        "duration": 4.621,
        "text": "prompt retrieval and inference then"
      },
      {
        "start": 120.78,
        "duration": 4.26,
        "text": "Jonathan Ellis founder of data sacks"
      },
      {
        "start": 123.06,
        "duration": 4.019,
        "text": "will do a deep dive into how you can"
      },
      {
        "start": 125.04,
        "duration": 4.439,
        "text": "maximize performance on Vector search"
      },
      {
        "start": 127.079,
        "duration": 4.981,
        "text": "and really get under the hood on how it"
      },
      {
        "start": 129.479,
        "duration": 4.26,
        "text": "works finally myself and Brian kirschner"
      },
      {
        "start": 132.06,
        "duration": 3.179,
        "text": "our VP of strategy will lead a"
      },
      {
        "start": 133.739,
        "duration": 3.541,
        "text": "discussion on what you should think"
      },
      {
        "start": 135.239,
        "duration": 3.72,
        "text": "about before they before putting AI"
      },
      {
        "start": 137.28,
        "duration": 3.319,
        "text": "agents into production"
      },
      {
        "start": 138.959,
        "duration": 4.981,
        "text": "want to think about hallucinations"
      },
      {
        "start": 140.599,
        "duration": 6.701,
        "text": "privacy security and other emerging best"
      },
      {
        "start": 143.94,
        "duration": 6.24,
        "text": "practices so again welcome to Agent X"
      },
      {
        "start": 147.3,
        "duration": 4.799,
        "text": "the agent AI experience and now let's"
      },
      {
        "start": 150.18,
        "duration": 4.5,
        "text": "get started and let me introduce you to"
      },
      {
        "start": 152.099,
        "duration": 5.061,
        "text": "our chief product officer at enough take"
      },
      {
        "start": 154.68,
        "duration": 2.48,
        "text": "it away Ed"
      },
      {
        "start": 157.319,
        "duration": 6.201,
        "text": "foreign"
      },
      {
        "start": 159.22,
        "duration": 4.3,
        "text": "[Music]"
      },
      {
        "start": 164.84,
        "duration": 4.96,
        "text": "the chief product officer of data stacks"
      },
      {
        "start": 167.94,
        "duration": 4.439,
        "text": "today I'm going to talk to you about how"
      },
      {
        "start": 169.8,
        "duration": 4.5,
        "text": "generative AI has completely redefined"
      },
      {
        "start": 172.379,
        "duration": 3.421,
        "text": "the software stack for building"
      },
      {
        "start": 174.3,
        "duration": 3.42,
        "text": "applications"
      },
      {
        "start": 175.8,
        "duration": 4.32,
        "text": "in many ways the disruption that we're"
      },
      {
        "start": 177.72,
        "duration": 5.879,
        "text": "seeing is the most pronounced we've seen"
      },
      {
        "start": 180.12,
        "duration": 5.699,
        "text": "since 1994. when every company needed to"
      },
      {
        "start": 183.599,
        "duration": 4.321,
        "text": "get online needed to build websites"
      },
      {
        "start": 185.819,
        "duration": 4.621,
        "text": "needed to build e-commerce experiences"
      },
      {
        "start": 187.92,
        "duration": 6.36,
        "text": "now every company is trying to figure"
      },
      {
        "start": 190.44,
        "duration": 6.48,
        "text": "out how to build generative AI into"
      },
      {
        "start": 194.28,
        "duration": 4.2,
        "text": "every customer every partner every"
      },
      {
        "start": 196.92,
        "duration": 2.7,
        "text": "employee experience that they're"
      },
      {
        "start": 198.48,
        "duration": 3.179,
        "text": "delivering"
      },
      {
        "start": 199.62,
        "duration": 5.88,
        "text": "we can see this with the growth of chat"
      },
      {
        "start": 201.659,
        "duration": 6.421,
        "text": "GPT plugins we've all used chat GPT and"
      },
      {
        "start": 205.5,
        "duration": 5.28,
        "text": "recently they've made it available for"
      },
      {
        "start": 208.08,
        "duration": 6.0,
        "text": "developers to extend building plugins"
      },
      {
        "start": 210.78,
        "duration": 5.7,
        "text": "that hook into the chat gbt experience"
      },
      {
        "start": 214.08,
        "duration": 4.2,
        "text": "and allow you to go and couple it with"
      },
      {
        "start": 216.48,
        "duration": 4.5,
        "text": "external services"
      },
      {
        "start": 218.28,
        "duration": 6.3,
        "text": "and hundreds of developers have gone and"
      },
      {
        "start": 220.98,
        "duration": 8.46,
        "text": "built plugins and we can see here the"
      },
      {
        "start": 224.58,
        "duration": 6.239,
        "text": "growth of the chat gbt plug-in ecosystem"
      },
      {
        "start": 229.44,
        "duration": 3.719,
        "text": "now"
      },
      {
        "start": 230.819,
        "duration": 7.2,
        "text": "in many ways what we've seen with chat"
      },
      {
        "start": 233.159,
        "duration": 8.521,
        "text": "GPT is the emergence of generative AI as"
      },
      {
        "start": 238.019,
        "duration": 5.64,
        "text": "the latest step in a number of advances"
      },
      {
        "start": 241.68,
        "duration": 4.199,
        "text": "that we've seen within AI"
      },
      {
        "start": 243.659,
        "duration": 4.8,
        "text": "we saw deep learning machine learning"
      },
      {
        "start": 245.879,
        "duration": 5.961,
        "text": "become a very big deal a few years ago"
      },
      {
        "start": 248.459,
        "duration": 6.721,
        "text": "many of us started to research and learn"
      },
      {
        "start": 251.84,
        "duration": 5.92,
        "text": "this technology many of us have taken"
      },
      {
        "start": 255.18,
        "duration": 4.32,
        "text": "the lessons from Coursera and read all"
      },
      {
        "start": 257.76,
        "duration": 6.18,
        "text": "of the great O'Reilly books and all of"
      },
      {
        "start": 259.5,
        "duration": 8.34,
        "text": "that predictive AI was what we used this"
      },
      {
        "start": 263.94,
        "duration": 5.52,
        "text": "technology for how to figure out what a"
      },
      {
        "start": 267.84,
        "duration": 4.38,
        "text": "next best action would be or how to"
      },
      {
        "start": 269.46,
        "duration": 4.5,
        "text": "identify from an image whether it's a"
      },
      {
        "start": 272.22,
        "duration": 3.84,
        "text": "picture of a cat for example"
      },
      {
        "start": 273.96,
        "duration": 5.1,
        "text": "but along those same lines we saw"
      },
      {
        "start": 276.06,
        "duration": 5.88,
        "text": "generative AI emerge starting from text"
      },
      {
        "start": 279.06,
        "duration": 7.1,
        "text": "completion and text prediction to"
      },
      {
        "start": 281.94,
        "duration": 8.039,
        "text": "suddenly with the emergence of GPT gpt3"
      },
      {
        "start": 286.16,
        "duration": 6.96,
        "text": "and now most recently gpt4 where we're"
      },
      {
        "start": 289.979,
        "duration": 6.601,
        "text": "able to create conversational"
      },
      {
        "start": 293.12,
        "duration": 6.46,
        "text": "experiences that are really remarkable"
      },
      {
        "start": 296.58,
        "duration": 5.16,
        "text": "and we're seeing the next Generations of"
      },
      {
        "start": 299.58,
        "duration": 5.24,
        "text": "that we're seeing things like Lang chain"
      },
      {
        "start": 301.74,
        "duration": 6.48,
        "text": "being used to build more complicated"
      },
      {
        "start": 304.82,
        "duration": 5.86,
        "text": "language interfaces towards the creation"
      },
      {
        "start": 308.22,
        "duration": 3.84,
        "text": "of autonomous agents at the same time"
      },
      {
        "start": 310.68,
        "duration": 3.959,
        "text": "we're seeing a lot of optimization"
      },
      {
        "start": 312.06,
        "duration": 5.699,
        "text": "happening where models are actually"
      },
      {
        "start": 314.639,
        "duration": 4.861,
        "text": "running on your device so a lot of"
      },
      {
        "start": 317.759,
        "duration": 4.5,
        "text": "exciting stuff has happened a lot of"
      },
      {
        "start": 319.5,
        "duration": 4.32,
        "text": "development that has happened to get"
      },
      {
        "start": 322.259,
        "duration": 3.541,
        "text": "here in fact if we were to look at the"
      },
      {
        "start": 323.82,
        "duration": 5.099,
        "text": "entire history of AI this stretches Back"
      },
      {
        "start": 325.8,
        "duration": 5.22,
        "text": "40 or more years but certainly in the"
      },
      {
        "start": 328.919,
        "duration": 3.901,
        "text": "last decade we've started to see this"
      },
      {
        "start": 331.02,
        "duration": 4.679,
        "text": "accelerate and I think anybody who's"
      },
      {
        "start": 332.82,
        "duration": 4.379,
        "text": "following social media feeds or reading"
      },
      {
        "start": 335.699,
        "duration": 3.121,
        "text": "the latest developments have seen just"
      },
      {
        "start": 337.199,
        "duration": 5.161,
        "text": "how fast it's moving now"
      },
      {
        "start": 338.82,
        "duration": 6.12,
        "text": "so what's exciting to us is how we use"
      },
      {
        "start": 342.36,
        "duration": 4.5,
        "text": "this to build applications because the"
      },
      {
        "start": 344.94,
        "duration": 4.56,
        "text": "technology itself is very interesting"
      },
      {
        "start": 346.86,
        "duration": 6.119,
        "text": "but what's really cool is putting this"
      },
      {
        "start": 349.5,
        "duration": 5.94,
        "text": "to work in a way that users can actually"
      },
      {
        "start": 352.979,
        "duration": 5.821,
        "text": "experience and can benefit from"
      },
      {
        "start": 355.44,
        "duration": 6.02,
        "text": "and so as we look at that let's take a"
      },
      {
        "start": 358.8,
        "duration": 5.76,
        "text": "look at the application stack and see"
      },
      {
        "start": 361.46,
        "duration": 6.579,
        "text": "how it's being reinvented how every"
      },
      {
        "start": 364.56,
        "duration": 6.419,
        "text": "stage of it is being changed by the"
      },
      {
        "start": 368.039,
        "duration": 5.521,
        "text": "emergence and application of generative"
      },
      {
        "start": 370.979,
        "duration": 4.801,
        "text": "AI so let's take a look at how"
      },
      {
        "start": 373.56,
        "duration": 4.62,
        "text": "generative AI has redefined the"
      },
      {
        "start": 375.78,
        "duration": 4.979,
        "text": "application here if we look at every"
      },
      {
        "start": 378.18,
        "duration": 5.459,
        "text": "level of the stack everything from the"
      },
      {
        "start": 380.759,
        "duration": 5.28,
        "text": "user experience through business logic"
      },
      {
        "start": 383.639,
        "duration": 4.741,
        "text": "through the integration tier through the"
      },
      {
        "start": 386.039,
        "duration": 5.461,
        "text": "data tier we can see that generative AI"
      },
      {
        "start": 388.38,
        "duration": 5.099,
        "text": "is transforming every piece of the stack"
      },
      {
        "start": 391.5,
        "duration": 5.28,
        "text": "at the ux tier"
      },
      {
        "start": 393.479,
        "duration": 5.34,
        "text": "generative AI is not just powering the"
      },
      {
        "start": 396.78,
        "duration": 3.84,
        "text": "textual interaction a lot of people look"
      },
      {
        "start": 398.819,
        "duration": 3.841,
        "text": "at"
      },
      {
        "start": 400.62,
        "duration": 4.199,
        "text": "generative Ai and they go and they say"
      },
      {
        "start": 402.66,
        "duration": 3.78,
        "text": "look I don't like chat interfaces well"
      },
      {
        "start": 404.819,
        "duration": 4.201,
        "text": "the reality is you're actually going to"
      },
      {
        "start": 406.44,
        "duration": 7.08,
        "text": "see graphical user interfaces you see"
      },
      {
        "start": 409.02,
        "duration": 7.92,
        "text": "image recognition in addition to actual"
      },
      {
        "start": 413.52,
        "duration": 7.26,
        "text": "voice as well as chat all able to be"
      },
      {
        "start": 416.94,
        "duration": 5.819,
        "text": "used as inputs and outputs at the ux"
      },
      {
        "start": 420.78,
        "duration": 4.44,
        "text": "tier so this isn't going to just be chat"
      },
      {
        "start": 422.759,
        "duration": 5.701,
        "text": "Bots or chat agents although many of the"
      },
      {
        "start": 425.22,
        "duration": 5.46,
        "text": "early applications do take that form"
      },
      {
        "start": 428.46,
        "duration": 5.1,
        "text": "at the business logic level we see the"
      },
      {
        "start": 430.68,
        "duration": 5.639,
        "text": "ability to actually go and Define the"
      },
      {
        "start": 433.56,
        "duration": 5.34,
        "text": "business actions in more of a do what I"
      },
      {
        "start": 436.319,
        "duration": 6.561,
        "text": "mean format versus actually having to go"
      },
      {
        "start": 438.9,
        "duration": 6.66,
        "text": "and code it explicit uh"
      },
      {
        "start": 442.88,
        "duration": 4.719,
        "text": "programmatic rules for how the"
      },
      {
        "start": 445.56,
        "duration": 4.62,
        "text": "application works at the integration"
      },
      {
        "start": 447.599,
        "duration": 4.741,
        "text": "tier we've seen examples of this with"
      },
      {
        "start": 450.18,
        "duration": 4.5,
        "text": "the way that chatgpt plugins are built"
      },
      {
        "start": 452.34,
        "duration": 4.979,
        "text": "where they're able to couple directly to"
      },
      {
        "start": 454.68,
        "duration": 5.4,
        "text": "API definitions so the integration tier"
      },
      {
        "start": 457.319,
        "duration": 5.1,
        "text": "being able to pull in data and connect"
      },
      {
        "start": 460.08,
        "duration": 5.28,
        "text": "to other services is actually going to"
      },
      {
        "start": 462.419,
        "duration": 5.461,
        "text": "be more powerful and easier to do than"
      },
      {
        "start": 465.36,
        "duration": 4.679,
        "text": "it ever has been in previous generations"
      },
      {
        "start": 467.88,
        "duration": 4.14,
        "text": "of software development and then finally"
      },
      {
        "start": 470.039,
        "duration": 5.041,
        "text": "in order for this stall to be possible"
      },
      {
        "start": 472.02,
        "duration": 6.42,
        "text": "you need a very robust high performance"
      },
      {
        "start": 475.08,
        "duration": 6.54,
        "text": "data tier that's actually able to go and"
      },
      {
        "start": 478.44,
        "duration": 7.319,
        "text": "speak the native language of llms which"
      },
      {
        "start": 481.62,
        "duration": 6.359,
        "text": "is vectors so that you can have your"
      },
      {
        "start": 485.759,
        "duration": 5.761,
        "text": "large language models retrieve the data"
      },
      {
        "start": 487.979,
        "duration": 7.021,
        "text": "that they need as they need it so"
      },
      {
        "start": 491.52,
        "duration": 5.7,
        "text": "from an end standpoint we see that the"
      },
      {
        "start": 495.0,
        "duration": 3.419,
        "text": "stack is has been pretty significantly"
      },
      {
        "start": 497.22,
        "duration": 2.4,
        "text": "impacted"
      },
      {
        "start": 498.419,
        "duration": 3.9,
        "text": "so"
      },
      {
        "start": 499.62,
        "duration": 4.5,
        "text": "let's take a look at how that's"
      },
      {
        "start": 502.319,
        "duration": 6.361,
        "text": "typically used from an architectural"
      },
      {
        "start": 504.12,
        "duration": 7.68,
        "text": "standpoint so the typical way that an"
      },
      {
        "start": 508.68,
        "duration": 4.799,
        "text": "application that uses llms is built is"
      },
      {
        "start": 511.8,
        "duration": 3.419,
        "text": "using something called retrieval"
      },
      {
        "start": 513.479,
        "duration": 5.041,
        "text": "augmented generation"
      },
      {
        "start": 515.219,
        "duration": 5.521,
        "text": "and what this means is that when I'm"
      },
      {
        "start": 518.52,
        "duration": 5.939,
        "text": "building the context that I'm sending to"
      },
      {
        "start": 520.74,
        "duration": 6.12,
        "text": "the llm I actually am using existing"
      },
      {
        "start": 524.459,
        "duration": 4.44,
        "text": "data sources many of them are going to"
      },
      {
        "start": 526.86,
        "duration": 3.96,
        "text": "be within databases although they can be"
      },
      {
        "start": 528.899,
        "duration": 3.601,
        "text": "also retrieved from apis and other"
      },
      {
        "start": 530.82,
        "duration": 5.4,
        "text": "sources"
      },
      {
        "start": 532.5,
        "duration": 4.86,
        "text": "that context is what provides memory to"
      },
      {
        "start": 536.22,
        "duration": 4.559,
        "text": "the llm"
      },
      {
        "start": 537.36,
        "duration": 6.74,
        "text": "and also is what makes it personalized"
      },
      {
        "start": 540.779,
        "duration": 7.581,
        "text": "it's what allows it to provide real-time"
      },
      {
        "start": 544.1,
        "duration": 7.54,
        "text": "relevant responses llms are stateless"
      },
      {
        "start": 548.36,
        "duration": 5.5,
        "text": "they don't learn except at training time"
      },
      {
        "start": 551.64,
        "duration": 5.94,
        "text": "and that training time may have happened"
      },
      {
        "start": 553.86,
        "duration": 6.539,
        "text": "days weeks months even years ago"
      },
      {
        "start": 557.58,
        "duration": 6.24,
        "text": "so how is it possible that an llm is"
      },
      {
        "start": 560.399,
        "duration": 5.821,
        "text": "able to give you a response that might"
      },
      {
        "start": 563.82,
        "duration": 5.28,
        "text": "have the latest weather as of this"
      },
      {
        "start": 566.22,
        "duration": 4.08,
        "text": "morning or stock prices from 15 minutes"
      },
      {
        "start": 569.1,
        "duration": 3.06,
        "text": "ago"
      },
      {
        "start": 570.3,
        "duration": 4.2,
        "text": "retrieval augmented generation is what"
      },
      {
        "start": 572.16,
        "duration": 6.0,
        "text": "makes it possible that data the data"
      },
      {
        "start": 574.5,
        "duration": 6.18,
        "text": "that the llm needs is retrieved real"
      },
      {
        "start": 578.16,
        "duration": 4.32,
        "text": "time and passed into the context"
      },
      {
        "start": 580.68,
        "duration": 3.24,
        "text": "and then the llm"
      },
      {
        "start": 582.48,
        "duration": 5.52,
        "text": "takes that"
      },
      {
        "start": 583.92,
        "duration": 5.58,
        "text": "and uses it as a crafts the response"
      },
      {
        "start": 588.0,
        "duration": 4.32,
        "text": "so"
      },
      {
        "start": 589.5,
        "duration": 5.399,
        "text": "part of how we build"
      },
      {
        "start": 592.32,
        "duration": 5.16,
        "text": "a generative AI application is typically"
      },
      {
        "start": 594.899,
        "duration": 5.581,
        "text": "to use one or more models"
      },
      {
        "start": 597.48,
        "duration": 4.799,
        "text": "you may use a small data set which I"
      },
      {
        "start": 600.48,
        "duration": 5.76,
        "text": "just talked about that data set in fact"
      },
      {
        "start": 602.279,
        "duration": 5.521,
        "text": "might be real-time data that comes from"
      },
      {
        "start": 606.24,
        "duration": 5.279,
        "text": "your databases"
      },
      {
        "start": 607.8,
        "duration": 6.719,
        "text": "and you might have a smaller model which"
      },
      {
        "start": 611.519,
        "duration": 5.641,
        "text": "pre-processes that data and determines"
      },
      {
        "start": 614.519,
        "duration": 4.5,
        "text": "what additional data is required"
      },
      {
        "start": 617.16,
        "duration": 4.14,
        "text": "that then goes into the larger"
      },
      {
        "start": 619.019,
        "duration": 4.801,
        "text": "foundational model which perhaps you've"
      },
      {
        "start": 621.3,
        "duration": 4.26,
        "text": "tuned for your use cases but many times"
      },
      {
        "start": 623.82,
        "duration": 2.82,
        "text": "that's not even necessary"
      },
      {
        "start": 625.56,
        "duration": 4.92,
        "text": "but"
      },
      {
        "start": 626.64,
        "duration": 6.36,
        "text": "you're providing a robust set of context"
      },
      {
        "start": 630.48,
        "duration": 5.76,
        "text": "that comes from your data sources"
      },
      {
        "start": 633.0,
        "duration": 6.6,
        "text": "that's processed by one or more models"
      },
      {
        "start": 636.24,
        "duration": 5.88,
        "text": "that gets augmented as necessary using"
      },
      {
        "start": 639.6,
        "duration": 4.2,
        "text": "Vector retrieval from your database"
      },
      {
        "start": 642.12,
        "duration": 4.14,
        "text": "and then goes into these foundational"
      },
      {
        "start": 643.8,
        "duration": 5.94,
        "text": "models you often hear about Technologies"
      },
      {
        "start": 646.26,
        "duration": 5.639,
        "text": "like Lang chain that are used to make"
      },
      {
        "start": 649.74,
        "duration": 4.56,
        "text": "this possible as the name would imply"
      },
      {
        "start": 651.899,
        "duration": 5.12,
        "text": "you typically have"
      },
      {
        "start": 654.3,
        "duration": 5.7,
        "text": "two or more models that go into the"
      },
      {
        "start": 657.019,
        "duration": 4.781,
        "text": "processing and generation of a response"
      },
      {
        "start": 660.0,
        "duration": 3.66,
        "text": "before it goes to the users so these"
      },
      {
        "start": 661.8,
        "duration": 5.159,
        "text": "models are chained together"
      },
      {
        "start": 663.66,
        "duration": 7.16,
        "text": "they build and augment context and then"
      },
      {
        "start": 666.959,
        "duration": 3.861,
        "text": "deliver that personalized response"
      },
      {
        "start": 671.7,
        "duration": 5.16,
        "text": "one of the things that I that I did just"
      },
      {
        "start": 673.38,
        "duration": 5.579,
        "text": "touch upon was that llms are stateless I"
      },
      {
        "start": 676.86,
        "duration": 3.06,
        "text": "often hear about people going and saying"
      },
      {
        "start": 678.959,
        "duration": 3.481,
        "text": "oh"
      },
      {
        "start": 679.92,
        "duration": 4.5,
        "text": "you know gpt4 it's going to steal your"
      },
      {
        "start": 682.44,
        "duration": 4.38,
        "text": "information be careful what you give it"
      },
      {
        "start": 684.42,
        "duration": 4.38,
        "text": "well the model itself isn't going to"
      },
      {
        "start": 686.82,
        "duration": 5.459,
        "text": "steal anything the model itself has no"
      },
      {
        "start": 688.8,
        "duration": 6.539,
        "text": "memory as I said it is frozen in time"
      },
      {
        "start": 692.279,
        "duration": 5.821,
        "text": "from the point that it's been trained"
      },
      {
        "start": 695.339,
        "duration": 7.381,
        "text": "but clearly we've all experienced using"
      },
      {
        "start": 698.1,
        "duration": 6.479,
        "text": "chat GPT and other you know agents that"
      },
      {
        "start": 702.72,
        "duration": 3.72,
        "text": "they do have a memory well the memory"
      },
      {
        "start": 704.579,
        "duration": 4.26,
        "text": "comes from the database"
      },
      {
        "start": 706.44,
        "duration": 3.72,
        "text": "so it's the database that remembers what"
      },
      {
        "start": 708.839,
        "duration": 5.041,
        "text": "you input"
      },
      {
        "start": 710.16,
        "duration": 6.84,
        "text": "and as we're using databases with llms"
      },
      {
        "start": 713.88,
        "duration": 5.94,
        "text": "one of the things that makes them more"
      },
      {
        "start": 717.0,
        "duration": 4.92,
        "text": "effective is if the database is able to"
      },
      {
        "start": 719.82,
        "duration": 4.32,
        "text": "query on vectors"
      },
      {
        "start": 721.92,
        "duration": 4.4,
        "text": "if we think about what a vector is a"
      },
      {
        "start": 724.14,
        "duration": 4.62,
        "text": "vector is a mathematical representation"
      },
      {
        "start": 726.32,
        "duration": 6.94,
        "text": "of a concept"
      },
      {
        "start": 728.76,
        "duration": 6.96,
        "text": "it basically goes and takes whatever the"
      },
      {
        "start": 733.26,
        "duration": 5.519,
        "text": "topic is whatever the content is and"
      },
      {
        "start": 735.72,
        "duration": 5.34,
        "text": "Maps it in multiple dimensions and the"
      },
      {
        "start": 738.779,
        "duration": 4.8,
        "text": "important thing about exploding this"
      },
      {
        "start": 741.06,
        "duration": 4.56,
        "text": "data out or embedding it in a"
      },
      {
        "start": 743.579,
        "duration": 3.781,
        "text": "dimensional space is that you can"
      },
      {
        "start": 745.62,
        "duration": 4.08,
        "text": "compare the distance from any two"
      },
      {
        "start": 747.36,
        "duration": 4.26,
        "text": "positions in space"
      },
      {
        "start": 749.7,
        "duration": 4.92,
        "text": "and those positions"
      },
      {
        "start": 751.62,
        "duration": 6.3,
        "text": "determine the distance"
      },
      {
        "start": 754.62,
        "duration": 6.6,
        "text": "goes into determining the the similarity"
      },
      {
        "start": 757.92,
        "duration": 6.12,
        "text": "so if I am looking conceptually for"
      },
      {
        "start": 761.22,
        "duration": 5.22,
        "text": "something if I am shopping and I am"
      },
      {
        "start": 764.04,
        "duration": 5.34,
        "text": "looking for for"
      },
      {
        "start": 766.44,
        "duration": 5.519,
        "text": "you know one type of product and another"
      },
      {
        "start": 769.38,
        "duration": 4.86,
        "text": "product is very similar to it they're"
      },
      {
        "start": 771.959,
        "duration": 3.361,
        "text": "going to be close similar in dimensional"
      },
      {
        "start": 774.24,
        "duration": 5.099,
        "text": "space"
      },
      {
        "start": 775.32,
        "duration": 5.34,
        "text": "and llms typically represent their"
      },
      {
        "start": 779.339,
        "duration": 4.081,
        "text": "knowledge particularly when they're"
      },
      {
        "start": 780.66,
        "duration": 4.56,
        "text": "inputting and outputting uh either the"
      },
      {
        "start": 783.42,
        "duration": 4.68,
        "text": "taking in context or outputting a"
      },
      {
        "start": 785.22,
        "duration": 4.32,
        "text": "response in the form of these vectors so"
      },
      {
        "start": 788.1,
        "duration": 3.359,
        "text": "you can take those vectors and use them"
      },
      {
        "start": 789.54,
        "duration": 4.38,
        "text": "to look something up if you have a"
      },
      {
        "start": 791.459,
        "duration": 5.041,
        "text": "product catalog for example that's in a"
      },
      {
        "start": 793.92,
        "duration": 5.4,
        "text": "vector database you can now look up your"
      },
      {
        "start": 796.5,
        "duration": 6.06,
        "text": "products by concept"
      },
      {
        "start": 799.32,
        "duration": 5.4,
        "text": "is it summertime outdoor furniture"
      },
      {
        "start": 802.56,
        "duration": 4.32,
        "text": "is it indoor living room furniture"
      },
      {
        "start": 804.72,
        "duration": 4.38,
        "text": "obviously there are different concepts"
      },
      {
        "start": 806.88,
        "duration": 4.5,
        "text": "associated with those and so if I'm"
      },
      {
        "start": 809.1,
        "duration": 4.38,
        "text": "going and saying recommend to me"
      },
      {
        "start": 811.38,
        "duration": 5.1,
        "text": "furniture for my living room"
      },
      {
        "start": 813.48,
        "duration": 4.62,
        "text": "it's going to be able to go and find a"
      },
      {
        "start": 816.48,
        "duration": 3.599,
        "text": "set of furniture that's appropriate for"
      },
      {
        "start": 818.1,
        "duration": 5.22,
        "text": "that versus what perhaps I want to go"
      },
      {
        "start": 820.079,
        "duration": 6.661,
        "text": "and put out in my backyard by the pool"
      },
      {
        "start": 823.32,
        "duration": 5.519,
        "text": "so this allows me to take the data sets"
      },
      {
        "start": 826.74,
        "duration": 5.099,
        "text": "that I already have that I've Vector"
      },
      {
        "start": 828.839,
        "duration": 5.101,
        "text": "encoded and use them"
      },
      {
        "start": 831.839,
        "duration": 3.901,
        "text": "with my llms it's an important"
      },
      {
        "start": 833.94,
        "duration": 4.139,
        "text": "capability that makes it possible to"
      },
      {
        "start": 835.74,
        "duration": 5.46,
        "text": "build these types of applications"
      },
      {
        "start": 838.079,
        "duration": 6.661,
        "text": "so typically when I build this out I do"
      },
      {
        "start": 841.2,
        "duration": 7.68,
        "text": "this in an architecture that"
      },
      {
        "start": 844.74,
        "duration": 6.599,
        "text": "leverages Vector databases leverages an"
      },
      {
        "start": 848.88,
        "duration": 4.68,
        "text": "embedding API that allows me to convert"
      },
      {
        "start": 851.339,
        "duration": 5.041,
        "text": "unstructured content perhaps a question"
      },
      {
        "start": 853.56,
        "duration": 5.399,
        "text": "from the user into a vector that I can"
      },
      {
        "start": 856.38,
        "duration": 5.639,
        "text": "be used for looking things up"
      },
      {
        "start": 858.959,
        "duration": 4.801,
        "text": "and I'm delivering that at runtime you"
      },
      {
        "start": 862.019,
        "duration": 3.361,
        "text": "know microservice based application that"
      },
      {
        "start": 863.76,
        "duration": 4.68,
        "text": "can be used to deliver application"
      },
      {
        "start": 865.38,
        "duration": 4.139,
        "text": "experiences wherever my users are"
      },
      {
        "start": 868.44,
        "duration": 2.639,
        "text": "interacting"
      },
      {
        "start": 869.519,
        "duration": 4.62,
        "text": "whether it's mobile apps whether it's"
      },
      {
        "start": 871.079,
        "duration": 5.581,
        "text": "web apps however they are interacting"
      },
      {
        "start": 874.139,
        "duration": 4.621,
        "text": "with my systems because llms typically"
      },
      {
        "start": 876.66,
        "duration": 4.38,
        "text": "are powering not exclusively but"
      },
      {
        "start": 878.76,
        "duration": 3.66,
        "text": "typically are powering interactive"
      },
      {
        "start": 881.04,
        "duration": 2.88,
        "text": "applications that's where you're"
      },
      {
        "start": 882.42,
        "duration": 5.76,
        "text": "probably going to put these agents"
      },
      {
        "start": 883.92,
        "duration": 6.479,
        "text": "agents is on the front lines of where"
      },
      {
        "start": 888.18,
        "duration": 4.339,
        "text": "customers are interacting with your"
      },
      {
        "start": 890.399,
        "duration": 2.12,
        "text": "business"
      },
      {
        "start": 893.339,
        "duration": 4.56,
        "text": "and as we look at the technologies that"
      },
      {
        "start": 895.56,
        "duration": 4.44,
        "text": "go into this architecture we can see"
      },
      {
        "start": 897.899,
        "duration": 5.88,
        "text": "that Vector databases like Cassandra"
      },
      {
        "start": 900.0,
        "duration": 6.66,
        "text": "that is part of our Astra database as"
      },
      {
        "start": 903.779,
        "duration": 7.381,
        "text": "well as Predictive Technologies all play"
      },
      {
        "start": 906.66,
        "duration": 6.66,
        "text": "A Part within this you'll also Leverage"
      },
      {
        "start": 911.16,
        "duration": 6.78,
        "text": "online"
      },
      {
        "start": 913.32,
        "duration": 7.8,
        "text": "inference Services things like vertex AI"
      },
      {
        "start": 917.94,
        "duration": 7.259,
        "text": "from Google or sagemaker"
      },
      {
        "start": 921.12,
        "duration": 6.36,
        "text": "to generate embeddings and also to go"
      },
      {
        "start": 925.199,
        "duration": 5.101,
        "text": "and refine your data and actually help"
      },
      {
        "start": 927.48,
        "duration": 5.28,
        "text": "build better context that goes into"
      },
      {
        "start": 930.3,
        "duration": 4.02,
        "text": "these llms"
      },
      {
        "start": 932.76,
        "duration": 5.34,
        "text": "and that's part of"
      },
      {
        "start": 934.32,
        "duration": 5.759,
        "text": "slide talks about which is that when"
      },
      {
        "start": 938.1,
        "duration": 5.22,
        "text": "you're building these real-time apps"
      },
      {
        "start": 940.079,
        "duration": 6.181,
        "text": "you're actually combining generative Ai"
      },
      {
        "start": 943.32,
        "duration": 6.54,
        "text": "and predictive AI predictive AI allows"
      },
      {
        "start": 946.26,
        "duration": 5.579,
        "text": "me to go and look at historical data"
      },
      {
        "start": 949.86,
        "duration": 3.539,
        "text": "sets and perhaps generate Dynamic"
      },
      {
        "start": 951.839,
        "duration": 4.201,
        "text": "pricing"
      },
      {
        "start": 953.399,
        "duration": 6.781,
        "text": "offering up a discount for example"
      },
      {
        "start": 956.04,
        "duration": 5.7,
        "text": "it might be able to go and look at a set"
      },
      {
        "start": 960.18,
        "duration": 3.36,
        "text": "of data points coming from an iot"
      },
      {
        "start": 961.74,
        "duration": 4.74,
        "text": "application to determine maintenance is"
      },
      {
        "start": 963.54,
        "duration": 4.919,
        "text": "necessary or it might be able to go and"
      },
      {
        "start": 966.48,
        "duration": 4.859,
        "text": "look at inventory stock levels and"
      },
      {
        "start": 968.459,
        "duration": 4.68,
        "text": "figure out do I need to reorder or do I"
      },
      {
        "start": 971.339,
        "duration": 4.62,
        "text": "have a surplus that perhaps"
      },
      {
        "start": 973.139,
        "duration": 4.021,
        "text": "would generate a flash sale for an"
      },
      {
        "start": 975.959,
        "duration": 3.721,
        "text": "e-commerce site"
      },
      {
        "start": 977.16,
        "duration": 5.58,
        "text": "but all of those inputs"
      },
      {
        "start": 979.68,
        "duration": 5.04,
        "text": "need to be delivered to the user in a"
      },
      {
        "start": 982.74,
        "duration": 4.5,
        "text": "form that makes sense to them and that's"
      },
      {
        "start": 984.72,
        "duration": 6.84,
        "text": "where generative AI fits into the"
      },
      {
        "start": 987.24,
        "duration": 6.3,
        "text": "picture general of AI is fantastic at"
      },
      {
        "start": 991.56,
        "duration": 4.5,
        "text": "the places where the user is interacting"
      },
      {
        "start": 993.54,
        "duration": 5.52,
        "text": "whether it's conversationally whether"
      },
      {
        "start": 996.06,
        "duration": 6.66,
        "text": "it's for example many of us experience"
      },
      {
        "start": 999.06,
        "duration": 6.779,
        "text": "with GitHub co-pilot where code Auto"
      },
      {
        "start": 1002.72,
        "duration": 5.52,
        "text": "completion happens as you're typing"
      },
      {
        "start": 1005.839,
        "duration": 4.021,
        "text": "and fills out sections of your code base"
      },
      {
        "start": 1008.24,
        "duration": 3.36,
        "text": "allowing you to develop in a much more"
      },
      {
        "start": 1009.86,
        "duration": 5.279,
        "text": "rapid fashion"
      },
      {
        "start": 1011.6,
        "duration": 4.919,
        "text": "obviously generative AI is and"
      },
      {
        "start": 1015.139,
        "duration": 2.64,
        "text": "predictive AI are used within"
      },
      {
        "start": 1016.519,
        "duration": 3.421,
        "text": "advertising"
      },
      {
        "start": 1017.779,
        "duration": 4.081,
        "text": "predictive AI figures out which ad to"
      },
      {
        "start": 1019.94,
        "duration": 3.78,
        "text": "show to which person but generative AI"
      },
      {
        "start": 1021.86,
        "duration": 4.199,
        "text": "can actually create an advertisement and"
      },
      {
        "start": 1023.72,
        "duration": 4.38,
        "text": "offer a promotion an email that's"
      },
      {
        "start": 1026.059,
        "duration": 4.941,
        "text": "tailored to an individual it's a very"
      },
      {
        "start": 1028.1,
        "duration": 7.8,
        "text": "powerful conversation combination"
      },
      {
        "start": 1031.0,
        "duration": 6.28,
        "text": "or going and taking a set of content and"
      },
      {
        "start": 1035.9,
        "duration": 3.299,
        "text": "going and condensing it down and"
      },
      {
        "start": 1037.28,
        "duration": 3.539,
        "text": "delivering the right message at the"
      },
      {
        "start": 1039.199,
        "duration": 3.961,
        "text": "right time"
      },
      {
        "start": 1040.819,
        "duration": 4.201,
        "text": "so"
      },
      {
        "start": 1043.16,
        "duration": 4.44,
        "text": "we've actually practiced what we"
      },
      {
        "start": 1045.02,
        "duration": 5.039,
        "text": "preached we've used these capabilities"
      },
      {
        "start": 1047.6,
        "duration": 3.42,
        "text": "to build something that we call Astra"
      },
      {
        "start": 1050.059,
        "duration": 3.0,
        "text": "assistant"
      },
      {
        "start": 1051.02,
        "duration": 4.74,
        "text": "which is designed to make it possible"
      },
      {
        "start": 1053.059,
        "duration": 5.941,
        "text": "for any developer that is using"
      },
      {
        "start": 1055.76,
        "duration": 6.9,
        "text": "Cassandra that's using Astra DB to be"
      },
      {
        "start": 1059.0,
        "duration": 6.96,
        "text": "able to go and build queries to be able"
      },
      {
        "start": 1062.66,
        "duration": 6.6,
        "text": "to go and Define their database schemas"
      },
      {
        "start": 1065.96,
        "duration": 5.219,
        "text": "or generate sample code that they can"
      },
      {
        "start": 1069.26,
        "duration": 3.659,
        "text": "use in their applications"
      },
      {
        "start": 1071.179,
        "duration": 3.541,
        "text": "and this is something that's live today"
      },
      {
        "start": 1072.919,
        "duration": 4.62,
        "text": "we're talking about it in some of the"
      },
      {
        "start": 1074.72,
        "duration": 6.9,
        "text": "other sessions today on how we built it"
      },
      {
        "start": 1077.539,
        "duration": 7.26,
        "text": "it's a very cool use case that will give"
      },
      {
        "start": 1081.62,
        "duration": 4.799,
        "text": "you a tangible perspective of how you"
      },
      {
        "start": 1084.799,
        "duration": 3.901,
        "text": "can put one of these applications to use"
      },
      {
        "start": 1086.419,
        "duration": 4.981,
        "text": "for you so"
      },
      {
        "start": 1088.7,
        "duration": 5.479,
        "text": "stick around a lot more great stuff to"
      },
      {
        "start": 1091.4,
        "duration": 2.779,
        "text": "show you thank you"
      },
      {
        "start": 1096.34,
        "duration": 3.21,
        "text": "[Music]"
      },
      {
        "start": 1101.539,
        "duration": 4.321,
        "text": "the architecture is for llms and agents"
      },
      {
        "start": 1103.7,
        "duration": 3.78,
        "text": "things are changing quickly and it's an"
      },
      {
        "start": 1105.86,
        "duration": 3.78,
        "text": "exciting time to be an architect or"
      },
      {
        "start": 1107.48,
        "duration": 4.5,
        "text": "designer next we're going to see an"
      },
      {
        "start": 1109.64,
        "duration": 4.08,
        "text": "agent architecture in action by diving"
      },
      {
        "start": 1111.98,
        "duration": 4.199,
        "text": "into our own generative AI application"
      },
      {
        "start": 1113.72,
        "duration": 4.44,
        "text": "and case study in our next session two"
      },
      {
        "start": 1116.179,
        "duration": 4.081,
        "text": "of our AI leaders Alan ho and Alex"
      },
      {
        "start": 1118.16,
        "duration": 4.8,
        "text": "levantera will deep that into the code"
      },
      {
        "start": 1120.26,
        "duration": 5.4,
        "text": "of an AI agent and Lessons Learned in"
      },
      {
        "start": 1122.96,
        "duration": 4.62,
        "text": "data and prompt engineering Alan ho is"
      },
      {
        "start": 1125.66,
        "duration": 3.72,
        "text": "an AI strategy lead at datastacks and"
      },
      {
        "start": 1127.58,
        "duration": 3.839,
        "text": "Alex leventer is one of our developers"
      },
      {
        "start": 1129.38,
        "duration": 5.539,
        "text": "building AI applications here at data"
      },
      {
        "start": 1131.419,
        "duration": 3.5,
        "text": "Stacks take it away Alan"
      },
      {
        "start": 1134.96,
        "duration": 6.48,
        "text": "foreign"
      },
      {
        "start": 1137.88,
        "duration": 6.56,
        "text": "[Music]"
      },
      {
        "start": 1141.44,
        "duration": 3.0,
        "text": "so"
      },
      {
        "start": 1144.679,
        "duration": 7.38,
        "text": "we're going to be sharing with you how"
      },
      {
        "start": 1148.46,
        "duration": 6.3,
        "text": "to actually build Hands-On"
      },
      {
        "start": 1152.059,
        "duration": 5.341,
        "text": "a generative AI agent"
      },
      {
        "start": 1154.76,
        "duration": 5.7,
        "text": "and so what we've done today is that we"
      },
      {
        "start": 1157.4,
        "duration": 5.94,
        "text": "decided to show you our journey in"
      },
      {
        "start": 1160.46,
        "duration": 5.82,
        "text": "building our own generative AI agent for"
      },
      {
        "start": 1163.34,
        "duration": 5.94,
        "text": "our Astra assistant and I'll be going"
      },
      {
        "start": 1166.28,
        "duration": 5.58,
        "text": "over some of the lessons learned and how"
      },
      {
        "start": 1169.28,
        "duration": 4.5,
        "text": "to do it and then Alex going to be"
      },
      {
        "start": 1171.86,
        "duration": 4.199,
        "text": "helping us get dived right into the code"
      },
      {
        "start": 1173.78,
        "duration": 5.899,
        "text": "and showing you exactly where you need"
      },
      {
        "start": 1176.059,
        "duration": 3.62,
        "text": "to code things up to build your own"
      },
      {
        "start": 1179.84,
        "duration": 4.14,
        "text": "all right so I'm going to first give you"
      },
      {
        "start": 1182.12,
        "duration": 3.36,
        "text": "an overview of the assistant of what"
      },
      {
        "start": 1183.98,
        "duration": 3.96,
        "text": "we're trying to do then I'm going to"
      },
      {
        "start": 1185.48,
        "duration": 4.92,
        "text": "talk a little bit about architecture"
      },
      {
        "start": 1187.94,
        "duration": 4.619,
        "text": "um how the generative AI workflow and"
      },
      {
        "start": 1190.4,
        "duration": 4.86,
        "text": "the various components needed then I'm"
      },
      {
        "start": 1192.559,
        "duration": 5.041,
        "text": "going to go step by step on how to build"
      },
      {
        "start": 1195.26,
        "duration": 4.56,
        "text": "a retrieval augmented generation"
      },
      {
        "start": 1197.6,
        "duration": 4.68,
        "text": "workflow we'll talk about the data"
      },
      {
        "start": 1199.82,
        "duration": 5.219,
        "text": "pre-processing that's needed how to"
      },
      {
        "start": 1202.28,
        "duration": 4.38,
        "text": "generate highly contextual prompts and"
      },
      {
        "start": 1205.039,
        "duration": 3.661,
        "text": "how to execute the prompts and what to"
      },
      {
        "start": 1206.66,
        "duration": 3.78,
        "text": "do afterwards"
      },
      {
        "start": 1208.7,
        "duration": 3.54,
        "text": "to give you some context of what we're"
      },
      {
        "start": 1210.44,
        "duration": 5.099,
        "text": "doing at data Stacks is that we're using"
      },
      {
        "start": 1212.24,
        "duration": 6.24,
        "text": "generative AI to improve the results of"
      },
      {
        "start": 1215.539,
        "duration": 4.621,
        "text": "our self-service business so when"
      },
      {
        "start": 1218.48,
        "duration": 5.04,
        "text": "customers use our website and they want"
      },
      {
        "start": 1220.16,
        "duration": 6.3,
        "text": "to use our app we've already used AI to"
      },
      {
        "start": 1223.52,
        "duration": 5.94,
        "text": "increase our subscription business by 5X"
      },
      {
        "start": 1226.46,
        "duration": 4.62,
        "text": "so now what we're doing with uh with a"
      },
      {
        "start": 1229.46,
        "duration": 5.52,
        "text": "generative AI which is different from"
      },
      {
        "start": 1231.08,
        "duration": 5.94,
        "text": "predictive AI is require the reduce the"
      },
      {
        "start": 1234.98,
        "duration": 4.5,
        "text": "time it takes for people to actually"
      },
      {
        "start": 1237.02,
        "duration": 4.26,
        "text": "deploy their apps for production and so"
      },
      {
        "start": 1239.48,
        "duration": 4.079,
        "text": "today because databases are very"
      },
      {
        "start": 1241.28,
        "duration": 4.92,
        "text": "complicated pieces of software most of"
      },
      {
        "start": 1243.559,
        "duration": 5.041,
        "text": "the time we use actual real agents real"
      },
      {
        "start": 1246.2,
        "duration": 6.06,
        "text": "people to help them out and this is our"
      },
      {
        "start": 1248.6,
        "duration": 6.78,
        "text": "journey in replacing those people or"
      },
      {
        "start": 1252.26,
        "duration": 5.22,
        "text": "augmenting those people uh with a"
      },
      {
        "start": 1255.38,
        "duration": 5.06,
        "text": "generative AI agents"
      },
      {
        "start": 1257.48,
        "duration": 5.939,
        "text": "so our key metric to get to production"
      },
      {
        "start": 1260.44,
        "duration": 4.599,
        "text": "is our key metric that we care about is"
      },
      {
        "start": 1263.419,
        "duration": 4.021,
        "text": "shorten the data time to production"
      },
      {
        "start": 1265.039,
        "duration": 5.581,
        "text": "because the typical thing that happens"
      },
      {
        "start": 1267.44,
        "duration": 5.88,
        "text": "for a software service like ours is that"
      },
      {
        "start": 1270.62,
        "duration": 5.28,
        "text": "once they start using the software and"
      },
      {
        "start": 1273.32,
        "duration": 4.56,
        "text": "they finish their first use case then"
      },
      {
        "start": 1275.9,
        "duration": 4.2,
        "text": "they tend to spend more money for their"
      },
      {
        "start": 1277.88,
        "duration": 3.9,
        "text": "second use case and we get them to the"
      },
      {
        "start": 1280.1,
        "duration": 3.48,
        "text": "cycle through this uh a lot of times"
      },
      {
        "start": 1281.78,
        "duration": 3.84,
        "text": "people just use the chat app to help"
      },
      {
        "start": 1283.58,
        "duration": 4.92,
        "text": "them get through this cycle"
      },
      {
        "start": 1285.62,
        "duration": 6.059,
        "text": "so our as Astra assistant that we built"
      },
      {
        "start": 1288.5,
        "duration": 5.58,
        "text": "it's primarily focused on answering some"
      },
      {
        "start": 1291.679,
        "duration": 5.401,
        "text": "of the uh technical questions it"
      },
      {
        "start": 1294.08,
        "duration": 4.2,
        "text": "leverages our chat history and the"
      },
      {
        "start": 1297.08,
        "duration": 4.68,
        "text": "documentation"
      },
      {
        "start": 1298.28,
        "duration": 7.019,
        "text": "Etc to answer these questions and"
      },
      {
        "start": 1301.76,
        "duration": 5.399,
        "text": "um we also use it as a mechanism to"
      },
      {
        "start": 1305.299,
        "duration": 5.461,
        "text": "um tell people about how much it costs"
      },
      {
        "start": 1307.159,
        "duration": 6.481,
        "text": "as well as uh get help and there's also"
      },
      {
        "start": 1310.76,
        "duration": 5.52,
        "text": "the ability within it to refer or"
      },
      {
        "start": 1313.64,
        "duration": 5.82,
        "text": "escalate to an actual person so we don't"
      },
      {
        "start": 1316.28,
        "duration": 6.48,
        "text": "have uh we don't have 24x7 support for"
      },
      {
        "start": 1319.46,
        "duration": 5.04,
        "text": "our our this chat bot so we're primarily"
      },
      {
        "start": 1322.76,
        "duration": 3.6,
        "text": "thinking about using it especially"
      },
      {
        "start": 1324.5,
        "duration": 3.659,
        "text": "during the off hours"
      },
      {
        "start": 1326.36,
        "duration": 3.9,
        "text": "all right with that I'm going to hand it"
      },
      {
        "start": 1328.159,
        "duration": 4.26,
        "text": "over to Alex who's going to show you a"
      },
      {
        "start": 1330.26,
        "duration": 5.58,
        "text": "demo of how it works"
      },
      {
        "start": 1332.419,
        "duration": 5.64,
        "text": "awesome thank you very much"
      },
      {
        "start": 1335.84,
        "duration": 5.28,
        "text": "cool um so what you're seeing here is"
      },
      {
        "start": 1338.059,
        "duration": 4.441,
        "text": "the dashboard of our product Astra"
      },
      {
        "start": 1341.12,
        "duration": 2.84,
        "text": "um if you haven't created an account you"
      },
      {
        "start": 1342.5,
        "duration": 3.72,
        "text": "can create a free account at"
      },
      {
        "start": 1343.96,
        "duration": 4.3,
        "text": "ashrae.datorsex.com register"
      },
      {
        "start": 1346.22,
        "duration": 4.26,
        "text": "and on the bottom right of the screen"
      },
      {
        "start": 1348.26,
        "duration": 4.38,
        "text": "you'll see this chat icon"
      },
      {
        "start": 1350.48,
        "duration": 4.74,
        "text": "um so this chat icon for the past couple"
      },
      {
        "start": 1352.64,
        "duration": 5.519,
        "text": "years has been managed by a team of"
      },
      {
        "start": 1355.22,
        "duration": 4.62,
        "text": "sport engineers and just recently"
      },
      {
        "start": 1358.159,
        "duration": 3.481,
        "text": "um it's now powered by the assisting"
      },
      {
        "start": 1359.84,
        "duration": 3.719,
        "text": "column suspensions"
      },
      {
        "start": 1361.64,
        "duration": 3.96,
        "text": "um so I'm first gonna start by asking a"
      },
      {
        "start": 1363.559,
        "duration": 4.86,
        "text": "simple query"
      },
      {
        "start": 1365.6,
        "duration": 5.16,
        "text": "hello generate token"
      },
      {
        "start": 1368.419,
        "duration": 5.64,
        "text": "and I'm going to add this new context"
      },
      {
        "start": 1370.76,
        "duration": 5.46,
        "text": "text and what no context does"
      },
      {
        "start": 1374.059,
        "duration": 3.841,
        "text": "is it tells the assistant that we don't"
      },
      {
        "start": 1376.22,
        "duration": 5.42,
        "text": "want to inject any additional"
      },
      {
        "start": 1377.9,
        "duration": 3.74,
        "text": "information in the prompts"
      },
      {
        "start": 1383.84,
        "duration": 3.839,
        "text": "and you'll see"
      },
      {
        "start": 1385.4,
        "duration": 4.38,
        "text": "um because we haven't injected any"
      },
      {
        "start": 1387.679,
        "duration": 3.781,
        "text": "additional information in the prompt"
      },
      {
        "start": 1389.78,
        "duration": 4.5,
        "text": "um we get an answer from the bot that"
      },
      {
        "start": 1391.46,
        "duration": 4.26,
        "text": "has nothing to do with data Stacks to"
      },
      {
        "start": 1394.28,
        "duration": 3.06,
        "text": "actually work Cassandra"
      },
      {
        "start": 1395.72,
        "duration": 4.02,
        "text": "um so you can see we get some"
      },
      {
        "start": 1397.34,
        "duration": 3.12,
        "text": "information on how to generate"
      },
      {
        "start": 1399.74,
        "duration": 4.2,
        "text": "um"
      },
      {
        "start": 1400.46,
        "duration": 4.079,
        "text": "an API token from Google documentation"
      },
      {
        "start": 1403.94,
        "duration": 4.44,
        "text": "um"
      },
      {
        "start": 1404.539,
        "duration": 6.241,
        "text": "has nothing to do with Ashraf is super"
      },
      {
        "start": 1408.38,
        "duration": 4.86,
        "text": "um you know unhelpful for our users so"
      },
      {
        "start": 1410.78,
        "duration": 5.279,
        "text": "I'm going to ask that same question with"
      },
      {
        "start": 1413.24,
        "duration": 6.5,
        "text": "information from our documentation in"
      },
      {
        "start": 1416.059,
        "duration": 3.681,
        "text": "our users in the problems"
      },
      {
        "start": 1425.0,
        "duration": 4.38,
        "text": "and you'll see"
      },
      {
        "start": 1426.919,
        "duration": 4.921,
        "text": "um with the improved prompts we get and"
      },
      {
        "start": 1429.38,
        "duration": 3.9,
        "text": "enter the super relevant to Astro"
      },
      {
        "start": 1431.84,
        "duration": 2.64,
        "text": "um so here you can see exactly how to"
      },
      {
        "start": 1433.28,
        "duration": 2.7,
        "text": "generate a token some of this"
      },
      {
        "start": 1434.48,
        "duration": 3.9,
        "text": "information was pulled directly from the"
      },
      {
        "start": 1435.98,
        "duration": 4.199,
        "text": "docs I'm in in later in the session"
      },
      {
        "start": 1438.38,
        "duration": 4.08,
        "text": "we'll we'll teach you exactly how to do"
      },
      {
        "start": 1440.179,
        "duration": 3.601,
        "text": "this and the importance of prompt"
      },
      {
        "start": 1442.46,
        "duration": 2.82,
        "text": "engineering"
      },
      {
        "start": 1443.78,
        "duration": 3.12,
        "text": "um and how you could potentially build"
      },
      {
        "start": 1445.28,
        "duration": 3.12,
        "text": "your own ball like this"
      },
      {
        "start": 1446.9,
        "duration": 2.94,
        "text": "um we can also ask more advanced"
      },
      {
        "start": 1448.4,
        "duration": 4.04,
        "text": "questions like tell me about my"
      },
      {
        "start": 1449.84,
        "duration": 2.6,
        "text": "databases"
      },
      {
        "start": 1455.679,
        "duration": 4.48,
        "text": "and because in the prompt we also"
      },
      {
        "start": 1458.179,
        "duration": 3.36,
        "text": "include information on the user all"
      },
      {
        "start": 1460.159,
        "duration": 3.0,
        "text": "their activity"
      },
      {
        "start": 1461.539,
        "duration": 4.561,
        "text": "um we get this super awesome answer back"
      },
      {
        "start": 1463.159,
        "duration": 5.64,
        "text": "on you know exactly the databases the"
      },
      {
        "start": 1466.1,
        "duration": 6.92,
        "text": "user has created and we can also ask the"
      },
      {
        "start": 1468.799,
        "duration": 4.221,
        "text": "bot to generate sample code"
      },
      {
        "start": 1478.48,
        "duration": 4.66,
        "text": "so here you can see"
      },
      {
        "start": 1481.46,
        "duration": 3.719,
        "text": "um we have"
      },
      {
        "start": 1483.14,
        "duration": 4.14,
        "text": "the bot generated just a simple sample"
      },
      {
        "start": 1485.179,
        "duration": 3.601,
        "text": "app on exactly how"
      },
      {
        "start": 1487.28,
        "duration": 3.06,
        "text": "um you know you connect to asteroid"
      },
      {
        "start": 1488.78,
        "duration": 3.6,
        "text": "using python"
      },
      {
        "start": 1490.34,
        "duration": 3.06,
        "text": "um and I'll hand it back over to Alan to"
      },
      {
        "start": 1492.38,
        "duration": 2.76,
        "text": "talk through"
      },
      {
        "start": 1493.4,
        "duration": 4.32,
        "text": "um exactly how you can build this on"
      },
      {
        "start": 1495.14,
        "duration": 4.38,
        "text": "your end all right thank you so there"
      },
      {
        "start": 1497.72,
        "duration": 3.959,
        "text": "was a lot of magic being behind the"
      },
      {
        "start": 1499.52,
        "duration": 3.6,
        "text": "scene that was going on so uh stick"
      },
      {
        "start": 1501.679,
        "duration": 3.901,
        "text": "around and we'll be able to show you"
      },
      {
        "start": 1503.12,
        "duration": 4.08,
        "text": "that magic before we get started to get"
      },
      {
        "start": 1505.58,
        "duration": 3.42,
        "text": "to that magic let's talk a little bit"
      },
      {
        "start": 1507.2,
        "duration": 4.38,
        "text": "about the workflow you need to build"
      },
      {
        "start": 1509.0,
        "duration": 5.7,
        "text": "these what we call retrieval augmented"
      },
      {
        "start": 1511.58,
        "duration": 5.54,
        "text": "generation workflows these are workflow"
      },
      {
        "start": 1514.7,
        "duration": 5.04,
        "text": "General retrieval augmented generation"
      },
      {
        "start": 1517.12,
        "duration": 5.679,
        "text": "basically means pairing up the large"
      },
      {
        "start": 1519.74,
        "duration": 5.039,
        "text": "language model with your data set and by"
      },
      {
        "start": 1522.799,
        "duration": 4.561,
        "text": "putting the two together you generate"
      },
      {
        "start": 1524.779,
        "duration": 5.041,
        "text": "these highly relevant prompts that allow"
      },
      {
        "start": 1527.36,
        "duration": 4.02,
        "text": "the the llm to give you good responses"
      },
      {
        "start": 1529.82,
        "duration": 3.3,
        "text": "like what you just saw"
      },
      {
        "start": 1531.38,
        "duration": 3.36,
        "text": "so at a high level the first thing you"
      },
      {
        "start": 1533.12,
        "duration": 3.36,
        "text": "need to do is you want to find what your"
      },
      {
        "start": 1534.74,
        "duration": 3.6,
        "text": "agent does"
      },
      {
        "start": 1536.48,
        "duration": 5.28,
        "text": "um is it a chat bot is an Enterprise"
      },
      {
        "start": 1538.34,
        "duration": 5.1,
        "text": "Search application is it a GitHub"
      },
      {
        "start": 1541.76,
        "duration": 4.38,
        "text": "co-pilot you want to do some definition"
      },
      {
        "start": 1543.44,
        "duration": 5.16,
        "text": "of what it actually is doing"
      },
      {
        "start": 1546.14,
        "duration": 5.46,
        "text": "um then you want to provide"
      },
      {
        "start": 1548.6,
        "duration": 5.579,
        "text": "um you want to select which llm you use"
      },
      {
        "start": 1551.6,
        "duration": 4.26,
        "text": "then you want to figure out what is the"
      },
      {
        "start": 1554.179,
        "duration": 4.38,
        "text": "data that you're going to be using to"
      },
      {
        "start": 1555.86,
        "duration": 4.86,
        "text": "augment the large language model with"
      },
      {
        "start": 1558.559,
        "duration": 6.48,
        "text": "and then figure out how to take that"
      },
      {
        "start": 1560.72,
        "duration": 7.92,
        "text": "that the that data and properly put it"
      },
      {
        "start": 1565.039,
        "duration": 9.061,
        "text": "into what we call vectors these allow"
      },
      {
        "start": 1568.64,
        "duration": 7.8,
        "text": "for the llm to semantically figure out"
      },
      {
        "start": 1574.1,
        "duration": 4.98,
        "text": "what is relevant information for it to"
      },
      {
        "start": 1576.44,
        "duration": 4.859,
        "text": "process and build that prompt then we"
      },
      {
        "start": 1579.08,
        "duration": 4.32,
        "text": "use then we leverage that information to"
      },
      {
        "start": 1581.299,
        "duration": 3.781,
        "text": "build the prompt and you code it up and"
      },
      {
        "start": 1583.4,
        "duration": 3.779,
        "text": "then we have to figure out how when we"
      },
      {
        "start": 1585.08,
        "duration": 4.14,
        "text": "actually launch in production how you're"
      },
      {
        "start": 1587.179,
        "duration": 4.321,
        "text": "going to maintain this uh regenerative"
      },
      {
        "start": 1589.22,
        "duration": 3.24,
        "text": "AI agent and how you're going to do it"
      },
      {
        "start": 1591.5,
        "duration": 3.44,
        "text": "how you're going to deal with your"
      },
      {
        "start": 1592.46,
        "duration": 2.48,
        "text": "operations"
      },
      {
        "start": 1594.98,
        "duration": 3.319,
        "text": "so with that there's a lot of various"
      },
      {
        "start": 1596.72,
        "duration": 3.959,
        "text": "different uh"
      },
      {
        "start": 1598.299,
        "duration": 3.88,
        "text": "tools and there's a lot of different"
      },
      {
        "start": 1600.679,
        "duration": 3.48,
        "text": "decisions that you have to ask yourself"
      },
      {
        "start": 1602.179,
        "duration": 3.421,
        "text": "when going through all these decisions"
      },
      {
        "start": 1604.159,
        "duration": 3.361,
        "text": "so this is kind of the architectural"
      },
      {
        "start": 1605.6,
        "duration": 3.54,
        "text": "work that you need to do up front"
      },
      {
        "start": 1607.52,
        "duration": 4.5,
        "text": "all right let's take a look at the the"
      },
      {
        "start": 1609.14,
        "duration": 5.82,
        "text": "uh the the um the overall um"
      },
      {
        "start": 1612.02,
        "duration": 4.62,
        "text": "architecture so at another high level we"
      },
      {
        "start": 1614.96,
        "duration": 4.26,
        "text": "showed you is an application which was"
      },
      {
        "start": 1616.64,
        "duration": 4.139,
        "text": "the chat application it's talking to"
      },
      {
        "start": 1619.22,
        "duration": 4.14,
        "text": "your agent this is the agent that you"
      },
      {
        "start": 1620.779,
        "duration": 5.101,
        "text": "code up in our case we cut it up in"
      },
      {
        "start": 1623.36,
        "duration": 4.559,
        "text": "Python and it's leveraged a combination"
      },
      {
        "start": 1625.88,
        "duration": 3.72,
        "text": "of proprietary data this is a"
      },
      {
        "start": 1627.919,
        "duration": 3.181,
        "text": "proprietary structured data just like"
      },
      {
        "start": 1629.6,
        "duration": 4.559,
        "text": "any kind of data you'd store on"
      },
      {
        "start": 1631.1,
        "duration": 6.9,
        "text": "Cassandra day and then it also leverages"
      },
      {
        "start": 1634.159,
        "duration": 5.941,
        "text": "uh proprietary unstructured data this is"
      },
      {
        "start": 1638.0,
        "duration": 3.659,
        "text": "the data that's retrieved via Vector"
      },
      {
        "start": 1640.1,
        "duration": 3.8,
        "text": "search and this is a brand new"
      },
      {
        "start": 1641.659,
        "duration": 5.64,
        "text": "capability that we've added to"
      },
      {
        "start": 1643.9,
        "duration": 4.96,
        "text": "astrodb Cassandra very recently"
      },
      {
        "start": 1647.299,
        "duration": 4.021,
        "text": "now in order to deal with your"
      },
      {
        "start": 1648.86,
        "duration": 4.62,
        "text": "unstructured data you need to leverage"
      },
      {
        "start": 1651.32,
        "duration": 5.4,
        "text": "something what we call an embeddings API"
      },
      {
        "start": 1653.48,
        "duration": 5.699,
        "text": "so this embeddings API takes"
      },
      {
        "start": 1656.72,
        "duration": 4.319,
        "text": "unstructured data and turns them into"
      },
      {
        "start": 1659.179,
        "duration": 3.061,
        "text": "the vectors as needed so you can do the"
      },
      {
        "start": 1661.039,
        "duration": 4.081,
        "text": "semantic search we'll get into more"
      },
      {
        "start": 1662.24,
        "duration": 5.179,
        "text": "details on it later and then last but"
      },
      {
        "start": 1665.12,
        "duration": 5.46,
        "text": "not least when the agent is being"
      },
      {
        "start": 1667.419,
        "duration": 6.64,
        "text": "executing it calls these data sources"
      },
      {
        "start": 1670.58,
        "duration": 4.92,
        "text": "and the lln in order to return it to the"
      },
      {
        "start": 1674.059,
        "duration": 2.641,
        "text": "application"
      },
      {
        "start": 1675.5,
        "duration": 3.179,
        "text": "now there's a lot of different"
      },
      {
        "start": 1676.7,
        "duration": 3.3,
        "text": "Technologies you could use and so today"
      },
      {
        "start": 1678.679,
        "duration": 4.021,
        "text": "what we're going to demonstrate is"
      },
      {
        "start": 1680.0,
        "duration": 5.279,
        "text": "obviously leveraging data Stacks"
      },
      {
        "start": 1682.7,
        "duration": 6.599,
        "text": "technology for proprieted structured"
      },
      {
        "start": 1685.279,
        "duration": 5.461,
        "text": "data we're leveraging Astro DB for our"
      },
      {
        "start": 1689.299,
        "duration": 4.26,
        "text": "Vector data we're also leveraging"
      },
      {
        "start": 1690.74,
        "duration": 4.62,
        "text": "astrodb leveraging the new Vector"
      },
      {
        "start": 1693.559,
        "duration": 3.661,
        "text": "database capabilities"
      },
      {
        "start": 1695.36,
        "duration": 2.88,
        "text": "then betting apis there's a lot of"
      },
      {
        "start": 1697.22,
        "duration": 3.0,
        "text": "different Technologies you could use"
      },
      {
        "start": 1698.24,
        "duration": 5.46,
        "text": "there's Technologies from vertex AI"
      },
      {
        "start": 1700.22,
        "duration": 6.0,
        "text": "That's from Google open AI Sage maker"
      },
      {
        "start": 1703.7,
        "duration": 4.5,
        "text": "hugging faces Etc there's a lot of"
      },
      {
        "start": 1706.22,
        "duration": 4.319,
        "text": "different embedding models today and"
      },
      {
        "start": 1708.2,
        "duration": 3.839,
        "text": "then today for llms there's also a lot"
      },
      {
        "start": 1710.539,
        "duration": 4.26,
        "text": "of various different applications"
      },
      {
        "start": 1712.039,
        "duration": 6.781,
        "text": "available today open AI"
      },
      {
        "start": 1714.799,
        "duration": 5.641,
        "text": "Google Cloud Google vertex AI hugging"
      },
      {
        "start": 1718.82,
        "duration": 3.719,
        "text": "faces Etc"
      },
      {
        "start": 1720.44,
        "duration": 3.839,
        "text": "and we want to make sure that when we're"
      },
      {
        "start": 1722.539,
        "duration": 3.601,
        "text": "doing this execution we want to actually"
      },
      {
        "start": 1724.279,
        "duration": 5.041,
        "text": "actually have it execute in one of the"
      },
      {
        "start": 1726.14,
        "duration": 5.22,
        "text": "clouds because latency becomes very very"
      },
      {
        "start": 1729.32,
        "duration": 3.719,
        "text": "important we want to make sure that all"
      },
      {
        "start": 1731.36,
        "duration": 4.679,
        "text": "these services are running in one cloud"
      },
      {
        "start": 1733.039,
        "duration": 4.921,
        "text": "in a secure location"
      },
      {
        "start": 1736.039,
        "duration": 4.081,
        "text": "so the technology that we're using on"
      },
      {
        "start": 1737.96,
        "duration": 4.62,
        "text": "needs ahead for this one is the llms"
      },
      {
        "start": 1740.12,
        "duration": 4.62,
        "text": "from Google we're using the vector"
      },
      {
        "start": 1742.58,
        "duration": 4.38,
        "text": "database of that we have we're"
      },
      {
        "start": 1744.74,
        "duration": 4.86,
        "text": "leveraging the embeddings API from"
      },
      {
        "start": 1746.96,
        "duration": 5.52,
        "text": "Google vertex AI we're leveraging Lang"
      },
      {
        "start": 1749.6,
        "duration": 5.459,
        "text": "chain and then we've also open sourced a"
      },
      {
        "start": 1752.48,
        "duration": 4.86,
        "text": "new project called cast IO which"
      },
      {
        "start": 1755.059,
        "duration": 6.061,
        "text": "basically allows you to create these"
      },
      {
        "start": 1757.34,
        "duration": 6.719,
        "text": "highly scalable agent memory which plugs"
      },
      {
        "start": 1761.12,
        "duration": 5.279,
        "text": "into all the very common Frameworks such"
      },
      {
        "start": 1764.059,
        "duration": 4.261,
        "text": "as line chain and hopefully very soon"
      },
      {
        "start": 1766.399,
        "duration": 4.081,
        "text": "Lama index"
      },
      {
        "start": 1768.32,
        "duration": 3.54,
        "text": "so why don't I go into talk a little bit"
      },
      {
        "start": 1770.48,
        "duration": 2.819,
        "text": "about the first step the data"
      },
      {
        "start": 1771.86,
        "duration": 3.179,
        "text": "pre-processing"
      },
      {
        "start": 1773.299,
        "duration": 4.081,
        "text": "so I wanted to give you a little context"
      },
      {
        "start": 1775.039,
        "duration": 5.461,
        "text": "of what Vector search is so Vector"
      },
      {
        "start": 1777.38,
        "duration": 5.22,
        "text": "search is a way that you can"
      },
      {
        "start": 1780.5,
        "duration": 4.86,
        "text": "um you can interact with unstructured"
      },
      {
        "start": 1782.6,
        "duration": 6.6,
        "text": "data and the reason why this is so"
      },
      {
        "start": 1785.36,
        "duration": 5.34,
        "text": "important is because the llms need to"
      },
      {
        "start": 1789.2,
        "duration": 3.24,
        "text": "have context it needs to have"
      },
      {
        "start": 1790.7,
        "duration": 4.74,
        "text": "information proprietary information"
      },
      {
        "start": 1792.44,
        "duration": 4.56,
        "text": "especially unstructured information in"
      },
      {
        "start": 1795.44,
        "duration": 4.08,
        "text": "order to generate very relevant"
      },
      {
        "start": 1797.0,
        "duration": 5.34,
        "text": "responses these can be information from"
      },
      {
        "start": 1799.52,
        "duration": 5.039,
        "text": "your chat's history system from actual"
      },
      {
        "start": 1802.34,
        "duration": 4.38,
        "text": "people conversations this can be"
      },
      {
        "start": 1804.559,
        "duration": 2.941,
        "text": "documentation this can be policy"
      },
      {
        "start": 1806.72,
        "duration": 1.62,
        "text": "information"
      },
      {
        "start": 1807.5,
        "duration": 2.7,
        "text": "Etc"
      },
      {
        "start": 1808.34,
        "duration": 3.959,
        "text": "and the reason why this is so important"
      },
      {
        "start": 1810.2,
        "duration": 4.64,
        "text": "is because if the llm doesn't have"
      },
      {
        "start": 1812.299,
        "duration": 4.98,
        "text": "context if it doesn't have information"
      },
      {
        "start": 1814.84,
        "duration": 4.3,
        "text": "then a lot of times that's the reason"
      },
      {
        "start": 1817.279,
        "duration": 3.601,
        "text": "why it actually creates a lot of"
      },
      {
        "start": 1819.14,
        "duration": 3.3,
        "text": "hallucinations"
      },
      {
        "start": 1820.88,
        "duration": 3.96,
        "text": "so let's talk a little bit about how"
      },
      {
        "start": 1822.44,
        "duration": 4.2,
        "text": "embeddings work so the first thing is"
      },
      {
        "start": 1824.84,
        "duration": 4.38,
        "text": "that you what you would do is that you"
      },
      {
        "start": 1826.64,
        "duration": 5.1,
        "text": "would map your database items into an"
      },
      {
        "start": 1829.22,
        "duration": 5.459,
        "text": "embank space so we're leveraging an"
      },
      {
        "start": 1831.74,
        "duration": 5.34,
        "text": "off-the-shelf model machine a neural"
      },
      {
        "start": 1834.679,
        "duration": 4.201,
        "text": "network to generate these embeddings and"
      },
      {
        "start": 1837.08,
        "duration": 5.339,
        "text": "you can see for every single piece of"
      },
      {
        "start": 1838.88,
        "duration": 5.64,
        "text": "data it's mapping into this space"
      },
      {
        "start": 1842.419,
        "duration": 4.26,
        "text": "now the time of inference when you"
      },
      {
        "start": 1844.52,
        "duration": 4.259,
        "text": "actually uh doing the vector simulator"
      },
      {
        "start": 1846.679,
        "duration": 4.921,
        "text": "search what it does is that when"
      },
      {
        "start": 1848.779,
        "duration": 5.581,
        "text": "somebody types in a question it figures"
      },
      {
        "start": 1851.6,
        "duration": 5.04,
        "text": "out what are the documents that are near"
      },
      {
        "start": 1854.36,
        "duration": 4.559,
        "text": "it so in this case this in this scenario"
      },
      {
        "start": 1856.64,
        "duration": 3.48,
        "text": "what it's doing is that it's mapping"
      },
      {
        "start": 1858.919,
        "duration": 4.14,
        "text": "different"
      },
      {
        "start": 1860.12,
        "duration": 5.1,
        "text": "um pieces of text to this embedding"
      },
      {
        "start": 1863.059,
        "duration": 4.801,
        "text": "space and then somebody's asking a"
      },
      {
        "start": 1865.22,
        "duration": 4.62,
        "text": "question what is like a good Shakespeare"
      },
      {
        "start": 1867.86,
        "duration": 4.14,
        "text": "tragedy and it finds out the very"
      },
      {
        "start": 1869.84,
        "duration": 3.78,
        "text": "nearest neighbors like Romeo Juliet et"
      },
      {
        "start": 1872.0,
        "duration": 4.02,
        "text": "cetera"
      },
      {
        "start": 1873.62,
        "duration": 4.08,
        "text": "so for our use case we have two sources"
      },
      {
        "start": 1876.02,
        "duration": 4.44,
        "text": "of unstructured data that we care about"
      },
      {
        "start": 1877.7,
        "duration": 4.38,
        "text": "one is our product documentation this"
      },
      {
        "start": 1880.46,
        "duration": 3.9,
        "text": "tells people how to use the product this"
      },
      {
        "start": 1882.08,
        "duration": 4.86,
        "text": "is probably the primary source of it and"
      },
      {
        "start": 1884.36,
        "duration": 4.86,
        "text": "the second is chat history and what the"
      },
      {
        "start": 1886.94,
        "duration": 5.7,
        "text": "chat history does is that it gives the"
      },
      {
        "start": 1889.22,
        "duration": 5.64,
        "text": "uh it gives the AI agent example"
      },
      {
        "start": 1892.64,
        "duration": 5.1,
        "text": "questions and answers that people ask"
      },
      {
        "start": 1894.86,
        "duration": 5.52,
        "text": "and this is important for not"
      },
      {
        "start": 1897.74,
        "duration": 4.74,
        "text": "necessarily for finding the answer but"
      },
      {
        "start": 1900.38,
        "duration": 3.179,
        "text": "especially in the scenario because that"
      },
      {
        "start": 1902.48,
        "duration": 3.72,
        "text": "information is in the product"
      },
      {
        "start": 1903.559,
        "duration": 4.141,
        "text": "documentation but it gives kind of an"
      },
      {
        "start": 1906.2,
        "duration": 4.92,
        "text": "example this is also what we call"
      },
      {
        "start": 1907.7,
        "duration": 4.94,
        "text": "fuchsia learning of how to answer these"
      },
      {
        "start": 1911.12,
        "duration": 4.919,
        "text": "questions"
      },
      {
        "start": 1912.64,
        "duration": 5.68,
        "text": "so we take this information we generate"
      },
      {
        "start": 1916.039,
        "duration": 4.201,
        "text": "embeddings and then we write the raw the"
      },
      {
        "start": 1918.32,
        "duration": 4.2,
        "text": "embeddings and the raw text directly"
      },
      {
        "start": 1920.24,
        "duration": 4.62,
        "text": "into the database so what does a raw"
      },
      {
        "start": 1922.52,
        "duration": 5.1,
        "text": "text look like so for example this"
      },
      {
        "start": 1924.86,
        "duration": 6.0,
        "text": "information here is"
      },
      {
        "start": 1927.62,
        "duration": 5.34,
        "text": "um from right from our documentation and"
      },
      {
        "start": 1930.86,
        "duration": 3.319,
        "text": "it generates about a thousand five"
      },
      {
        "start": 1932.96,
        "duration": 5.88,
        "text": "hundred"
      },
      {
        "start": 1934.179,
        "duration": 5.201,
        "text": "vectors sorry 5000 1 500"
      },
      {
        "start": 1938.84,
        "duration": 3.959,
        "text": "um"
      },
      {
        "start": 1939.38,
        "duration": 5.22,
        "text": "uh scalar values to create a vector and"
      },
      {
        "start": 1942.799,
        "duration": 3.901,
        "text": "this is the data that allows you to do a"
      },
      {
        "start": 1944.6,
        "duration": 3.84,
        "text": "similarity search"
      },
      {
        "start": 1946.7,
        "duration": 3.0,
        "text": "so with that I'm going to head it over"
      },
      {
        "start": 1948.44,
        "duration": 4.02,
        "text": "to"
      },
      {
        "start": 1949.7,
        "duration": 5.64,
        "text": "um uh um I'm going to hand it over to"
      },
      {
        "start": 1952.46,
        "duration": 5.099,
        "text": "Alex to show a little bit about how the"
      },
      {
        "start": 1955.34,
        "duration": 5.939,
        "text": "pre-processing works"
      },
      {
        "start": 1957.559,
        "duration": 6.48,
        "text": "yeah let me share my screen again"
      },
      {
        "start": 1961.279,
        "duration": 4.321,
        "text": "um so what you're looking at here is a"
      },
      {
        "start": 1964.039,
        "duration": 3.12,
        "text": "collab notebook"
      },
      {
        "start": 1965.6,
        "duration": 3.12,
        "text": "um where you where we demonstrate how"
      },
      {
        "start": 1967.159,
        "duration": 4.201,
        "text": "you can potentially"
      },
      {
        "start": 1968.72,
        "duration": 4.62,
        "text": "um you know process all your data like"
      },
      {
        "start": 1971.36,
        "duration": 3.78,
        "text": "Alan just mentioned um so I'm just going"
      },
      {
        "start": 1973.34,
        "duration": 3.54,
        "text": "to walk through all the cells and how"
      },
      {
        "start": 1975.14,
        "duration": 3.0,
        "text": "you can potentially build an app to do"
      },
      {
        "start": 1976.88,
        "duration": 2.88,
        "text": "this"
      },
      {
        "start": 1978.14,
        "duration": 3.6,
        "text": "um so at the top"
      },
      {
        "start": 1979.76,
        "duration": 4.44,
        "text": "um really basic setup right we install"
      },
      {
        "start": 1981.74,
        "duration": 4.439,
        "text": "our necessary dependencies"
      },
      {
        "start": 1984.2,
        "duration": 3.839,
        "text": "um like Alan said we're using uh vertex"
      },
      {
        "start": 1986.179,
        "duration": 4.62,
        "text": "AI for this demo right so we need to"
      },
      {
        "start": 1988.039,
        "duration": 5.161,
        "text": "install Google dependencies along with"
      },
      {
        "start": 1990.799,
        "duration": 5.401,
        "text": "the python driver our own python driver"
      },
      {
        "start": 1993.2,
        "duration": 5.339,
        "text": "that supports vectors"
      },
      {
        "start": 1996.2,
        "duration": 4.38,
        "text": "um we do some basic environment"
      },
      {
        "start": 1998.539,
        "duration": 5.041,
        "text": "variables set up right we need the"
      },
      {
        "start": 2000.58,
        "duration": 3.839,
        "text": "intercom API we need Cassandra creds we"
      },
      {
        "start": 2003.58,
        "duration": 1.699,
        "text": "need"
      },
      {
        "start": 2004.419,
        "duration": 3.6,
        "text": "um"
      },
      {
        "start": 2005.279,
        "duration": 5.02,
        "text": "uh protects AI credits"
      },
      {
        "start": 2008.019,
        "duration": 3.78,
        "text": "um and then the process is pretty"
      },
      {
        "start": 2010.299,
        "duration": 3.24,
        "text": "straightforward"
      },
      {
        "start": 2011.799,
        "duration": 3.6,
        "text": "um so first of all what we do is we call"
      },
      {
        "start": 2013.539,
        "duration": 3.36,
        "text": "the intercom API to pull all our"
      },
      {
        "start": 2015.399,
        "duration": 2.88,
        "text": "conversation history"
      },
      {
        "start": 2016.899,
        "duration": 4.321,
        "text": "um so this is just kind of a simple call"
      },
      {
        "start": 2018.279,
        "duration": 4.981,
        "text": "to intercom to um get all the"
      },
      {
        "start": 2021.22,
        "duration": 3.839,
        "text": "conversations and then iterate through"
      },
      {
        "start": 2023.26,
        "duration": 4.7,
        "text": "each of the pages"
      },
      {
        "start": 2025.059,
        "duration": 2.901,
        "text": "of conversations"
      },
      {
        "start": 2028.299,
        "duration": 3.6,
        "text": "um down here we do some Google vertex AI"
      },
      {
        "start": 2030.82,
        "duration": 2.52,
        "text": "setup"
      },
      {
        "start": 2031.899,
        "duration": 4.38,
        "text": "um we were at the library we set our"
      },
      {
        "start": 2033.34,
        "duration": 5.4,
        "text": "project ID we load our credentials"
      },
      {
        "start": 2036.279,
        "duration": 5.341,
        "text": "um and it really is just a couple API"
      },
      {
        "start": 2038.74,
        "duration": 5.159,
        "text": "calls to um create the embeddings right"
      },
      {
        "start": 2041.62,
        "duration": 4.02,
        "text": "so here we can see here's just kind of"
      },
      {
        "start": 2043.899,
        "duration": 4.141,
        "text": "example of how you create embeddings"
      },
      {
        "start": 2045.64,
        "duration": 5.16,
        "text": "Google has a very simple get embeddings"
      },
      {
        "start": 2048.04,
        "duration": 4.74,
        "text": "API where you can feed in the Raw"
      },
      {
        "start": 2050.8,
        "duration": 3.359,
        "text": "unstructured text and you get back into"
      },
      {
        "start": 2052.78,
        "duration": 4.139,
        "text": "bedding"
      },
      {
        "start": 2054.159,
        "duration": 4.321,
        "text": "um so if you wanted to kind of create"
      },
      {
        "start": 2056.919,
        "duration": 2.94,
        "text": "embeddings for all of the intercom"
      },
      {
        "start": 2058.48,
        "duration": 3.54,
        "text": "conversations"
      },
      {
        "start": 2059.859,
        "duration": 3.961,
        "text": "it's pretty straightforward right so"
      },
      {
        "start": 2062.02,
        "duration": 4.26,
        "text": "here we're doing some basic setup of the"
      },
      {
        "start": 2063.82,
        "duration": 4.799,
        "text": "tables I'm in the vector store right"
      },
      {
        "start": 2066.28,
        "duration": 4.859,
        "text": "we're creating the chat table we're"
      },
      {
        "start": 2068.619,
        "duration": 4.381,
        "text": "creating an Sai index"
      },
      {
        "start": 2071.139,
        "duration": 2.941,
        "text": "um and then all we do is we iterate"
      },
      {
        "start": 2073.0,
        "duration": 2.94,
        "text": "through each"
      },
      {
        "start": 2074.08,
        "duration": 3.12,
        "text": "we iterate through a list of"
      },
      {
        "start": 2075.94,
        "duration": 3.3,
        "text": "conversations"
      },
      {
        "start": 2077.2,
        "duration": 3.899,
        "text": "um for each conversation we generate the"
      },
      {
        "start": 2079.24,
        "duration": 3.54,
        "text": "embedding and then we're loading it all"
      },
      {
        "start": 2081.099,
        "duration": 4.441,
        "text": "into Cassandra"
      },
      {
        "start": 2082.78,
        "duration": 4.559,
        "text": "um so pretty straightforward"
      },
      {
        "start": 2085.54,
        "duration": 4.68,
        "text": "um and down here we just have kind of an"
      },
      {
        "start": 2087.339,
        "duration": 4.26,
        "text": "example of how you potentially make uh a"
      },
      {
        "start": 2090.22,
        "duration": 2.34,
        "text": "vector search once you've load all this"
      },
      {
        "start": 2091.599,
        "duration": 3.121,
        "text": "data"
      },
      {
        "start": 2092.56,
        "duration": 4.079,
        "text": "um so in order to actually query the"
      },
      {
        "start": 2094.72,
        "duration": 3.42,
        "text": "vector store you would need to convert"
      },
      {
        "start": 2096.639,
        "duration": 3.061,
        "text": "your search query into an embedding"
      },
      {
        "start": 2098.14,
        "duration": 2.76,
        "text": "right we're doing that right here so"
      },
      {
        "start": 2099.7,
        "duration": 3.419,
        "text": "let's say the question is where do I"
      },
      {
        "start": 2100.9,
        "duration": 3.42,
        "text": "find a token we'd first generate the"
      },
      {
        "start": 2103.119,
        "duration": 3.361,
        "text": "embedding"
      },
      {
        "start": 2104.32,
        "duration": 4.2,
        "text": "um and then it's a really simple query"
      },
      {
        "start": 2106.48,
        "duration": 3.84,
        "text": "right get the raw text where the"
      },
      {
        "start": 2108.52,
        "duration": 3.36,
        "text": "embedding is the embedding which is"
      },
      {
        "start": 2110.32,
        "duration": 3.48,
        "text": "generated"
      },
      {
        "start": 2111.88,
        "duration": 4.44,
        "text": "um and I'll hand it back to Alan to walk"
      },
      {
        "start": 2113.8,
        "duration": 4.799,
        "text": "through the next step of the process"
      },
      {
        "start": 2116.32,
        "duration": 3.96,
        "text": "so there's a few tips that you that you"
      },
      {
        "start": 2118.599,
        "duration": 4.26,
        "text": "should remember when you're doing your"
      },
      {
        "start": 2120.28,
        "duration": 5.22,
        "text": "data pre-processing number one is that"
      },
      {
        "start": 2122.859,
        "duration": 5.521,
        "text": "clean data matters so if you garbage in"
      },
      {
        "start": 2125.5,
        "duration": 4.44,
        "text": "garbage out if you put irrelevant if you"
      },
      {
        "start": 2128.38,
        "duration": 3.84,
        "text": "put data that's"
      },
      {
        "start": 2129.94,
        "duration": 4.32,
        "text": "um that's uh it's actually irrelevant"
      },
      {
        "start": 2132.22,
        "duration": 4.2,
        "text": "data is not that important that's not"
      },
      {
        "start": 2134.26,
        "duration": 3.96,
        "text": "not that terrible it's actually data"
      },
      {
        "start": 2136.42,
        "duration": 4.56,
        "text": "that's wrong so if you put in answers"
      },
      {
        "start": 2138.22,
        "duration": 5.7,
        "text": "that are semantically wrong your llm is"
      },
      {
        "start": 2140.98,
        "duration": 6.06,
        "text": "going to give you actually wrong answers"
      },
      {
        "start": 2143.92,
        "duration": 5.22,
        "text": "the second thing is to actually properly"
      },
      {
        "start": 2147.04,
        "duration": 7.38,
        "text": "split your data and this is kind of a"
      },
      {
        "start": 2149.14,
        "duration": 6.959,
        "text": "200 level type of a a tip here so when"
      },
      {
        "start": 2154.42,
        "duration": 3.36,
        "text": "you're processing a very large document"
      },
      {
        "start": 2156.099,
        "duration": 4.621,
        "text": "you don't want to store the entire"
      },
      {
        "start": 2157.78,
        "duration": 4.38,
        "text": "document into the vector database what"
      },
      {
        "start": 2160.72,
        "duration": 5.28,
        "text": "you actually want to do is you want to"
      },
      {
        "start": 2162.16,
        "duration": 6.06,
        "text": "Chunk Up the document into pieces that"
      },
      {
        "start": 2166.0,
        "duration": 4.02,
        "text": "are small enough that you could put in"
      },
      {
        "start": 2168.22,
        "duration": 4.2,
        "text": "the large language model"
      },
      {
        "start": 2170.02,
        "duration": 4.86,
        "text": "and when you split when you chunk it up"
      },
      {
        "start": 2172.42,
        "duration": 4.919,
        "text": "you don't chunk it up let's say take a"
      },
      {
        "start": 2174.88,
        "duration": 4.44,
        "text": "document and evenly split into 10 chunks"
      },
      {
        "start": 2177.339,
        "duration": 4.381,
        "text": "what you would do is that you would take"
      },
      {
        "start": 2179.32,
        "duration": 4.98,
        "text": "the document and you would split it into"
      },
      {
        "start": 2181.72,
        "duration": 4.8,
        "text": "10 overlapping chunks and this is"
      },
      {
        "start": 2184.3,
        "duration": 5.46,
        "text": "necessary because the semantic search"
      },
      {
        "start": 2186.52,
        "duration": 5.52,
        "text": "capability needs to have"
      },
      {
        "start": 2189.76,
        "duration": 5.04,
        "text": "um a little bit for for a particular"
      },
      {
        "start": 2192.04,
        "duration": 4.319,
        "text": "paragraph of text it only makes sense if"
      },
      {
        "start": 2194.8,
        "duration": 3.9,
        "text": "it has a little bit of information about"
      },
      {
        "start": 2196.359,
        "duration": 3.661,
        "text": "the previous paragraph of text so you"
      },
      {
        "start": 2198.7,
        "duration": 2.82,
        "text": "have to be very smart and this is"
      },
      {
        "start": 2200.02,
        "duration": 3.66,
        "text": "something that you need to there's no"
      },
      {
        "start": 2201.52,
        "duration": 3.9,
        "text": "out of the magic answer for every piece"
      },
      {
        "start": 2203.68,
        "duration": 3.419,
        "text": "of the text you got to be really smart"
      },
      {
        "start": 2205.42,
        "duration": 3.36,
        "text": "about it"
      },
      {
        "start": 2207.099,
        "duration": 3.541,
        "text": "uh the third thing that you want to do"
      },
      {
        "start": 2208.78,
        "duration": 4.26,
        "text": "is you want to be careful about loading"
      },
      {
        "start": 2210.64,
        "duration": 4.5,
        "text": "the information at speed so one of the"
      },
      {
        "start": 2213.04,
        "duration": 5.6,
        "text": "nice things about Cassandra is that it"
      },
      {
        "start": 2215.14,
        "duration": 6.06,
        "text": "can it can do a lot of parallel"
      },
      {
        "start": 2218.64,
        "duration": 5.74,
        "text": "inserts at the order of thousands of"
      },
      {
        "start": 2221.2,
        "duration": 5.28,
        "text": "Records per second for just even like a"
      },
      {
        "start": 2224.38,
        "duration": 4.14,
        "text": "three note a small three node cluster so"
      },
      {
        "start": 2226.48,
        "duration": 4.2,
        "text": "you want to leverage parallelization we"
      },
      {
        "start": 2228.52,
        "duration": 4.5,
        "text": "have some tools called DS bulk that"
      },
      {
        "start": 2230.68,
        "duration": 4.5,
        "text": "helps you load the data faster"
      },
      {
        "start": 2233.02,
        "duration": 5.24,
        "text": "now another part that we have to be very"
      },
      {
        "start": 2235.18,
        "duration": 5.22,
        "text": "careful is keeping the data safe"
      },
      {
        "start": 2238.26,
        "duration": 4.359,
        "text": "especially if you're trying to leverage"
      },
      {
        "start": 2240.4,
        "duration": 3.26,
        "text": "your chat history for a few shot"
      },
      {
        "start": 2242.619,
        "duration": 3.661,
        "text": "learning"
      },
      {
        "start": 2243.66,
        "duration": 5.919,
        "text": "you have to make sure you strip out all"
      },
      {
        "start": 2246.28,
        "duration": 5.52,
        "text": "the pii data so you know customers often"
      },
      {
        "start": 2249.579,
        "duration": 3.78,
        "text": "do things like put in their tokens in"
      },
      {
        "start": 2251.8,
        "duration": 4.08,
        "text": "the chat history things of that sort"
      },
      {
        "start": 2253.359,
        "duration": 4.681,
        "text": "their their proprietary secrets you want"
      },
      {
        "start": 2255.88,
        "duration": 3.12,
        "text": "to make sure that's outside kept on your"
      },
      {
        "start": 2258.04,
        "duration": 3.66,
        "text": "data set"
      },
      {
        "start": 2259.0,
        "duration": 4.8,
        "text": "and last but not least the large"
      },
      {
        "start": 2261.7,
        "duration": 4.32,
        "text": "language model is only as good as data"
      },
      {
        "start": 2263.8,
        "duration": 5.279,
        "text": "it has so you need to make sure that the"
      },
      {
        "start": 2266.02,
        "duration": 5.94,
        "text": "information you have is relevant uh and"
      },
      {
        "start": 2269.079,
        "duration": 5.221,
        "text": "updated on a regular basis"
      },
      {
        "start": 2271.96,
        "duration": 5.34,
        "text": "okay with that let's kind of get into"
      },
      {
        "start": 2274.3,
        "duration": 6.539,
        "text": "the details of prompt engineering"
      },
      {
        "start": 2277.3,
        "duration": 6.72,
        "text": "so the purpose of prompt engineering is"
      },
      {
        "start": 2280.839,
        "duration": 5.541,
        "text": "to make sure that the data that's being"
      },
      {
        "start": 2284.02,
        "duration": 6.42,
        "text": "the prompt that's being sent to the llm"
      },
      {
        "start": 2286.38,
        "duration": 6.42,
        "text": "is as contextually relevant as possible"
      },
      {
        "start": 2290.44,
        "duration": 4.679,
        "text": "and this is not easy because"
      },
      {
        "start": 2292.8,
        "duration": 4.12,
        "text": "LMS are inherently they're kind of like"
      },
      {
        "start": 2295.119,
        "duration": 3.841,
        "text": "human beings there's only a certain"
      },
      {
        "start": 2296.92,
        "duration": 3.96,
        "text": "amount of working memory that it has so"
      },
      {
        "start": 2298.96,
        "duration": 4.619,
        "text": "it has it has something called a token"
      },
      {
        "start": 2300.88,
        "duration": 5.219,
        "text": "limit you can't throw your entire"
      },
      {
        "start": 2303.579,
        "duration": 5.28,
        "text": "knowledge base into the llm because"
      },
      {
        "start": 2306.099,
        "duration": 5.701,
        "text": "there's simply not enough memory for the"
      },
      {
        "start": 2308.859,
        "duration": 5.881,
        "text": "working memory for the llm to take it to"
      },
      {
        "start": 2311.8,
        "duration": 5.7,
        "text": "do so so we have prompt templates and"
      },
      {
        "start": 2314.74,
        "duration": 4.379,
        "text": "testing framework that you can use and"
      },
      {
        "start": 2317.5,
        "duration": 3.839,
        "text": "it's really important to kind of compare"
      },
      {
        "start": 2319.119,
        "duration": 3.901,
        "text": "all the various prompts"
      },
      {
        "start": 2321.339,
        "duration": 5.161,
        "text": "now there's a there's a bunch of"
      },
      {
        "start": 2323.02,
        "duration": 7.2,
        "text": "challenges so the first challenge is how"
      },
      {
        "start": 2326.5,
        "duration": 5.52,
        "text": "do we make sure that the results are"
      },
      {
        "start": 2330.22,
        "duration": 4.619,
        "text": "qualitatively good so how do you"
      },
      {
        "start": 2332.02,
        "duration": 4.92,
        "text": "actually go do around that testing"
      },
      {
        "start": 2334.839,
        "duration": 3.901,
        "text": "um sometimes there's a balance of detail"
      },
      {
        "start": 2336.94,
        "duration": 3.96,
        "text": "you want to give enough information to"
      },
      {
        "start": 2338.74,
        "duration": 4.619,
        "text": "the llm but you don't want to give too"
      },
      {
        "start": 2340.9,
        "duration": 4.679,
        "text": "much llm information to the llm"
      },
      {
        "start": 2343.359,
        "duration": 5.821,
        "text": "otherwise it gives a very vague"
      },
      {
        "start": 2345.579,
        "duration": 5.881,
        "text": "responses and then last but not least"
      },
      {
        "start": 2349.18,
        "duration": 4.919,
        "text": "um there's relevancy and safety so how"
      },
      {
        "start": 2351.46,
        "duration": 5.04,
        "text": "do we prevent the llm from saying"
      },
      {
        "start": 2354.099,
        "duration": 4.141,
        "text": "harmful things irrelevant things or very"
      },
      {
        "start": 2356.5,
        "duration": 3.0,
        "text": "inappropriate things how do we do all"
      },
      {
        "start": 2358.24,
        "duration": 3.78,
        "text": "that"
      },
      {
        "start": 2359.5,
        "duration": 4.32,
        "text": "so Kyle let's go into the data flow so"
      },
      {
        "start": 2362.02,
        "duration": 5.12,
        "text": "let's say a customer uses the"
      },
      {
        "start": 2363.82,
        "duration": 6.9,
        "text": "application calls the nosql assistant"
      },
      {
        "start": 2367.14,
        "duration": 5.439,
        "text": "to with a question the first thing that"
      },
      {
        "start": 2370.72,
        "duration": 4.98,
        "text": "happens is that the nosql assistant"
      },
      {
        "start": 2372.579,
        "duration": 6.301,
        "text": "calls the vertex AI to get the embedding"
      },
      {
        "start": 2375.7,
        "duration": 6.659,
        "text": "uh so what for that question semantic"
      },
      {
        "start": 2378.88,
        "duration": 5.4,
        "text": "what are vectors that are close to it"
      },
      {
        "start": 2382.359,
        "duration": 5.941,
        "text": "um and then this this way is I just just"
      },
      {
        "start": 2384.28,
        "duration": 5.819,
        "text": "a reminder of how this is done is that"
      },
      {
        "start": 2388.3,
        "duration": 5.039,
        "text": "um you know there's there's the the"
      },
      {
        "start": 2390.099,
        "duration": 6.121,
        "text": "database already has all these vectors"
      },
      {
        "start": 2393.339,
        "duration": 6.361,
        "text": "in it and the question that's being"
      },
      {
        "start": 2396.22,
        "duration": 6.0,
        "text": "asked is related to some of those uh"
      },
      {
        "start": 2399.7,
        "duration": 4.32,
        "text": "those uh pieces of data in this case"
      },
      {
        "start": 2402.22,
        "duration": 4.56,
        "text": "product information and Returns the"
      },
      {
        "start": 2404.02,
        "duration": 5.16,
        "text": "correct product information"
      },
      {
        "start": 2406.78,
        "duration": 6.18,
        "text": "so for example how do I generate a token"
      },
      {
        "start": 2409.18,
        "duration": 6.48,
        "text": "this turns back into the 1500 floatware"
      },
      {
        "start": 2412.96,
        "duration": 3.84,
        "text": "float vector and it finds the closest"
      },
      {
        "start": 2415.66,
        "duration": 3.3,
        "text": "ones"
      },
      {
        "start": 2416.8,
        "duration": 4.559,
        "text": "and so when you're doing the vector when"
      },
      {
        "start": 2418.96,
        "duration": 4.379,
        "text": "you're calling the vector store not only"
      },
      {
        "start": 2421.359,
        "duration": 4.861,
        "text": "are you calling the vector"
      },
      {
        "start": 2423.339,
        "duration": 5.041,
        "text": "content but you're also calling the"
      },
      {
        "start": 2426.22,
        "duration": 4.619,
        "text": "vector store uh the the Cassandra"
      },
      {
        "start": 2428.38,
        "duration": 4.02,
        "text": "database for information that's relevant"
      },
      {
        "start": 2430.839,
        "duration": 3.721,
        "text": "about the user"
      },
      {
        "start": 2432.4,
        "duration": 4.8,
        "text": "so what are what is the elements of a"
      },
      {
        "start": 2434.56,
        "duration": 4.74,
        "text": "good prompt so a good prompt has the"
      },
      {
        "start": 2437.2,
        "duration": 4.62,
        "text": "directive so what what the kind of what"
      },
      {
        "start": 2439.3,
        "duration": 4.98,
        "text": "kind of Bot that person is it usually"
      },
      {
        "start": 2441.82,
        "duration": 5.16,
        "text": "has information about the user the user"
      },
      {
        "start": 2444.28,
        "duration": 5.28,
        "text": "itself like who's this particular person"
      },
      {
        "start": 2446.98,
        "duration": 3.66,
        "text": "whether that person particularly done in"
      },
      {
        "start": 2449.56,
        "duration": 3.6,
        "text": "the past"
      },
      {
        "start": 2450.64,
        "duration": 4.02,
        "text": "and then also the previous interactions"
      },
      {
        "start": 2453.16,
        "duration": 3.36,
        "text": "with the user"
      },
      {
        "start": 2454.66,
        "duration": 4.199,
        "text": "so this is what we often call the chat"
      },
      {
        "start": 2456.52,
        "duration": 4.26,
        "text": "history so when somebody's typing in and"
      },
      {
        "start": 2458.859,
        "duration": 4.681,
        "text": "having a conversation just like a"
      },
      {
        "start": 2460.78,
        "duration": 4.5,
        "text": "regular human being the llm needs to"
      },
      {
        "start": 2463.54,
        "duration": 4.02,
        "text": "know what are the previous chats it had"
      },
      {
        "start": 2465.28,
        "duration": 4.559,
        "text": "in the past in the context for that"
      },
      {
        "start": 2467.56,
        "duration": 3.779,
        "text": "then it would have then also the prompt"
      },
      {
        "start": 2469.839,
        "duration": 3.421,
        "text": "would have relevant information like"
      },
      {
        "start": 2471.339,
        "duration": 3.961,
        "text": "sorry the relevant documents that give"
      },
      {
        "start": 2473.26,
        "duration": 4.38,
        "text": "the answer and then finally of course"
      },
      {
        "start": 2475.3,
        "duration": 4.38,
        "text": "the prompt has a user's question"
      },
      {
        "start": 2477.64,
        "duration": 3.78,
        "text": "so let's take a look at this is kind of"
      },
      {
        "start": 2479.68,
        "duration": 4.62,
        "text": "a subset of all this information but"
      },
      {
        "start": 2481.42,
        "duration": 5.04,
        "text": "let's take a look at uh if the prompt"
      },
      {
        "start": 2484.3,
        "duration": 4.62,
        "text": "asks if the user asks how do I generate"
      },
      {
        "start": 2486.46,
        "duration": 5.399,
        "text": "a token inside the prompt it would have"
      },
      {
        "start": 2488.92,
        "duration": 5.159,
        "text": "a directive so in this case we're"
      },
      {
        "start": 2491.859,
        "duration": 4.921,
        "text": "telling using the using the person's"
      },
      {
        "start": 2494.079,
        "duration": 4.861,
        "text": "profile information we determine that"
      },
      {
        "start": 2496.78,
        "duration": 4.92,
        "text": "this is an advanced user trying to"
      },
      {
        "start": 2498.94,
        "duration": 4.5,
        "text": "perform some actions in the database and"
      },
      {
        "start": 2501.7,
        "duration": 3.899,
        "text": "this is very helpful because if you tell"
      },
      {
        "start": 2503.44,
        "duration": 5.58,
        "text": "that llm that this is an advanced User"
      },
      {
        "start": 2505.599,
        "duration": 6.061,
        "text": "it's it ends up not providing it"
      },
      {
        "start": 2509.02,
        "duration": 4.74,
        "text": "providing the user more advanced answers"
      },
      {
        "start": 2511.66,
        "duration": 3.84,
        "text": "so like more sustained code and things"
      },
      {
        "start": 2513.76,
        "duration": 5.0,
        "text": "of that sort it assumes that the user"
      },
      {
        "start": 2515.5,
        "duration": 3.26,
        "text": "knows how to use Cassandra already"
      },
      {
        "start": 2519.099,
        "duration": 4.561,
        "text": "um now we also provide uh information"
      },
      {
        "start": 2521.32,
        "duration": 4.62,
        "text": "about the user and some of the databases"
      },
      {
        "start": 2523.66,
        "duration": 5.22,
        "text": "that the person has created these are"
      },
      {
        "start": 2525.94,
        "duration": 6.0,
        "text": "just regular key value pair lookups um"
      },
      {
        "start": 2528.88,
        "duration": 6.3,
        "text": "in Cassandra nothing fancy here and then"
      },
      {
        "start": 2531.94,
        "duration": 5.76,
        "text": "we have a bunch of unstructured uh data"
      },
      {
        "start": 2535.18,
        "duration": 5.1,
        "text": "in this case we're showing similar chat"
      },
      {
        "start": 2537.7,
        "duration": 5.28,
        "text": "history conversations with this user and"
      },
      {
        "start": 2540.28,
        "duration": 5.579,
        "text": "you can see over here there's like three"
      },
      {
        "start": 2542.98,
        "duration": 5.04,
        "text": "different conversations with various"
      },
      {
        "start": 2545.859,
        "duration": 5.161,
        "text": "folks like Melissa who's one of our chat"
      },
      {
        "start": 2548.02,
        "duration": 6.0,
        "text": "support reps who gives answers on how to"
      },
      {
        "start": 2551.02,
        "duration": 6.68,
        "text": "connect to the how to connect to uh to a"
      },
      {
        "start": 2554.02,
        "duration": 6.12,
        "text": "particular API for generating a token"
      },
      {
        "start": 2557.7,
        "duration": 4.899,
        "text": "a talk a little bit about prompt"
      },
      {
        "start": 2560.14,
        "duration": 3.959,
        "text": "selection so what you do is that for"
      },
      {
        "start": 2562.599,
        "duration": 3.841,
        "text": "prompt selection you would use the"
      },
      {
        "start": 2564.099,
        "duration": 4.441,
        "text": "information about the user's profile the"
      },
      {
        "start": 2566.44,
        "duration": 4.56,
        "text": "recent history and then select a"
      },
      {
        "start": 2568.54,
        "duration": 4.26,
        "text": "particular prompt to use and this prompt"
      },
      {
        "start": 2571.0,
        "duration": 3.839,
        "text": "is basic template you would select a"
      },
      {
        "start": 2572.8,
        "duration": 3.779,
        "text": "prompt template to use so in this"
      },
      {
        "start": 2574.839,
        "duration": 4.861,
        "text": "scenario we have two types of prompts"
      },
      {
        "start": 2576.579,
        "duration": 6.421,
        "text": "one is a learner prompt template another"
      },
      {
        "start": 2579.7,
        "duration": 5.22,
        "text": "is a qualified user prompt template and"
      },
      {
        "start": 2583.0,
        "duration": 4.5,
        "text": "this is what a qualified user prompt"
      },
      {
        "start": 2584.92,
        "duration": 5.52,
        "text": "template would look like it would say oh"
      },
      {
        "start": 2587.5,
        "duration": 5.339,
        "text": "you know you should answer"
      },
      {
        "start": 2590.44,
        "duration": 4.74,
        "text": "um okay sorry this is a little bit cut"
      },
      {
        "start": 2592.839,
        "duration": 4.5,
        "text": "off but say you should answer customer"
      },
      {
        "start": 2595.18,
        "duration": 4.8,
        "text": "questions it tells that this person is a"
      },
      {
        "start": 2597.339,
        "duration": 5.581,
        "text": "qualified user and then the learner"
      },
      {
        "start": 2599.98,
        "duration": 5.099,
        "text": "template what it says that you are"
      },
      {
        "start": 2602.92,
        "duration": 4.439,
        "text": "giving answers to someone who's new to"
      },
      {
        "start": 2605.079,
        "duration": 5.28,
        "text": "data stacks and Cassandra so be"
      },
      {
        "start": 2607.359,
        "duration": 4.74,
        "text": "especially helpful and and be a little"
      },
      {
        "start": 2610.359,
        "duration": 3.781,
        "text": "bit more verbose on how you answer the"
      },
      {
        "start": 2612.099,
        "duration": 5.041,
        "text": "questions and if you look inside the"
      },
      {
        "start": 2614.14,
        "duration": 7.38,
        "text": "prompt template it has both a little bit"
      },
      {
        "start": 2617.14,
        "duration": 6.84,
        "text": "as um information about uh"
      },
      {
        "start": 2621.52,
        "duration": 4.319,
        "text": "um you know helpful responses from"
      },
      {
        "start": 2623.98,
        "duration": 4.44,
        "text": "intercom so this is used for prompt"
      },
      {
        "start": 2625.839,
        "duration": 5.28,
        "text": "templating sorry for a few shop uh"
      },
      {
        "start": 2628.42,
        "duration": 5.58,
        "text": "learning it has the user information it"
      },
      {
        "start": 2631.119,
        "duration": 4.921,
        "text": "has the user's questions"
      },
      {
        "start": 2634.0,
        "duration": 4.38,
        "text": "so we're going to give a demo of this so"
      },
      {
        "start": 2636.04,
        "duration": 4.44,
        "text": "we'll Alex will show"
      },
      {
        "start": 2638.38,
        "duration": 5.699,
        "text": "um the Astro assistant how to construct"
      },
      {
        "start": 2640.48,
        "duration": 5.639,
        "text": "The Prompt The Prompt templates and how"
      },
      {
        "start": 2644.079,
        "duration": 4.801,
        "text": "to test with some of History"
      },
      {
        "start": 2646.119,
        "duration": 5.581,
        "text": "okay headed over to you"
      },
      {
        "start": 2648.88,
        "duration": 7.199,
        "text": "awesome um so what you're seeing here is"
      },
      {
        "start": 2651.7,
        "duration": 7.26,
        "text": "just my ID icon and to manage prompts we"
      },
      {
        "start": 2656.079,
        "duration": 5.641,
        "text": "use just the line chain prompt templates"
      },
      {
        "start": 2658.96,
        "duration": 4.379,
        "text": "um so both of the templates that Alan"
      },
      {
        "start": 2661.72,
        "duration": 4.859,
        "text": "just mentioned are within a prompts"
      },
      {
        "start": 2663.339,
        "duration": 4.081,
        "text": "directory they're both yaml files and"
      },
      {
        "start": 2666.579,
        "duration": 3.121,
        "text": "these templates are pretty"
      },
      {
        "start": 2667.42,
        "duration": 5.52,
        "text": "straightforward essentially we Define"
      },
      {
        "start": 2669.7,
        "duration": 5.76,
        "text": "that you know the XML file is a prompt"
      },
      {
        "start": 2672.94,
        "duration": 4.139,
        "text": "um we Define input variables right here"
      },
      {
        "start": 2675.46,
        "duration": 4.08,
        "text": "there's only a single input variable"
      },
      {
        "start": 2677.079,
        "duration": 5.461,
        "text": "which is the output of the vector search"
      },
      {
        "start": 2679.54,
        "duration": 5.64,
        "text": "and that's pretty much about it"
      },
      {
        "start": 2682.54,
        "duration": 5.7,
        "text": "um the more interesting part is how the"
      },
      {
        "start": 2685.18,
        "duration": 6.06,
        "text": "prompt template is actually selected"
      },
      {
        "start": 2688.24,
        "duration": 5.099,
        "text": "um So within kind of our main bot code"
      },
      {
        "start": 2691.24,
        "duration": 4.5,
        "text": "um we have a very simple"
      },
      {
        "start": 2693.339,
        "duration": 4.081,
        "text": "um you know method to get the Persona"
      },
      {
        "start": 2695.74,
        "duration": 4.32,
        "text": "right we check"
      },
      {
        "start": 2697.42,
        "duration": 4.56,
        "text": "um the user's information on user right"
      },
      {
        "start": 2700.06,
        "duration": 3.72,
        "text": "when it when a user registers for a"
      },
      {
        "start": 2701.98,
        "duration": 3.66,
        "text": "product we basically ask them a"
      },
      {
        "start": 2703.78,
        "duration": 3.36,
        "text": "questionnaire on if the use consider"
      },
      {
        "start": 2705.64,
        "duration": 2.88,
        "text": "before what their primary programming"
      },
      {
        "start": 2707.14,
        "duration": 3.78,
        "text": "which is"
      },
      {
        "start": 2708.52,
        "duration": 3.839,
        "text": "um and and basically based on these"
      },
      {
        "start": 2710.92,
        "duration": 3.24,
        "text": "responses"
      },
      {
        "start": 2712.359,
        "duration": 5.22,
        "text": "um we determine if we want to give them"
      },
      {
        "start": 2714.16,
        "duration": 5.1,
        "text": "the learner template or"
      },
      {
        "start": 2717.579,
        "duration": 2.821,
        "text": "um the qualified template that on this"
      },
      {
        "start": 2719.26,
        "duration": 2.88,
        "text": "mentions"
      },
      {
        "start": 2720.4,
        "duration": 3.6,
        "text": "um so here's where we actually select"
      },
      {
        "start": 2722.14,
        "duration": 3.0,
        "text": "the template and then"
      },
      {
        "start": 2724.0,
        "duration": 3.839,
        "text": "um you know you've seen the single"
      },
      {
        "start": 2725.14,
        "duration": 7.02,
        "text": "products now here the actual templates"
      },
      {
        "start": 2727.839,
        "duration": 6.361,
        "text": "um and we really struggled with kind of"
      },
      {
        "start": 2732.16,
        "duration": 3.84,
        "text": "how do you actually test these templates"
      },
      {
        "start": 2734.2,
        "duration": 4.32,
        "text": "how do you actually make sure that the"
      },
      {
        "start": 2736.0,
        "duration": 5.16,
        "text": "responses are high quality"
      },
      {
        "start": 2738.52,
        "duration": 5.64,
        "text": "um so to basically debug this process"
      },
      {
        "start": 2741.16,
        "duration": 7.199,
        "text": "and consistently improve our templates"
      },
      {
        "start": 2744.16,
        "duration": 5.58,
        "text": "um we built a pretty cool slack bot"
      },
      {
        "start": 2748.359,
        "duration": 4.921,
        "text": "um so let me"
      },
      {
        "start": 2749.74,
        "duration": 6.119,
        "text": "set that up real quick okay awesome"
      },
      {
        "start": 2753.28,
        "duration": 4.62,
        "text": "um so on the left we have"
      },
      {
        "start": 2755.859,
        "duration": 4.861,
        "text": "just the desperate of our product in the"
      },
      {
        "start": 2757.9,
        "duration": 5.4,
        "text": "chat chatbot and on the right we have a"
      },
      {
        "start": 2760.72,
        "duration": 5.04,
        "text": "slack Channel we set up to automatically"
      },
      {
        "start": 2763.3,
        "duration": 4.38,
        "text": "log all the prompts along with the"
      },
      {
        "start": 2765.76,
        "duration": 3.9,
        "text": "responses um so we can keep reviewing"
      },
      {
        "start": 2767.68,
        "duration": 3.12,
        "text": "and improving the quality"
      },
      {
        "start": 2769.66,
        "duration": 2.34,
        "text": "um so I'm just going to ask a simple"
      },
      {
        "start": 2770.8,
        "duration": 4.279,
        "text": "question"
      },
      {
        "start": 2772.0,
        "duration": 3.079,
        "text": "the chatbot"
      },
      {
        "start": 2779.88,
        "duration": 5.56,
        "text": "and on the right"
      },
      {
        "start": 2782.56,
        "duration": 4.559,
        "text": "um we can see the entire prompt along"
      },
      {
        "start": 2785.44,
        "duration": 5.48,
        "text": "with the response"
      },
      {
        "start": 2787.119,
        "duration": 3.801,
        "text": "I'm just going to scroll up a bit"
      },
      {
        "start": 2793.42,
        "duration": 3.659,
        "text": "um so here we can see we have the"
      },
      {
        "start": 2794.8,
        "duration": 5.039,
        "text": "directives basically over here is the"
      },
      {
        "start": 2797.079,
        "duration": 5.52,
        "text": "full prompt that I'll just talked about"
      },
      {
        "start": 2799.839,
        "duration": 4.321,
        "text": "um so first we have the directive"
      },
      {
        "start": 2802.599,
        "duration": 4.621,
        "text": "um"
      },
      {
        "start": 2804.16,
        "duration": 4.439,
        "text": "we then have the output of the vector"
      },
      {
        "start": 2807.22,
        "duration": 4.7,
        "text": "search right through here you can see a"
      },
      {
        "start": 2808.599,
        "duration": 3.321,
        "text": "couple pieces of documentation"
      },
      {
        "start": 2812.2,
        "duration": 5.04,
        "text": "we have information on the user so"
      },
      {
        "start": 2814.839,
        "duration": 4.081,
        "text": "here's the username their email"
      },
      {
        "start": 2817.24,
        "duration": 3.839,
        "text": "um their primary programming language"
      },
      {
        "start": 2818.92,
        "duration": 4.08,
        "text": "and we have information on all of our"
      },
      {
        "start": 2821.079,
        "duration": 5.101,
        "text": "databases what's really interesting here"
      },
      {
        "start": 2823.0,
        "duration": 5.16,
        "text": "is you can be the LM raw"
      },
      {
        "start": 2826.18,
        "duration": 4.74,
        "text": "um Json and table two"
      },
      {
        "start": 2828.16,
        "duration": 5.159,
        "text": "interpret all that data"
      },
      {
        "start": 2830.92,
        "duration": 5.22,
        "text": "and then finally we have the response"
      },
      {
        "start": 2833.319,
        "duration": 4.621,
        "text": "that was actually fed back to the user"
      },
      {
        "start": 2836.14,
        "duration": 3.719,
        "text": "um so really our process of testing and"
      },
      {
        "start": 2837.94,
        "duration": 3.54,
        "text": "improving the prompt has been I'm"
      },
      {
        "start": 2839.859,
        "duration": 3.361,
        "text": "reviewing all the prompts reviewing all"
      },
      {
        "start": 2841.48,
        "duration": 4.2,
        "text": "the responses"
      },
      {
        "start": 2843.22,
        "duration": 3.72,
        "text": "um you know let's say we get"
      },
      {
        "start": 2845.68,
        "duration": 3.899,
        "text": "um"
      },
      {
        "start": 2846.94,
        "duration": 5.1,
        "text": "you know bad documentation left but"
      },
      {
        "start": 2849.579,
        "duration": 3.78,
        "text": "sorry bad documentation back maybe we'd"
      },
      {
        "start": 2852.04,
        "duration": 3.18,
        "text": "update the data in the vector store"
      },
      {
        "start": 2853.359,
        "duration": 4.5,
        "text": "let's say you know maybe there isn't"
      },
      {
        "start": 2855.22,
        "duration": 4.32,
        "text": "enough information on a topic we insert"
      },
      {
        "start": 2857.859,
        "duration": 3.421,
        "text": "new documentation in the vector store"
      },
      {
        "start": 2859.54,
        "duration": 3.779,
        "text": "but really it's just been kind of this"
      },
      {
        "start": 2861.28,
        "duration": 4.559,
        "text": "iterative process of"
      },
      {
        "start": 2863.319,
        "duration": 5.641,
        "text": "um you know we review uh The Prompt the"
      },
      {
        "start": 2865.839,
        "duration": 5.461,
        "text": "response and we kind of keep improving"
      },
      {
        "start": 2868.96,
        "duration": 3.78,
        "text": "um and now I'll hand it back to you just"
      },
      {
        "start": 2871.3,
        "duration": 3.36,
        "text": "before you go there"
      },
      {
        "start": 2872.74,
        "duration": 3.0,
        "text": "um actually if you bring up your screen"
      },
      {
        "start": 2874.66,
        "duration": 3.78,
        "text": "for a second"
      },
      {
        "start": 2875.74,
        "duration": 4.5,
        "text": "yes go back to there you can see if you"
      },
      {
        "start": 2878.44,
        "duration": 4.74,
        "text": "look in the document in the prompt"
      },
      {
        "start": 2880.24,
        "duration": 5.339,
        "text": "template in the in the prompt there's a"
      },
      {
        "start": 2883.18,
        "duration": 4.56,
        "text": "lot of information in here"
      },
      {
        "start": 2885.579,
        "duration": 3.601,
        "text": "um and if you go up into the into the"
      },
      {
        "start": 2887.74,
        "duration": 3.48,
        "text": "the responses"
      },
      {
        "start": 2889.18,
        "duration": 5.82,
        "text": "The Prompt template even to the human"
      },
      {
        "start": 2891.22,
        "duration": 6.06,
        "text": "eye doesn't look super understandable so"
      },
      {
        "start": 2895.0,
        "duration": 4.44,
        "text": "for example there's this in the middle"
      },
      {
        "start": 2897.28,
        "duration": 4.38,
        "text": "of the screen that says Ah the token"
      },
      {
        "start": 2899.44,
        "duration": 4.56,
        "text": "will no longer exist the length of time"
      },
      {
        "start": 2901.66,
        "duration": 5.1,
        "text": "to persist the token is configurable"
      },
      {
        "start": 2904.0,
        "duration": 4.859,
        "text": "that's unrelated to how to generate a"
      },
      {
        "start": 2906.76,
        "duration": 5.04,
        "text": "token and so one of the things that we"
      },
      {
        "start": 2908.859,
        "duration": 4.921,
        "text": "notice is that the semantic search that"
      },
      {
        "start": 2911.8,
        "duration": 4.14,
        "text": "you're doing from the vector store it's"
      },
      {
        "start": 2913.78,
        "duration": 4.86,
        "text": "more of a gross level search it doesn't"
      },
      {
        "start": 2915.94,
        "duration": 6.12,
        "text": "it doesn't exactly understand the the"
      },
      {
        "start": 2918.64,
        "duration": 5.16,
        "text": "the the context of the question it's"
      },
      {
        "start": 2922.06,
        "duration": 2.82,
        "text": "able to get things that are related to"
      },
      {
        "start": 2923.8,
        "duration": 5.16,
        "text": "it"
      },
      {
        "start": 2924.88,
        "duration": 6.479,
        "text": "um and the llm is extremely good at"
      },
      {
        "start": 2928.96,
        "duration": 4.74,
        "text": "ignoring irrelevant information"
      },
      {
        "start": 2931.359,
        "duration": 4.141,
        "text": "so if we kind of go back to I'm going to"
      },
      {
        "start": 2933.7,
        "duration": 3.119,
        "text": "kind of Select so talking a little bit"
      },
      {
        "start": 2935.5,
        "duration": 3.839,
        "text": "about"
      },
      {
        "start": 2936.819,
        "duration": 5.101,
        "text": "um if you can stop your share screen I'm"
      },
      {
        "start": 2939.339,
        "duration": 4.681,
        "text": "going to show you some particular tips"
      },
      {
        "start": 2941.92,
        "duration": 4.38,
        "text": "that you can use to help improve your"
      },
      {
        "start": 2944.02,
        "duration": 5.099,
        "text": "prompts"
      },
      {
        "start": 2946.3,
        "duration": 4.92,
        "text": "so the first thing is uh Vector search"
      },
      {
        "start": 2949.119,
        "duration": 3.801,
        "text": "so what are the things that you really"
      },
      {
        "start": 2951.22,
        "duration": 5.22,
        "text": "care about the vector search number one"
      },
      {
        "start": 2952.92,
        "duration": 6.399,
        "text": "is that if there's no l if there's no"
      },
      {
        "start": 2956.44,
        "duration": 5.879,
        "text": "data that's coming back relevant from"
      },
      {
        "start": 2959.319,
        "duration": 6.121,
        "text": "your knowledge base there's a very high"
      },
      {
        "start": 2962.319,
        "duration": 5.161,
        "text": "probability that you're going to have a"
      },
      {
        "start": 2965.44,
        "duration": 3.84,
        "text": "hallucination"
      },
      {
        "start": 2967.48,
        "duration": 3.96,
        "text": "um and so and the other thing too is"
      },
      {
        "start": 2969.28,
        "duration": 4.86,
        "text": "that because lens are so good at"
      },
      {
        "start": 2971.44,
        "duration": 5.34,
        "text": "rejecting irrelevant information it's"
      },
      {
        "start": 2974.14,
        "duration": 5.04,
        "text": "really good idea to get as many nearest"
      },
      {
        "start": 2976.78,
        "duration": 5.4,
        "text": "neighbors as possible at least whatever"
      },
      {
        "start": 2979.18,
        "duration": 4.62,
        "text": "would fit within the prompt"
      },
      {
        "start": 2982.18,
        "duration": 5.399,
        "text": "um and then actually there's another"
      },
      {
        "start": 2983.8,
        "duration": 6.299,
        "text": "Advanced algorithm called uh maximal"
      },
      {
        "start": 2987.579,
        "duration": 5.221,
        "text": "marginal relevance that what it does is"
      },
      {
        "start": 2990.099,
        "duration": 5.581,
        "text": "that it takes the let's say a hundred"
      },
      {
        "start": 2992.8,
        "duration": 6.779,
        "text": "nearest neighbors and it can isolate it"
      },
      {
        "start": 2995.68,
        "duration": 6.72,
        "text": "down to the 70 or sorry the the 10 most"
      },
      {
        "start": 2999.579,
        "duration": 5.76,
        "text": "nearest neighbors that's uh most"
      },
      {
        "start": 3002.4,
        "duration": 4.919,
        "text": "relevant for for this particular prompt"
      },
      {
        "start": 3005.339,
        "duration": 5.101,
        "text": "and the reason why that's very important"
      },
      {
        "start": 3007.319,
        "duration": 6.421,
        "text": "this is a kind of a 200 level type of"
      },
      {
        "start": 3010.44,
        "duration": 5.159,
        "text": "um of use case is that approximate"
      },
      {
        "start": 3013.74,
        "duration": 3.24,
        "text": "nearest neighbor algorithm which is the"
      },
      {
        "start": 3015.599,
        "duration": 4.921,
        "text": "underlying algorithm used for nearest"
      },
      {
        "start": 3016.98,
        "duration": 7.02,
        "text": "neighbor is very very susceptible for a"
      },
      {
        "start": 3020.52,
        "duration": 6.059,
        "text": "duplicate or near duplicate vectors in"
      },
      {
        "start": 3024.0,
        "duration": 4.68,
        "text": "your prompt so for example if you're if"
      },
      {
        "start": 3026.579,
        "duration": 5.701,
        "text": "you're if you're using your scripting or"
      },
      {
        "start": 3028.68,
        "duration": 5.76,
        "text": "web page uh your web pages and you have"
      },
      {
        "start": 3032.28,
        "duration": 3.44,
        "text": "five web pages with almost identical"
      },
      {
        "start": 3034.44,
        "duration": 4.679,
        "text": "information"
      },
      {
        "start": 3035.72,
        "duration": 5.92,
        "text": "what ends up happening is that it it"
      },
      {
        "start": 3039.119,
        "duration": 5.341,
        "text": "fills up the database with those those"
      },
      {
        "start": 3041.64,
        "duration": 4.8,
        "text": "vectors if it so happens that the vector"
      },
      {
        "start": 3044.46,
        "duration": 4.32,
        "text": "infra those duplicates were also"
      },
      {
        "start": 3046.44,
        "duration": 5.58,
        "text": "irrelevant what happens is we use search"
      },
      {
        "start": 3048.78,
        "duration": 5.579,
        "text": "for let's say uh five nearest neighbors"
      },
      {
        "start": 3052.02,
        "duration": 4.44,
        "text": "then end up what happens at all five"
      },
      {
        "start": 3054.359,
        "duration": 5.101,
        "text": "nearest neighbors end up being very uh"
      },
      {
        "start": 3056.46,
        "duration": 7.139,
        "text": "data becomes irrelevant whereas if you"
      },
      {
        "start": 3059.46,
        "duration": 8.04,
        "text": "use a wider search let's say 50"
      },
      {
        "start": 3063.599,
        "duration": 8.041,
        "text": "um and then you for 50 nearest neighbors"
      },
      {
        "start": 3067.5,
        "duration": 7.2,
        "text": "and then ask for the five most different"
      },
      {
        "start": 3071.64,
        "duration": 5.52,
        "text": "um uh documents within that set of 50"
      },
      {
        "start": 3074.7,
        "duration": 4.68,
        "text": "all the duplicates automatically get"
      },
      {
        "start": 3077.16,
        "duration": 4.08,
        "text": "removed because of this maximal marginal"
      },
      {
        "start": 3079.38,
        "duration": 4.76,
        "text": "relevance it basically tries to find out"
      },
      {
        "start": 3081.24,
        "duration": 5.099,
        "text": "the most distinct pieces of information"
      },
      {
        "start": 3084.14,
        "duration": 3.699,
        "text": "so this is kind of an advanced thing"
      },
      {
        "start": 3086.339,
        "duration": 3.841,
        "text": "that you should remember to use"
      },
      {
        "start": 3087.839,
        "duration": 5.101,
        "text": "especially if you're concerned of your"
      },
      {
        "start": 3090.18,
        "duration": 4.679,
        "text": "of your uh your system having duplicate"
      },
      {
        "start": 3092.94,
        "duration": 3.899,
        "text": "data"
      },
      {
        "start": 3094.859,
        "duration": 3.361,
        "text": "uh there's also another bunch of other"
      },
      {
        "start": 3096.839,
        "duration": 3.181,
        "text": "things that you that we've learned to"
      },
      {
        "start": 3098.22,
        "duration": 4.859,
        "text": "prompt engineering so number one is"
      },
      {
        "start": 3100.02,
        "duration": 5.7,
        "text": "because this is a real-time application"
      },
      {
        "start": 3103.079,
        "duration": 5.28,
        "text": "um we you want to respond to the user as"
      },
      {
        "start": 3105.72,
        "duration": 4.859,
        "text": "quickly as possible so you want to fetch"
      },
      {
        "start": 3108.359,
        "duration": 4.921,
        "text": "as much context from your data your your"
      },
      {
        "start": 3110.579,
        "duration": 4.201,
        "text": "database in parallel so one of the"
      },
      {
        "start": 3113.28,
        "duration": 3.779,
        "text": "optimizations that you didn't see"
      },
      {
        "start": 3114.78,
        "duration": 4.68,
        "text": "underneath the hood is that the data is"
      },
      {
        "start": 3117.059,
        "duration": 4.441,
        "text": "being fetched in parallel like calling"
      },
      {
        "start": 3119.46,
        "duration": 6.18,
        "text": "the vector search for getting the user"
      },
      {
        "start": 3121.5,
        "duration": 7.98,
        "text": "information getting uh um the the user"
      },
      {
        "start": 3125.64,
        "duration": 6.479,
        "text": "activity uh getting the chat history and"
      },
      {
        "start": 3129.48,
        "duration": 4.8,
        "text": "as well as getting the documents that"
      },
      {
        "start": 3132.119,
        "duration": 4.081,
        "text": "are relevant that's all being happening"
      },
      {
        "start": 3134.28,
        "duration": 3.779,
        "text": "in parallel against Cassandra which is"
      },
      {
        "start": 3136.2,
        "duration": 5.72,
        "text": "great because Cassandra itself is built"
      },
      {
        "start": 3138.059,
        "duration": 3.861,
        "text": "as a paralyzable database"
      },
      {
        "start": 3141.96,
        "duration": 3.78,
        "text": "um you got to be very careful about"
      },
      {
        "start": 3143.16,
        "duration": 5.64,
        "text": "managing the token size so this is"
      },
      {
        "start": 3145.74,
        "duration": 6.66,
        "text": "important not just for making sure that"
      },
      {
        "start": 3148.8,
        "duration": 6.0,
        "text": "you um it's it's not just important for"
      },
      {
        "start": 3152.4,
        "duration": 5.04,
        "text": "Speed but it's also important to use a"
      },
      {
        "start": 3154.8,
        "duration": 5.039,
        "text": "cost and then you want to have all the"
      },
      {
        "start": 3157.44,
        "duration": 4.44,
        "text": "tools you need to simplify your"
      },
      {
        "start": 3159.839,
        "duration": 3.841,
        "text": "development so prompt templates and in"
      },
      {
        "start": 3161.88,
        "duration": 4.679,
        "text": "particular is a very very easy way to"
      },
      {
        "start": 3163.68,
        "duration": 5.399,
        "text": "simplify the development and in our case"
      },
      {
        "start": 3166.559,
        "duration": 4.741,
        "text": "because our our company is a slack so"
      },
      {
        "start": 3169.079,
        "duration": 4.381,
        "text": "much instead of if we want to"
      },
      {
        "start": 3171.3,
        "duration": 5.88,
        "text": "democratize more people testing out"
      },
      {
        "start": 3173.46,
        "duration": 5.46,
        "text": "these LMS we use slack as a way that our"
      },
      {
        "start": 3177.18,
        "duration": 3.3,
        "text": "customer support reps can look at all"
      },
      {
        "start": 3178.92,
        "duration": 4.56,
        "text": "the data that's coming back and forth"
      },
      {
        "start": 3180.48,
        "duration": 4.92,
        "text": "from that particular bot and last but"
      },
      {
        "start": 3183.48,
        "duration": 4.879,
        "text": "not least you have to for hallucinations"
      },
      {
        "start": 3185.4,
        "duration": 5.219,
        "text": "you really got to kind of test it out"
      },
      {
        "start": 3188.359,
        "duration": 4.901,
        "text": "and then figure out what's the problem"
      },
      {
        "start": 3190.619,
        "duration": 6.901,
        "text": "iterate and constantly improve the uh"
      },
      {
        "start": 3193.26,
        "duration": 5.22,
        "text": "reduce the the the um that'll improve"
      },
      {
        "start": 3197.52,
        "duration": 3.12,
        "text": "your problems to reduce the"
      },
      {
        "start": 3198.48,
        "duration": 4.74,
        "text": "hallucinations"
      },
      {
        "start": 3200.64,
        "duration": 4.56,
        "text": "all right so with that the next portion"
      },
      {
        "start": 3203.22,
        "duration": 3.96,
        "text": "is actually and so we've talked about"
      },
      {
        "start": 3205.2,
        "duration": 4.44,
        "text": "generating The Prompt now let's talk"
      },
      {
        "start": 3207.18,
        "duration": 4.26,
        "text": "about execution of the llm"
      },
      {
        "start": 3209.64,
        "duration": 4.02,
        "text": "and so what the way it would happen is"
      },
      {
        "start": 3211.44,
        "duration": 4.5,
        "text": "that we're going to start from after the"
      },
      {
        "start": 3213.66,
        "duration": 3.72,
        "text": "prompt is generated the first thing that"
      },
      {
        "start": 3215.94,
        "duration": 4.26,
        "text": "we're going to do is that instead of"
      },
      {
        "start": 3217.38,
        "duration": 5.52,
        "text": "calling the LM directly we do actually a"
      },
      {
        "start": 3220.2,
        "duration": 4.98,
        "text": "cache lookup so we're calling the vector"
      },
      {
        "start": 3222.9,
        "duration": 4.679,
        "text": "store to get see if that particular"
      },
      {
        "start": 3225.18,
        "duration": 5.04,
        "text": "question has been asked before"
      },
      {
        "start": 3227.579,
        "duration": 4.861,
        "text": "and if it's not in the cache that's at"
      },
      {
        "start": 3230.22,
        "duration": 4.339,
        "text": "the time when we actually invoke the LM"
      },
      {
        "start": 3232.44,
        "duration": 4.919,
        "text": "so calling this text bison"
      },
      {
        "start": 3234.559,
        "duration": 4.841,
        "text": "llm model and getting the answer"
      },
      {
        "start": 3237.359,
        "duration": 4.801,
        "text": "and at that point after we get the"
      },
      {
        "start": 3239.4,
        "duration": 5.04,
        "text": "answer we store the chat history back in"
      },
      {
        "start": 3242.16,
        "duration": 4.5,
        "text": "the vector store we also store the"
      },
      {
        "start": 3244.44,
        "duration": 4.679,
        "text": "cached value into the vector store as"
      },
      {
        "start": 3246.66,
        "duration": 5.459,
        "text": "well for further use and then we respond"
      },
      {
        "start": 3249.119,
        "duration": 5.401,
        "text": "return the the response to the to the"
      },
      {
        "start": 3252.119,
        "duration": 5.041,
        "text": "user so just to give you some context to"
      },
      {
        "start": 3254.52,
        "duration": 4.44,
        "text": "hear some other Lessons Learned uh when"
      },
      {
        "start": 3257.16,
        "duration": 5.76,
        "text": "we're doing caching you got to be really"
      },
      {
        "start": 3258.96,
        "duration": 6.26,
        "text": "careful about uh privacy so especially"
      },
      {
        "start": 3262.92,
        "duration": 4.8,
        "text": "when you're doing a chat bot"
      },
      {
        "start": 3265.22,
        "duration": 5.92,
        "text": "usually people are having one-on-one"
      },
      {
        "start": 3267.72,
        "duration": 5.52,
        "text": "conversations that are private and so"
      },
      {
        "start": 3271.14,
        "duration": 3.959,
        "text": "this doesn't lend itself very well like"
      },
      {
        "start": 3273.24,
        "duration": 4.379,
        "text": "our Astro agent unfortunately doesn't"
      },
      {
        "start": 3275.099,
        "duration": 4.921,
        "text": "lend itself very well to caching because"
      },
      {
        "start": 3277.619,
        "duration": 5.641,
        "text": "usually the conversations are in the"
      },
      {
        "start": 3280.02,
        "duration": 5.28,
        "text": "context of that particular person"
      },
      {
        "start": 3283.26,
        "duration": 4.44,
        "text": "um and then to also reduce the"
      },
      {
        "start": 3285.3,
        "duration": 5.1,
        "text": "hallucinations there's a lot of settings"
      },
      {
        "start": 3287.7,
        "duration": 5.7,
        "text": "in the llm the temperature the K value"
      },
      {
        "start": 3290.4,
        "duration": 4.08,
        "text": "p-value Etc that you can play with in"
      },
      {
        "start": 3293.4,
        "duration": 4.32,
        "text": "order to"
      },
      {
        "start": 3294.48,
        "duration": 5.639,
        "text": "um to uh to generate to see whether or"
      },
      {
        "start": 3297.72,
        "duration": 3.78,
        "text": "not the answers are correct and last but"
      },
      {
        "start": 3300.119,
        "duration": 3.841,
        "text": "not least one of the things you need to"
      },
      {
        "start": 3301.5,
        "duration": 4.319,
        "text": "do before you roll out is you need to"
      },
      {
        "start": 3303.96,
        "duration": 3.72,
        "text": "capture qualitatively these"
      },
      {
        "start": 3305.819,
        "duration": 4.381,
        "text": "conversations are good conversations or"
      },
      {
        "start": 3307.68,
        "duration": 4.86,
        "text": "bad conversations so"
      },
      {
        "start": 3310.2,
        "duration": 4.919,
        "text": "what we do is that we we actually"
      },
      {
        "start": 3312.54,
        "duration": 4.559,
        "text": "capture a thumbs up thumbs down of these"
      },
      {
        "start": 3315.119,
        "duration": 4.44,
        "text": "conversations or good conversations and"
      },
      {
        "start": 3317.099,
        "duration": 4.681,
        "text": "bad conversations and then we use that"
      },
      {
        "start": 3319.559,
        "duration": 5.461,
        "text": "to understand"
      },
      {
        "start": 3321.78,
        "duration": 4.74,
        "text": "um to we use that as feedback to improve"
      },
      {
        "start": 3325.02,
        "duration": 3.96,
        "text": "our prompt templates and improve our"
      },
      {
        "start": 3326.52,
        "duration": 4.26,
        "text": "prompts down the road"
      },
      {
        "start": 3328.98,
        "duration": 5.22,
        "text": "one of the other things that we've made"
      },
      {
        "start": 3330.78,
        "duration": 6.059,
        "text": "a lot of usage of is land chain so Lang"
      },
      {
        "start": 3334.2,
        "duration": 4.2,
        "text": "chain has these key abstractions for"
      },
      {
        "start": 3336.839,
        "duration": 3.661,
        "text": "accelerating development of your"
      },
      {
        "start": 3338.4,
        "duration": 3.78,
        "text": "application the first key abstraction"
      },
      {
        "start": 3340.5,
        "duration": 4.98,
        "text": "that we use this thing called indexes"
      },
      {
        "start": 3342.18,
        "duration": 6.899,
        "text": "this is the data like for retrieving the"
      },
      {
        "start": 3345.48,
        "duration": 8.579,
        "text": "um the the the the chat history as well"
      },
      {
        "start": 3349.079,
        "duration": 5.941,
        "text": "as the um the the uh the DB"
      },
      {
        "start": 3354.059,
        "duration": 3.361,
        "text": "um"
      },
      {
        "start": 3355.02,
        "duration": 5.52,
        "text": "uh sorry retrieving the product"
      },
      {
        "start": 3357.42,
        "duration": 5.159,
        "text": "documentation we use the CH The Prompt"
      },
      {
        "start": 3360.54,
        "duration": 4.26,
        "text": "template capability to contextually"
      },
      {
        "start": 3362.579,
        "duration": 5.04,
        "text": "generate contextually relevant prompts"
      },
      {
        "start": 3364.8,
        "duration": 4.98,
        "text": "in fact we have this feature in Cass i o"
      },
      {
        "start": 3367.619,
        "duration": 4.98,
        "text": "that you know you can write the prompt"
      },
      {
        "start": 3369.78,
        "duration": 4.86,
        "text": "template and then the prompt template uh"
      },
      {
        "start": 3372.599,
        "duration": 5.48,
        "text": "and in the prompt template itself you"
      },
      {
        "start": 3374.64,
        "duration": 6.0,
        "text": "can kind of feed in uh cql type"
      },
      {
        "start": 3378.079,
        "duration": 5.861,
        "text": "functions such as Auto populate the prop"
      },
      {
        "start": 3380.64,
        "duration": 5.28,
        "text": "template uh we use this application a"
      },
      {
        "start": 3383.94,
        "duration": 3.659,
        "text": "algorithm called the Maxwell marginal"
      },
      {
        "start": 3385.92,
        "duration": 3.6,
        "text": "relevance this is kind of an advanced"
      },
      {
        "start": 3387.599,
        "duration": 4.441,
        "text": "algorithm on top of vector search to"
      },
      {
        "start": 3389.52,
        "duration": 5.039,
        "text": "improve the relevancy of the data coming"
      },
      {
        "start": 3392.04,
        "duration": 5.7,
        "text": "back from our database we leverage the"
      },
      {
        "start": 3394.559,
        "duration": 5.101,
        "text": "memory abstraction uh to record all the"
      },
      {
        "start": 3397.74,
        "duration": 5.16,
        "text": "chat history"
      },
      {
        "start": 3399.66,
        "duration": 4.62,
        "text": "um and uh underneath the hood there's a"
      },
      {
        "start": 3402.9,
        "duration": 4.8,
        "text": "lever you can leverage something called"
      },
      {
        "start": 3404.28,
        "duration": 5.46,
        "text": "a semantic memory so instead of just"
      },
      {
        "start": 3407.7,
        "duration": 5.159,
        "text": "pulling all like let's say the 10 last"
      },
      {
        "start": 3409.74,
        "duration": 5.22,
        "text": "100 chat interactions it's able to"
      },
      {
        "start": 3412.859,
        "duration": 3.901,
        "text": "semantically figure out what are the"
      },
      {
        "start": 3414.96,
        "duration": 4.32,
        "text": "previous interactions are relevant to"
      },
      {
        "start": 3416.76,
        "duration": 4.98,
        "text": "the con the question the person is using"
      },
      {
        "start": 3419.28,
        "duration": 5.1,
        "text": "and then last we use the cache for"
      },
      {
        "start": 3421.74,
        "duration": 6.0,
        "text": "improving the performance and the cost"
      },
      {
        "start": 3424.38,
        "duration": 5.459,
        "text": "all right so with that I recommend you"
      },
      {
        "start": 3427.74,
        "duration": 3.78,
        "text": "guys to get started"
      },
      {
        "start": 3429.839,
        "duration": 4.801,
        "text": "um we can do a little demo why don't"
      },
      {
        "start": 3431.52,
        "duration": 5.64,
        "text": "Alex can you show exactly how you would"
      },
      {
        "start": 3434.64,
        "duration": 4.919,
        "text": "provision the vector database"
      },
      {
        "start": 3437.16,
        "duration": 4.5,
        "text": "so if you haven't done so I'll link it"
      },
      {
        "start": 3439.559,
        "duration": 3.661,
        "text": "on set you can create an account and ask"
      },
      {
        "start": 3441.66,
        "duration": 3.419,
        "text": "for account at Azure daystax.com"
      },
      {
        "start": 3443.22,
        "duration": 3.599,
        "text": "register"
      },
      {
        "start": 3445.079,
        "duration": 3.901,
        "text": "um once you've done that you'll see the"
      },
      {
        "start": 3446.819,
        "duration": 4.441,
        "text": "dashboard that looks like this and you"
      },
      {
        "start": 3448.98,
        "duration": 3.599,
        "text": "really can create a vector store in just"
      },
      {
        "start": 3451.26,
        "duration": 2.339,
        "text": "a couple minutes"
      },
      {
        "start": 3452.579,
        "duration": 2.641,
        "text": "um so you're just going to click this"
      },
      {
        "start": 3453.599,
        "duration": 4.141,
        "text": "create database button at the top of the"
      },
      {
        "start": 3455.22,
        "duration": 4.26,
        "text": "screen you're going to select serverless"
      },
      {
        "start": 3457.74,
        "duration": 3.48,
        "text": "with Vector"
      },
      {
        "start": 3459.48,
        "duration": 4.32,
        "text": "um enter some basic details like your"
      },
      {
        "start": 3461.22,
        "duration": 6.06,
        "text": "database name your key space name"
      },
      {
        "start": 3463.8,
        "duration": 5.519,
        "text": "um finally choose a region and a couple"
      },
      {
        "start": 3467.28,
        "duration": 4.5,
        "text": "minutes later you will have"
      },
      {
        "start": 3469.319,
        "duration": 4.26,
        "text": "um your vector database in the in the"
      },
      {
        "start": 3471.78,
        "duration": 3.779,
        "text": "region you select"
      },
      {
        "start": 3473.579,
        "duration": 4.02,
        "text": "um so really I encourage everyone to"
      },
      {
        "start": 3475.559,
        "duration": 4.381,
        "text": "give this a try"
      },
      {
        "start": 3477.599,
        "duration": 4.561,
        "text": "um and if you need any additional help"
      },
      {
        "start": 3479.94,
        "duration": 4.86,
        "text": "feel free to ask the uh no SQL assistant"
      },
      {
        "start": 3482.16,
        "duration": 4.679,
        "text": "at the bottom of the screen all right so"
      },
      {
        "start": 3484.8,
        "duration": 3.24,
        "text": "with that um I want to tell you a little"
      },
      {
        "start": 3486.839,
        "duration": 3.181,
        "text": "bit what we're going to be doing in the"
      },
      {
        "start": 3488.04,
        "duration": 3.779,
        "text": "future uh so what we've done is that"
      },
      {
        "start": 3490.02,
        "duration": 4.319,
        "text": "we've connected our chat bot to our"
      },
      {
        "start": 3491.819,
        "duration": 4.561,
        "text": "internal data warehouse which is"
      },
      {
        "start": 3494.339,
        "duration": 3.961,
        "text": "um uh bigquery and we're going to"
      },
      {
        "start": 3496.38,
        "duration": 4.439,
        "text": "leverage looker which is a reporting"
      },
      {
        "start": 3498.3,
        "duration": 4.74,
        "text": "system on top of bigquery to generate a"
      },
      {
        "start": 3500.819,
        "duration": 5.161,
        "text": "lot of cement reports on how people are"
      },
      {
        "start": 3503.04,
        "duration": 4.799,
        "text": "doing and using the chatbot uh what we"
      },
      {
        "start": 3505.98,
        "duration": 4.68,
        "text": "also hope to do is that right now the"
      },
      {
        "start": 3507.839,
        "duration": 5.461,
        "text": "chatbot is integrated into the nosql"
      },
      {
        "start": 3510.66,
        "duration": 4.639,
        "text": "into the into the page but there's no"
      },
      {
        "start": 3513.3,
        "duration": 5.64,
        "text": "reason why that this is the only"
      },
      {
        "start": 3515.299,
        "duration": 5.921,
        "text": "location where you can integrate the"
      },
      {
        "start": 3518.94,
        "duration": 4.859,
        "text": "chat bot so what we plan to actually do"
      },
      {
        "start": 3521.22,
        "duration": 5.22,
        "text": "is integrate that into the shell the cql"
      },
      {
        "start": 3523.799,
        "duration": 7.381,
        "text": "shell on the website or potentially"
      },
      {
        "start": 3526.44,
        "duration": 7.02,
        "text": "integrated directly into chat GPT as a"
      },
      {
        "start": 3531.18,
        "duration": 3.899,
        "text": "chat GPT plugin"
      },
      {
        "start": 3533.46,
        "duration": 4.619,
        "text": "and last but not least we plan to"
      },
      {
        "start": 3535.079,
        "duration": 6.301,
        "text": "leverage the nosql assistant to also do"
      },
      {
        "start": 3538.079,
        "duration": 5.641,
        "text": "email follow-ups so not only can your"
      },
      {
        "start": 3541.38,
        "duration": 4.199,
        "text": "agent be reactive you know you ask me a"
      },
      {
        "start": 3543.72,
        "duration": 4.859,
        "text": "question it gives an answer but you can"
      },
      {
        "start": 3545.579,
        "duration": 5.161,
        "text": "also use the agent to predict when the"
      },
      {
        "start": 3548.579,
        "duration": 4.381,
        "text": "person needs to would would prefer would"
      },
      {
        "start": 3550.74,
        "duration": 5.28,
        "text": "want to interact with you and generate"
      },
      {
        "start": 3552.96,
        "duration": 6.24,
        "text": "automatically relevant emails to help"
      },
      {
        "start": 3556.02,
        "duration": 5.46,
        "text": "that user along their Journey"
      },
      {
        "start": 3559.2,
        "duration": 3.659,
        "text": "so this is just kind of the beginning uh"
      },
      {
        "start": 3561.48,
        "duration": 2.94,
        "text": "there's a lot of other things that we"
      },
      {
        "start": 3562.859,
        "duration": 3.72,
        "text": "want to talk about in this in the next"
      },
      {
        "start": 3564.42,
        "duration": 4.74,
        "text": "while so we're going to talk a lot about"
      },
      {
        "start": 3566.579,
        "duration": 5.22,
        "text": "autonomous AI agents you may have heard"
      },
      {
        "start": 3569.16,
        "duration": 6.06,
        "text": "this thing called Auto GPT so this is"
      },
      {
        "start": 3571.799,
        "duration": 6.841,
        "text": "the idea of leveraging multiple llm"
      },
      {
        "start": 3575.22,
        "duration": 6.839,
        "text": "calls in in sequence or in parallel to"
      },
      {
        "start": 3578.64,
        "duration": 6.419,
        "text": "get to a particular uh to to enable a"
      },
      {
        "start": 3582.059,
        "duration": 4.56,
        "text": "particular outcome whether it's fixing"
      },
      {
        "start": 3585.059,
        "duration": 2.941,
        "text": "your database things of that sort so"
      },
      {
        "start": 3586.619,
        "duration": 3.421,
        "text": "we're going to be experimenting a lot"
      },
      {
        "start": 3588.0,
        "duration": 3.9,
        "text": "with that and talking about it we're"
      },
      {
        "start": 3590.04,
        "duration": 4.38,
        "text": "also going to be talking about Advanced"
      },
      {
        "start": 3591.9,
        "duration": 5.04,
        "text": "retrieval augmentation generation"
      },
      {
        "start": 3594.42,
        "duration": 5.04,
        "text": "algorithms so we talked a little bit"
      },
      {
        "start": 3596.94,
        "duration": 4.98,
        "text": "about the mark uh the the Maxwell"
      },
      {
        "start": 3599.46,
        "duration": 3.72,
        "text": "marginal relevance algorithm uh next"
      },
      {
        "start": 3601.92,
        "duration": 4.199,
        "text": "time around I really want to get into"
      },
      {
        "start": 3603.18,
        "duration": 5.939,
        "text": "the details of the math behind it why is"
      },
      {
        "start": 3606.119,
        "duration": 6.18,
        "text": "approximate nearest neighbor and uh K"
      },
      {
        "start": 3609.119,
        "duration": 5.041,
        "text": "nearest neighbor algorithm useful but"
      },
      {
        "start": 3612.299,
        "duration": 2.82,
        "text": "also what are the limitations so we want"
      },
      {
        "start": 3614.16,
        "duration": 3.3,
        "text": "to talk a little bit about the"
      },
      {
        "start": 3615.119,
        "duration": 3.901,
        "text": "limitations of vector search and then"
      },
      {
        "start": 3617.46,
        "duration": 4.139,
        "text": "we're going to talk about things such as"
      },
      {
        "start": 3619.02,
        "duration": 4.94,
        "text": "the forward-looking augmented retrieval"
      },
      {
        "start": 3621.599,
        "duration": 5.101,
        "text": "generation algorithm also known as flare"
      },
      {
        "start": 3623.96,
        "duration": 5.08,
        "text": "these are out these are Advanced"
      },
      {
        "start": 3626.7,
        "duration": 4.8,
        "text": "algorithms that instead of doing just"
      },
      {
        "start": 3629.04,
        "duration": 6.36,
        "text": "one API call to your vector store"
      },
      {
        "start": 3631.5,
        "duration": 6.42,
        "text": "getting the content and then uh getting"
      },
      {
        "start": 3635.4,
        "duration": 5.82,
        "text": "content and putting into the llm and to"
      },
      {
        "start": 3637.92,
        "duration": 6.179,
        "text": "summarize a response this is whereby the"
      },
      {
        "start": 3641.22,
        "duration": 6.2,
        "text": "uh the the agent can actually make"
      },
      {
        "start": 3644.099,
        "duration": 6.601,
        "text": "multiple calls to the the vector store"
      },
      {
        "start": 3647.42,
        "duration": 4.78,
        "text": "and multiple calls to the llms to"
      },
      {
        "start": 3650.7,
        "duration": 3.48,
        "text": "actually summarize and extract"
      },
      {
        "start": 3652.2,
        "duration": 3.899,
        "text": "information which is very similar to"
      },
      {
        "start": 3654.18,
        "duration": 4.08,
        "text": "what us humans do so for example if"
      },
      {
        "start": 3656.099,
        "duration": 4.98,
        "text": "we're reading a book and if we want to"
      },
      {
        "start": 3658.26,
        "duration": 4.38,
        "text": "get an answer we would go reap different"
      },
      {
        "start": 3661.079,
        "duration": 3.961,
        "text": "portions of the book we'd jump around"
      },
      {
        "start": 3662.64,
        "duration": 4.74,
        "text": "with a book summarize go back to the"
      },
      {
        "start": 3665.04,
        "duration": 4.319,
        "text": "places we forgot about and then try to"
      },
      {
        "start": 3667.38,
        "duration": 5.1,
        "text": "come up with the answer by going to"
      },
      {
        "start": 3669.359,
        "duration": 4.26,
        "text": "iteratively calling uh looking at"
      },
      {
        "start": 3672.48,
        "duration": 3.42,
        "text": "different sections of the book"
      },
      {
        "start": 3673.619,
        "duration": 4.081,
        "text": "processing Etc"
      },
      {
        "start": 3675.9,
        "duration": 4.56,
        "text": "and then last but not least we want to"
      },
      {
        "start": 3677.7,
        "duration": 4.98,
        "text": "talk about what this agent memory is so"
      },
      {
        "start": 3680.46,
        "duration": 5.879,
        "text": "the idea behind how do we create a"
      },
      {
        "start": 3682.68,
        "duration": 5.639,
        "text": "human-like memory for uh for autonomous"
      },
      {
        "start": 3686.339,
        "duration": 5.041,
        "text": "AI agents and so we're going to get into"
      },
      {
        "start": 3688.319,
        "duration": 4.98,
        "text": "the details of how data stacks and the"
      },
      {
        "start": 3691.38,
        "duration": 3.84,
        "text": "Cassandra Community are trying to push"
      },
      {
        "start": 3693.299,
        "duration": 4.741,
        "text": "this technology forward"
      },
      {
        "start": 3695.22,
        "duration": 5.099,
        "text": "so we got a lot of resources that we've"
      },
      {
        "start": 3698.04,
        "duration": 5.34,
        "text": "uh that we've put together that you can"
      },
      {
        "start": 3700.319,
        "duration": 5.401,
        "text": "do more reading and um you know in terms"
      },
      {
        "start": 3703.38,
        "duration": 4.32,
        "text": "of The Next Step uh we love to see what"
      },
      {
        "start": 3705.72,
        "duration": 4.68,
        "text": "you build uh so if you're going to build"
      },
      {
        "start": 3707.7,
        "duration": 4.74,
        "text": "something uh let's uh let's schedule a"
      },
      {
        "start": 3710.4,
        "duration": 3.48,
        "text": "live end-to-end demonstration uh we can"
      },
      {
        "start": 3712.44,
        "duration": 4.2,
        "text": "demonstrate what we have and we'd love"
      },
      {
        "start": 3713.88,
        "duration": 5.04,
        "text": "to see yours uh let's schedule a"
      },
      {
        "start": 3716.64,
        "duration": 4.26,
        "text": "collaborative Workshop so in the future"
      },
      {
        "start": 3718.92,
        "duration": 5.699,
        "text": "what I hope to do is build a community"
      },
      {
        "start": 3720.9,
        "duration": 4.98,
        "text": "of uh AI agent Builders and we're going"
      },
      {
        "start": 3724.619,
        "duration": 3.48,
        "text": "to show each other what we've been"
      },
      {
        "start": 3725.88,
        "duration": 4.02,
        "text": "building over the last month and so we"
      },
      {
        "start": 3728.099,
        "duration": 3.841,
        "text": "want to have a regular touch point with"
      },
      {
        "start": 3729.9,
        "duration": 4.86,
        "text": "the community to do so and last but not"
      },
      {
        "start": 3731.94,
        "duration": 5.399,
        "text": "least uh try Astra DB with Vector search"
      },
      {
        "start": 3734.76,
        "duration": 5.72,
        "text": "and build some awesome llms and awesome"
      },
      {
        "start": 3737.339,
        "duration": 6.201,
        "text": "AI agents thank you"
      },
      {
        "start": 3740.48,
        "duration": 3.06,
        "text": "thank you"
      },
      {
        "start": 3743.8,
        "duration": 5.059,
        "text": "[Music]"
      },
      {
        "start": 3746.119,
        "duration": 4.18,
        "text": "thank you Alan and Alex I think the"
      },
      {
        "start": 3748.859,
        "duration": 3.601,
        "text": "takeaway from the session that we just"
      },
      {
        "start": 3750.299,
        "duration": 3.661,
        "text": "saw is that you can dive in and start to"
      },
      {
        "start": 3752.46,
        "duration": 4.5,
        "text": "build some pretty amazing agent"
      },
      {
        "start": 3753.96,
        "duration": 5.339,
        "text": "experiences with your own data right now"
      },
      {
        "start": 3756.96,
        "duration": 4.619,
        "text": "next I'm delighted to introduce Jonathan"
      },
      {
        "start": 3759.299,
        "duration": 4.201,
        "text": "Ellis co-founder of data Stacks Jonathan"
      },
      {
        "start": 3761.579,
        "duration": 3.901,
        "text": "is going to do a deep dive on after a"
      },
      {
        "start": 3763.5,
        "duration": 4.079,
        "text": "vector search especially how you can get"
      },
      {
        "start": 3765.48,
        "duration": 6.839,
        "text": "the most out of it in performance and"
      },
      {
        "start": 3767.579,
        "duration": 7.191,
        "text": "scalability and now here's Jonathan"
      },
      {
        "start": 3772.319,
        "duration": 6.751,
        "text": "foreign"
      },
      {
        "start": 3774.77,
        "duration": 4.3,
        "text": "[Music]"
      },
      {
        "start": 3780.26,
        "duration": 4.78,
        "text": "I'd like to explain a little bit about"
      },
      {
        "start": 3782.64,
        "duration": 4.979,
        "text": "what's going on under the hood with our"
      },
      {
        "start": 3785.04,
        "duration": 4.259,
        "text": "implementation of vector search and how"
      },
      {
        "start": 3787.619,
        "duration": 3.48,
        "text": "you can use that information to make"
      },
      {
        "start": 3789.299,
        "duration": 3.901,
        "text": "better decisions as you build"
      },
      {
        "start": 3791.099,
        "duration": 4.321,
        "text": "applications on top of it"
      },
      {
        "start": 3793.2,
        "duration": 5.82,
        "text": "the first thing to understand is that"
      },
      {
        "start": 3795.42,
        "duration": 7.02,
        "text": "Vector search is built on top of Sai or"
      },
      {
        "start": 3799.02,
        "duration": 6.779,
        "text": "rather it's built as a part of Sai Sai"
      },
      {
        "start": 3802.44,
        "duration": 6.0,
        "text": "stands for storage attached indexes and"
      },
      {
        "start": 3805.799,
        "duration": 5.941,
        "text": "it's a Next Generation Cassandra index"
      },
      {
        "start": 3808.44,
        "duration": 6.6,
        "text": "coming in Cassandra 5.0 and it's"
      },
      {
        "start": 3811.74,
        "duration": 6.359,
        "text": "available now in datastax Astra"
      },
      {
        "start": 3815.04,
        "duration": 6.12,
        "text": "Sai gives you a couple things"
      },
      {
        "start": 3818.099,
        "duration": 6.96,
        "text": "in this graph we're showing the effects"
      },
      {
        "start": 3821.16,
        "duration": 6.78,
        "text": "of indexing a billion rows with 10"
      },
      {
        "start": 3825.059,
        "duration": 7.861,
        "text": "indexes defined across the rows in that"
      },
      {
        "start": 3827.94,
        "duration": 8.1,
        "text": "table on the left is the space used by"
      },
      {
        "start": 3832.92,
        "duration": 7.379,
        "text": "the raw row data itself so that's about"
      },
      {
        "start": 3836.04,
        "duration": 6.84,
        "text": "160 gigabytes of data and then we have"
      },
      {
        "start": 3840.299,
        "duration": 6.421,
        "text": "in red an experimental kind of index"
      },
      {
        "start": 3842.88,
        "duration": 6.36,
        "text": "called sassy in the green we have uh"
      },
      {
        "start": 3846.72,
        "duration": 4.44,
        "text": "traditional Cassandra secondary indexes"
      },
      {
        "start": 3849.24,
        "duration": 4.379,
        "text": "so that's what you get if you download"
      },
      {
        "start": 3851.16,
        "duration": 4.98,
        "text": "Cassandra fortado and you say create"
      },
      {
        "start": 3853.619,
        "duration": 7.381,
        "text": "index whatever the green bar is what you"
      },
      {
        "start": 3856.14,
        "duration": 9.3,
        "text": "get and then the blue is the Sai index"
      },
      {
        "start": 3861.0,
        "duration": 6.38,
        "text": "so you can see that Sai is more"
      },
      {
        "start": 3865.44,
        "duration": 5.58,
        "text": "efficient by a factor of about five"
      },
      {
        "start": 3867.38,
        "duration": 5.08,
        "text": "compared to the next closest index"
      },
      {
        "start": 3871.02,
        "duration": 3.12,
        "text": "implementation"
      },
      {
        "start": 3872.46,
        "duration": 5.46,
        "text": "the other thing maybe even more"
      },
      {
        "start": 3874.14,
        "duration": 5.84,
        "text": "important is that with Sai you get much"
      },
      {
        "start": 3877.92,
        "duration": 5.76,
        "text": "more powerful queries"
      },
      {
        "start": 3879.98,
        "duration": 8.44,
        "text": "the traditional indexes the green ones"
      },
      {
        "start": 3883.68,
        "duration": 7.5,
        "text": "in the last graph could only index exact"
      },
      {
        "start": 3888.42,
        "duration": 5.58,
        "text": "match queries and they can only combine"
      },
      {
        "start": 3891.18,
        "duration": 5.82,
        "text": "predicates using and"
      },
      {
        "start": 3894.0,
        "duration": 5.7,
        "text": "so you could select column one equals X"
      },
      {
        "start": 3897.0,
        "duration": 5.579,
        "text": "and column two equals y but you cannot"
      },
      {
        "start": 3899.7,
        "duration": 5.099,
        "text": "select column one equals X or column two"
      },
      {
        "start": 3902.579,
        "duration": 5.341,
        "text": "equals y you couldn't do that before you"
      },
      {
        "start": 3904.799,
        "duration": 6.241,
        "text": "can do that now with Sai and you can"
      },
      {
        "start": 3907.92,
        "duration": 6.24,
        "text": "also do inequalities with Sai and you"
      },
      {
        "start": 3911.04,
        "duration": 7.259,
        "text": "can also do nested predicates with Sai"
      },
      {
        "start": 3914.16,
        "duration": 7.74,
        "text": "so here is an example of doing nested uh"
      },
      {
        "start": 3918.299,
        "duration": 5.581,
        "text": "predicates with an or uh using Sai"
      },
      {
        "start": 3921.9,
        "duration": 5.64,
        "text": "that's completely legal you can do that"
      },
      {
        "start": 3923.88,
        "duration": 6.479,
        "text": "today again uh in Astra"
      },
      {
        "start": 3927.54,
        "duration": 5.759,
        "text": "so we're going to apply this to indexing"
      },
      {
        "start": 3930.359,
        "duration": 5.22,
        "text": "vectors and to do that I'm going to back"
      },
      {
        "start": 3933.299,
        "duration": 5.161,
        "text": "up just a little bit and talk about what"
      },
      {
        "start": 3935.579,
        "duration": 4.081,
        "text": "we mean when we're comparing vectors in"
      },
      {
        "start": 3938.46,
        "duration": 2.52,
        "text": "the first place"
      },
      {
        "start": 3939.66,
        "duration": 5.639,
        "text": "uh"
      },
      {
        "start": 3940.98,
        "duration": 6.3,
        "text": "so one of the the our goal is to given a"
      },
      {
        "start": 3945.299,
        "duration": 6.3,
        "text": "bunch of vectors that we've inserted in"
      },
      {
        "start": 3947.28,
        "duration": 8.46,
        "text": "the database find the closest match to a"
      },
      {
        "start": 3951.599,
        "duration": 6.0,
        "text": "given query and you you'll see discussed"
      },
      {
        "start": 3955.74,
        "duration": 5.28,
        "text": "a couple different abbreviations one of"
      },
      {
        "start": 3957.599,
        "duration": 5.7,
        "text": "them is k n k n stands for the K nearest"
      },
      {
        "start": 3961.02,
        "duration": 3.72,
        "text": "neighbors usually when people are"
      },
      {
        "start": 3963.299,
        "duration": 3.421,
        "text": "talking about this they're talking about"
      },
      {
        "start": 3964.74,
        "duration": 5.16,
        "text": "the exact"
      },
      {
        "start": 3966.72,
        "duration": 6.359,
        "text": "nearest neighbors so if I've got 10"
      },
      {
        "start": 3969.9,
        "duration": 6.3,
        "text": "million vectors I'm going to find the 10"
      },
      {
        "start": 3973.079,
        "duration": 5.821,
        "text": "out of those 10 million that are closest"
      },
      {
        "start": 3976.2,
        "duration": 4.139,
        "text": "to the query the thing is that to do"
      },
      {
        "start": 3978.9,
        "duration": 4.199,
        "text": "this"
      },
      {
        "start": 3980.339,
        "duration": 8.22,
        "text": "you're you have to compare the query"
      },
      {
        "start": 3983.099,
        "duration": 6.661,
        "text": "with each Vector in the source"
      },
      {
        "start": 3988.559,
        "duration": 3.121,
        "text": "um"
      },
      {
        "start": 3989.76,
        "duration": 4.14,
        "text": "there's there's more clever there's some"
      },
      {
        "start": 3991.68,
        "duration": 5.04,
        "text": "clever tricks we can play if we only"
      },
      {
        "start": 3993.9,
        "duration": 7.02,
        "text": "have a vector of Dimension two or three"
      },
      {
        "start": 3996.72,
        "duration": 7.56,
        "text": "or ten however those those algorithms"
      },
      {
        "start": 4000.92,
        "duration": 8.1,
        "text": "don't work uh when you start to get to"
      },
      {
        "start": 4004.28,
        "duration": 7.019,
        "text": "vectors of dimensionality 300 700 a"
      },
      {
        "start": 4009.02,
        "duration": 4.14,
        "text": "thousand and when you're dealing with"
      },
      {
        "start": 4011.299,
        "duration": 4.26,
        "text": "machine learning vectors which is kind"
      },
      {
        "start": 4013.16,
        "duration": 7.139,
        "text": "of the most interesting Vector to Index"
      },
      {
        "start": 4015.559,
        "duration": 7.921,
        "text": "right now uh you start at several"
      },
      {
        "start": 4020.299,
        "duration": 5.461,
        "text": "hundred and so when you're doing vectors"
      },
      {
        "start": 4023.48,
        "duration": 4.44,
        "text": "of several hundred Dimensions Brute"
      },
      {
        "start": 4025.76,
        "duration": 6.72,
        "text": "Force is is the best we've been able to"
      },
      {
        "start": 4027.92,
        "duration": 6.78,
        "text": "come up with to get exact matches"
      },
      {
        "start": 4032.48,
        "duration": 4.379,
        "text": "um and so naturally when you're when you"
      },
      {
        "start": 4034.7,
        "duration": 5.7,
        "text": "start to get 10 million 100 million a"
      },
      {
        "start": 4036.859,
        "duration": 5.7,
        "text": "billion vectors inserted uh doing that"
      },
      {
        "start": 4040.4,
        "duration": 5.04,
        "text": "Brute Force gets less and less"
      },
      {
        "start": 4042.559,
        "duration": 6.181,
        "text": "attractive and you can actually get down"
      },
      {
        "start": 4045.44,
        "duration": 4.859,
        "text": "to logarithmic queries uh if you are"
      },
      {
        "start": 4048.74,
        "duration": 4.68,
        "text": "willing to say"
      },
      {
        "start": 4050.299,
        "duration": 7.921,
        "text": "I'm I can live with approximate results"
      },
      {
        "start": 4053.42,
        "duration": 7.8,
        "text": "instead of exact results so this is uh"
      },
      {
        "start": 4058.22,
        "duration": 4.98,
        "text": "usually abbreviated a n approximate"
      },
      {
        "start": 4061.22,
        "duration": 5.819,
        "text": "nearest neighbors and we can do"
      },
      {
        "start": 4063.2,
        "duration": 6.06,
        "text": "logarithmic uh query time with that"
      },
      {
        "start": 4067.039,
        "duration": 4.5,
        "text": "so uh when we're"
      },
      {
        "start": 4069.26,
        "duration": 5.46,
        "text": "going there there's a bunch of different"
      },
      {
        "start": 4071.539,
        "duration": 6.421,
        "text": "uh implementation options for getting"
      },
      {
        "start": 4074.72,
        "duration": 5.04,
        "text": "logarithmic query results uh but when"
      },
      {
        "start": 4077.96,
        "duration": 3.599,
        "text": "we're applying this to Cassandra we've"
      },
      {
        "start": 4079.76,
        "duration": 5.579,
        "text": "got some specific requirements that we"
      },
      {
        "start": 4081.559,
        "duration": 6.3,
        "text": "need to meet that narrows down our uh"
      },
      {
        "start": 4085.339,
        "duration": 4.921,
        "text": "our options uh the first is that we need"
      },
      {
        "start": 4087.859,
        "duration": 5.281,
        "text": "to have a reasonable performance from"
      },
      {
        "start": 4090.26,
        "duration": 5.4,
        "text": "disk uh if the index only really works"
      },
      {
        "start": 4093.14,
        "duration": 4.86,
        "text": "when it's 100 in memory then it's not a"
      },
      {
        "start": 4095.66,
        "duration": 4.699,
        "text": "great fit because in Cassandra we're"
      },
      {
        "start": 4098.0,
        "duration": 6.359,
        "text": "typically dealing with billions of rows"
      },
      {
        "start": 4100.359,
        "duration": 6.281,
        "text": "uh and so we don't want to have to spend"
      },
      {
        "start": 4104.359,
        "duration": 3.96,
        "text": "enough money to fit all of that in"
      },
      {
        "start": 4106.64,
        "duration": 4.199,
        "text": "memory"
      },
      {
        "start": 4108.319,
        "duration": 4.801,
        "text": "the second requirement is that"
      },
      {
        "start": 4110.839,
        "duration": 5.52,
        "text": "the index needs to be incrementally"
      },
      {
        "start": 4113.12,
        "duration": 6.96,
        "text": "buildable and immediately queryable what"
      },
      {
        "start": 4116.359,
        "duration": 6.96,
        "text": "I mean by that is uh as I insert rows"
      },
      {
        "start": 4120.08,
        "duration": 5.699,
        "text": "into my table uh I should be able to do"
      },
      {
        "start": 4123.319,
        "duration": 4.44,
        "text": "that one row at a time and as soon as"
      },
      {
        "start": 4125.779,
        "duration": 6.54,
        "text": "I've inserted that one row I should be"
      },
      {
        "start": 4127.759,
        "duration": 7.861,
        "text": "able to use it in a query uh"
      },
      {
        "start": 4132.319,
        "duration": 5.701,
        "text": "I shouldn't have to do batch inserts of"
      },
      {
        "start": 4135.62,
        "duration": 6.0,
        "text": "thousands of rows uh I shouldn't have to"
      },
      {
        "start": 4138.02,
        "duration": 5.88,
        "text": "wait for some kind of indexing process"
      },
      {
        "start": 4141.62,
        "duration": 4.5,
        "text": "to happen in the background I want it to"
      },
      {
        "start": 4143.9,
        "duration": 5.879,
        "text": "be available as soon as I've inserted it"
      },
      {
        "start": 4146.12,
        "duration": 5.699,
        "text": "and then finally uh we don't know the"
      },
      {
        "start": 4149.779,
        "duration": 5.341,
        "text": "data set in advance and some of these"
      },
      {
        "start": 4151.819,
        "duration": 5.101,
        "text": "index implementations for vectors they"
      },
      {
        "start": 4155.12,
        "duration": 4.679,
        "text": "only work if you do know that in advance"
      },
      {
        "start": 4156.92,
        "duration": 4.379,
        "text": "and so where those get ruled out as well"
      },
      {
        "start": 4159.799,
        "duration": 2.281,
        "text": "so"
      },
      {
        "start": 4161.299,
        "duration": 4.261,
        "text": "um"
      },
      {
        "start": 4162.08,
        "duration": 6.719,
        "text": "this is a graph of some of the index"
      },
      {
        "start": 4165.56,
        "duration": 6.239,
        "text": "implementations in Facebook ai's"
      },
      {
        "start": 4168.799,
        "duration": 5.4,
        "text": "approximate similarity search Library uh"
      },
      {
        "start": 4171.799,
        "duration": 5.52,
        "text": "there's literally dozens of combinations"
      },
      {
        "start": 4174.199,
        "duration": 4.681,
        "text": "you can put together here to to test and"
      },
      {
        "start": 4177.319,
        "duration": 4.98,
        "text": "what you're seeing on the axes by the"
      },
      {
        "start": 4178.88,
        "duration": 6.299,
        "text": "way is that the x-axis is recall that's"
      },
      {
        "start": 4182.299,
        "duration": 6.06,
        "text": "how accurate are my results and that's"
      },
      {
        "start": 4185.179,
        "duration": 6.301,
        "text": "measured as a percentage or a fraction"
      },
      {
        "start": 4188.359,
        "duration": 4.94,
        "text": "from zero to one where one is I got a"
      },
      {
        "start": 4191.48,
        "duration": 6.42,
        "text": "hundred percent perfect results"
      },
      {
        "start": 4193.299,
        "duration": 6.521,
        "text": "and then the y-axis here is how many uh"
      },
      {
        "start": 4197.9,
        "duration": 4.74,
        "text": "queries per second I can do"
      },
      {
        "start": 4199.82,
        "duration": 4.74,
        "text": "and the y-axis is logarithmic I'll point"
      },
      {
        "start": 4202.64,
        "duration": 4.98,
        "text": "that out as well"
      },
      {
        "start": 4204.56,
        "duration": 6.24,
        "text": "um and so all of these index types you"
      },
      {
        "start": 4207.62,
        "duration": 5.04,
        "text": "can trade off between how accurate do I"
      },
      {
        "start": 4210.8,
        "duration": 4.08,
        "text": "need my results to be and how fast do I"
      },
      {
        "start": 4212.66,
        "duration": 5.46,
        "text": "want it to be but there's clearly some"
      },
      {
        "start": 4214.88,
        "duration": 5.1,
        "text": "algorithms are better than others uh"
      },
      {
        "start": 4218.12,
        "duration": 4.079,
        "text": "even given the fact that that we can"
      },
      {
        "start": 4219.98,
        "duration": 6.36,
        "text": "make different trade-offs"
      },
      {
        "start": 4222.199,
        "duration": 7.201,
        "text": "um and so applying the uh"
      },
      {
        "start": 4226.34,
        "duration": 5.58,
        "text": "criteria that Cassandra has we end up"
      },
      {
        "start": 4229.4,
        "duration": 5.22,
        "text": "with basically one appropriate candidate"
      },
      {
        "start": 4231.92,
        "duration": 5.279,
        "text": "for building the uh indexes and that's"
      },
      {
        "start": 4234.62,
        "duration": 5.94,
        "text": "called hnsw which stands for"
      },
      {
        "start": 4237.199,
        "duration": 6.841,
        "text": "hierarchical navigable small world and"
      },
      {
        "start": 4240.56,
        "duration": 6.24,
        "text": "the I the idea here is similar to a"
      },
      {
        "start": 4244.04,
        "duration": 5.28,
        "text": "classic skip list where I've got I'm"
      },
      {
        "start": 4246.8,
        "duration": 6.24,
        "text": "going to create multiple levels in my"
      },
      {
        "start": 4249.32,
        "duration": 7.26,
        "text": "index and at level zero the bottom all"
      },
      {
        "start": 4253.04,
        "duration": 6.06,
        "text": "of my vectors live in in level zero but"
      },
      {
        "start": 4256.58,
        "duration": 5.7,
        "text": "every level I go up from there I only"
      },
      {
        "start": 4259.1,
        "duration": 5.82,
        "text": "promote 10 of the vectors to that next"
      },
      {
        "start": 4262.28,
        "duration": 4.919,
        "text": "level so I you know I've got a hundred"
      },
      {
        "start": 4264.92,
        "duration": 4.38,
        "text": "percent of my vectors in level zero I've"
      },
      {
        "start": 4267.199,
        "duration": 4.621,
        "text": "got ten percent in level one I've got"
      },
      {
        "start": 4269.3,
        "duration": 5.22,
        "text": "one percent in level two point one"
      },
      {
        "start": 4271.82,
        "duration": 7.56,
        "text": "percent and level three and so forth and"
      },
      {
        "start": 4274.52,
        "duration": 9.179,
        "text": "what we do is when we search for a query"
      },
      {
        "start": 4279.38,
        "duration": 6.66,
        "text": "vector and the the page here is uh"
      },
      {
        "start": 4283.699,
        "duration": 5.101,
        "text": "giving an example of a query Vector in"
      },
      {
        "start": 4286.04,
        "duration": 6.659,
        "text": "red what we do is we go to the top level"
      },
      {
        "start": 4288.8,
        "duration": 6.72,
        "text": "in our our index and we compare it to uh"
      },
      {
        "start": 4292.699,
        "duration": 4.02,
        "text": "the the nodes the vectors in that top"
      },
      {
        "start": 4295.52,
        "duration": 4.139,
        "text": "level"
      },
      {
        "start": 4296.719,
        "duration": 5.821,
        "text": "and then the one that is closest to we"
      },
      {
        "start": 4299.659,
        "duration": 6.0,
        "text": "look for The Neighbors of that node in"
      },
      {
        "start": 4302.54,
        "duration": 4.8,
        "text": "the next level down and then we take the"
      },
      {
        "start": 4305.659,
        "duration": 4.381,
        "text": "one that it's closest to in that level"
      },
      {
        "start": 4307.34,
        "duration": 4.62,
        "text": "go to the next level down and so forth"
      },
      {
        "start": 4310.04,
        "duration": 5.04,
        "text": "until we get down to the bottom level"
      },
      {
        "start": 4311.96,
        "duration": 5.88,
        "text": "where all of our vectors live and"
      },
      {
        "start": 4315.08,
        "duration": 4.8,
        "text": "we've narrowed down the vicinity of"
      },
      {
        "start": 4317.84,
        "duration": 6.0,
        "text": "where we're going to look by following"
      },
      {
        "start": 4319.88,
        "duration": 5.94,
        "text": "that chain down from the top and so we"
      },
      {
        "start": 4323.84,
        "duration": 5.7,
        "text": "get it's not an exact search again"
      },
      {
        "start": 4325.82,
        "duration": 5.22,
        "text": "because each we're only remembering a"
      },
      {
        "start": 4329.54,
        "duration": 3.48,
        "text": "small fraction of the potential"
      },
      {
        "start": 4331.04,
        "duration": 4.199,
        "text": "neighbors for each Vector first of all"
      },
      {
        "start": 4333.02,
        "duration": 6.42,
        "text": "and then second of all we're bounding"
      },
      {
        "start": 4335.239,
        "duration": 5.761,
        "text": "how far we search on each level but you"
      },
      {
        "start": 4339.44,
        "duration": 5.12,
        "text": "can see how we get the logarithmic"
      },
      {
        "start": 4341.0,
        "duration": 6.78,
        "text": "behavior from uh the graph construction"
      },
      {
        "start": 4344.56,
        "duration": 7.179,
        "text": "of one-tenth of the nodes on each level"
      },
      {
        "start": 4347.78,
        "duration": 5.64,
        "text": "uh the details of how we build that uh I"
      },
      {
        "start": 4351.739,
        "duration": 3.42,
        "text": "would if you're interested I definitely"
      },
      {
        "start": 4353.42,
        "duration": 4.34,
        "text": "encourage you to read the paper that"
      },
      {
        "start": 4355.159,
        "duration": 5.821,
        "text": "this comes from uh it's super readable"
      },
      {
        "start": 4357.76,
        "duration": 6.64,
        "text": "uh the core of the algorithm is just"
      },
      {
        "start": 4360.98,
        "duration": 7.259,
        "text": "about one page uh so at its core it's"
      },
      {
        "start": 4364.4,
        "duration": 5.64,
        "text": "it's very concise and simple and elegant"
      },
      {
        "start": 4368.239,
        "duration": 4.081,
        "text": "um I won't go into it in more detail"
      },
      {
        "start": 4370.04,
        "duration": 5.0,
        "text": "here except to say that you know this"
      },
      {
        "start": 4372.32,
        "duration": 5.04,
        "text": "this is something that we can do"
      },
      {
        "start": 4375.04,
        "duration": 5.26,
        "text": "concurrently and give you those"
      },
      {
        "start": 4377.36,
        "duration": 5.94,
        "text": "logarithmic query results"
      },
      {
        "start": 4380.3,
        "duration": 5.7,
        "text": "so some more details of our"
      },
      {
        "start": 4383.3,
        "duration": 7.919,
        "text": "implementation is that this is actually"
      },
      {
        "start": 4386.0,
        "duration": 8.4,
        "text": "based on a h s w index from leucine but"
      },
      {
        "start": 4391.219,
        "duration": 5.821,
        "text": "we've made a few changes because leucine"
      },
      {
        "start": 4394.4,
        "duration": 7.44,
        "text": "does not qualify for one of those"
      },
      {
        "start": 4397.04,
        "duration": 6.659,
        "text": "requirements that I gave of when I add a"
      },
      {
        "start": 4401.84,
        "duration": 4.62,
        "text": "value to the index it needs to be"
      },
      {
        "start": 4403.699,
        "duration": 5.701,
        "text": "immediately queryable leucine is built"
      },
      {
        "start": 4406.46,
        "duration": 5.58,
        "text": "around more of a bulk ingest kind of"
      },
      {
        "start": 4409.4,
        "duration": 5.46,
        "text": "design and it has what's called a commit"
      },
      {
        "start": 4412.04,
        "duration": 4.76,
        "text": "interval and it and that says we're"
      },
      {
        "start": 4414.86,
        "duration": 7.2,
        "text": "going to"
      },
      {
        "start": 4416.8,
        "duration": 7.96,
        "text": "go through and serialize all of the data"
      },
      {
        "start": 4422.06,
        "duration": 4.98,
        "text": "that we've had in the last interval and"
      },
      {
        "start": 4424.76,
        "duration": 4.919,
        "text": "make that available for queries but"
      },
      {
        "start": 4427.04,
        "duration": 4.86,
        "text": "until we do that until we do that commit"
      },
      {
        "start": 4429.679,
        "duration": 5.161,
        "text": "it is not available for queries so"
      },
      {
        "start": 4431.9,
        "duration": 5.4,
        "text": "there's a a Time window where you know"
      },
      {
        "start": 4434.84,
        "duration": 4.8,
        "text": "I've inserted my new record but I can't"
      },
      {
        "start": 4437.3,
        "duration": 5.16,
        "text": "see it yet so that's not a good fit for"
      },
      {
        "start": 4439.64,
        "duration": 7.82,
        "text": "Cassandra and so what we did is we"
      },
      {
        "start": 4442.46,
        "duration": 9.66,
        "text": "modified uh the leucine index structure"
      },
      {
        "start": 4447.46,
        "duration": 10.42,
        "text": "to be able to index that and query that"
      },
      {
        "start": 4452.12,
        "duration": 9.3,
        "text": "without having a commit delay uh and as"
      },
      {
        "start": 4457.88,
        "duration": 6.359,
        "text": "part of that we were able to handle not"
      },
      {
        "start": 4461.42,
        "duration": 5.16,
        "text": "just doing reads instantly but doing"
      },
      {
        "start": 4464.239,
        "duration": 4.261,
        "text": "reads while rights are happening at the"
      },
      {
        "start": 4466.58,
        "duration": 4.74,
        "text": "same time and even multiple rights"
      },
      {
        "start": 4468.5,
        "duration": 6.3,
        "text": "happening at the same time"
      },
      {
        "start": 4471.32,
        "duration": 5.399,
        "text": "uh the last thing is that I mentioned"
      },
      {
        "start": 4474.8,
        "duration": 4.02,
        "text": "that dis you know serving this index"
      },
      {
        "start": 4476.719,
        "duration": 5.181,
        "text": "from disk is also very important to"
      },
      {
        "start": 4478.82,
        "duration": 5.82,
        "text": "Cassandra and so we took a look at how"
      },
      {
        "start": 4481.9,
        "duration": 5.86,
        "text": "uh the"
      },
      {
        "start": 4484.64,
        "duration": 6.42,
        "text": "you know we cache parts of the index in"
      },
      {
        "start": 4487.76,
        "duration": 6.36,
        "text": "memory uh and we adjusted that as well"
      },
      {
        "start": 4491.06,
        "duration": 7.38,
        "text": "and so what what our strategy is is that"
      },
      {
        "start": 4494.12,
        "duration": 8.4,
        "text": "uh since the top levels of the hnsw"
      },
      {
        "start": 4498.44,
        "duration": 6.66,
        "text": "index are accessed more frequently we're"
      },
      {
        "start": 4502.52,
        "duration": 5.0,
        "text": "going to uh"
      },
      {
        "start": 4505.1,
        "duration": 6.119,
        "text": "preferentially"
      },
      {
        "start": 4507.52,
        "duration": 7.36,
        "text": "store those levels in cash so that we're"
      },
      {
        "start": 4511.219,
        "duration": 6.421,
        "text": "not having to read from disk as often"
      },
      {
        "start": 4514.88,
        "duration": 8.52,
        "text": "so putting those together"
      },
      {
        "start": 4517.64,
        "duration": 8.34,
        "text": "uh we have a vector index that's part of"
      },
      {
        "start": 4523.4,
        "duration": 7.68,
        "text": "our Sai implementation"
      },
      {
        "start": 4525.98,
        "duration": 7.5,
        "text": "uh and so you can do a simple uh give me"
      },
      {
        "start": 4531.08,
        "duration": 6.06,
        "text": "the closest vectors anywhere in the"
      },
      {
        "start": 4533.48,
        "duration": 6.3,
        "text": "table that's this first uh code fragment"
      },
      {
        "start": 4537.14,
        "duration": 4.5,
        "text": "here where I'm just going to say you"
      },
      {
        "start": 4539.78,
        "duration": 3.419,
        "text": "know I'm not specifying any other"
      },
      {
        "start": 4541.64,
        "duration": 4.14,
        "text": "predicates I'm just saying give me the"
      },
      {
        "start": 4543.199,
        "duration": 5.941,
        "text": "closest vectors so here's the Syntax for"
      },
      {
        "start": 4545.78,
        "duration": 5.52,
        "text": "that I just say order by my Vector"
      },
      {
        "start": 4549.14,
        "duration": 6.12,
        "text": "column name the column name here is"
      },
      {
        "start": 4551.3,
        "duration": 6.3,
        "text": "embedding and then A and N of my query"
      },
      {
        "start": 4555.26,
        "duration": 6.0,
        "text": "Vector so the question mark would be the"
      },
      {
        "start": 4557.6,
        "duration": 5.88,
        "text": "bind variable that my query Vector gets"
      },
      {
        "start": 4561.26,
        "duration": 4.56,
        "text": "attached to and then I can say you know"
      },
      {
        "start": 4563.48,
        "duration": 5.04,
        "text": "how many results do I want that's the"
      },
      {
        "start": 4565.82,
        "duration": 7.08,
        "text": "the limit 10. so that's the the simplest"
      },
      {
        "start": 4568.52,
        "duration": 6.179,
        "text": "uh version the the next one is that uh"
      },
      {
        "start": 4572.9,
        "duration": 4.799,
        "text": "you know the most maybe the most common"
      },
      {
        "start": 4574.699,
        "duration": 6.0,
        "text": "kind of predicate in Cassandra is I want"
      },
      {
        "start": 4577.699,
        "duration": 5.581,
        "text": "to limit this to a single partition of"
      },
      {
        "start": 4580.699,
        "duration": 6.54,
        "text": "data so here I've added where partition"
      },
      {
        "start": 4583.28,
        "duration": 5.64,
        "text": "ID equals some other Vine variable uh"
      },
      {
        "start": 4587.239,
        "duration": 5.581,
        "text": "and so that you know"
      },
      {
        "start": 4588.92,
        "duration": 5.819,
        "text": "that composes with the vector ordering"
      },
      {
        "start": 4592.82,
        "duration": 5.76,
        "text": "that we get from our index"
      },
      {
        "start": 4594.739,
        "duration": 6.541,
        "text": "and then we can also include other index"
      },
      {
        "start": 4598.58,
        "duration": 4.02,
        "text": "predicates uh with or without a"
      },
      {
        "start": 4601.28,
        "duration": 3.54,
        "text": "partition ID"
      },
      {
        "start": 4602.6,
        "duration": 5.54,
        "text": "and that's what this third example shows"
      },
      {
        "start": 4604.82,
        "duration": 6.48,
        "text": "is you know I've got that nested Boolean"
      },
      {
        "start": 4608.14,
        "duration": 6.039,
        "text": "uh query and then I'm saying I want to"
      },
      {
        "start": 4611.3,
        "duration": 5.64,
        "text": "order that by Vector similarity so we"
      },
      {
        "start": 4614.179,
        "duration": 5.221,
        "text": "can do any of these all of these uh and"
      },
      {
        "start": 4616.94,
        "duration": 5.16,
        "text": "it and it just works"
      },
      {
        "start": 4619.4,
        "duration": 5.22,
        "text": "so our philosophy is that you know"
      },
      {
        "start": 4622.1,
        "duration": 5.46,
        "text": "Vector search is a feature it's an"
      },
      {
        "start": 4624.62,
        "duration": 5.099,
        "text": "important feature but it's not the only"
      },
      {
        "start": 4627.56,
        "duration": 6.54,
        "text": "feature you want you want Vector search"
      },
      {
        "start": 4629.719,
        "duration": 6.96,
        "text": "to play nice and be part of uh the rest"
      },
      {
        "start": 4634.1,
        "duration": 5.04,
        "text": "of your database and compose with what"
      },
      {
        "start": 4636.679,
        "duration": 4.441,
        "text": "the rest of your database does your app"
      },
      {
        "start": 4639.14,
        "duration": 4.68,
        "text": "needs to do you know normal crud"
      },
      {
        "start": 4641.12,
        "duration": 6.36,
        "text": "operations uh you know create retrieve"
      },
      {
        "start": 4643.82,
        "duration": 6.18,
        "text": "update delete and also Vector search"
      },
      {
        "start": 4647.48,
        "duration": 5.28,
        "text": "typically Vector embeddings are derived"
      },
      {
        "start": 4650.0,
        "duration": 4.92,
        "text": "from the rest of your app data so"
      },
      {
        "start": 4652.76,
        "duration": 4.919,
        "text": "the most straightforward way to deal"
      },
      {
        "start": 4654.92,
        "duration": 5.52,
        "text": "with that is to add a vector column to"
      },
      {
        "start": 4657.679,
        "duration": 5.52,
        "text": "your existing table and then put the"
      },
      {
        "start": 4660.44,
        "duration": 5.82,
        "text": "embedding uh into that column instead of"
      },
      {
        "start": 4663.199,
        "duration": 4.861,
        "text": "doing it in a separate system if you do"
      },
      {
        "start": 4666.26,
        "duration": 3.36,
        "text": "it in a separate system you know now you"
      },
      {
        "start": 4668.06,
        "duration": 4.32,
        "text": "have to deal with all the pain of"
      },
      {
        "start": 4669.62,
        "duration": 5.94,
        "text": "keeping those in sync uh two different"
      },
      {
        "start": 4672.38,
        "duration": 5.46,
        "text": "systems to administer it it's just you"
      },
      {
        "start": 4675.56,
        "duration": 5.88,
        "text": "know this is why you know the the"
      },
      {
        "start": 4677.84,
        "duration": 5.7,
        "text": "history of of databases has been I want"
      },
      {
        "start": 4681.44,
        "duration": 6.06,
        "text": "to use the smallest number of systems as"
      },
      {
        "start": 4683.54,
        "duration": 8.1,
        "text": "possible that let me accomplish my goal"
      },
      {
        "start": 4687.5,
        "duration": 5.94,
        "text": "uh and then it helps that the ordering"
      },
      {
        "start": 4691.64,
        "duration": 4.92,
        "text": "by Vector similarity is a natural"
      },
      {
        "start": 4693.44,
        "duration": 4.739,
        "text": "extension of cql the Cassandra query"
      },
      {
        "start": 4696.56,
        "duration": 3.36,
        "text": "language"
      },
      {
        "start": 4698.179,
        "duration": 3.241,
        "text": "um and then you know"
      },
      {
        "start": 4699.92,
        "duration": 4.2,
        "text": "uh"
      },
      {
        "start": 4701.42,
        "duration": 4.56,
        "text": "as as part of our implementation we"
      },
      {
        "start": 4704.12,
        "duration": 4.14,
        "text": "designed it that"
      },
      {
        "start": 4705.98,
        "duration": 4.259,
        "text": "uh such that"
      },
      {
        "start": 4708.26,
        "duration": 3.479,
        "text": "you know it's not going to surprise you"
      },
      {
        "start": 4710.239,
        "duration": 4.801,
        "text": "you know reads and writes can happen"
      },
      {
        "start": 4711.739,
        "duration": 5.761,
        "text": "concurrently that is is uh immediately"
      },
      {
        "start": 4715.04,
        "duration": 5.4,
        "text": "queryable on insert it composes the way"
      },
      {
        "start": 4717.5,
        "duration": 3.9,
        "text": "you'd expect with other predicates we we"
      },
      {
        "start": 4720.44,
        "duration": 3.36,
        "text": "want"
      },
      {
        "start": 4721.4,
        "duration": 4.7,
        "text": "we want this to just work as much as"
      },
      {
        "start": 4723.8,
        "duration": 5.76,
        "text": "possible"
      },
      {
        "start": 4726.1,
        "duration": 7.68,
        "text": "so a few implications for uh building"
      },
      {
        "start": 4729.56,
        "duration": 7.619,
        "text": "your application so the first is that"
      },
      {
        "start": 4733.78,
        "duration": 6.58,
        "text": "as as far as performance is concerned"
      },
      {
        "start": 4737.179,
        "duration": 5.161,
        "text": "the most important variable is how large"
      },
      {
        "start": 4740.36,
        "duration": 5.24,
        "text": "your vectors are"
      },
      {
        "start": 4742.34,
        "duration": 6.72,
        "text": "um so I put three examples on this page"
      },
      {
        "start": 4745.6,
        "duration": 5.2,
        "text": "uh probably the most well-known at this"
      },
      {
        "start": 4749.06,
        "duration": 4.98,
        "text": "point I guess it's kind of the default"
      },
      {
        "start": 4750.8,
        "duration": 7.5,
        "text": "for a lot of people now is open AI is"
      },
      {
        "start": 4754.04,
        "duration": 7.02,
        "text": "Ada uh embeddings model and that's going"
      },
      {
        "start": 4758.3,
        "duration": 4.5,
        "text": "to give you vectors of 1576"
      },
      {
        "start": 4761.06,
        "duration": 6.48,
        "text": "dimensionality"
      },
      {
        "start": 4762.8,
        "duration": 7.56,
        "text": "uh Google's uh gecko embeddings uh which"
      },
      {
        "start": 4767.54,
        "duration": 7.139,
        "text": "are based on their Palm architecture are"
      },
      {
        "start": 4770.36,
        "duration": 6.9,
        "text": "going to give you half that size 768 uh"
      },
      {
        "start": 4774.679,
        "duration": 4.461,
        "text": "and these are competitive in terms of"
      },
      {
        "start": 4777.26,
        "duration": 5.16,
        "text": "precision like in terms of"
      },
      {
        "start": 4779.14,
        "duration": 5.16,
        "text": "distinguishing uh actually similar"
      },
      {
        "start": 4782.42,
        "duration": 5.94,
        "text": "pieces of text"
      },
      {
        "start": 4784.3,
        "duration": 7.66,
        "text": "uh Gekko is competitive with Ada and so"
      },
      {
        "start": 4788.36,
        "duration": 6.42,
        "text": "other things being equal uh"
      },
      {
        "start": 4791.96,
        "duration": 5.82,
        "text": "I would prefer to use the gecko"
      },
      {
        "start": 4794.78,
        "duration": 5.04,
        "text": "embeddings because every time that I'm"
      },
      {
        "start": 4797.78,
        "duration": 8.28,
        "text": "doing a comparison as I'm navigating"
      },
      {
        "start": 4799.82,
        "duration": 9.48,
        "text": "down that search index I'm Computing uh"
      },
      {
        "start": 4806.06,
        "duration": 6.179,
        "text": "cosine or a DOT product of my query"
      },
      {
        "start": 4809.3,
        "duration": 7.32,
        "text": "Vector with the vector in the index and"
      },
      {
        "start": 4812.239,
        "duration": 8.221,
        "text": "so it's literally twice as slow to do"
      },
      {
        "start": 4816.62,
        "duration": 5.52,
        "text": "that with 1576 Dimensions as it is with"
      },
      {
        "start": 4820.46,
        "duration": 3.96,
        "text": "768."
      },
      {
        "start": 4822.14,
        "duration": 6.24,
        "text": "I've got one more model on this page and"
      },
      {
        "start": 4824.42,
        "duration": 6.84,
        "text": "that's the mini ilium L6 model from"
      },
      {
        "start": 4828.38,
        "duration": 7.98,
        "text": "hugging face and that's half the size"
      },
      {
        "start": 4831.26,
        "duration": 10.399,
        "text": "yet smaller of uh the gecko model"
      },
      {
        "start": 4836.36,
        "duration": 9.42,
        "text": "here is where we do start to see uh"
      },
      {
        "start": 4841.659,
        "duration": 6.341,
        "text": "worse performance on the dimension of"
      },
      {
        "start": 4845.78,
        "duration": 3.66,
        "text": "how good is it at distinguishing these"
      },
      {
        "start": 4848.0,
        "duration": 5.219,
        "text": "pieces of text"
      },
      {
        "start": 4849.44,
        "duration": 9.299,
        "text": "in my opinion you should use a model"
      },
      {
        "start": 4853.219,
        "duration": 8.761,
        "text": "like mini LM L6 carefully because if it"
      },
      {
        "start": 4858.739,
        "duration": 5.341,
        "text": "does give you enough uh"
      },
      {
        "start": 4861.98,
        "duration": 5.94,
        "text": "distinguishability across the vectors"
      },
      {
        "start": 4864.08,
        "duration": 7.139,
        "text": "then great like you're you're faster and"
      },
      {
        "start": 4867.92,
        "duration": 5.58,
        "text": "you're less expensive uh"
      },
      {
        "start": 4871.219,
        "duration": 6.241,
        "text": "so if it solves your problem definitely"
      },
      {
        "start": 4873.5,
        "duration": 6.48,
        "text": "use it however uh it I would not put it"
      },
      {
        "start": 4877.46,
        "duration": 4.98,
        "text": "in the same category of you can probably"
      },
      {
        "start": 4879.98,
        "duration": 6.38,
        "text": "just use it and have it just work like"
      },
      {
        "start": 4882.44,
        "duration": 3.92,
        "text": "the other two larger models"
      },
      {
        "start": 4886.699,
        "duration": 3.741,
        "text": "uh the next thing is that"
      },
      {
        "start": 4888.98,
        "duration": 5.04,
        "text": "I"
      },
      {
        "start": 4890.44,
        "duration": 5.68,
        "text": "you should use normalized vectors and"
      },
      {
        "start": 4894.02,
        "duration": 3.86,
        "text": "Dot products instead of unnormalized"
      },
      {
        "start": 4896.12,
        "duration": 6.18,
        "text": "vectors and cosines"
      },
      {
        "start": 4897.88,
        "duration": 6.22,
        "text": "and all that means is that if you're if"
      },
      {
        "start": 4902.3,
        "duration": 5.22,
        "text": "you're embedding vectors have been"
      },
      {
        "start": 4904.1,
        "duration": 6.66,
        "text": "normalized to length one then we can"
      },
      {
        "start": 4907.52,
        "duration": 6.24,
        "text": "compute the similarity as a simple dot"
      },
      {
        "start": 4910.76,
        "duration": 6.84,
        "text": "product between the components of those"
      },
      {
        "start": 4913.76,
        "duration": 7.5,
        "text": "vectors which is mathematically more"
      },
      {
        "start": 4917.6,
        "duration": 6.599,
        "text": "efficient than Computing the cosine"
      },
      {
        "start": 4921.26,
        "duration": 5.1,
        "text": "between two large vectors all of the"
      },
      {
        "start": 4924.199,
        "duration": 5.281,
        "text": "models on the previous page they give"
      },
      {
        "start": 4926.36,
        "duration": 5.52,
        "text": "you normalized vectors so you should use"
      },
      {
        "start": 4929.48,
        "duration": 4.56,
        "text": "dot product and here's the syntax that"
      },
      {
        "start": 4931.88,
        "duration": 6.06,
        "text": "you use to tell"
      },
      {
        "start": 4934.04,
        "duration": 7.74,
        "text": "Astra that you want to create a"
      },
      {
        "start": 4937.94,
        "duration": 6.0,
        "text": "index that uses dot product so that will"
      },
      {
        "start": 4941.78,
        "duration": 6.3,
        "text": "give you a performance difference about"
      },
      {
        "start": 4943.94,
        "duration": 6.0,
        "text": "40 percent it's about 40 faster if you"
      },
      {
        "start": 4948.08,
        "duration": 5.639,
        "text": "use dot product than cosine so that's"
      },
      {
        "start": 4949.94,
        "duration": 6.12,
        "text": "material the reason that we don't make"
      },
      {
        "start": 4953.719,
        "duration": 4.801,
        "text": "that the default is that if your vectors"
      },
      {
        "start": 4956.06,
        "duration": 5.34,
        "text": "aren't normalized and we have no way of"
      },
      {
        "start": 4958.52,
        "duration": 5.04,
        "text": "knowing ahead of time uh whether they"
      },
      {
        "start": 4961.4,
        "duration": 4.319,
        "text": "are unless you tell us by using this"
      },
      {
        "start": 4963.56,
        "duration": 3.72,
        "text": "syntax if your vectors aren't normalized"
      },
      {
        "start": 4965.719,
        "duration": 3.48,
        "text": "then dot product will give you"
      },
      {
        "start": 4967.28,
        "duration": 3.78,
        "text": "effectively nonsense"
      },
      {
        "start": 4969.199,
        "duration": 4.921,
        "text": "and so that's why we default to cosine"
      },
      {
        "start": 4971.06,
        "duration": 4.32,
        "text": "but in general uh you know check to make"
      },
      {
        "start": 4974.12,
        "duration": 4.98,
        "text": "sure that your model is giving you"
      },
      {
        "start": 4975.38,
        "duration": 6.14,
        "text": "normalized vectors and then opt into the"
      },
      {
        "start": 4979.1,
        "duration": 2.42,
        "text": "dot product"
      },
      {
        "start": 4981.8,
        "duration": 5.399,
        "text": "uh another thing is that you should"
      },
      {
        "start": 4984.32,
        "duration": 6.96,
        "text": "avoid deletes if possible so if it's not"
      },
      {
        "start": 4987.199,
        "duration": 5.96,
        "text": "possible then no it's not and uh you"
      },
      {
        "start": 4991.28,
        "duration": 5.34,
        "text": "know Cassandra will handle it just fine"
      },
      {
        "start": 4993.159,
        "duration": 7.481,
        "text": "however uh it does become less efficient"
      },
      {
        "start": 4996.62,
        "duration": 6.539,
        "text": "and that's because of how the uh index"
      },
      {
        "start": 5000.64,
        "duration": 6.48,
        "text": "Works remember that the index is"
      },
      {
        "start": 5003.159,
        "duration": 5.881,
        "text": "basically a graph of nodes representing"
      },
      {
        "start": 5007.12,
        "duration": 6.66,
        "text": "each vector and the neighbors of that"
      },
      {
        "start": 5009.04,
        "duration": 7.139,
        "text": "node and so if I delete a vector uh"
      },
      {
        "start": 5013.78,
        "duration": 4.98,
        "text": "there's no way to pull that out of the"
      },
      {
        "start": 5016.179,
        "duration": 7.861,
        "text": "graph without causing collateral damage"
      },
      {
        "start": 5018.76,
        "duration": 7.38,
        "text": "and so what we do is we uh leave the"
      },
      {
        "start": 5024.04,
        "duration": 3.96,
        "text": "vector in the graph but we mark it as"
      },
      {
        "start": 5026.14,
        "duration": 2.64,
        "text": "hey this has been deleted it's not used"
      },
      {
        "start": 5028.0,
        "duration": 1.98,
        "text": "anymore"
      },
      {
        "start": 5028.78,
        "duration": 4.8,
        "text": "so"
      },
      {
        "start": 5029.98,
        "duration": 5.64,
        "text": "if you so deletes work but it does make"
      },
      {
        "start": 5033.58,
        "duration": 4.2,
        "text": "things slow down because now I have to"
      },
      {
        "start": 5035.62,
        "duration": 5.34,
        "text": "go and check and say is this Vector"
      },
      {
        "start": 5037.78,
        "duration": 4.98,
        "text": "actually still valid but if you don't do"
      },
      {
        "start": 5040.96,
        "duration": 4.64,
        "text": "deletes then we don't need to do that"
      },
      {
        "start": 5042.76,
        "duration": 2.84,
        "text": "check and it's faster"
      },
      {
        "start": 5046.239,
        "duration": 4.5,
        "text": "um another piece of advice is this"
      },
      {
        "start": 5049.0,
        "duration": 4.38,
        "text": "applies to all of Cassandra not just"
      },
      {
        "start": 5050.739,
        "duration": 6.301,
        "text": "vectors is that Cassandra's designed for"
      },
      {
        "start": 5053.38,
        "duration": 6.66,
        "text": "concurrency uh and especially when"
      },
      {
        "start": 5057.04,
        "duration": 6.54,
        "text": "you're adding data to the system uh I"
      },
      {
        "start": 5060.04,
        "duration": 6.0,
        "text": "I'd say 90 of the time our inclination"
      },
      {
        "start": 5063.58,
        "duration": 4.2,
        "text": "as programmers is to write code that"
      },
      {
        "start": 5066.04,
        "duration": 4.32,
        "text": "looks like this where you know I'll"
      },
      {
        "start": 5067.78,
        "duration": 4.14,
        "text": "prepare my statement because I know that"
      },
      {
        "start": 5070.36,
        "duration": 5.1,
        "text": "that's more efficient than making"
      },
      {
        "start": 5071.92,
        "duration": 6.06,
        "text": "Cassandra Parson uh fresh every time but"
      },
      {
        "start": 5075.46,
        "duration": 5.82,
        "text": "then I'll go through uh every item in my"
      },
      {
        "start": 5077.98,
        "duration": 5.64,
        "text": "collection and execute that prepared"
      },
      {
        "start": 5081.28,
        "duration": 5.22,
        "text": "insert with that item"
      },
      {
        "start": 5083.62,
        "duration": 5.94,
        "text": "that will work but what that means is"
      },
      {
        "start": 5086.5,
        "duration": 5.64,
        "text": "that I'm sending a request to Cassandra"
      },
      {
        "start": 5089.56,
        "duration": 4.38,
        "text": "I'm waiting for it to get back and then"
      },
      {
        "start": 5092.14,
        "duration": 4.079,
        "text": "I'm sending another request waiting for"
      },
      {
        "start": 5093.94,
        "duration": 5.219,
        "text": "it to get back and so you know I've got"
      },
      {
        "start": 5096.219,
        "duration": 5.401,
        "text": "you know this this server of you know"
      },
      {
        "start": 5099.159,
        "duration": 5.58,
        "text": "dozens of cores on the other side of"
      },
      {
        "start": 5101.62,
        "duration": 5.76,
        "text": "that request and I'm just doing one at a"
      },
      {
        "start": 5104.739,
        "duration": 5.161,
        "text": "time uh with all the latency of the"
      },
      {
        "start": 5107.38,
        "duration": 7.38,
        "text": "network and the way of doing that so"
      },
      {
        "start": 5109.9,
        "duration": 7.92,
        "text": "it's much more uh it's much faster if I"
      },
      {
        "start": 5114.76,
        "duration": 7.979,
        "text": "just do those those inserts concurrently"
      },
      {
        "start": 5117.82,
        "duration": 6.96,
        "text": "and the Cassandra drivers uh provide uh"
      },
      {
        "start": 5122.739,
        "duration": 4.081,
        "text": "this functionality out of the box so"
      },
      {
        "start": 5124.78,
        "duration": 5.1,
        "text": "this is an example with the python"
      },
      {
        "start": 5126.82,
        "duration": 5.7,
        "text": "driver so I'm importing at the top I'm"
      },
      {
        "start": 5129.88,
        "duration": 5.46,
        "text": "importing execute concurrent with args"
      },
      {
        "start": 5132.52,
        "duration": 6.42,
        "text": "and what that means is I want to execute"
      },
      {
        "start": 5135.34,
        "duration": 6.78,
        "text": "a single statement against all the"
      },
      {
        "start": 5138.94,
        "duration": 5.88,
        "text": "arguments that I give it in that uh"
      },
      {
        "start": 5142.12,
        "duration": 5.34,
        "text": "function call so I'm calling that with"
      },
      {
        "start": 5144.82,
        "duration": 4.56,
        "text": "my session object with my prepared"
      },
      {
        "start": 5147.46,
        "duration": 5.88,
        "text": "request and then I just give it my"
      },
      {
        "start": 5149.38,
        "duration": 6.72,
        "text": "collection of tuples of data to apply to"
      },
      {
        "start": 5153.34,
        "duration": 5.879,
        "text": "the bind variables in my statement and"
      },
      {
        "start": 5156.1,
        "duration": 4.86,
        "text": "now the driver can go and do that uh you"
      },
      {
        "start": 5159.219,
        "duration": 5.52,
        "text": "know it can it will parallelize that and"
      },
      {
        "start": 5160.96,
        "duration": 6.9,
        "text": "issue multiple requests uh concurrently"
      },
      {
        "start": 5164.739,
        "duration": 5.881,
        "text": "so this is kind of the tutorial version"
      },
      {
        "start": 5167.86,
        "duration": 6.359,
        "text": "of this code in the next slide I'm going"
      },
      {
        "start": 5170.62,
        "duration": 6.24,
        "text": "to show you uh a more realistic version"
      },
      {
        "start": 5174.219,
        "duration": 6.121,
        "text": "because in the real world sometimes"
      },
      {
        "start": 5176.86,
        "duration": 5.94,
        "text": "requests fail because uh the server was"
      },
      {
        "start": 5180.34,
        "duration": 3.72,
        "text": "overloaded or because there was a"
      },
      {
        "start": 5182.8,
        "duration": 3.419,
        "text": "network hiccup"
      },
      {
        "start": 5184.06,
        "duration": 5.52,
        "text": "uh and so we we want to be able to to"
      },
      {
        "start": 5186.219,
        "duration": 7.261,
        "text": "deal with that uh without starting over"
      },
      {
        "start": 5189.58,
        "duration": 6.18,
        "text": "and so this is actually from some code I"
      },
      {
        "start": 5193.48,
        "duration": 4.02,
        "text": "wrote uh over the weekend just playing"
      },
      {
        "start": 5195.76,
        "duration": 4.5,
        "text": "around"
      },
      {
        "start": 5197.5,
        "duration": 4.98,
        "text": "um and"
      },
      {
        "start": 5200.26,
        "duration": 4.439,
        "text": "you can see a couple differences from"
      },
      {
        "start": 5202.48,
        "duration": 3.9,
        "text": "kind of the tutorial version here and"
      },
      {
        "start": 5204.699,
        "duration": 4.321,
        "text": "I've bolded a couple of those so the"
      },
      {
        "start": 5206.38,
        "duration": 5.7,
        "text": "first is that when we're running that"
      },
      {
        "start": 5209.02,
        "duration": 6.0,
        "text": "function execute concurrent with args I"
      },
      {
        "start": 5212.08,
        "duration": 4.86,
        "text": "want to I want to specify how how many"
      },
      {
        "start": 5215.02,
        "duration": 4.26,
        "text": "requests in parallel do you want to make"
      },
      {
        "start": 5216.94,
        "duration": 4.739,
        "text": "that's the concurrency argument and then"
      },
      {
        "start": 5219.28,
        "duration": 4.439,
        "text": "the second one says don't just give up"
      },
      {
        "start": 5221.679,
        "duration": 4.02,
        "text": "if you get an error"
      },
      {
        "start": 5223.719,
        "duration": 5.52,
        "text": "default is as soon as I get an error I'm"
      },
      {
        "start": 5225.699,
        "duration": 5.281,
        "text": "gonna throw an exception back to you"
      },
      {
        "start": 5229.239,
        "duration": 3.96,
        "text": "to you but that's not what we want we"
      },
      {
        "start": 5230.98,
        "duration": 6.48,
        "text": "want you to keep going and then when"
      },
      {
        "start": 5233.199,
        "duration": 6.841,
        "text": "you're done give me for each request did"
      },
      {
        "start": 5237.46,
        "duration": 4.62,
        "text": "it succeed or fail so then that that"
      },
      {
        "start": 5240.04,
        "duration": 5.82,
        "text": "next line there where I've got the list"
      },
      {
        "start": 5242.08,
        "duration": 7.2,
        "text": "comprehension I'm saying uh go through"
      },
      {
        "start": 5245.86,
        "duration": 5.1,
        "text": "all of those uh items that I sent the"
      },
      {
        "start": 5249.28,
        "duration": 3.3,
        "text": "database the chunks that I sent in the"
      },
      {
        "start": 5250.96,
        "duration": 5.52,
        "text": "database"
      },
      {
        "start": 5252.58,
        "duration": 5.88,
        "text": "um and then uh associate it with its"
      },
      {
        "start": 5256.48,
        "duration": 6.06,
        "text": "successor failure result"
      },
      {
        "start": 5258.46,
        "duration": 7.1,
        "text": "then pull out the ones that failed into"
      },
      {
        "start": 5262.54,
        "duration": 6.179,
        "text": "a new list and then we'll loop again"
      },
      {
        "start": 5265.56,
        "duration": 7.36,
        "text": "with only the failed ones we'll retry"
      },
      {
        "start": 5268.719,
        "duration": 7.621,
        "text": "those that's what's going on there"
      },
      {
        "start": 5272.92,
        "duration": 5.759,
        "text": "oh another another easy uh way to"
      },
      {
        "start": 5276.34,
        "duration": 6.54,
        "text": "improve your performance is to do your"
      },
      {
        "start": 5278.679,
        "duration": 6.901,
        "text": "reads at consistency level of local one"
      },
      {
        "start": 5282.88,
        "duration": 4.68,
        "text": "so the default consistency level is"
      },
      {
        "start": 5285.58,
        "duration": 4.38,
        "text": "Quorum which means we're going to"
      },
      {
        "start": 5287.56,
        "duration": 7.98,
        "text": "compare results from two out of three"
      },
      {
        "start": 5289.96,
        "duration": 7.5,
        "text": "replicas but uh this is not the this is"
      },
      {
        "start": 5295.54,
        "duration": 6.42,
        "text": "not the best thing to do in the case of"
      },
      {
        "start": 5297.46,
        "duration": 6.719,
        "text": "vector data where we usually depends on"
      },
      {
        "start": 5301.96,
        "duration": 4.38,
        "text": "your application but almost always in"
      },
      {
        "start": 5304.179,
        "duration": 5.581,
        "text": "the case of vector data it's not"
      },
      {
        "start": 5306.34,
        "duration": 6.54,
        "text": "something that you change frequently uh"
      },
      {
        "start": 5309.76,
        "duration": 5.82,
        "text": "typically you uh you know your your"
      },
      {
        "start": 5312.88,
        "duration": 5.7,
        "text": "vector embedding is computed based on"
      },
      {
        "start": 5315.58,
        "duration": 6.86,
        "text": "the data in your row once and that tends"
      },
      {
        "start": 5318.58,
        "duration": 8.159,
        "text": "to let that tends to live a long time"
      },
      {
        "start": 5322.44,
        "duration": 8.199,
        "text": "uh and so rather than"
      },
      {
        "start": 5326.739,
        "duration": 6.181,
        "text": "wasting time checking two replicas that"
      },
      {
        "start": 5330.639,
        "duration": 5.52,
        "text": "are pretty much guaranteed to have the"
      },
      {
        "start": 5332.92,
        "duration": 5.759,
        "text": "same data on them uh we'll just"
      },
      {
        "start": 5336.159,
        "duration": 5.04,
        "text": "request that Cassandra check a single"
      },
      {
        "start": 5338.679,
        "duration": 6.421,
        "text": "replica so that's half the work it's"
      },
      {
        "start": 5341.199,
        "duration": 5.881,
        "text": "twice as fast uh it's just a good idea"
      },
      {
        "start": 5345.1,
        "duration": 5.639,
        "text": "the the last tip I'm going to leave you"
      },
      {
        "start": 5347.08,
        "duration": 6.72,
        "text": "with also depends on a little bit of"
      },
      {
        "start": 5350.739,
        "duration": 5.4,
        "text": "understanding of how Cassandra works and"
      },
      {
        "start": 5353.8,
        "duration": 4.16,
        "text": "I I referred to this earlier when I"
      },
      {
        "start": 5356.139,
        "duration": 5.641,
        "text": "talked about how often when we're"
      },
      {
        "start": 5357.96,
        "duration": 6.64,
        "text": "querying indexes uh we'll want to narrow"
      },
      {
        "start": 5361.78,
        "duration": 5.459,
        "text": "down what the index has to search by"
      },
      {
        "start": 5364.6,
        "duration": 5.52,
        "text": "giving it a partition ID now in"
      },
      {
        "start": 5367.239,
        "duration": 5.821,
        "text": "Cassandra you can Partition by user you"
      },
      {
        "start": 5370.12,
        "duration": 6.36,
        "text": "can Partition by time you can Partition"
      },
      {
        "start": 5373.06,
        "duration": 5.7,
        "text": "by lots of different strategies but uh"
      },
      {
        "start": 5376.48,
        "duration": 5.4,
        "text": "the bottom line is by specifying that"
      },
      {
        "start": 5378.76,
        "duration": 5.399,
        "text": "partition uh the index doesn't have to"
      },
      {
        "start": 5381.88,
        "duration": 5.52,
        "text": "do an exhaustive search it can limit its"
      },
      {
        "start": 5384.159,
        "duration": 5.821,
        "text": "search to that one partition uh and so"
      },
      {
        "start": 5387.4,
        "duration": 5.1,
        "text": "not only does that matter on an"
      },
      {
        "start": 5389.98,
        "duration": 6.84,
        "text": "individual replica but it also means"
      },
      {
        "start": 5392.5,
        "duration": 7.38,
        "text": "that Cassandra has to check fewer uh"
      },
      {
        "start": 5396.82,
        "duration": 7.919,
        "text": "nodes in the Cassandra cluster because"
      },
      {
        "start": 5399.88,
        "duration": 8.88,
        "text": "each partition is uniquely assigned to a"
      },
      {
        "start": 5404.739,
        "duration": 6.541,
        "text": "a triple of replicas and we we know that"
      },
      {
        "start": 5408.76,
        "duration": 7.979,
        "text": "we only need to touch those specific"
      },
      {
        "start": 5411.28,
        "duration": 8.16,
        "text": "replicas so if if your data model is"
      },
      {
        "start": 5416.739,
        "duration": 7.021,
        "text": "appropriate then"
      },
      {
        "start": 5419.44,
        "duration": 7.259,
        "text": "uh giving this hint to the query is"
      },
      {
        "start": 5423.76,
        "duration": 6.24,
        "text": "going to make your workloads more"
      },
      {
        "start": 5426.699,
        "duration": 6.841,
        "text": "scalable as well as giving you lower"
      },
      {
        "start": 5430.0,
        "duration": 6.78,
        "text": "latency for each of those queries and so"
      },
      {
        "start": 5433.54,
        "duration": 6.199,
        "text": "with that uh thank you and I'll turn it"
      },
      {
        "start": 5436.78,
        "duration": 6.19,
        "text": "back over to the team"
      },
      {
        "start": 5439.739,
        "duration": 6.341,
        "text": "thank you"
      },
      {
        "start": 5442.97,
        "duration": 5.33,
        "text": "[Music]"
      },
      {
        "start": 5446.08,
        "duration": 4.02,
        "text": "thank you Jonathan that was awesome in"
      },
      {
        "start": 5448.3,
        "duration": 3.6,
        "text": "our final session for today I got the"
      },
      {
        "start": 5450.1,
        "duration": 4.139,
        "text": "chance to catch up with Brian kirschner"
      },
      {
        "start": 5451.9,
        "duration": 4.08,
        "text": "VPO strategy at data Stacks the other"
      },
      {
        "start": 5454.239,
        "duration": 3.601,
        "text": "day for a discussion on things"
      },
      {
        "start": 5455.98,
        "duration": 4.679,
        "text": "Architects and practitioners should"
      },
      {
        "start": 5457.84,
        "duration": 5.16,
        "text": "consider as you put your first AI agents"
      },
      {
        "start": 5460.659,
        "duration": 4.321,
        "text": "in production we got the chance to talk"
      },
      {
        "start": 5463.0,
        "duration": 3.84,
        "text": "about many of the issues you will no"
      },
      {
        "start": 5464.98,
        "duration": 4.02,
        "text": "doubt be debating yourselves in your"
      },
      {
        "start": 5466.84,
        "duration": 4.379,
        "text": "journey from security and privacy to"
      },
      {
        "start": 5469.0,
        "duration": 5.06,
        "text": "relevance and hallucinations to the"
      },
      {
        "start": 5471.219,
        "duration": 2.841,
        "text": "ethics of AI"
      },
      {
        "start": 5477.0,
        "duration": 3.06,
        "text": "thank you"
      },
      {
        "start": 5480.58,
        "duration": 5.82,
        "text": "hi I'm Sharna Parky um I'm the real-time"
      },
      {
        "start": 5483.88,
        "duration": 5.339,
        "text": "AI product and strategy leader at data"
      },
      {
        "start": 5486.4,
        "duration": 5.16,
        "text": "stacks and here with me I have Brian as"
      },
      {
        "start": 5489.219,
        "duration": 3.96,
        "text": "the VPO strategy and data Stacks with a"
      },
      {
        "start": 5491.56,
        "duration": 3.659,
        "text": "long history of working with business"
      },
      {
        "start": 5493.179,
        "duration": 4.98,
        "text": "and technology leaders on digital"
      },
      {
        "start": 5495.219,
        "duration": 4.561,
        "text": "transformation and AI strategy it's"
      },
      {
        "start": 5498.159,
        "duration": 3.901,
        "text": "really good to have you here I'm very"
      },
      {
        "start": 5499.78,
        "duration": 3.3,
        "text": "excited about this conversation glad to"
      },
      {
        "start": 5502.06,
        "duration": 3.3,
        "text": "be here"
      },
      {
        "start": 5503.08,
        "duration": 4.619,
        "text": "uh let's start with you"
      },
      {
        "start": 5505.36,
        "duration": 5.22,
        "text": "um so I know that you've been thinking"
      },
      {
        "start": 5507.699,
        "duration": 5.641,
        "text": "deeply lately about challenges and risks"
      },
      {
        "start": 5510.58,
        "duration": 6.599,
        "text": "in this space the agent space the AI"
      },
      {
        "start": 5513.34,
        "duration": 5.7,
        "text": "space the generative AI space and"
      },
      {
        "start": 5517.179,
        "duration": 3.241,
        "text": "um we were talking to our CEO the other"
      },
      {
        "start": 5519.04,
        "duration": 4.56,
        "text": "day and we know that he's talked about"
      },
      {
        "start": 5520.42,
        "duration": 7.2,
        "text": "how we believe we reach our best future"
      },
      {
        "start": 5523.6,
        "duration": 6.599,
        "text": "of AI for all fastest if the right"
      },
      {
        "start": 5527.62,
        "duration": 6.96,
        "text": "people who want to do the right things"
      },
      {
        "start": 5530.199,
        "duration": 6.721,
        "text": "in the right way can learn by doing so"
      },
      {
        "start": 5534.58,
        "duration": 4.32,
        "text": "how can we mitigate those risks and"
      },
      {
        "start": 5536.92,
        "duration": 4.68,
        "text": "manage challenges when so many people"
      },
      {
        "start": 5538.9,
        "duration": 4.68,
        "text": "are working on AI yeah I think that"
      },
      {
        "start": 5541.6,
        "duration": 3.9,
        "text": "that's a great question and I think you"
      },
      {
        "start": 5543.58,
        "duration": 5.34,
        "text": "know it's worth thinking about this in"
      },
      {
        "start": 5545.5,
        "duration": 5.46,
        "text": "the context of the opportunity the risks"
      },
      {
        "start": 5548.92,
        "duration": 3.48,
        "text": "and the challenges and the precedent we"
      },
      {
        "start": 5550.96,
        "duration": 3.179,
        "text": "have right so if you think about"
      },
      {
        "start": 5552.4,
        "duration": 5.4,
        "text": "smartphones"
      },
      {
        "start": 5554.139,
        "duration": 6.06,
        "text": "about 85 penetration worldwide with"
      },
      {
        "start": 5557.8,
        "duration": 4.939,
        "text": "smartphones right yeah there's no reason"
      },
      {
        "start": 5560.199,
        "duration": 5.281,
        "text": "why AI won't be a hundred percent"
      },
      {
        "start": 5562.739,
        "duration": 4.781,
        "text": "worldwide and certainly in everybody's"
      },
      {
        "start": 5565.48,
        "duration": 3.96,
        "text": "organization right so if you didn't"
      },
      {
        "start": 5567.52,
        "duration": 5.04,
        "text": "start to think about that you know you"
      },
      {
        "start": 5569.44,
        "duration": 6.6,
        "text": "kind of back into look AI will redefine"
      },
      {
        "start": 5572.56,
        "duration": 6.42,
        "text": "Industries where your company sits will"
      },
      {
        "start": 5576.04,
        "duration": 4.8,
        "text": "depend on what you do with AI we know"
      },
      {
        "start": 5578.98,
        "duration": 4.5,
        "text": "it's going to be part of everybody's job"
      },
      {
        "start": 5580.84,
        "duration": 5.46,
        "text": "both doing their own work and serving"
      },
      {
        "start": 5583.48,
        "duration": 4.92,
        "text": "their internal external customers so the"
      },
      {
        "start": 5586.3,
        "duration": 4.919,
        "text": "sooner you Embrace that and say yes it's"
      },
      {
        "start": 5588.4,
        "duration": 4.68,
        "text": "a hundred percent of everybody's job"
      },
      {
        "start": 5591.219,
        "duration": 3.48,
        "text": "um then you start to think about how do"
      },
      {
        "start": 5593.08,
        "duration": 4.079,
        "text": "you do that safely you know how do you"
      },
      {
        "start": 5594.699,
        "duration": 4.621,
        "text": "do that right and I think one pillar we"
      },
      {
        "start": 5597.159,
        "duration": 5.101,
        "text": "think about is openness right being"
      },
      {
        "start": 5599.32,
        "duration": 5.52,
        "text": "explicit this is the journey we're on"
      },
      {
        "start": 5602.26,
        "duration": 4.379,
        "text": "it's all part of our jobs and we're all"
      },
      {
        "start": 5604.84,
        "duration": 2.94,
        "text": "learning how to do it"
      },
      {
        "start": 5606.639,
        "duration": 3.421,
        "text": "um and I think there's you know there's"
      },
      {
        "start": 5607.78,
        "duration": 4.379,
        "text": "a couple angles on that one is we all"
      },
      {
        "start": 5610.06,
        "duration": 5.099,
        "text": "know the pain of technical debt if we've"
      },
      {
        "start": 5612.159,
        "duration": 5.821,
        "text": "been around in in technology you've got"
      },
      {
        "start": 5615.159,
        "duration": 5.04,
        "text": "the chance not just to avoid technical"
      },
      {
        "start": 5617.98,
        "duration": 5.34,
        "text": "debt as you start off on this journey by"
      },
      {
        "start": 5620.199,
        "duration": 6.0,
        "text": "doing it right but you know use AI to"
      },
      {
        "start": 5623.32,
        "duration": 6.54,
        "text": "document every meeting create the body"
      },
      {
        "start": 5626.199,
        "duration": 5.52,
        "text": "of knowledge and text and data about all"
      },
      {
        "start": 5629.86,
        "duration": 4.58,
        "text": "the questions people ask you know what"
      },
      {
        "start": 5631.719,
        "duration": 5.221,
        "text": "harm could we do how would we know"
      },
      {
        "start": 5634.44,
        "duration": 4.54,
        "text": "what level of trust are we trying to"
      },
      {
        "start": 5636.94,
        "duration": 3.719,
        "text": "achieve what would have to be true to"
      },
      {
        "start": 5638.98,
        "duration": 3.84,
        "text": "achieve that you're actually creating a"
      },
      {
        "start": 5640.659,
        "duration": 5.161,
        "text": "body of knowledge you can point AI at"
      },
      {
        "start": 5642.82,
        "duration": 5.46,
        "text": "right the faster you proceed by learning"
      },
      {
        "start": 5645.82,
        "duration": 5.04,
        "text": "by doing carefully cautiously you"
      },
      {
        "start": 5648.28,
        "duration": 5.52,
        "text": "actually build this possibility of an AI"
      },
      {
        "start": 5650.86,
        "duration": 6.6,
        "text": "agent to help people do AI"
      },
      {
        "start": 5653.8,
        "duration": 5.46,
        "text": "organization with your context with your"
      },
      {
        "start": 5657.46,
        "duration": 6.36,
        "text": "customers you know in the geographies"
      },
      {
        "start": 5659.26,
        "duration": 6.3,
        "text": "you operate that's bespoke to you"
      },
      {
        "start": 5663.82,
        "duration": 3.359,
        "text": "um and then I think you know so many"
      },
      {
        "start": 5665.56,
        "duration": 3.079,
        "text": "people working on it then gets back"
      },
      {
        "start": 5667.179,
        "duration": 3.661,
        "text": "again to openness"
      },
      {
        "start": 5668.639,
        "duration": 4.441,
        "text": "ensure everybody feels like they can do"
      },
      {
        "start": 5670.84,
        "duration": 5.52,
        "text": "their work in the open right including"
      },
      {
        "start": 5673.08,
        "duration": 5.8,
        "text": "surfacing a question of we're not sure"
      },
      {
        "start": 5676.36,
        "duration": 5.1,
        "text": "how to achieve this level of trust right"
      },
      {
        "start": 5678.88,
        "duration": 4.02,
        "text": "we don't know how we would detect this"
      },
      {
        "start": 5681.46,
        "duration": 2.699,
        "text": "type of harm"
      },
      {
        "start": 5682.9,
        "duration": 3.06,
        "text": "um and then it shouldn't be going alone"
      },
      {
        "start": 5684.159,
        "duration": 3.661,
        "text": "in your organization and your"
      },
      {
        "start": 5685.96,
        "duration": 5.219,
        "text": "organization shouldn't go it alone right"
      },
      {
        "start": 5687.82,
        "duration": 5.94,
        "text": "like whatever vertical you're in you"
      },
      {
        "start": 5691.179,
        "duration": 4.321,
        "text": "probably lean into understanding you"
      },
      {
        "start": 5693.76,
        "duration": 4.32,
        "text": "know Trends in consumer marketing and"
      },
      {
        "start": 5695.5,
        "duration": 4.8,
        "text": "participate maybe in an academic body or"
      },
      {
        "start": 5698.08,
        "duration": 4.2,
        "text": "you know Trends in financial regulation"
      },
      {
        "start": 5700.3,
        "duration": 4.32,
        "text": "and you go to those those bodies and"
      },
      {
        "start": 5702.28,
        "duration": 5.879,
        "text": "you've got experts do the same for AI"
      },
      {
        "start": 5704.62,
        "duration": 6.18,
        "text": "right in each uh company but also each"
      },
      {
        "start": 5708.159,
        "duration": 5.401,
        "text": "function each vertical uh or each line"
      },
      {
        "start": 5710.8,
        "duration": 5.7,
        "text": "of business lean into picking up the"
      },
      {
        "start": 5713.56,
        "duration": 5.579,
        "text": "phone or engaging with folks"
      },
      {
        "start": 5716.5,
        "duration": 5.239,
        "text": "um I think you know we all see and I"
      },
      {
        "start": 5719.139,
        "duration": 5.821,
        "text": "mean we meaning business government"
      },
      {
        "start": 5721.739,
        "duration": 5.381,
        "text": "Academia we all see the potential if you"
      },
      {
        "start": 5724.96,
        "duration": 4.679,
        "text": "reach out to talk about what you want to"
      },
      {
        "start": 5727.12,
        "duration": 4.619,
        "text": "do with AI as an organization and people"
      },
      {
        "start": 5729.639,
        "duration": 3.6,
        "text": "will probably be very interested in"
      },
      {
        "start": 5731.739,
        "duration": 3.96,
        "text": "talking to you"
      },
      {
        "start": 5733.239,
        "duration": 3.721,
        "text": "yeah that is definitely I mean in my"
      },
      {
        "start": 5735.699,
        "duration": 2.94,
        "text": "experience recently been the case"
      },
      {
        "start": 5736.96,
        "duration": 3.779,
        "text": "anytime I've reached out to want to talk"
      },
      {
        "start": 5738.639,
        "duration": 5.1,
        "text": "about it people are engaging it we're"
      },
      {
        "start": 5740.739,
        "duration": 4.561,
        "text": "all learning this together and so but"
      },
      {
        "start": 5743.739,
        "duration": 3.841,
        "text": "the topics like some of the words you"
      },
      {
        "start": 5745.3,
        "duration": 5.52,
        "text": "just use safety and trustworthiness it"
      },
      {
        "start": 5747.58,
        "duration": 5.579,
        "text": "seems like those are key where you know"
      },
      {
        "start": 5750.82,
        "duration": 4.319,
        "text": "we have to there's there are feelings"
      },
      {
        "start": 5753.159,
        "duration": 4.621,
        "text": "we're trying to evoke right feelings of"
      },
      {
        "start": 5755.139,
        "duration": 6.0,
        "text": "safety feelings of trust so how do we"
      },
      {
        "start": 5757.78,
        "duration": 5.939,
        "text": "sort of Ensure we're building for safety"
      },
      {
        "start": 5761.139,
        "duration": 4.981,
        "text": "and for trust versus kind of doing it"
      },
      {
        "start": 5763.719,
        "duration": 4.321,
        "text": "after the fact yes absolutely I think"
      },
      {
        "start": 5766.12,
        "duration": 3.96,
        "text": "you know one two two other things I"
      },
      {
        "start": 5768.04,
        "duration": 6.119,
        "text": "think about a lot are the sense of being"
      },
      {
        "start": 5770.08,
        "duration": 7.02,
        "text": "Mission driven and context aware"
      },
      {
        "start": 5774.159,
        "duration": 5.101,
        "text": "um and so you know every time you're"
      },
      {
        "start": 5777.1,
        "duration": 5.22,
        "text": "going to deploy an AI you are entailing"
      },
      {
        "start": 5779.26,
        "duration": 5.22,
        "text": "some risk right the return will be"
      },
      {
        "start": 5782.32,
        "duration": 4.68,
        "text": "higher for those things that really"
      },
      {
        "start": 5784.48,
        "duration": 4.259,
        "text": "deliver on your brand promise that"
      },
      {
        "start": 5787.0,
        "duration": 4.32,
        "text": "connect to what customers really want"
      },
      {
        "start": 5788.739,
        "duration": 4.141,
        "text": "from you right so so first off is"
      },
      {
        "start": 5791.32,
        "duration": 3.66,
        "text": "thinking about what's going to give us"
      },
      {
        "start": 5792.88,
        "duration": 5.759,
        "text": "the highest return for the level of risk"
      },
      {
        "start": 5794.98,
        "duration": 6.239,
        "text": "we take right and sort of lean into what"
      },
      {
        "start": 5798.639,
        "duration": 4.741,
        "text": "what do folks really value from us right"
      },
      {
        "start": 5801.219,
        "duration": 4.141,
        "text": "yeah and and the second is you're going"
      },
      {
        "start": 5803.38,
        "duration": 5.4,
        "text": "to do best probably at what you know"
      },
      {
        "start": 5805.36,
        "duration": 5.76,
        "text": "well right so stay aware of the context"
      },
      {
        "start": 5808.78,
        "duration": 4.68,
        "text": "are you reaching outside of the context"
      },
      {
        "start": 5811.12,
        "duration": 5.64,
        "text": "where you've got good customer knowledge"
      },
      {
        "start": 5813.46,
        "duration": 5.04,
        "text": "and intimacy and domain expertise"
      },
      {
        "start": 5816.76,
        "duration": 4.8,
        "text": "um probably not the first AIS you want"
      },
      {
        "start": 5818.5,
        "duration": 6.659,
        "text": "to deploy right figure out how you can"
      },
      {
        "start": 5821.56,
        "duration": 5.7,
        "text": "enrich or extend what you know well"
      },
      {
        "start": 5825.159,
        "duration": 4.381,
        "text": "um which will then Pro you know if you"
      },
      {
        "start": 5827.26,
        "duration": 5.1,
        "text": "learn how to deliver safe trustworthy"
      },
      {
        "start": 5829.54,
        "duration": 5.52,
        "text": "experiences in a space that's defined"
      },
      {
        "start": 5832.36,
        "duration": 5.279,
        "text": "and scoped the tools you learn the"
      },
      {
        "start": 5835.06,
        "duration": 4.74,
        "text": "techniques you learn right the the ways"
      },
      {
        "start": 5837.639,
        "duration": 3.901,
        "text": "to measure and monitor you learn will"
      },
      {
        "start": 5839.8,
        "duration": 3.18,
        "text": "probably build your skills on you know"
      },
      {
        "start": 5841.54,
        "duration": 4.02,
        "text": "okay maybe we're now going to extend"
      },
      {
        "start": 5842.98,
        "duration": 4.199,
        "text": "into more foreign territory but the"
      },
      {
        "start": 5845.56,
        "duration": 2.76,
        "text": "foreign territory shouldn't be your"
      },
      {
        "start": 5847.179,
        "duration": 2.221,
        "text": "first foray"
      },
      {
        "start": 5848.32,
        "duration": 4.58,
        "text": "right"
      },
      {
        "start": 5849.4,
        "duration": 3.5,
        "text": "so if we're if we're putting this in"
      },
      {
        "start": 5853.36,
        "duration": 4.02,
        "text": "con today or"
      },
      {
        "start": 5854.679,
        "duration": 4.861,
        "text": "listeners or architects what are some"
      },
      {
        "start": 5857.38,
        "duration": 4.62,
        "text": "maybe more concrete actions they can"
      },
      {
        "start": 5859.54,
        "duration": 4.5,
        "text": "take that can help yeah that's a great"
      },
      {
        "start": 5862.0,
        "duration": 4.08,
        "text": "question I think you know the the bottom"
      },
      {
        "start": 5864.04,
        "duration": 4.32,
        "text": "line is as a technical practitioner and"
      },
      {
        "start": 5866.08,
        "duration": 4.86,
        "text": "architect your organization can't do"
      },
      {
        "start": 5868.36,
        "duration": 4.74,
        "text": "this without you can't succeed at safety"
      },
      {
        "start": 5870.94,
        "duration": 5.1,
        "text": "and Trust without you your your skills"
      },
      {
        "start": 5873.1,
        "duration": 5.34,
        "text": "are critical but also cross-functional"
      },
      {
        "start": 5876.04,
        "duration": 4.8,
        "text": "teams is already an established pattern"
      },
      {
        "start": 5878.44,
        "duration": 4.739,
        "text": "for success and I think becomes even"
      },
      {
        "start": 5880.84,
        "duration": 4.319,
        "text": "more important in when you think about"
      },
      {
        "start": 5883.179,
        "duration": 4.441,
        "text": "generative AI where there's"
      },
      {
        "start": 5885.159,
        "duration": 4.801,
        "text": "probabilistic interaction you know with"
      },
      {
        "start": 5887.62,
        "duration": 5.039,
        "text": "the end user right lots of things start"
      },
      {
        "start": 5889.96,
        "duration": 4.92,
        "text": "to matter you know everything from you"
      },
      {
        "start": 5892.659,
        "duration": 4.861,
        "text": "know their brand perception of you to"
      },
      {
        "start": 5894.88,
        "duration": 5.88,
        "text": "their age to their reading level to you"
      },
      {
        "start": 5897.52,
        "duration": 5.28,
        "text": "know cultural nuances and so as a"
      },
      {
        "start": 5900.76,
        "duration": 4.08,
        "text": "technologist you know a service you can"
      },
      {
        "start": 5902.8,
        "duration": 5.52,
        "text": "do not only to your organization but to"
      },
      {
        "start": 5904.84,
        "duration": 5.879,
        "text": "yourself is to really emphasize that the"
      },
      {
        "start": 5908.32,
        "duration": 4.74,
        "text": "path to success for this type of"
      },
      {
        "start": 5910.719,
        "duration": 6.5,
        "text": "application is a diverse team a"
      },
      {
        "start": 5913.06,
        "duration": 6.84,
        "text": "cross-functional team is you know making"
      },
      {
        "start": 5917.219,
        "duration": 5.381,
        "text": "wrestling with or chasing down the"
      },
      {
        "start": 5919.9,
        "duration": 5.46,
        "text": "qualitative dimensions of things just a"
      },
      {
        "start": 5922.6,
        "duration": 4.38,
        "text": "first order you know task and not"
      },
      {
        "start": 5925.36,
        "duration": 3.48,
        "text": "something you've you know we figured out"
      },
      {
        "start": 5926.98,
        "duration": 4.259,
        "text": "how to deploy a capability let's sort of"
      },
      {
        "start": 5928.84,
        "duration": 4.14,
        "text": "shoehorn it into what we think will work"
      },
      {
        "start": 5931.239,
        "duration": 4.5,
        "text": "like no build build from the customer"
      },
      {
        "start": 5932.98,
        "duration": 4.44,
        "text": "build from that emotion you want to"
      },
      {
        "start": 5935.739,
        "duration": 3.241,
        "text": "create and of course at the end of the"
      },
      {
        "start": 5937.42,
        "duration": 3.96,
        "text": "day the prize for this is you know"
      },
      {
        "start": 5938.98,
        "duration": 5.28,
        "text": "obviously losing trust is bad but the"
      },
      {
        "start": 5941.38,
        "duration": 4.98,
        "text": "prize is if you build trust people will"
      },
      {
        "start": 5944.26,
        "duration": 4.379,
        "text": "be willing to share that much more data"
      },
      {
        "start": 5946.36,
        "duration": 5.4,
        "text": "with you over time which you know is is"
      },
      {
        "start": 5948.639,
        "duration": 5.58,
        "text": "the route to competitive Advantage right"
      },
      {
        "start": 5951.76,
        "duration": 4.5,
        "text": "right exactly and I mean as"
      },
      {
        "start": 5954.219,
        "duration": 3.901,
        "text": "practitioners we all want more data for"
      },
      {
        "start": 5956.26,
        "duration": 4.14,
        "text": "all of those answers we're trying to"
      },
      {
        "start": 5958.12,
        "duration": 4.38,
        "text": "come up with and getting a more complete"
      },
      {
        "start": 5960.4,
        "duration": 4.38,
        "text": "picture of you know our users and things"
      },
      {
        "start": 5962.5,
        "duration": 4.8,
        "text": "like that right uh on that on that note"
      },
      {
        "start": 5964.78,
        "duration": 4.56,
        "text": "uh you know I know there's actually a"
      },
      {
        "start": 5967.3,
        "duration": 5.1,
        "text": "body of knowledge under the cognitive"
      },
      {
        "start": 5969.34,
        "duration": 5.52,
        "text": "handle of responsible AI that you are"
      },
      {
        "start": 5972.4,
        "duration": 4.2,
        "text": "extremely well versed in and do a lot of"
      },
      {
        "start": 5974.86,
        "duration": 3.42,
        "text": "thinking thinking about"
      },
      {
        "start": 5976.6,
        "duration": 3.18,
        "text": "um and I think it's important to surface"
      },
      {
        "start": 5978.28,
        "duration": 2.82,
        "text": "that you don't have to reinvent the"
      },
      {
        "start": 5979.78,
        "duration": 2.76,
        "text": "wheel here"
      },
      {
        "start": 5981.1,
        "duration": 2.579,
        "text": "um you should stand on the shoulders of"
      },
      {
        "start": 5982.54,
        "duration": 2.46,
        "text": "giants"
      },
      {
        "start": 5983.679,
        "duration": 3.241,
        "text": "um so you tell me a little about you"
      },
      {
        "start": 5985.0,
        "duration": 4.139,
        "text": "know how would you express your point of"
      },
      {
        "start": 5986.92,
        "duration": 5.279,
        "text": "view on responsible Ai and what that"
      },
      {
        "start": 5989.139,
        "duration": 4.921,
        "text": "means to folks who are listening yeah I"
      },
      {
        "start": 5992.199,
        "duration": 3.301,
        "text": "mean we may have heard about this in in"
      },
      {
        "start": 5994.06,
        "duration": 3.78,
        "text": "several different ways over the last"
      },
      {
        "start": 5995.5,
        "duration": 5.159,
        "text": "couple of years right it may be"
      },
      {
        "start": 5997.84,
        "duration": 4.62,
        "text": "responsibility or ethics or transparency"
      },
      {
        "start": 6000.659,
        "duration": 3.961,
        "text": "accountability they're all kind of"
      },
      {
        "start": 6002.46,
        "duration": 4.44,
        "text": "interconnected and they can't really"
      },
      {
        "start": 6004.62,
        "duration": 5.34,
        "text": "exist in a vacuum"
      },
      {
        "start": 6006.9,
        "duration": 5.759,
        "text": "um basically anything that we do with AI"
      },
      {
        "start": 6009.96,
        "duration": 4.739,
        "text": "is going to impact people in process and"
      },
      {
        "start": 6012.659,
        "duration": 5.281,
        "text": "Technology I tend to think about this"
      },
      {
        "start": 6014.699,
        "duration": 5.101,
        "text": "instead of about you know AI like just"
      },
      {
        "start": 6017.94,
        "duration": 4.259,
        "text": "responsibility in general because what"
      },
      {
        "start": 6019.8,
        "duration": 4.859,
        "text": "we're trying to do is drive Behavior"
      },
      {
        "start": 6022.199,
        "duration": 5.701,
        "text": "change some change in some outcomes so"
      },
      {
        "start": 6024.659,
        "duration": 5.641,
        "text": "it means that we have to consider the"
      },
      {
        "start": 6027.9,
        "duration": 4.5,
        "text": "people the behaviors trying to drive and"
      },
      {
        "start": 6030.3,
        "duration": 3.54,
        "text": "the ethics of that in the products that"
      },
      {
        "start": 6032.4,
        "duration": 4.62,
        "text": "we're making intended to create change"
      },
      {
        "start": 6033.84,
        "duration": 4.5,
        "text": "so for me part of ensuring something is"
      },
      {
        "start": 6037.02,
        "duration": 2.52,
        "text": "ethical"
      },
      {
        "start": 6038.34,
        "duration": 4.74,
        "text": "um since there are no real like"
      },
      {
        "start": 6039.54,
        "duration": 5.58,
        "text": "objective values is this consumable"
      },
      {
        "start": 6043.08,
        "duration": 4.32,
        "text": "communication to the reader so what you"
      },
      {
        "start": 6045.12,
        "duration": 5.82,
        "text": "were talking about earlier where it's"
      },
      {
        "start": 6047.4,
        "duration": 5.46,
        "text": "not just that you can willy-nilly make"
      },
      {
        "start": 6050.94,
        "duration": 4.32,
        "text": "some someone go do something that you"
      },
      {
        "start": 6052.86,
        "duration": 4.859,
        "text": "want them to do that that is not ethical"
      },
      {
        "start": 6055.26,
        "duration": 5.7,
        "text": "if it's not aligned with you know their"
      },
      {
        "start": 6057.719,
        "duration": 5.221,
        "text": "intention or their goal or some of their"
      },
      {
        "start": 6060.96,
        "duration": 3.84,
        "text": "values in some way and so like you said"
      },
      {
        "start": 6062.94,
        "duration": 3.96,
        "text": "we can stand on the shoulders and Giants"
      },
      {
        "start": 6064.8,
        "duration": 4.62,
        "text": "here and look into things like"
      },
      {
        "start": 6066.9,
        "duration": 4.02,
        "text": "um behavioral psychology like"
      },
      {
        "start": 6069.42,
        "duration": 2.94,
        "text": "um products that create change there are"
      },
      {
        "start": 6070.92,
        "duration": 4.259,
        "text": "a lot of books out there that we can"
      },
      {
        "start": 6072.36,
        "duration": 5.4,
        "text": "read about this Behavior change I mean I"
      },
      {
        "start": 6075.179,
        "duration": 4.681,
        "text": "think most of us well I'm dating myself"
      },
      {
        "start": 6077.76,
        "duration": 4.379,
        "text": "most of us remember"
      },
      {
        "start": 6079.86,
        "duration": 4.68,
        "text": "um like when like using cigarettes and"
      },
      {
        "start": 6082.139,
        "duration": 5.821,
        "text": "different things in movies were banned"
      },
      {
        "start": 6084.54,
        "duration": 7.679,
        "text": "yes unless right we made it look cool"
      },
      {
        "start": 6087.96,
        "duration": 6.719,
        "text": "but the the impact of the sort of"
      },
      {
        "start": 6092.219,
        "duration": 4.02,
        "text": "promoting this was that people were like"
      },
      {
        "start": 6094.679,
        "duration": 3.241,
        "text": "oh I'm cool that they're not thinking"
      },
      {
        "start": 6096.239,
        "duration": 3.721,
        "text": "about like it caused me death that sort"
      },
      {
        "start": 6097.92,
        "duration": 4.739,
        "text": "of a thing so we have to be careful with"
      },
      {
        "start": 6099.96,
        "duration": 4.199,
        "text": "AI in that regard as well are there a"
      },
      {
        "start": 6102.659,
        "duration": 3.661,
        "text": "couple examples that you would cite"
      },
      {
        "start": 6104.159,
        "duration": 4.261,
        "text": "about you know what it means to to build"
      },
      {
        "start": 6106.32,
        "duration": 5.22,
        "text": "responsibly"
      },
      {
        "start": 6108.42,
        "duration": 5.4,
        "text": "yeah I mean I like to think kind of of"
      },
      {
        "start": 6111.54,
        "duration": 4.5,
        "text": "responsible and irresponsible and so"
      },
      {
        "start": 6113.82,
        "duration": 5.58,
        "text": "irresponsible examples are a little"
      },
      {
        "start": 6116.04,
        "duration": 5.22,
        "text": "easier for me to kind of pull out so"
      },
      {
        "start": 6119.4,
        "duration": 4.08,
        "text": "um and maybe I can just negate them as"
      },
      {
        "start": 6121.26,
        "duration": 4.919,
        "text": "we're talking so I I would find it"
      },
      {
        "start": 6123.48,
        "duration": 5.159,
        "text": "irresponsible especially in AI if you"
      },
      {
        "start": 6126.179,
        "duration": 6.06,
        "text": "didn't provide some sort of feedback"
      },
      {
        "start": 6128.639,
        "duration": 5.161,
        "text": "loop right AI is learning from the data"
      },
      {
        "start": 6132.239,
        "duration": 3.181,
        "text": "and learning from different outcomes and"
      },
      {
        "start": 6133.8,
        "duration": 3.899,
        "text": "so if you're not providing any"
      },
      {
        "start": 6135.42,
        "duration": 4.08,
        "text": "mechanisms for closing that feedback"
      },
      {
        "start": 6137.699,
        "duration": 4.081,
        "text": "loop like you're keeping"
      },
      {
        "start": 6139.5,
        "duration": 4.44,
        "text": "um your idea of someone or the state of"
      },
      {
        "start": 6141.78,
        "duration": 4.98,
        "text": "the world constant there's no change"
      },
      {
        "start": 6143.94,
        "duration": 4.739,
        "text": "that would be irresponsible we know I"
      },
      {
        "start": 6146.76,
        "duration": 4.2,
        "text": "mean just look at how things are so"
      },
      {
        "start": 6148.679,
        "duration": 5.341,
        "text": "rapidly changing we know language"
      },
      {
        "start": 6150.96,
        "duration": 6.06,
        "text": "changes over time we know that who a"
      },
      {
        "start": 6154.02,
        "duration": 5.52,
        "text": "person is doesn't equate to the order of"
      },
      {
        "start": 6157.02,
        "duration": 5.099,
        "text": "like credit rights or things like that"
      },
      {
        "start": 6159.54,
        "duration": 4.619,
        "text": "we don't know how things are going to"
      },
      {
        "start": 6162.119,
        "duration": 4.08,
        "text": "change and we don't know what kind of"
      },
      {
        "start": 6164.159,
        "duration": 4.261,
        "text": "Concepts we're using today that should"
      },
      {
        "start": 6166.199,
        "duration": 4.5,
        "text": "not be codified so it would be very"
      },
      {
        "start": 6168.42,
        "duration": 4.44,
        "text": "irresponsible to keep sort of that"
      },
      {
        "start": 6170.699,
        "duration": 4.5,
        "text": "feedback loop broken"
      },
      {
        "start": 6172.86,
        "duration": 4.5,
        "text": "yeah and I love I love that example"
      },
      {
        "start": 6175.199,
        "duration": 3.901,
        "text": "because you know if you think about you"
      },
      {
        "start": 6177.36,
        "duration": 3.299,
        "text": "know however let's call it a"
      },
      {
        "start": 6179.1,
        "duration": 3.96,
        "text": "manipulative system to stay in the"
      },
      {
        "start": 6180.659,
        "duration": 3.781,
        "text": "irresponsible Zone however effective it"
      },
      {
        "start": 6183.06,
        "duration": 3.3,
        "text": "is today"
      },
      {
        "start": 6184.44,
        "duration": 3.84,
        "text": "um if it's not a learning system it may"
      },
      {
        "start": 6186.36,
        "duration": 4.08,
        "text": "become less effective and now obviously"
      },
      {
        "start": 6188.28,
        "duration": 4.859,
        "text": "the win-win is you're not manipulative"
      },
      {
        "start": 6190.44,
        "duration": 4.199,
        "text": "but you are a learning system and you"
      },
      {
        "start": 6193.139,
        "duration": 2.58,
        "text": "kind of miss the boat if you're not a"
      },
      {
        "start": 6194.639,
        "duration": 3.54,
        "text": "learning system"
      },
      {
        "start": 6195.719,
        "duration": 3.721,
        "text": "yeah yeah and I think part of this"
      },
      {
        "start": 6198.179,
        "duration": 3.601,
        "text": "openness that you were talking about"
      },
      {
        "start": 6199.44,
        "duration": 5.1,
        "text": "before also applies to"
      },
      {
        "start": 6201.78,
        "duration": 4.62,
        "text": "telling the end user about how you're"
      },
      {
        "start": 6204.54,
        "duration": 4.8,
        "text": "updating this data how you're updating"
      },
      {
        "start": 6206.4,
        "duration": 5.219,
        "text": "the model so you you have to communicate"
      },
      {
        "start": 6209.34,
        "duration": 4.859,
        "text": "the assumptions that you took when you"
      },
      {
        "start": 6211.619,
        "duration": 4.381,
        "text": "built the model uh the motivations that"
      },
      {
        "start": 6214.199,
        "duration": 4.98,
        "text": "you had in order to like change the"
      },
      {
        "start": 6216.0,
        "duration": 3.84,
        "text": "behavior this this idea of trust"
      },
      {
        "start": 6219.179,
        "duration": 3.301,
        "text": "um"
      },
      {
        "start": 6219.84,
        "duration": 5.339,
        "text": "it doesn't come with secrecy right I"
      },
      {
        "start": 6222.48,
        "duration": 5.52,
        "text": "don't know like we can think about just"
      },
      {
        "start": 6225.179,
        "duration": 5.221,
        "text": "like teaching our children how to trust"
      },
      {
        "start": 6228.0,
        "duration": 4.38,
        "text": "other people like it comes from a"
      },
      {
        "start": 6230.4,
        "duration": 3.0,
        "text": "hundred thousand little interactions so"
      },
      {
        "start": 6232.38,
        "duration": 3.239,
        "text": "that"
      },
      {
        "start": 6233.4,
        "duration": 3.48,
        "text": "we can kind of build this up over time"
      },
      {
        "start": 6235.619,
        "duration": 4.321,
        "text": "and that's going to be with"
      },
      {
        "start": 6236.88,
        "duration": 4.799,
        "text": "communication that's just yes yeah and I"
      },
      {
        "start": 6239.94,
        "duration": 3.179,
        "text": "think in the moment we're in it's"
      },
      {
        "start": 6241.679,
        "duration": 2.94,
        "text": "actually pretty fascinating I think"
      },
      {
        "start": 6243.119,
        "duration": 4.261,
        "text": "you're hitting on something which people"
      },
      {
        "start": 6244.619,
        "duration": 5.701,
        "text": "should lean into because you know in the"
      },
      {
        "start": 6247.38,
        "duration": 5.04,
        "text": "past lots of folks weren't really aware"
      },
      {
        "start": 6250.32,
        "duration": 5.58,
        "text": "of how AI was working in the background"
      },
      {
        "start": 6252.42,
        "duration": 5.58,
        "text": "but now because of generative AI because"
      },
      {
        "start": 6255.9,
        "duration": 4.319,
        "text": "AI will be in every you know every"
      },
      {
        "start": 6258.0,
        "duration": 5.52,
        "text": "Microsoft Office or every Google app"
      },
      {
        "start": 6260.219,
        "duration": 5.221,
        "text": "your customers will understand that"
      },
      {
        "start": 6263.52,
        "duration": 4.8,
        "text": "they're interacting with AI and you can"
      },
      {
        "start": 6265.44,
        "duration": 5.04,
        "text": "actually disclose this information and"
      },
      {
        "start": 6268.32,
        "duration": 4.319,
        "text": "they'll get it right and that that's uh"
      },
      {
        "start": 6270.48,
        "duration": 3.719,
        "text": "that's an opportunity really"
      },
      {
        "start": 6272.639,
        "duration": 3.361,
        "text": "um so to bring this back to the final"
      },
      {
        "start": 6274.199,
        "duration": 4.141,
        "text": "question you asked me"
      },
      {
        "start": 6276.0,
        "duration": 4.56,
        "text": "um you know what advice do you have for"
      },
      {
        "start": 6278.34,
        "duration": 3.6,
        "text": "the practitioners and architects in our"
      },
      {
        "start": 6280.56,
        "duration": 4.32,
        "text": "audience"
      },
      {
        "start": 6281.94,
        "duration": 5.16,
        "text": "yeah I think in general when I think of"
      },
      {
        "start": 6284.88,
        "duration": 4.98,
        "text": "responsibility it's a it's a shared"
      },
      {
        "start": 6287.1,
        "duration": 5.099,
        "text": "framework we all have some"
      },
      {
        "start": 6289.86,
        "duration": 4.68,
        "text": "responsibility in this our users our"
      },
      {
        "start": 6292.199,
        "duration": 4.321,
        "text": "Builders the providers of the technology"
      },
      {
        "start": 6294.54,
        "duration": 4.079,
        "text": "collectors and providers of data there's"
      },
      {
        "start": 6296.52,
        "duration": 3.06,
        "text": "some level of responsibility and similar"
      },
      {
        "start": 6298.619,
        "duration": 2.761,
        "text": "to what you were saying I would"
      },
      {
        "start": 6299.58,
        "duration": 2.659,
        "text": "encourage the practitioners to"
      },
      {
        "start": 6301.38,
        "duration": 3.9,
        "text": "understand"
      },
      {
        "start": 6302.239,
        "duration": 4.721,
        "text": "where do they sit in terms of what"
      },
      {
        "start": 6305.28,
        "duration": 4.14,
        "text": "they're accountable for and who they're"
      },
      {
        "start": 6306.96,
        "duration": 5.34,
        "text": "accountable to if you're building"
      },
      {
        "start": 6309.42,
        "duration": 7.44,
        "text": "something that's going to impact an end"
      },
      {
        "start": 6312.3,
        "duration": 7.62,
        "text": "user are you building in mechanisms for"
      },
      {
        "start": 6316.86,
        "duration": 5.64,
        "text": "communicating the risks to the user"
      },
      {
        "start": 6319.92,
        "duration": 4.64,
        "text": "that's associated with using the content"
      },
      {
        "start": 6322.5,
        "duration": 5.28,
        "text": "that's being generated with your app or"
      },
      {
        "start": 6324.56,
        "duration": 6.04,
        "text": "maybe that what your application May"
      },
      {
        "start": 6327.78,
        "duration": 4.5,
        "text": "prevent them from doing you know what"
      },
      {
        "start": 6330.6,
        "duration": 4.68,
        "text": "what are you communicating and how are"
      },
      {
        "start": 6332.28,
        "duration": 4.379,
        "text": "you letting that sort of come out in"
      },
      {
        "start": 6335.28,
        "duration": 4.919,
        "text": "your end user experience as you're"
      },
      {
        "start": 6336.659,
        "duration": 6.121,
        "text": "building but also part of what you're"
      },
      {
        "start": 6340.199,
        "duration": 4.081,
        "text": "doing by exposing this information and"
      },
      {
        "start": 6342.78,
        "duration": 5.22,
        "text": "building in a more transparent and open"
      },
      {
        "start": 6344.28,
        "duration": 7.5,
        "text": "way is you're enabling your end users to"
      },
      {
        "start": 6348.0,
        "duration": 5.4,
        "text": "take part in their own responsibility as"
      },
      {
        "start": 6351.78,
        "duration": 4.5,
        "text": "part of this shared responsibility model"
      },
      {
        "start": 6353.4,
        "duration": 4.92,
        "text": "right so you know there are ways to do"
      },
      {
        "start": 6356.28,
        "duration": 3.419,
        "text": "this whether it's you know terms of"
      },
      {
        "start": 6358.32,
        "duration": 3.12,
        "text": "services not the way that you're going"
      },
      {
        "start": 6359.699,
        "duration": 4.561,
        "text": "to communicate this to your end user you"
      },
      {
        "start": 6361.44,
        "duration": 5.699,
        "text": "need these simple experiences that are"
      },
      {
        "start": 6364.26,
        "duration": 4.979,
        "text": "understandable that allow a user to know"
      },
      {
        "start": 6367.139,
        "duration": 4.261,
        "text": "like hey I just saw this friend"
      },
      {
        "start": 6369.239,
        "duration": 3.361,
        "text": "recommendation maybe you have a little"
      },
      {
        "start": 6371.4,
        "duration": 2.759,
        "text": "pop-up that said where did this"
      },
      {
        "start": 6372.6,
        "duration": 3.9,
        "text": "recommendation come from or how did it"
      },
      {
        "start": 6374.159,
        "duration": 4.441,
        "text": "get here letting them opt in and out of"
      },
      {
        "start": 6376.5,
        "duration": 5.1,
        "text": "different components of your experience"
      },
      {
        "start": 6378.6,
        "duration": 5.4,
        "text": "and applications so yeah I would yeah"
      },
      {
        "start": 6381.6,
        "duration": 5.039,
        "text": "mostly encourage like individualize put"
      },
      {
        "start": 6384.0,
        "duration": 5.1,
        "text": "yourself in that ecosystem of shared"
      },
      {
        "start": 6386.639,
        "duration": 4.08,
        "text": "trust and responsibility and I love the"
      },
      {
        "start": 6389.1,
        "duration": 3.42,
        "text": "scoping and the contextualization"
      },
      {
        "start": 6390.719,
        "duration": 3.601,
        "text": "because you know we've got these models"
      },
      {
        "start": 6392.52,
        "duration": 4.08,
        "text": "now that we can interact with and have a"
      },
      {
        "start": 6394.32,
        "duration": 4.319,
        "text": "conversation about anything that's not"
      },
      {
        "start": 6396.6,
        "duration": 4.559,
        "text": "why your customers come to you right"
      },
      {
        "start": 6398.639,
        "duration": 4.261,
        "text": "right don't come for some generic"
      },
      {
        "start": 6401.159,
        "duration": 3.601,
        "text": "all-encompassing thing they come for"
      },
      {
        "start": 6402.9,
        "duration": 4.62,
        "text": "something specific where you can hone in"
      },
      {
        "start": 6404.76,
        "duration": 4.879,
        "text": "on oh a great feedback mechanism for"
      },
      {
        "start": 6407.52,
        "duration": 4.619,
        "text": "this specific interaction is this"
      },
      {
        "start": 6409.639,
        "duration": 5.261,
        "text": "because they're not here to chat about"
      },
      {
        "start": 6412.139,
        "duration": 5.04,
        "text": "all things under the sun right I mean"
      },
      {
        "start": 6414.9,
        "duration": 3.96,
        "text": "that brings us to the Practical example"
      },
      {
        "start": 6417.179,
        "duration": 3.54,
        "text": "that we were using earlier in our"
      },
      {
        "start": 6418.86,
        "duration": 5.16,
        "text": "earlier assessments we saw a demo of an"
      },
      {
        "start": 6420.719,
        "duration": 5.341,
        "text": "AI agent that datastex built and so"
      },
      {
        "start": 6424.02,
        "duration": 4.5,
        "text": "maybe we should go into some of the"
      },
      {
        "start": 6426.06,
        "duration": 4.92,
        "text": "considerations that they had and"
      },
      {
        "start": 6428.52,
        "duration": 5.28,
        "text": "learnings from that experience to kind"
      },
      {
        "start": 6430.98,
        "duration": 6.6,
        "text": "of help contextualize this to you know"
      },
      {
        "start": 6433.8,
        "duration": 6.12,
        "text": "the agent we did sounds great so yeah so"
      },
      {
        "start": 6437.58,
        "duration": 4.26,
        "text": "um I think the first thing"
      },
      {
        "start": 6439.92,
        "duration": 4.86,
        "text": "um when I spoke with a team that they"
      },
      {
        "start": 6441.84,
        "duration": 5.64,
        "text": "were considering is like you said what's"
      },
      {
        "start": 6444.78,
        "duration": 5.1,
        "text": "the customer experience what what part"
      },
      {
        "start": 6447.48,
        "duration": 4.259,
        "text": "of our brand and what part of our"
      },
      {
        "start": 6449.88,
        "duration": 3.66,
        "text": "business requirements are actually going"
      },
      {
        "start": 6451.739,
        "duration": 5.101,
        "text": "into this agent because"
      },
      {
        "start": 6453.54,
        "duration": 5.28,
        "text": "we knew that our agents had to be as"
      },
      {
        "start": 6456.84,
        "duration": 5.22,
        "text": "effective as our current support and"
      },
      {
        "start": 6458.82,
        "duration": 6.359,
        "text": "sales teams right our current human chat"
      },
      {
        "start": 6462.06,
        "duration": 6.0,
        "text": "is critical in go to market like we are"
      },
      {
        "start": 6465.179,
        "duration": 4.681,
        "text": "a developer Community like most of our"
      },
      {
        "start": 6468.06,
        "duration": 4.74,
        "text": "all of our products are open source and"
      },
      {
        "start": 6469.86,
        "duration": 4.92,
        "text": "so everything we do it's critical that"
      },
      {
        "start": 6472.8,
        "duration": 3.72,
        "text": "we are having that that communication"
      },
      {
        "start": 6474.78,
        "duration": 3.3,
        "text": "experience and so"
      },
      {
        "start": 6476.52,
        "duration": 3.78,
        "text": "when we were thinking about like what"
      },
      {
        "start": 6478.08,
        "duration": 4.8,
        "text": "the requirements are that was going to"
      },
      {
        "start": 6480.3,
        "duration": 5.1,
        "text": "be a very important part of the brand"
      },
      {
        "start": 6482.88,
        "duration": 4.259,
        "text": "experience as sort of that primary"
      },
      {
        "start": 6485.4,
        "duration": 3.54,
        "text": "interaction"
      },
      {
        "start": 6487.139,
        "duration": 3.421,
        "text": "um you know from a business perspective"
      },
      {
        "start": 6488.94,
        "duration": 3.48,
        "text": "it's going to impact that promoter score"
      },
      {
        "start": 6490.56,
        "duration": 3.9,
        "text": "and other things like that right but"
      },
      {
        "start": 6492.42,
        "duration": 3.6,
        "text": "also you know as"
      },
      {
        "start": 6494.46,
        "duration": 3.239,
        "text": "as we were thinking about that we were"
      },
      {
        "start": 6496.02,
        "duration": 4.02,
        "text": "thinking like okay well what are the"
      },
      {
        "start": 6497.699,
        "duration": 5.101,
        "text": "mitigation steps we need to take"
      },
      {
        "start": 6500.04,
        "duration": 4.56,
        "text": "and to your point earlier we were we had"
      },
      {
        "start": 6502.8,
        "duration": 4.62,
        "text": "to like limit the audience first we"
      },
      {
        "start": 6504.6,
        "duration": 4.68,
        "text": "needed to run tests we only deployed the"
      },
      {
        "start": 6507.42,
        "duration": 4.739,
        "text": "first assistant to like a small"
      },
      {
        "start": 6509.28,
        "duration": 4.56,
        "text": "percentage of our audience and we put in"
      },
      {
        "start": 6512.159,
        "duration": 3.181,
        "text": "some checks there right we had human"
      },
      {
        "start": 6513.84,
        "duration": 3.299,
        "text": "review"
      },
      {
        "start": 6515.34,
        "duration": 3.18,
        "text": "um so we're we're still working on this"
      },
      {
        "start": 6517.139,
        "duration": 2.281,
        "text": "and we're going to experiment with the"
      },
      {
        "start": 6518.52,
        "duration": 3.96,
        "text": "agent"
      },
      {
        "start": 6519.42,
        "duration": 5.58,
        "text": "um but the the human part of that loop"
      },
      {
        "start": 6522.48,
        "duration": 4.8,
        "text": "we're keeping intact because we need to"
      },
      {
        "start": 6525.0,
        "duration": 3.9,
        "text": "kind of like as we expand outside of"
      },
      {
        "start": 6527.28,
        "duration": 4.56,
        "text": "chat to writing emails and other"
      },
      {
        "start": 6528.9,
        "duration": 6.06,
        "text": "multimodal agents we need this sort of"
      },
      {
        "start": 6531.84,
        "duration": 5.16,
        "text": "expert review to make sure that you know"
      },
      {
        "start": 6534.96,
        "duration": 4.14,
        "text": "the audience that we're speaking to and"
      },
      {
        "start": 6537.0,
        "duration": 4.38,
        "text": "the brand as it's being represented is"
      },
      {
        "start": 6539.1,
        "duration": 4.74,
        "text": "still within you know what's acceptable"
      },
      {
        "start": 6541.38,
        "duration": 4.08,
        "text": "to us yes absolutely and I think you"
      },
      {
        "start": 6543.84,
        "duration": 3.66,
        "text": "know there's a there's a there's a human"
      },
      {
        "start": 6545.46,
        "duration": 3.779,
        "text": "side of cross-functional teams as a"
      },
      {
        "start": 6547.5,
        "duration": 3.48,
        "text": "pattern for success but there's a"
      },
      {
        "start": 6549.239,
        "duration": 3.841,
        "text": "there's another side which is you know"
      },
      {
        "start": 6550.98,
        "duration": 5.28,
        "text": "no one goes it alone that includes the"
      },
      {
        "start": 6553.08,
        "duration": 5.76,
        "text": "AIS yes right there's just no AI that"
      },
      {
        "start": 6556.26,
        "duration": 5.58,
        "text": "doesn't have a steward a shepherd on an"
      },
      {
        "start": 6558.84,
        "duration": 5.06,
        "text": "ongoing basis exactly yeah"
      },
      {
        "start": 6561.84,
        "duration": 2.06,
        "text": "exactly"
      },
      {
        "start": 6566.04,
        "duration": 4.079,
        "text": "okay so you know when you were thinking"
      },
      {
        "start": 6568.32,
        "duration": 2.76,
        "text": "about that with this application what"
      },
      {
        "start": 6570.119,
        "duration": 3.901,
        "text": "kind of"
      },
      {
        "start": 6571.08,
        "duration": 4.559,
        "text": "things did we hit there yeah I mean so I"
      },
      {
        "start": 6574.02,
        "duration": 3.659,
        "text": "think this the support is very"
      },
      {
        "start": 6575.639,
        "duration": 3.721,
        "text": "interesting because as a as a SAS"
      },
      {
        "start": 6577.679,
        "duration": 4.141,
        "text": "company a cloud company we've of course"
      },
      {
        "start": 6579.36,
        "duration": 3.9,
        "text": "got all kinds of operational data"
      },
      {
        "start": 6581.82,
        "duration": 3.54,
        "text": "um we've got data about the customers"
      },
      {
        "start": 6583.26,
        "duration": 4.14,
        "text": "the customers are communicating data"
      },
      {
        "start": 6585.36,
        "duration": 4.2,
        "text": "about their situation in the support"
      },
      {
        "start": 6587.4,
        "duration": 6.779,
        "text": "chat and so you really have to think"
      },
      {
        "start": 6589.56,
        "duration": 7.32,
        "text": "about you know isolating users"
      },
      {
        "start": 6594.179,
        "duration": 4.801,
        "text": "um and and even also you know putting"
      },
      {
        "start": 6596.88,
        "duration": 4.319,
        "text": "boundaries around around what type of"
      },
      {
        "start": 6598.98,
        "duration": 5.52,
        "text": "user information winds up getting into"
      },
      {
        "start": 6601.199,
        "duration": 5.46,
        "text": "the system so no Pai you know"
      },
      {
        "start": 6604.5,
        "duration": 3.54,
        "text": "anonymizing names"
      },
      {
        "start": 6606.659,
        "duration": 3.96,
        "text": "um you know you don't want to create"
      },
      {
        "start": 6608.04,
        "duration": 4.8,
        "text": "traces of individual users that can be"
      },
      {
        "start": 6610.619,
        "duration": 4.141,
        "text": "identified in the data set you're"
      },
      {
        "start": 6612.84,
        "duration": 4.08,
        "text": "building right yeah"
      },
      {
        "start": 6614.76,
        "duration": 3.899,
        "text": "um and you have to you know and and you"
      },
      {
        "start": 6616.92,
        "duration": 3.06,
        "text": "know one of the things I love about the"
      },
      {
        "start": 6618.659,
        "duration": 4.141,
        "text": "the system we're building is of course"
      },
      {
        "start": 6619.98,
        "duration": 4.62,
        "text": "you know uh uh Apache Cassandra data"
      },
      {
        "start": 6622.8,
        "duration": 4.98,
        "text": "Stacks asteroid can be used to give an"
      },
      {
        "start": 6624.6,
        "duration": 5.4,
        "text": "LM memory essentially but you need to"
      },
      {
        "start": 6627.78,
        "duration": 4.68,
        "text": "scope that context that you're using"
      },
      {
        "start": 6630.0,
        "duration": 4.38,
        "text": "that user's history"
      },
      {
        "start": 6632.46,
        "duration": 4.62,
        "text": "um and exclusively that user's history"
      },
      {
        "start": 6634.38,
        "duration": 4.259,
        "text": "to provide context"
      },
      {
        "start": 6637.08,
        "duration": 3.48,
        "text": "um and so I think the team did a pretty"
      },
      {
        "start": 6638.639,
        "duration": 4.681,
        "text": "thoughtful job about"
      },
      {
        "start": 6640.56,
        "duration": 5.7,
        "text": "building the the boundaries and boundary"
      },
      {
        "start": 6643.32,
        "duration": 6.06,
        "text": "conditions around all that yeah yeah for"
      },
      {
        "start": 6646.26,
        "duration": 5.58,
        "text": "sure and I know that there was quite a"
      },
      {
        "start": 6649.38,
        "duration": 3.96,
        "text": "bit of work around the sort of feature"
      },
      {
        "start": 6651.84,
        "duration": 4.92,
        "text": "engineering side of that right because"
      },
      {
        "start": 6653.34,
        "duration": 6.0,
        "text": "you're trying to summarize you know this"
      },
      {
        "start": 6656.76,
        "duration": 5.879,
        "text": "user's experience in history but also"
      },
      {
        "start": 6659.34,
        "duration": 5.04,
        "text": "contextualize it into what have people"
      },
      {
        "start": 6662.639,
        "duration": 4.441,
        "text": "at this stage of their journey journey"
      },
      {
        "start": 6664.38,
        "duration": 4.14,
        "text": "done yeah and so that's kind of a you"
      },
      {
        "start": 6667.08,
        "duration": 4.26,
        "text": "know like you're saying anonymization"
      },
      {
        "start": 6668.52,
        "duration": 4.619,
        "text": "but also engineering the right features"
      },
      {
        "start": 6671.34,
        "duration": 5.7,
        "text": "to provide context was incredibly"
      },
      {
        "start": 6673.139,
        "duration": 6.6,
        "text": "important yes right yes absolutely"
      },
      {
        "start": 6677.04,
        "duration": 4.679,
        "text": "um but for all the engineering we did as"
      },
      {
        "start": 6679.739,
        "duration": 3.661,
        "text": "you mentioned humans are still in the"
      },
      {
        "start": 6681.719,
        "duration": 3.541,
        "text": "loop in a couple different ways and"
      },
      {
        "start": 6683.4,
        "duration": 3.66,
        "text": "we've got I think both some automated"
      },
      {
        "start": 6685.26,
        "duration": 5.34,
        "text": "triggers"
      },
      {
        "start": 6687.06,
        "duration": 6.599,
        "text": "um and some human interventions uh in"
      },
      {
        "start": 6690.6,
        "duration": 3.84,
        "text": "terms of sentiments and escalation can"
      },
      {
        "start": 6693.659,
        "duration": 3.361,
        "text": "you"
      },
      {
        "start": 6694.44,
        "duration": 5.58,
        "text": "talk about that a little bit yeah yeah"
      },
      {
        "start": 6697.02,
        "duration": 5.219,
        "text": "so um part of what we knew like I said"
      },
      {
        "start": 6700.02,
        "duration": 5.159,
        "text": "before is that that Human Experience is"
      },
      {
        "start": 6702.239,
        "duration": 5.641,
        "text": "very important and we needed to be able"
      },
      {
        "start": 6705.179,
        "duration": 6.241,
        "text": "to know when was the right moment to"
      },
      {
        "start": 6707.88,
        "duration": 5.52,
        "text": "escalate to a human if a user has like"
      },
      {
        "start": 6711.42,
        "duration": 4.68,
        "text": "said something negative to the chatbot"
      },
      {
        "start": 6713.4,
        "duration": 4.14,
        "text": "no you know certainly sometimes users"
      },
      {
        "start": 6716.1,
        "duration": 3.72,
        "text": "are going to just play with a bot and"
      },
      {
        "start": 6717.54,
        "duration": 4.02,
        "text": "see like what's going on right"
      },
      {
        "start": 6719.82,
        "duration": 4.02,
        "text": "um but the difference between I think a"
      },
      {
        "start": 6721.56,
        "duration": 4.74,
        "text": "bot and an agent is an agent knows when"
      },
      {
        "start": 6723.84,
        "duration": 3.66,
        "text": "to escalate they've you know looked at"
      },
      {
        "start": 6726.3,
        "duration": 2.7,
        "text": "the sentiment of what's Happening"
      },
      {
        "start": 6727.5,
        "duration": 3.659,
        "text": "they've sort of seen a journey in there"
      },
      {
        "start": 6729.0,
        "duration": 4.8,
        "text": "and they're like I can't handle this"
      },
      {
        "start": 6731.159,
        "duration": 4.441,
        "text": "anymore right it's sort of this idea we"
      },
      {
        "start": 6733.8,
        "duration": 4.74,
        "text": "talked about where it's like an agent"
      },
      {
        "start": 6735.6,
        "duration": 4.8,
        "text": "can act on your behalf a lot but they"
      },
      {
        "start": 6738.54,
        "duration": 3.599,
        "text": "can't they can't do everything right"
      },
      {
        "start": 6740.4,
        "duration": 3.299,
        "text": "they they kind of know when to give it"
      },
      {
        "start": 6742.139,
        "duration": 3.6,
        "text": "back to you"
      },
      {
        "start": 6743.699,
        "duration": 4.98,
        "text": "um yes and we also had a little bit of"
      },
      {
        "start": 6745.739,
        "duration": 6.121,
        "text": "um uh intense detection in there as well"
      },
      {
        "start": 6748.679,
        "duration": 5.161,
        "text": "so if there was started to be questions"
      },
      {
        "start": 6751.86,
        "duration": 3.359,
        "text": "around not just like how do I use this"
      },
      {
        "start": 6753.84,
        "duration": 3.66,
        "text": "product how do I get unblocked but"
      },
      {
        "start": 6755.219,
        "duration": 4.141,
        "text": "around like pricing and how would I"
      },
      {
        "start": 6757.5,
        "duration": 4.38,
        "text": "actually put this into other things then"
      },
      {
        "start": 6759.36,
        "duration": 4.859,
        "text": "we would bring them to an expert in that"
      },
      {
        "start": 6761.88,
        "duration": 3.9,
        "text": "area because that's not what this agent"
      },
      {
        "start": 6764.219,
        "duration": 5.821,
        "text": "was trained on right they were trained"
      },
      {
        "start": 6765.78,
        "duration": 6.12,
        "text": "on how to assist not how to sell so yeah"
      },
      {
        "start": 6770.04,
        "duration": 3.24,
        "text": "other kinds of triggers right so that"
      },
      {
        "start": 6771.9,
        "duration": 3.36,
        "text": "and that that's actually a really"
      },
      {
        "start": 6773.28,
        "duration": 3.72,
        "text": "wonderful example about respecting user"
      },
      {
        "start": 6775.26,
        "duration": 4.26,
        "text": "intent which is we're not actually"
      },
      {
        "start": 6777.0,
        "duration": 4.56,
        "text": "trying to use this interaction to"
      },
      {
        "start": 6779.52,
        "duration": 4.38,
        "text": "persuade someone who's not here to buy"
      },
      {
        "start": 6781.56,
        "duration": 3.72,
        "text": "to buy but if they wind up coming"
      },
      {
        "start": 6783.9,
        "duration": 3.66,
        "text": "through this Channel and that's what"
      },
      {
        "start": 6785.28,
        "duration": 3.959,
        "text": "they're trying to do right we will"
      },
      {
        "start": 6787.56,
        "duration": 3.599,
        "text": "quickly connect them to the right"
      },
      {
        "start": 6789.239,
        "duration": 3.721,
        "text": "resource which I think is a great"
      },
      {
        "start": 6791.159,
        "duration": 4.98,
        "text": "example of you know the opposite of"
      },
      {
        "start": 6792.96,
        "duration": 4.62,
        "text": "manipulative it's respecting and"
      },
      {
        "start": 6796.139,
        "duration": 2.881,
        "text": "detecting if they're in the wrong place"
      },
      {
        "start": 6797.58,
        "duration": 4.139,
        "text": "and helping them out"
      },
      {
        "start": 6799.02,
        "duration": 4.38,
        "text": "right it you know in any business if"
      },
      {
        "start": 6801.719,
        "duration": 2.881,
        "text": "you're trying to manipulate someone to"
      },
      {
        "start": 6803.4,
        "duration": 2.759,
        "text": "buy some"
      },
      {
        "start": 6804.6,
        "duration": 3.48,
        "text": "essentially something they need like"
      },
      {
        "start": 6806.159,
        "duration": 3.0,
        "text": "you're going to lose that business you"
      },
      {
        "start": 6808.08,
        "duration": 4.079,
        "text": "don't want that kind of negative"
      },
      {
        "start": 6809.159,
        "duration": 5.121,
        "text": "experience so consider consider not"
      },
      {
        "start": 6812.159,
        "duration": 5.341,
        "text": "doing that um yes"
      },
      {
        "start": 6814.28,
        "duration": 5.26,
        "text": "so so hallucinations we hear a lot about"
      },
      {
        "start": 6817.5,
        "duration": 3.659,
        "text": "them everyone's afraid of them it's a"
      },
      {
        "start": 6819.54,
        "duration": 3.179,
        "text": "really hard problem I know we're always"
      },
      {
        "start": 6821.159,
        "duration": 5.121,
        "text": "working on it"
      },
      {
        "start": 6822.719,
        "duration": 3.561,
        "text": "um how have we been handling it today"
      },
      {
        "start": 6826.32,
        "duration": 4.859,
        "text": "um I think there's you know a lot of you"
      },
      {
        "start": 6829.32,
        "duration": 3.419,
        "text": "know prompt engineering and prompt"
      },
      {
        "start": 6831.179,
        "duration": 3.121,
        "text": "testing"
      },
      {
        "start": 6832.739,
        "duration": 3.361,
        "text": "um and I think there's also the humans"
      },
      {
        "start": 6834.3,
        "duration": 5.16,
        "text": "come into play right because we are"
      },
      {
        "start": 6836.1,
        "duration": 6.3,
        "text": "actually devoting human time and effort"
      },
      {
        "start": 6839.46,
        "duration": 4.14,
        "text": "to reviewing and tagging hallucinations"
      },
      {
        "start": 6842.4,
        "duration": 2.16,
        "text": "right"
      },
      {
        "start": 6843.6,
        "duration": 3.3,
        "text": "yeah"
      },
      {
        "start": 6844.56,
        "duration": 4.32,
        "text": "yeah I think there's a whole team in the"
      },
      {
        "start": 6846.9,
        "duration": 4.08,
        "text": "beginning who was doing this and now we"
      },
      {
        "start": 6848.88,
        "duration": 4.38,
        "text": "can sort of escalate the more likely"
      },
      {
        "start": 6850.98,
        "duration": 6.0,
        "text": "hallucinations but yeah it's such a hard"
      },
      {
        "start": 6853.26,
        "duration": 5.34,
        "text": "problem and I'm sure you know the"
      },
      {
        "start": 6856.98,
        "duration": 4.44,
        "text": "industry is going to find better and"
      },
      {
        "start": 6858.6,
        "duration": 5.34,
        "text": "better ways to do this but the the point"
      },
      {
        "start": 6861.42,
        "duration": 5.279,
        "text": "of generative AI is to create new"
      },
      {
        "start": 6863.94,
        "duration": 5.94,
        "text": "content and so you have to be pretty"
      },
      {
        "start": 6866.699,
        "duration": 6.781,
        "text": "careful about what is"
      },
      {
        "start": 6869.88,
        "duration": 5.64,
        "text": "real what's actually you know coming"
      },
      {
        "start": 6873.48,
        "duration": 4.62,
        "text": "from your support docs versus something"
      },
      {
        "start": 6875.52,
        "duration": 4.199,
        "text": "that's like it'd be cool if we had this"
      },
      {
        "start": 6878.1,
        "duration": 3.599,
        "text": "but we don't actually have that feature"
      },
      {
        "start": 6879.719,
        "duration": 4.621,
        "text": "today please don't tell in users about"
      },
      {
        "start": 6881.699,
        "duration": 4.701,
        "text": "it right right right you know this this"
      },
      {
        "start": 6884.34,
        "duration": 4.379,
        "text": "thing ties back to you know yes"
      },
      {
        "start": 6886.4,
        "duration": 4.92,
        "text": "hallucinations are at risk of learning"
      },
      {
        "start": 6888.719,
        "duration": 5.581,
        "text": "by doing but by learning by doing"
      },
      {
        "start": 6891.32,
        "duration": 5.319,
        "text": "responsibly and diligently you start to"
      },
      {
        "start": 6894.3,
        "duration": 4.859,
        "text": "understand the interaction of you know"
      },
      {
        "start": 6896.639,
        "duration": 4.741,
        "text": "your particular data and their domain"
      },
      {
        "start": 6899.159,
        "duration": 4.56,
        "text": "and what types of hallucinations get"
      },
      {
        "start": 6901.38,
        "duration": 5.16,
        "text": "produced and how you can detect and"
      },
      {
        "start": 6903.719,
        "duration": 4.681,
        "text": "prevent them and so you know as generic"
      },
      {
        "start": 6906.54,
        "duration": 4.199,
        "text": "tools you know general purpose tools"
      },
      {
        "start": 6908.4,
        "duration": 4.98,
        "text": "evolve in the ecosystem you'll also have"
      },
      {
        "start": 6910.739,
        "duration": 3.541,
        "text": "a leg up on kind of understanding you"
      },
      {
        "start": 6913.38,
        "duration": 3.6,
        "text": "know"
      },
      {
        "start": 6914.28,
        "duration": 3.859,
        "text": "how do hallucinations surface in our"
      },
      {
        "start": 6916.98,
        "duration": 3.78,
        "text": "context"
      },
      {
        "start": 6918.139,
        "duration": 4.08,
        "text": "and that's better to have done that by"
      },
      {
        "start": 6920.76,
        "duration": 4.439,
        "text": "learning by doing"
      },
      {
        "start": 6922.219,
        "duration": 5.861,
        "text": "and including investing in some human"
      },
      {
        "start": 6925.199,
        "duration": 5.281,
        "text": "you know oversight time than to start"
      },
      {
        "start": 6928.08,
        "duration": 3.3,
        "text": "out from from Square zero you know down"
      },
      {
        "start": 6930.48,
        "duration": 3.179,
        "text": "the road"
      },
      {
        "start": 6931.38,
        "duration": 5.7,
        "text": "yeah yeah"
      },
      {
        "start": 6933.659,
        "duration": 5.341,
        "text": "well um I know that we're about"
      },
      {
        "start": 6937.08,
        "duration": 3.78,
        "text": "points we were going to talk about"
      },
      {
        "start": 6939.0,
        "duration": 4.76,
        "text": "response format and relevancy and"
      },
      {
        "start": 6940.86,
        "duration": 4.92,
        "text": "testing and metrics maybe we do that in"
      },
      {
        "start": 6943.76,
        "duration": 3.34,
        "text": "shorter than what we wanted to because"
      },
      {
        "start": 6945.78,
        "duration": 4.98,
        "text": "we got a little deep in some of these"
      },
      {
        "start": 6947.1,
        "duration": 7.74,
        "text": "things I'll just like give a few points"
      },
      {
        "start": 6950.76,
        "duration": 7.2,
        "text": "um so you don't just need to get an out"
      },
      {
        "start": 6954.84,
        "duration": 5.46,
        "text": "of the box right we knew that we had to"
      },
      {
        "start": 6957.96,
        "duration": 5.64,
        "text": "tune various things like temperature"
      },
      {
        "start": 6960.3,
        "duration": 4.319,
        "text": "like the degree of Randomness versus a"
      },
      {
        "start": 6963.6,
        "duration": 3.66,
        "text": "varied"
      },
      {
        "start": 6964.619,
        "duration": 4.981,
        "text": "um an unexpected response and mistakes"
      },
      {
        "start": 6967.26,
        "duration": 3.899,
        "text": "and nonsense Etc like you can tune these"
      },
      {
        "start": 6969.6,
        "duration": 3.18,
        "text": "different settings between zero and one"
      },
      {
        "start": 6971.159,
        "duration": 4.381,
        "text": "there were other things that we had to"
      },
      {
        "start": 6972.78,
        "duration": 4.68,
        "text": "like EK values P values different prompt"
      },
      {
        "start": 6975.54,
        "duration": 3.659,
        "text": "settings that controlled the dispersion"
      },
      {
        "start": 6977.46,
        "duration": 3.179,
        "text": "in the content and the creativity of the"
      },
      {
        "start": 6979.199,
        "duration": 4.201,
        "text": "answers"
      },
      {
        "start": 6980.639,
        "duration": 4.5,
        "text": "um and so when you tell a prompt like"
      },
      {
        "start": 6983.4,
        "duration": 3.54,
        "text": "you should only answer a customer"
      },
      {
        "start": 6985.139,
        "duration": 3.6,
        "text": "questions about Cassandra data stacks"
      },
      {
        "start": 6986.94,
        "duration": 3.84,
        "text": "and its products that means that the"
      },
      {
        "start": 6988.739,
        "duration": 3.361,
        "text": "user isn't going to be able to ask the"
      },
      {
        "start": 6990.78,
        "duration": 3.0,
        "text": "irrelevant questions that are not"
      },
      {
        "start": 6992.1,
        "duration": 3.48,
        "text": "pertaining to what this agent's"
      },
      {
        "start": 6993.78,
        "duration": 2.939,
        "text": "expertise is and so we definitely"
      },
      {
        "start": 6995.58,
        "duration": 2.579,
        "text": "consider putting some of those"
      },
      {
        "start": 6996.719,
        "duration": 2.641,
        "text": "guardrails on"
      },
      {
        "start": 6998.159,
        "duration": 3.301,
        "text": "um and I know"
      },
      {
        "start": 6999.36,
        "duration": 4.02,
        "text": "um it this is going to just continue as"
      },
      {
        "start": 7001.46,
        "duration": 5.6,
        "text": "we start to make these specialized"
      },
      {
        "start": 7003.38,
        "duration": 3.68,
        "text": "agents across our product experience"
      },
      {
        "start": 7009.46,
        "duration": 4.6,
        "text": "yeah without any last remarks Brian"
      },
      {
        "start": 7012.08,
        "duration": 4.5,
        "text": "before we let everyone go"
      },
      {
        "start": 7014.06,
        "duration": 4.38,
        "text": "uh no it's just been a pleasure uh"
      },
      {
        "start": 7016.58,
        "duration": 3.599,
        "text": "talking to you and I hope folks take"
      },
      {
        "start": 7018.44,
        "duration": 4.199,
        "text": "away that you know"
      },
      {
        "start": 7020.179,
        "duration": 4.741,
        "text": "there's work to be done there's caution"
      },
      {
        "start": 7022.639,
        "duration": 4.56,
        "text": "that you need to be take uh but it is"
      },
      {
        "start": 7024.92,
        "duration": 4.739,
        "text": "doable right and you can start to think"
      },
      {
        "start": 7027.199,
        "duration": 6.241,
        "text": "about use cases and agents"
      },
      {
        "start": 7029.659,
        "duration": 5.52,
        "text": "um and again learn by doing documenting"
      },
      {
        "start": 7033.44,
        "duration": 4.98,
        "text": "um and the doing being the shepherding"
      },
      {
        "start": 7035.179,
        "duration": 5.221,
        "text": "and management after the fact as well"
      },
      {
        "start": 7038.42,
        "duration": 3.12,
        "text": "well thanks so much we'll see you guys"
      },
      {
        "start": 7040.4,
        "duration": 4.4,
        "text": "next time"
      },
      {
        "start": 7041.54,
        "duration": 3.26,
        "text": "Take Care thank you"
      },
      {
        "start": 7046.77,
        "duration": 4.969,
        "text": "[Music]"
      },
      {
        "start": 7049.52,
        "duration": 3.9,
        "text": "and that's around thanks for joining us"
      },
      {
        "start": 7051.739,
        "duration": 3.241,
        "text": "for these sessions we hope that you've"
      },
      {
        "start": 7053.42,
        "duration": 3.0,
        "text": "seen something here today that speeds up"
      },
      {
        "start": 7054.98,
        "duration": 3.48,
        "text": "your journey into generative AI"
      },
      {
        "start": 7056.42,
        "duration": 4.14,
        "text": "architecture and development I think the"
      },
      {
        "start": 7058.46,
        "duration": 4.199,
        "text": "big takeaway from today is the next wave"
      },
      {
        "start": 7060.56,
        "duration": 4.2,
        "text": "of generative AI applications are being"
      },
      {
        "start": 7062.659,
        "duration": 5.04,
        "text": "built today and the time to start"
      },
      {
        "start": 7064.76,
        "duration": 4.919,
        "text": "exploring agent architecture is now it's"
      },
      {
        "start": 7067.699,
        "duration": 3.601,
        "text": "an incredibly exciting time and you can"
      },
      {
        "start": 7069.679,
        "duration": 4.621,
        "text": "expect us to share more Deep dive"
      },
      {
        "start": 7071.3,
        "duration": 5.22,
        "text": "content demos and code with you over the"
      },
      {
        "start": 7074.3,
        "duration": 4.08,
        "text": "following months and you can start"
      },
      {
        "start": 7076.52,
        "duration": 5.119,
        "text": "building agents affect your search on"
      },
      {
        "start": 7078.38,
        "duration": 6.18,
        "text": "asker DB now just go to"
      },
      {
        "start": 7081.639,
        "duration": 4.841,
        "text": "askra.datastacks.com it's free we want"
      },
      {
        "start": 7084.56,
        "duration": 5.48,
        "text": "to hear from you find us on social email"
      },
      {
        "start": 7086.48,
        "duration": 3.56,
        "text": "us let us know what you're building"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T17:45:42.348102+00:00"
}