{
  "video_id": "ZRX-ROpmeSo",
  "title": "Mastering Apache Cassandra®: Make 2025 Your Most Successful Yet",
  "description": "Get ready for an insightful fireside chat with Cassandra pros Francesco Animali, Romain Anselin, and Mick Semb Wever! With years of hands-on experience running Apache Cassandra clusters, these experts will share their favorite tips, lessons learned, and practical advice for making the most out of Cassandra. Plus, we’ll dive into what’s new with Cassandra 5.0 and what’s coming in 2025.\n\nWhether you’re managing a cluster, planning to upgrade, or just curious about how Cassandra is evolving, this session is packed with valuable insights to help you stay ahead.\n\nDuring this session, you’ll learn:\n\n--Proven tips for managing clusters: Learn expert strategies to optimize performance and scalability.\n--What’s next for Cassandra: Get a peek into 2025 trends.\n--Real-world problem-solving: Hear about challenges and solutions from the experts’ personal experiences.\n--Future-ready Cassandra use cases: See how Cassandra is powering real-time, AI, and cloud-native applications.\n\n\nAdditional Resources:\n- DataStax Developer Hub: https://datastax.com/devs\n- DataStax Blog: https://www.datastax.com/blog/technical-how-tos\n- Langflow: https://langflow.datastax.com\n- Astra DB: https://astra.datastax.com\n____________________\n\nStay in touch:\n- Join our Discord Community: https://discord.gg/datastax\n- Follow us on X: https://x.com/DataStaxDevs\n\nChapters:\n00:00:00 Introduction\n_________",
  "published_at": "2024-12-06T05:20:11Z",
  "thumbnail": "https://i.ytimg.com/vi/ZRX-ROpmeSo/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "datastax",
    "apache_cassandra",
    "astra",
    "introduction",
    "performance"
  ],
  "url": "https://www.youtube.com/watch?v=ZRX-ROpmeSo",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "okay I believe we're live hello everyone thank you for joining us today um I'm I'm sure there's some recognizable names in the attendees lists and uh if if if we know each other already you watching this afterwards uh after the event um a warm welcome to you so today we've got a webinar on uh uh some typical problems in Cassandra that many of you have faced before and I've got two of my favorite colleagues joining me today Francesco and Roman uh who are both from support but they've been around for a long time and I've worked with them on a number of tickets with customers and always being thoroughly surprised inspired they've taught me some things along the way um so uh when I was asked a couple of weeks ago or maybe a month ago now if I could do a webinar my first thought was hey let's get together these two people and let's talk about problems um knowing that this type of informational uh this type of information is highly valuable for all of you some of the stuff that we'll be talking about today uh is problems with petitions problems with repairs how to set up discs the choice between jbods and lbms how to do upgrades well how to set up your system uh for the the Linux distribution properly and different tooling and operational methodologies also a quick touch onto what's coming in Cassandra uh both what's exciting uh we'll open up new use cases for you uh but also what can help make life easier as an operator for you and uh one of those things will be Vector search this is very hot at the moment so uh we wouldn't do uh justice if we didn't touch on that lightly and at the end there'll be an ask ask us anything section before we jump into that uh we've got a couple of introduction slides to go through so first off a bit more of introduction of the three of us I can go first um so I've been involved in Cassandra going back to 200 11 I wasn't interested in databases before Cassandra Cassandra was a technology that came along and in the way that it Sol distributed problems it felt more akin to how we were solved solving problems in a distributed space everywhere else in our industry that opened up a lot of doors for me uh the rest was history since then I've become a Cassandra committer and I'm now on the PMC and I've had many roles uh both as consultant and services as an engineer and ending product and back as an engineer now uh helping build the DSC and hcd products and thorough enjoying it a little bit personal about myself I am an Australian uh I'm met a Norwegian wife along the way uh moved to Norway on and off have been living in Norway for 20 years and one of the real blessings in life is the opportunity to live in Europe I absolutely love working with the diversity that's available here um and I'm hoping that that that Roman and CCO can also share a little bit about uh their backgrounds and and and what they what they bring to the table because I think working in teams working with different cultures it's it's it is for me one of the joys in life Francesco would you like to go next yes hi hi everybody my name is Francesa Nali I I'm Italian I I live currently live in Italy I've been living in in UK for about 15 years up to 2020 I uh I have known uh databases from a long time and I've been working on pretty much all the database types starting from uh MySQL postgress and uh Oracle db2 a lot for many years then I in 26 16 I I I I I fell in love with Cassandra and in in particular with the distributed databases that the technology that is behind that is it really opened my my eyes and uh because it's such such a difference there's such a dramatic difference between the relational databases and non relational distributed database like candra um so I I I love Cassandra been working with Cassandra for the past eight years plus enjoying the technical side of it enjoying working with customers of uh with with many different projects use cases uh it it really never uh uh never stops surprising and never stops interesting my and and and teasing my brain everything you we do in in support is about learning uh yet a new thing it seems uh kind of weird but in in eight years I can say that I pretty much learned something every day from my colleagues from customers from from the the engineering team anything so that's the very interesting part of my my job my my job in support uh it is also always you you need to work in teams you need to work in in in a group you need to exchange constantly your ideas with ideas of others so yeah I hope that we will uh bring some nice discussion about uh of things so I'm really looking forward hearing questions and I couldn't agree with you more franccesco I think it's uh learning new things every day it's a technology that you don't hit the bottom uh you you always can keep digging and and and learn your stuff and having a good group of people around you to smile with is is a wonderful thing for us Roman hello everyone uh my name is Roman uh I'm French I've been living in the UK for 18 years now uh been in the IT industry for a bit longer than that uh worked for 11 years in the business analytics area uh business intelligence more exactly um and I joined the data stacks and Cassandra community in 2018 so it's been six years and a half I've been working on on the technology bouncing on what Francesco and mck said I remember when I first started to learn about Cassandra um that I found it interesting to have to ditch all my existing Concepts about rdbms to actually work on no SQL database I remember this vividly shocked at first um fun fact about me I'm uh I I used to do uh cliff diving um but I I got wiser over the years and I don't think I'm at an age where I should be jumping or back flipping from 10 meters high CLI oh wow yeah that sounds awesome keep keep [Applause] going yes that's awesome we'll do it together one day how's that I'll get you back into it anytime okay so before we keep going um some house rules for us uh through the presentations there's going to be some links that pop up uh these are quter actions um or uh further reading on some of the things that we are talking about so if you see them uh pop up uh we recommend that you you click on them um save them read them later uh We've we've carefully selected them for you this whole webinar it's going to be recorded So and emails will be sent out afterwards uh so you can you can watch it again you can uh um you can pass it to your friends and colleagues Etc in this SQL uh um uh app this this this webinar page we do have a few tabs on the right hand side we've got chat and we've got Q&A uh if you've got questions throw them into Q&A we're going to hold some time uh off at the end and answer your all your questions if there is something which is pertinent in the moment uh feel free to also write something in in the chat um well we'll pick up on that as we go okay so jumping into the content uh this is how we're going to lay it out we're going to go through some of these uh hot topics for operators first and then wrap up with uh what's coming and what's uh what's hot in the use case and the development so first up we've got hot and wide and large petitions uh I think we've we've all done a lot of time on this one haven't we so before I jump into this uh remind I'm G to ask you uh uh a an initial question uh we often like people often just talk about large petitions and when we had when we were discussing this before we we laid this out between hot petitions and wide petitions and large petitions do you want to quickly give us the difference between them so the let let's start first with what is a hot partition the the way the way candre is design is to allow you to do a replication factor of three uh for your data and your driver will come with uh an agreement as to the consistency level it needs to reach to uh to receive its response from the database so in a local Quorum scenario you will ask two of the nodes to retrieve the data now the H partition is basically when you're always eating the same partition hence the name and that means that that in the way gason R is designed you will always be hitting the same notes so the a hot partition is something that however you scale your cluster will never be sorted by any scaling because the the way you request a partition key will hit a specific token and that token is always on the same three nodes so that's what defines a hot partition and so it doesn't matter if you have uh three nodes 10 nodes 100 nodes 300 nodes or more hot partition will always give you the that problem of uh creating a a funnel to a specific set of your environment white partitions is different in that it's the way uh you create your data in your partition key and so if you create a a partition key or no a primary key which contains let's say one column as the partition key and one column as the clustering column you can have multiple roles coming in your environment coming in that partition and the problem you get is when this the number of rols within a partition get out of control which means we say from a conservative perspective uh that a partition should avoid being above 100 Meg so from a support perspective we know it's a conservative value but that's because we know that when you start to see this you will end up potentially seeing these particular partitions grow bigger and bigger and the challenges around the white partitions is that they will create pressure on your system if they grow totally out of control they will o your system uh and so that's so that's the definition of the hot and the white partition hot partition is always eating the same node white partition white partitions will also give pressure on some systems but in a different manner one is just constantly asking the same data to the same node and the other is uh due to the blotting of the amount of data you request from the nodes and large large petitions uh which can also be large rows yeah they're in fact something different so a large row is um is when you try to push too much data within one specific row so typically uh using blobs and trying to push a full document within uh a push when you try to push a full document a full binary within the database so that's again something different um we do see uh one of one of the red flags for this is when you start to see the commit lock segment size increased in the environment this is not the only uh giveaway but that's generally a good indication that you're trying to push too much uh within the within the rules of your database um the issue is go ahead there are also some alerts in Cassandra right that prevent you to like you said the the mid log when when cander detects that you are pushing a mutation that is more than half the size of the commit log it will stop you right yeah a right which is a right which is half the size of a commit loog segment size will be uh will be refused by the database so that's why I said the commit loog segment size is one example where this can be some form of red flag there's another thing take that's why you should increase size of the commit logs uh well no well you should reduce the size you should reduce the size of your Ro I'm tricking you I'm tricking you no of course not yes so the the problem Associated to the sorry the problem Associated to the large roles and partition actually is inherent to the jvm your uh Young Generation is actually split in regions uh 1,24 to 248 of them and the problem you have is that when you're querying that data you generate what you call an Among Us humongous allocation which means that this data does not go through your Young Generation and will actually go straight in the old gen of the jvm and by doing that what this creates if you have a lot of large rows it means that your cluster is going to start doing stop the worlds GCS what we call full GCS and that's and and that really becomes a problem in your environment the advice for this being to actually take the documents and store them outside of the database in that case or to store your blob within a storage U that you link within the repository you have in your castom environment and there are some settings that can help uh protect against this now in newer versions you can you can set the max cell size you can there's a few things you can do isn't there yeah the guard rails which have been implemented is that what you're referring to Mick yeah yeah yeah um so so going back to hot petitions you did kind of mention that what's some uh what's some typical ways that this is obvious like when when when support tickets come in and The Operators are like the cluster keeps on falling over I don't know what it is and you see something and you're like I know exactly what it is so I I kind of gave the hint by explaining the way uh this reflects so because you're always pinning your work Cloud on the same nodes those nodes will have a totally different uh CPU usage compared to the others in general uh if 90% of your traffic is itting the same partition uh those nodes are going to exhibit way higher CPU level so what you will find in that case is three nodes uh or depending on your replication factor and your consistency level uh those nodes will actually be a lot hotter in term of CPU usage [Music] and and one way to look into it further is for example using the note Tool top partitions tool which is not one we use on a daily basis but uh when you start seeing that kind of significant CPU workload difference between your environment that's generally a good one to start uh to start looking at to sample your environment and find a bit the structure of the reads and the writes on on specific tables okay and and my last question before we jump on to the next slide is why partitions this one can be a little bit more tricky sometimes uh because sometimes you have for example time series uh data models like the idea of having a million rows in a petition sometimes that is legitimate so sometimes it can be a a a initially poorly designed data model or it may have been a data model that was originally correct and then something a change has happened along the way and that breaks it um what are the ways that you see it and and what are the ways that we can fix it so one so one of the thing I mean uh one of the thing that's noted here is that we have a tool such as not tool CF stats would allow you to see potentially the largest partition in your own vment however this can be misleading the I've recently encountered that where looking into a hip dump I realized that I had a a specific partition being called which was more than 4 gig in memory the it was a small castra cluster there was not a lot of Heap allocated to it and that for G query o and the environment and when I read in the Heap dump realized that it's a single partition rate so I'm not talking about a wrench query or anything it was really a single a single one however when I looked into the CF stats I realized that you have only one gig uh shown here so I think one of the key takeway I want to give here is that the CF stats give you only the largest partition Within an SS table uh across all the SS tables it doesn't give you uh this across all the different SS table so the there's no aggregation of the SS table metadata in the CF stats showing you that so what you have to understand is that when we tell you we see a 100 Meg partition if that partition exist across multiple SS table in your environment it can be actually quite significantly bigger a way to identify this is looking into the logs uh if you see the white partitions are notified and depending on the versions you use some of them will actually even tell you which partition key within a table is actually giving you trouble this is not uh always working because I just mentioned that the partition can be uh can be spread across multiple SS tabl so in a Time series this wouldn't occur uh but in yeah in stcs or in stcs you could end up with that LC LCS you would obviously see it based on the way LCS is designed on competion it should really uh flag it up I'm going to jump on I'm going to move on uh just with an eye on time um but I do want to say that that the both the you know that that that carier with the CF stats that was something that that you taught me uh when you mentioned it I was like really uh and and I thought about it I'm like yeah that I like from code I that makes sense I'm like and that's kind of embarrassing why why haven't we fixed that yet um that's that's the way things are so thank you I I had the same uh I had the same as you when I was looking at that him DP with four gig of data on a single R partition and I was looking at my CM stat on the other side and I had only one gig showing in the CF stat I I did have to go and dig uh and discuss around to understand where that was coming from so yeah he he come to surpris as you as it came to me one of those little hidden gems okay moving on to repairs uh Francesco you're our expert here um do you w to uh do you w to tell me about uh the problems that we hit with large petitions with tombstones with v noes with tables with dense nodes at first line um you can you can introduce repairs too but I think we can jump into yes you can go so repair repair is a is an amazing background service that it's a it's a misnomer because it shouldn't be called repair because repair gives a a wrong idea about what it what it is it's not it's not repairing anything is it's ensuring that data is consistent in addition bued environment like capja um cap and all the all the rest we are trading immediate consistency for for Speed and availability so in Canda we want the database to be extremely fast and we've got some amazing examples or very recent example of our customers using big cundra cluster reaching levels of four four Millions request per second 4.5 million request per second consistently so K is super fast when it comes to to speed is super fast when it comes to availability it is available always available but when it comes to consistency it will not be immediately consistent so that's what repair is repair is a is a is a background process that will ensure consistency what what it does is it will go Partition by partition node by node and will the nodes what's your what's your version of the of this piece of data what's your version what's your and then it will put them together and stream and and compare the data of one node and the other no and all the three replicas and then stream the data across so now it's a very a crucial service but it does a lot of work and it's got it has got some impact and is very sensitive yeah we often say customers uh um we often see customers either not doing repairs at all um or they come in and they do repairs they don't quite understand the the nuances what the mechanism is is really about and so they're just trying to repair everything in a kind of uh without tuning it and they can't get the repairs done in a week as you exactly what you say it's it's a heavy background process that's true we we've seen everything in in support about about repair and repair has been there for since the beginning of Cassandra so and in a way it hasn't evolved a lot in the past up until version four Invasion for the clear improvements we will talk about them later so repair is very sensitive to different things it's very sensitive to the amount of vodes you're using the amount of the large partitions that we were talking about before it's it's very sensitive to the total amount of data um in the cluster on the nodes so the old what what they were called anti patterns Cassandra anti patterns they are very significant they are very impactful for repair and repair needs to run quickly it needs to be consistent it needs to be predictable so it's very important because otherwise there there are other issues in in the data so the data will not be consistent eventually so if repair needs to be guaranteed a nice and easy life but there are many different things that so V nodes a cluster a Cassandra cluster with a very high number of vodes will will make sub range repair generate an amazing amount of tasks and sub ranges that eventually it can become a problem uh large partitions tombstones are all kind of roadblocks for repair because they they are they fall in the read path and and repair also uses the read path So reading a a very large partition with a lot of De and Deb partition it will will slow down repair tombstones will slow down repair um uh a lot of data will make repair go above again slow down repair and go above the threshold of the the very famous a very important GC grace period GC Grace second so repair is a very tricky therapist very crucial and the the user needs to be to to monitor that needs to ensure that repair has got an easy life within the time of GC gra seconds so and so with uh what changed in Cassandra 4 there's a few things that changed in Cassandra 4 and uh we're seeing people kind of having to learn new things and trip up yes uh on newer versions of Cassandra yes pentra 4 brings back an old an old uh um kind of an old project but an old repair repair was um was born for to to repair the the entire amount of dat in the cluster and initially in version two and version three incremental repair was brought in and it was set of default as well now increment so incremental repair as opposed to uh range or sub range repair will try will different will Mark SS tabl will Mark partitions as repair SS table as repaired so when you run incremental repair it will break down the assess tables will anti- compact the asss tables repair a number of partition and the number of ranges and create two different SS tables one with repaired data and one with unrepaired data and mark the repair data with a repaired app property in the in their yeah so the is good yeah I think one of the challenges that people have with it is you know how these two what what you said you have these two sets of of ss tables now from through through the lens of repair and unrepaired and a repaired and uh so what repairs are actually doing is they're doing twice as much work um and they be a a a amplification that's happening there but the incremental approach means that you can kind of just do the Deltas so the idea is that you even if the cost is double you chunking it down into smaller sizes and not having to worry about older stuff the idea works well it doesn't always work in practice it depends on a few things so understanding that is is important um yes Francisco yeah if I may in a in a in an Ideal World you wouldn't need repair but the the the way Cassandra works is try to give you just High availability and by doing so will sometimes allow a right to not make it to let's say the third node in your environment because you requested a right at local Quorum it would normally do this so the three nodes would receive the rights uh even if you request a local Quorum so in an Ideal World if your cluster is always healthy there will not be a need to run repair but the issue is it it will happen at one point or another that there's for a reason a system that goes down some nodes which are actually not responding in time because we mentioned some scenarios where this can happen and and when that happen you want to make sure the data gets resynchronized across the across the board and that's where so I am with that it should be called resynchronization rather than repair it's help with read latency because normally you would you want to uh uh you inforce uh consistency at request level anyway so it it could be right right level consistency or read level consisten so the data should be fairly fairly consistent already but this depends really on the type of the use case for very fast rise very mean repair is always there for for ensuring a consistency in the end but yes uh like you were saying Mick um incremental repair was a great invention because it it allows to to do repair only um Deltas or increment increments but yes the implementation then um didn't didn't provide the the expected uh performance in particular because um with um there there's a risk of some some SS Stables not being included into the repair uh if they uh happen if the the streaming happens after the medle tree has been built so there are some some edge cases some scenarios where you can in fact uh remain with some data that is not completely repaired uh the version four Cassandra 4 has has completely completely reworked the incremental repair fixed and fixed a lot of those bugs yes fixed a lot of bugs and changed the way the Merle tree and the time the tree is created um in this way it would the the um because the antic compaction was was also a major uh overhead for incremental repair compacting an anti- compacting all the times and for low systems systems with L discs that would already be uh impactful so um in this way uh anti compaction in Cassandra 4 anti compaction is done before the meal tree is created so in this way the Merle tree will contain the data from all the assess tables that are not prepared and then the m is exchanged and compar amongst nodes and eventually the streaming happens so candra 4 brings back incremental repair it's the default and it has completely changed with respect to version three so it's probably the best way to to do um repair now and also the entire just to finish the entire repair task so the each repair task is enclosed is wrapped into a a paxos transaction paxos also has been completely reworked version four we're not going to talk about that because it's it's a big thing but the entire repair task is a transaction now so it will if it doesn't complete completely it will fail it will roll back it's a it's it's able to roll back so yeah increment repair is is pretty good I think we're g to keep moving uh we do got some questions Landing U one of the questions uh is about repairs but we'll tackle that at the end and uh there's another question about uh oltp uh and olap systems uh that will touch on some of this as well so we see the questions we we'll get we'll get to you uh jumping on to the next topic jbod and lvm I'm really happy Roman that you brought this one up because uh it's also one of my favorite uh topics having worked at the consultant the last pickle for five years um you know jbod was one of those things we just wanted to throw throw out the window um so so tell us a bit more about this um yeah so the thing is that jbud uh JB look looks great on the paper when you when you first look at it and and I've I've had my share of uh my share of trouble with it as well um so looks great on this just just quickly describe because it may not everyone or Cassandra operators may not understand what we mean by of course of course JB but just a bunch of dis uh is actually a simple way to set up your environment and say I have six discs and I'm going to create six directories and in the data file directories of gas and Dro you just set up those six discs to be used by your environment okay so so the Cassandra configuration the yam is actually explicitly pointing to each separate dis absolutely each volume is separately notified and so you can you can just when you add a new dis put a put the seven pass in there so on paper it looks easy to administer uh but the reality of it is quite more complex than that first of all jbod act as a kind of subvod so it it will split your ranges into specific discs so you end up multiplying that there's another uh nasty effect I've I've realized over the course of the years when it comes to it is I mentioned the file descriptors so the thing is that when you normally write let's say uh your data into your environment you would write you would flush one ss table except that when you introduce jbod uh I me six discs uh so if you have six discs at that point you will flush six SS tables instead and so you're multiplying the number of file descriptors in your environment by the number of jbot discs you have uh you will hit the no files limit of your kernel if you haven't implemented the best practices the default kernel no files is generally at 100,000 rest assured that if you write a lot in your environment you will blow that number up at some point with a jbod implementation because as I said it acts as a direct multiplicator on the number of five descriptors you have in to connect to the SS tables because Cassandra has uh will have a f descriptors open for every SS table in your data directory so this is this is quite often there's the breaking point cluster crash because or a node crash because the no files limit has been hit so that we we'll push that as possible of the best practices information it's already documented in a fair few places but something to keep in mind um the other is quite it is quite tricky I'm going to ask you to to to to keep moving yeah absolutely so so why lvm and and how do we do it um so lvm so logical volume management is one of the Linux solution uh which allows you to group to bundle your discs together so you just create one partition which actually spreads across several dis the con uh the con of it is just that you lose a dis you lose your lvm but in the reality of it while it looks bad you get jbot gives you that false sense of I still have my data but bringing the disc back bringing a disc back in after you lost a disc in jbod is a bit painful to deal with you need to run around of repair quite frankly there's a replication factor in Cassandra for higher availability fail over if you lose a disc you lose the node it's not a problem you get a new disc you rebuild an lvm you re you reboot strap the n in the cluster I I think that's a really good point I think like it's a typical when we say you just use lvm it makes life simple people go oh but like I'm not setting up a like a ra redundancy in the lvm and if I lose a disc I lose that whole thing um and they think that JB's a better solution and you know the the conversation always comes back to but Cassandra was designed for this you've got distributed replicas the it's actually life is much easier with just one whole entity going down uh and being able to replace it and The Balancing Act in jbod becomes is sometimes extremely painful you can end up with an imbalance of the discs and fixing that is sometimes quite tricky the the tool which is used for that uh there's the not to relocate as a tables doesn't always yield the results you want so I would it's yeah it it looks nice on paper but the reality is an lvm is better and with a if you want to keep the performance that's where we recommend to use a striped lvm which means your volume will actually split in in small chunks across the different discs so if you have six discs you will have the performance of the disc multiplied by six when you're actually writing into the discs as opposed to L one which fills the first disc and then the second and then the S yeah and that that can be incredibly important uh if you are running dense nodes so good advice I love that we included this slide thank you next up major upgrades um so this is a slide that I'm going to talk to but we've all done our time uh around this uh the best bugs that we hit are um halfway through upgrades um or post upgrades that haven't been done well the first thing that I want to uh speak to is um uh what are the upgrade paths that we want you to do and and that we support So when you're doing an upgrade always first upgrade to the latest patch version on that major version that you're on so if you're on 311 upgrade to the latest 311 first uh it's this is because and you know hopefully it's it's obvious it's because with some uh of the bugs that we find we can only fix those bugs in the in the source version like in the version that you're upgrading from it doesn't happen too often but it happens inantly so that that this advice is is a wise one otherwise the since Cassandra 4 the the Cassandra Community has been uh more clear in that for online upgrades upgrades where you're doing a a rolling upgrade of each node while maintaining uh full availability and and online your services online we recommend that you only upgrade between adjacent major uh releases so what we mean by that is like the uh sorry adjacent major versions so you can upgrade from any three version to any four version but you can't upgrade from 3 to five and skip for um for offline we're getting better that you can uh upgrade SS tables from any uh version this is particularly important if you start to think about backups you want to pull out your backups and they be may be quite old and you still need to read them so we're at the the uh SS table format level we're trying to maintain compatibility um but when it comes to online upgrades there's just too much to test and worry about and the community only has so many resources to work with so this constraint makes life easier and safer for everybody a little text on this Slide the the summary of it is there really is a lot of homework you got to do with major upgrades there are prequisites that you should do there's preparation steps that you should do and the actual action um sequence of actions that you should take are all super important reach out to support uh and get help um don't underestimate any of this stuff you want a smooth experience okay jumping on uh system changes so Roman you had you brought this up um uh well so I think um yeah I think one one of the important Point here uh as part of as part of the discussion we had preparing this session is to mention that uh it's it's important to know your data model so it's just like nothing can replace that uh I regularly get uh CAU by uh request types that I didn't think uh existed um mentioning uh mentioning for example the case a case I had some quite a long time ago but secondary index uh based on a bullan value and so you you need to know your data model because in that case what happen is that a secondary index with a Boolean value means that if you have five million RS and they're spread evenly your cardinality of two because your bullan is z or one will mean you will bring 2.5 million document whether you're querying the zero or the one so those are those are important consideration you need to know uh what you have and how you're actually querying it and have some idea of how it's going to make the computer behave uh as a result so it's um I've I regularly have challenges around that front uh because this is something that Cassandra will not necessarily show you yeah and and and you know it's it's databases depend a lot on the underlying system and especially like a lot of the Linux distributions their default setup is not suitable for Gander technology um and so a lot of people come in and they just got start running and there's some baseline advice that we have to offer it's recommended check it out um but also I think one of the other things is that once they once the penny drops on that front and then they start investigating and learning all of the the the the Linux and underlying configuration changes that they can make they jump in and they change everything at once uh yeah that's what you mentioned here uh in the in the bullet point about doing the changes sequentially is just like one one step at a time is generally a better one at a time I'll make I'll make an ex I'll make an exception to the best practices which are generally a good of them to get your environment going setting assuring you that you have your reader heads correctly said that you know the values kernel parameters I mentioned no files earlier which is one that you can that you can hit and and when it when it hits you the problem is that the kernel will just stop your process so you end up with a not down so some of the the best practices really should be in place um on the as a general rule absolutely and and and and those things what like a what I've seen all too often is that they'll they'll they'll hit multiple nodes in a surprisingly uh similar point in time uh they like all of a sudden the cluster is just falling over node after node after node that's the last thing you want and Sandra set up properly uh it's incredibly resilient it's supposed to be an extremely resilient system that just keeps on working no matter what happens happens this is one of the gotes on that front I'm really happy that you mentioned read ahead readhead I think readhead and the GC tuning are your two loow hanging fruits to get huge improvements on systems uh so so thank you for that we are really pushing for time um so I'm gonna I'm going to jump over uh the tooling and the operational stop I'm going to jump um there is a core to action for this page which I think is is useful there's documentation on the Cassandra uh website that's really rich uh and it was it's recently new um go check that out it goes through a lot of the tools that we've mentioned here um and uh and how to use them and when to use them there was an item here Roman that uh it's really important for us to talk about the new Mac tell us about it so yeah I'll try I'll try to go quickly because I realize we're shot on time here um data Stacks uh and cundra have uh some some Metric toolings available but uh there's there's quite it's quite a bit let's let's put it that way it's the jungle out there when it comes to all the different tools available the Cassandra exporter and uh and we have designed ourselves mcak and we have the DS metric collector for data Stacks um and all of those are their pros and cons and we know that uh there's been a lot of there's been some issues with notably enak and the and the DSC metrics collector and those have there's genuine design flows in them uh which means they're really hard to fix uh we've worked with with uh our development team and uh with one of our colleagues in support uh and he came up with a full documentation on uh how to use MAAC on premise the idea being uh MAAC is a is actually part of the Kate Sandra operator uh uh offering so when you if I take it people have heard of Kate Sandra which is the one of the data stack open source solution to deploy your environment on kubernets and it has its own metric system which is different from the previous ones I mentioned but it's limited to kubernets and what we've done is extract all the information and extract the agent from the existing kubernetes originally provided in the code of uh of KRA operator uh and we have created a knowledge based article which will allow you to use MAAC for on Prime deployment now at this time the KB is designed toward DSC but the idea is that this is valid as well for Cassandra 4 Cassandra 41 and Cassandra 5 because I mentioned mcak earlier mcak doesn't go further than Cassandra 4.0 I believe we have a 41 Alpha which was released but never passed the alpha level so I think it's um a good takeaway as part of this discussion for people to be aware that we have been working on making this available even outside of katundra operator and we have our first released which came out literally two hours ago for the article nice one so so everyone who's struggling with enabling all your metrics monitoring uh on your Cassandra clusters uh and if anyone who's using mcac pay attention to this uh it's it's coming out thank you Peter for writing that blog post and I'm thank you I'm going to thank Michael Berman who I think is one of the engineers um who did a lot of this work um which has been asked for for a lot of people I'm GNA jump forward I'm gonna I'm going to run quickly over a minute or two to get to the questions that we have um so having a just a quick look Cassandra 5 what do we got coming the headliner has to be the Accord transactions this is going to make cassand ra an asset compliant system in fact it because it implements strictly serializable isolation it makes it uh the most asset compliance uh database out there on power with spanner um this is big stuff but probably more interesting to most people is what you can actually do with it on this slide there's an example syntax of the begin transaction um and how you can you can grab variables and then write a condition and then do something in that condition this means one of the the the big wins with this will be you can now do referential Integrity in your data model um this will be huge uh it's about to get merged to trunk okay so uh the next section I'm going to jump to one key slide we all know geni is massive at the moment and uh and and data Stacks is pumping out products around gen and Rag and Vector search because we're just we're seeing such huge adoption around this and the potential of what you can solve is is is everywhere and and it's brand new what you can do the one place where this really touches on Cassandra is the vector search Cassandra is a database which you know where seeing this brand new traffic shape and load uh land on the transactional database so it's perfectly positioned uh for this U being that scalable database that can just take anything and stay super fast on top of that we have the vector search the vector search has been implemented on top of the SII indexes in Cassandra and allows you to you do Vector search through the aut by clause and you can do like Global vector to search um in a table you can do it within a petition and you can also do hybrid against other indexes it's a very powerful feature it's put on top of the J Vector Library uh which is best in class if you have a look at the version three of that J Vector Library uh and how it's implemented dis and and a number of other industry-leading um Solutions it's it's blowing all the competitors uh and other open source Solutions the water I mentioned all the products that we're pumping out um so data sex has this AI uh pass solution so there's heaps here jump in um people talking in your company you got colleagues talking about J uh mention this to them um you know there's there's in every hype Circle there's so much noise it's a little bit hard to kind of grab hold of what's real what's happening this is one of those things and it's a really good opener so this is something you can just kind of show hey look here's a whole list of things for you to go check out and see if they match uh what you're currently exploring to do okay let's get to the questions yeah got a couple questions yeah there was a question in the chat as well which we can jump back to the first question I think I think there are I I I think the question in the chat was then put in the Q&A as well because I think that's the same one from aod Deli AI okay yeah a deli is asking about um incre changing parameters in yaml file for uh to Street between oltp and Olaf now should we should we each should we each give 10 seconds on this one uh what would be your your highlight yeah you can do analytics work using other tools like spark or other tools outside of a Cassandra but Cassandra itself is is a is a is a ntp type of small short um operations not really analytics operations so this is my 10 seconds that's right that's that's perfect was a good introduction Roman I'll give a try to it I'd say depending on the number of so it's all about the sizing of your box you can potentially increase the concurrent read and concurrent rights uh but you also have to make sure you monitor your environment accordingly going back to what we mentioned originally best practices should be in place rhead is something on any red flavor that will bite you uh when you so yeah it also depends on what what type of uh worklow do you have reads wres so if you have a right oriented workflow use LCS and no sorry use use sdcs if if you if you have a read oriented workflow use LCS so 10 to1 ratio for LCS that's my 10 seconds version but also check CPU and check the iio on your uh on your systems when you start doing this so use use no SQL bench DS bulk or some other tools to generate some stress on the environment see see how it it yeah it's a good question because it it it has a whole lot of challenges I mean you're you're going from transaction database to analytics database you're going from tuning for uh latency and removing uh long tails in the latency uh you're going for from probably a CP system to an AP system and in analytics you know you you don't you often don't care uh you can just restart a job um if if it crashes uh and still completes within in a certain window so you care more about throughput than you do latency and so you do end up with quite different uh y more files and quite different read ahead settings it goes all the way down the stack and so we often see people creating a separate Cassandra data center uh and having their clients do the analytics workload of that second data center where it can create as much instability as it wants and everything has been Che for throughput rather than low latency um so that's my takeaway um as far as individual yaml settings go uh there's too many to touch to give you a good answer Adela um you can ask us that uh offline if you like we'll be happy to help next question in Cassandra 4 are both unrepaired and repaired tables looked at for compaction or is only repaired tables compacted Francisco do you want to answer that one yeah well so if we're talking about Minor compaction all SS tables will be compacted that depends on the um compaction strategy that's not driven by by repair so if you talking about anti compaction in in in incremental repair then again all SS Stables will be anti- compacted because it could happen that SS Stables become can mix not not doing um become mixed uh with repaired and un repaired table so I would assume the question here is just about how normal compactions not the antic compaction so all yeah and and and you're right and I think it's we spoken before that there's now two different sets of ss tables there's unrepaired and there's repaired and the the compaction now kind of splits those sets separately so it will only compact the unrepaired SS tables together and it will only compact the repaired SS tables together so it does both but it has to do them separately yeah I'm with Nick on that one there's there's two streams of of compaction occurring separate so they the answer is go ahead sorry Francisco no no please go ahead um so are they compacted the answer is repair disc compacted and unrepaired disc compacted but separately and it can bring its share of surprises as well so we had a recent event where uh someone tried a major compaction on their environment and they were expecting to shave a lot of data with it and it didn't happen and the reason was that because they had a a rep table uh which dated back from 2022 and that couldn't compact with the unrepaired table uh and there was a tomb there were some tombstones in the repair table blocking the blocking the deletion of tombstones so this is really this happens a lot with time R compaction strategy as well and I think it's probably a really great answer for this question because if if you are seeing something that indicates this is not happening there are reasons for that worth investigating isn't that what we call Shadow tombstones uh or yes and there's a setting like for time winner competion strategy there's a setting called uh unsafe unsafe aggressive yeah um I think I was partly responsible for that naming um let's make it as scary as possible um but if that righty it's a it's a very effective a very effective option to turn on but you need you do need to turn it on both in the Amo and in its assistant property yes repairing repairing uh tables with with twcs or or LCS is not necessarily a great thing I mean uh they tables with uh it could mix up the the time the time series and and the time Windows of the of the S table so it's still that's one aspect I would I'm not sure what the challenge is with the LCS SP but I do agree we know twcs twcs is definitely something that that is challenging to repair I agree with that and this problem can happen uh like like the example you gave can happen elsewhere too well over time we're starting to to lose some people uh thank you for everyone who's hung around to the end um thank you ran and franccesco for doing this together it's been a pleasure um and as I said before I'm the one who brought Christmas to the group uh as you can see in my background uh so uh I hope you all have a white Christmas uh and a good holiday season thanks franisco and everyone for uh for being there goodbye thank you very Brad to",
    "segments": [
      {
        "start": 1.48,
        "duration": 6.319,
        "text": "okay I believe we're live hello everyone"
      },
      {
        "start": 4.04,
        "duration": 5.84,
        "text": "thank you for joining us today um I'm"
      },
      {
        "start": 7.799,
        "duration": 7.0,
        "text": "I'm sure there's some recognizable names"
      },
      {
        "start": 9.88,
        "duration": 6.44,
        "text": "in the attendees lists and uh if if if"
      },
      {
        "start": 14.799,
        "duration": 5.48,
        "text": "we know each other already you watching"
      },
      {
        "start": 16.32,
        "duration": 8.0,
        "text": "this afterwards uh after the event um a"
      },
      {
        "start": 20.279,
        "duration": 9.4,
        "text": "warm welcome to you so today we've got a"
      },
      {
        "start": 24.32,
        "duration": 8.36,
        "text": "webinar on uh uh some typical problems"
      },
      {
        "start": 29.679,
        "duration": 5.88,
        "text": "in Cassandra that many of you have faced"
      },
      {
        "start": 32.68,
        "duration": 5.559,
        "text": "before and I've got two of my favorite"
      },
      {
        "start": 35.559,
        "duration": 5.84,
        "text": "colleagues joining me today Francesco"
      },
      {
        "start": 38.239,
        "duration": 5.521,
        "text": "and Roman uh who are both from support"
      },
      {
        "start": 41.399,
        "duration": 5.441,
        "text": "but they've been around for a long time"
      },
      {
        "start": 43.76,
        "duration": 6.52,
        "text": "and I've worked with them on a number of"
      },
      {
        "start": 46.84,
        "duration": 6.64,
        "text": "tickets with customers and always being"
      },
      {
        "start": 50.28,
        "duration": 6.52,
        "text": "thoroughly surprised inspired they've"
      },
      {
        "start": 53.48,
        "duration": 7.0,
        "text": "taught me some things along the way um"
      },
      {
        "start": 56.8,
        "duration": 5.599,
        "text": "so uh when I was asked"
      },
      {
        "start": 60.48,
        "duration": 5.999,
        "text": "a couple of weeks ago or maybe a month"
      },
      {
        "start": 62.399,
        "duration": 6.72,
        "text": "ago now if I could do a webinar my first"
      },
      {
        "start": 66.479,
        "duration": 5.601,
        "text": "thought was hey let's get"
      },
      {
        "start": 69.119,
        "duration": 5.961,
        "text": "together these two people and let's talk"
      },
      {
        "start": 72.08,
        "duration": 4.079,
        "text": "about problems um knowing that this type"
      },
      {
        "start": 75.08,
        "duration": 3.12,
        "text": "of"
      },
      {
        "start": 76.159,
        "duration": 3.6,
        "text": "informational uh this type of"
      },
      {
        "start": 78.2,
        "duration": 2.599,
        "text": "information is highly valuable for all"
      },
      {
        "start": 79.759,
        "duration": 3.04,
        "text": "of"
      },
      {
        "start": 80.799,
        "duration": 6.161,
        "text": "you some of the stuff that we'll be"
      },
      {
        "start": 82.799,
        "duration": 8.241,
        "text": "talking about today uh"
      },
      {
        "start": 86.96,
        "duration": 6.76,
        "text": "is problems with petitions problems with"
      },
      {
        "start": 91.04,
        "duration": 5.039,
        "text": "repairs how to set up discs the choice"
      },
      {
        "start": 93.72,
        "duration": 6.039,
        "text": "between jbods and"
      },
      {
        "start": 96.079,
        "duration": 8.201,
        "text": "lbms how to do upgrades"
      },
      {
        "start": 99.759,
        "duration": 7.68,
        "text": "well how to set up your system uh for"
      },
      {
        "start": 104.28,
        "duration": 6.72,
        "text": "the the Linux distribution properly and"
      },
      {
        "start": 107.439,
        "duration": 6.601,
        "text": "different tooling and operational"
      },
      {
        "start": 111.0,
        "duration": 4.719,
        "text": "methodologies also a quick touch onto"
      },
      {
        "start": 114.04,
        "duration": 5.119,
        "text": "what's coming in"
      },
      {
        "start": 115.719,
        "duration": 6.121,
        "text": "Cassandra uh both what's exciting uh"
      },
      {
        "start": 119.159,
        "duration": 5.681,
        "text": "we'll open up new use cases for you uh"
      },
      {
        "start": 121.84,
        "duration": 8.08,
        "text": "but also what can help make life easier"
      },
      {
        "start": 124.84,
        "duration": 6.84,
        "text": "as an operator for you and uh one of"
      },
      {
        "start": 129.92,
        "duration": 4.959,
        "text": "those things will be Vector search this"
      },
      {
        "start": 131.68,
        "duration": 5.88,
        "text": "is very hot at the moment so uh we"
      },
      {
        "start": 134.879,
        "duration": 5.0,
        "text": "wouldn't do uh justice if we didn't"
      },
      {
        "start": 137.56,
        "duration": 6.679,
        "text": "touch on that lightly and at the end"
      },
      {
        "start": 139.879,
        "duration": 4.36,
        "text": "there'll be an ask ask us anything"
      },
      {
        "start": 144.72,
        "duration": 3.64,
        "text": "section before we jump into that uh"
      },
      {
        "start": 147.239,
        "duration": 3.801,
        "text": "we've got a couple of introduction"
      },
      {
        "start": 148.36,
        "duration": 5.72,
        "text": "slides to go through"
      },
      {
        "start": 151.04,
        "duration": 5.52,
        "text": "so first off a bit more of introduction"
      },
      {
        "start": 154.08,
        "duration": 7.239,
        "text": "of the three of"
      },
      {
        "start": 156.56,
        "duration": 8.48,
        "text": "us I can go first um so I've been"
      },
      {
        "start": 161.319,
        "duration": 7.721,
        "text": "involved in Cassandra going back to"
      },
      {
        "start": 165.04,
        "duration": 5.68,
        "text": "200 11 I wasn't interested in databases"
      },
      {
        "start": 169.04,
        "duration": 3.839,
        "text": "before Cassandra Cassandra was a"
      },
      {
        "start": 170.72,
        "duration": 5.64,
        "text": "technology that came along and in the"
      },
      {
        "start": 172.879,
        "duration": 6.961,
        "text": "way that it Sol distributed problems it"
      },
      {
        "start": 176.36,
        "duration": 6.92,
        "text": "felt more akin to how we were solved"
      },
      {
        "start": 179.84,
        "duration": 5.44,
        "text": "solving problems in a distributed space"
      },
      {
        "start": 183.28,
        "duration": 4.12,
        "text": "everywhere else in our industry that"
      },
      {
        "start": 185.28,
        "duration": 4.679,
        "text": "opened up a lot of doors for me uh the"
      },
      {
        "start": 187.4,
        "duration": 5.68,
        "text": "rest was history since then I've become"
      },
      {
        "start": 189.959,
        "duration": 7.321,
        "text": "a Cassandra committer and I'm now on the"
      },
      {
        "start": 193.08,
        "duration": 7.799,
        "text": "PMC and I've had many roles uh both as"
      },
      {
        "start": 197.28,
        "duration": 5.4,
        "text": "consultant and services as an engineer"
      },
      {
        "start": 200.879,
        "duration": 5.72,
        "text": "and ending product and back as an"
      },
      {
        "start": 202.68,
        "duration": 8.36,
        "text": "engineer now uh helping build the DSC"
      },
      {
        "start": 206.599,
        "duration": 8.041,
        "text": "and hcd products and thorough enjoying"
      },
      {
        "start": 211.04,
        "duration": 7.64,
        "text": "it a little bit personal about myself I"
      },
      {
        "start": 214.64,
        "duration": 7.36,
        "text": "am an Australian uh I'm met a Norwegian"
      },
      {
        "start": 218.68,
        "duration": 5.24,
        "text": "wife along the way uh moved to Norway on"
      },
      {
        "start": 222.0,
        "duration": 5.2,
        "text": "and off have been living in Norway for"
      },
      {
        "start": 223.92,
        "duration": 6.0,
        "text": "20 years and one of the real blessings"
      },
      {
        "start": 227.2,
        "duration": 6.48,
        "text": "in life is the opportunity to live in"
      },
      {
        "start": 229.92,
        "duration": 7.28,
        "text": "Europe I absolutely love working with"
      },
      {
        "start": 233.68,
        "duration": 6.16,
        "text": "the diversity that's available here um"
      },
      {
        "start": 237.2,
        "duration": 6.2,
        "text": "and I'm hoping that that that Roman and"
      },
      {
        "start": 239.84,
        "duration": 6.56,
        "text": "CCO can also share a little bit about uh"
      },
      {
        "start": 243.4,
        "duration": 4.679,
        "text": "their backgrounds and and and what they"
      },
      {
        "start": 246.4,
        "duration": 3.08,
        "text": "what they bring to the table because I"
      },
      {
        "start": 248.079,
        "duration": 4.281,
        "text": "think working in teams working with"
      },
      {
        "start": 249.48,
        "duration": 5.399,
        "text": "different cultures it's it's it is for"
      },
      {
        "start": 252.36,
        "duration": 5.36,
        "text": "me one of the joys in"
      },
      {
        "start": 254.879,
        "duration": 6.441,
        "text": "life Francesco would you like to go"
      },
      {
        "start": 257.72,
        "duration": 5.84,
        "text": "next yes hi hi everybody my name is"
      },
      {
        "start": 261.32,
        "duration": 5.72,
        "text": "Francesa Nali I I'm"
      },
      {
        "start": 263.56,
        "duration": 7.0,
        "text": "Italian I I live currently live in Italy"
      },
      {
        "start": 267.04,
        "duration": 6.999,
        "text": "I've been living in in UK for about 15"
      },
      {
        "start": 270.56,
        "duration": 8.4,
        "text": "years up to 2020"
      },
      {
        "start": 274.039,
        "duration": 8.281,
        "text": "I uh I have known uh databases"
      },
      {
        "start": 278.96,
        "duration": 5.76,
        "text": "from a long time and I've been working"
      },
      {
        "start": 282.32,
        "duration": 7.56,
        "text": "on pretty much all the"
      },
      {
        "start": 284.72,
        "duration": 10.28,
        "text": "database types starting from uh MySQL"
      },
      {
        "start": 289.88,
        "duration": 13.159,
        "text": "postgress and uh Oracle db2 a"
      },
      {
        "start": 295.0,
        "duration": 12.8,
        "text": "lot for many years then I in 26 16 I I"
      },
      {
        "start": 303.039,
        "duration": 7.201,
        "text": "I I I fell in love with Cassandra and in"
      },
      {
        "start": 307.8,
        "duration": 5.16,
        "text": "in particular with the distributed"
      },
      {
        "start": 310.24,
        "duration": 7.959,
        "text": "databases that the technology that is"
      },
      {
        "start": 312.96,
        "duration": 9.72,
        "text": "behind that is it really opened my my"
      },
      {
        "start": 318.199,
        "duration": 6.84,
        "text": "eyes and uh because it's such such a"
      },
      {
        "start": 322.68,
        "duration": 4.48,
        "text": "difference there's such a dramatic"
      },
      {
        "start": 325.039,
        "duration": 5.16,
        "text": "difference between the relational"
      },
      {
        "start": 327.16,
        "duration": 5.479,
        "text": "databases and non relational distributed"
      },
      {
        "start": 330.199,
        "duration": 4.161,
        "text": "database like"
      },
      {
        "start": 332.639,
        "duration": 5.56,
        "text": "candra"
      },
      {
        "start": 334.36,
        "duration": 9.839,
        "text": "um so I I I love Cassandra been working"
      },
      {
        "start": 338.199,
        "duration": 9.44,
        "text": "with Cassandra for the past eight years"
      },
      {
        "start": 344.199,
        "duration": 6.161,
        "text": "plus enjoying the technical side of it"
      },
      {
        "start": 347.639,
        "duration": 6.161,
        "text": "enjoying working with customers"
      },
      {
        "start": 350.36,
        "duration": 4.48,
        "text": "of uh with with many different projects"
      },
      {
        "start": 353.8,
        "duration": 7.76,
        "text": "use"
      },
      {
        "start": 354.84,
        "duration": 10.84,
        "text": "cases uh it it really never uh uh never"
      },
      {
        "start": 361.56,
        "duration": 8.16,
        "text": "stops surprising and never stops"
      },
      {
        "start": 365.68,
        "duration": 7.919,
        "text": "interesting my and and and teasing my"
      },
      {
        "start": 369.72,
        "duration": 5.759,
        "text": "brain everything you we do in in support"
      },
      {
        "start": 373.599,
        "duration": 6.401,
        "text": "is about"
      },
      {
        "start": 375.479,
        "duration": 6.921,
        "text": "learning uh yet a new thing it seems uh"
      },
      {
        "start": 380.0,
        "duration": 4.12,
        "text": "kind of weird but in in eight years I"
      },
      {
        "start": 382.4,
        "duration": 5.16,
        "text": "can say that I pretty much learned"
      },
      {
        "start": 384.12,
        "duration": 6.32,
        "text": "something every day from my colleagues"
      },
      {
        "start": 387.56,
        "duration": 5.88,
        "text": "from customers from"
      },
      {
        "start": 390.44,
        "duration": 7.599,
        "text": "from the the engineering team anything"
      },
      {
        "start": 393.44,
        "duration": 8.52,
        "text": "so that's the very interesting part of"
      },
      {
        "start": 398.039,
        "duration": 7.921,
        "text": "my my job my my job in"
      },
      {
        "start": 401.96,
        "duration": 7.079,
        "text": "support uh it is also"
      },
      {
        "start": 405.96,
        "duration": 5.359,
        "text": "always you you need to work in teams you"
      },
      {
        "start": 409.039,
        "duration": 5.241,
        "text": "need to work in in in a group you need"
      },
      {
        "start": 411.319,
        "duration": 6.88,
        "text": "to exchange constantly your ideas with"
      },
      {
        "start": 414.28,
        "duration": 6.08,
        "text": "ideas of others so yeah I hope that we"
      },
      {
        "start": 418.199,
        "duration": 6.961,
        "text": "will uh bring"
      },
      {
        "start": 420.36,
        "duration": 8.679,
        "text": "some nice discussion about uh of things"
      },
      {
        "start": 425.16,
        "duration": 5.68,
        "text": "so I'm really looking forward hearing"
      },
      {
        "start": 429.039,
        "duration": 4.6,
        "text": "questions and I couldn't agree with you"
      },
      {
        "start": 430.84,
        "duration": 5.199,
        "text": "more franccesco I think it's uh learning"
      },
      {
        "start": 433.639,
        "duration": 5.481,
        "text": "new things every day it's a technology"
      },
      {
        "start": 436.039,
        "duration": 5.081,
        "text": "that you don't hit the bottom uh you you"
      },
      {
        "start": 439.12,
        "duration": 4.04,
        "text": "always can keep digging and and and"
      },
      {
        "start": 441.12,
        "duration": 5.28,
        "text": "learn your stuff and having a good group"
      },
      {
        "start": 443.16,
        "duration": 7.879,
        "text": "of people around you to smile with is is"
      },
      {
        "start": 446.4,
        "duration": 7.96,
        "text": "a wonderful thing for us Roman"
      },
      {
        "start": 451.039,
        "duration": 7.0,
        "text": "hello everyone uh my name is Roman uh"
      },
      {
        "start": 454.36,
        "duration": 8.04,
        "text": "I'm French I've been living in the UK"
      },
      {
        "start": 458.039,
        "duration": 8.56,
        "text": "for 18 years now uh been in the IT"
      },
      {
        "start": 462.4,
        "duration": 6.759,
        "text": "industry for a bit longer than that uh"
      },
      {
        "start": 466.599,
        "duration": 3.921,
        "text": "worked for 11 years in the business"
      },
      {
        "start": 469.159,
        "duration": 4.841,
        "text": "analytics"
      },
      {
        "start": 470.52,
        "duration": 8.72,
        "text": "area uh business intelligence more"
      },
      {
        "start": 474.0,
        "duration": 6.879,
        "text": "exactly um and I joined the data stacks"
      },
      {
        "start": 479.24,
        "duration": 4.399,
        "text": "and"
      },
      {
        "start": 480.879,
        "duration": 5.88,
        "text": "Cassandra community in"
      },
      {
        "start": 483.639,
        "duration": 5.96,
        "text": "2018 so it's been six years and a half"
      },
      {
        "start": 486.759,
        "duration": 5.961,
        "text": "I've been working on on the technology"
      },
      {
        "start": 489.599,
        "duration": 5.921,
        "text": "bouncing on what Francesco and mck said"
      },
      {
        "start": 492.72,
        "duration": 4.0,
        "text": "I remember when I first started to learn"
      },
      {
        "start": 495.52,
        "duration": 3.84,
        "text": "about"
      },
      {
        "start": 496.72,
        "duration": 4.96,
        "text": "Cassandra um that I found it interesting"
      },
      {
        "start": 499.36,
        "duration": 6.399,
        "text": "to have to ditch all my existing"
      },
      {
        "start": 501.68,
        "duration": 7.199,
        "text": "Concepts about rdbms to actually work on"
      },
      {
        "start": 505.759,
        "duration": 4.84,
        "text": "no SQL database I remember this vividly"
      },
      {
        "start": 508.879,
        "duration": 3.241,
        "text": "shocked"
      },
      {
        "start": 510.599,
        "duration": 3.281,
        "text": "at"
      },
      {
        "start": 512.12,
        "duration": 5.12,
        "text": "first"
      },
      {
        "start": 513.88,
        "duration": 7.839,
        "text": "um fun fact about me I'm uh I I used to"
      },
      {
        "start": 517.24,
        "duration": 6.279,
        "text": "do uh cliff diving um but I I got wiser"
      },
      {
        "start": 521.719,
        "duration": 4.001,
        "text": "over the years and I don't think I'm at"
      },
      {
        "start": 523.519,
        "duration": 5.32,
        "text": "an age where I should be jumping or back"
      },
      {
        "start": 525.72,
        "duration": 7.119,
        "text": "flipping from 10 meters high"
      },
      {
        "start": 528.839,
        "duration": 6.061,
        "text": "CLI oh wow yeah that sounds awesome keep"
      },
      {
        "start": 532.839,
        "duration": 5.19,
        "text": "keep"
      },
      {
        "start": 534.9,
        "duration": 3.129,
        "text": "[Applause]"
      },
      {
        "start": 538.079,
        "duration": 5.241,
        "text": "going yes that's awesome we'll do it"
      },
      {
        "start": 540.64,
        "duration": 5.16,
        "text": "together one day how's that I'll get you"
      },
      {
        "start": 543.32,
        "duration": 5.639,
        "text": "back into it"
      },
      {
        "start": 545.8,
        "duration": 7.159,
        "text": "anytime okay so before we keep going um"
      },
      {
        "start": 548.959,
        "duration": 6.281,
        "text": "some house rules for us"
      },
      {
        "start": 552.959,
        "duration": 5.041,
        "text": "uh through the presentations there's"
      },
      {
        "start": 555.24,
        "duration": 6.599,
        "text": "going to be some links that pop up uh"
      },
      {
        "start": 558.0,
        "duration": 5.64,
        "text": "these are quter actions um or uh further"
      },
      {
        "start": 561.839,
        "duration": 4.481,
        "text": "reading on some of the things that we"
      },
      {
        "start": 563.64,
        "duration": 5.12,
        "text": "are talking about so if you see them uh"
      },
      {
        "start": 566.32,
        "duration": 3.68,
        "text": "pop up uh we recommend that you you"
      },
      {
        "start": 568.76,
        "duration": 4.319,
        "text": "click on them"
      },
      {
        "start": 570.0,
        "duration": 6.0,
        "text": "um save them read them later uh We've"
      },
      {
        "start": 573.079,
        "duration": 5.481,
        "text": "we've carefully selected them for"
      },
      {
        "start": 576.0,
        "duration": 6.12,
        "text": "you this whole webinar it's going to be"
      },
      {
        "start": 578.56,
        "duration": 5.68,
        "text": "recorded So and emails will be sent out"
      },
      {
        "start": 582.12,
        "duration": 6.6,
        "text": "afterwards uh so you can you can watch"
      },
      {
        "start": 584.24,
        "duration": 7.2,
        "text": "it again you can uh um you can pass it"
      },
      {
        "start": 588.72,
        "duration": 8.799,
        "text": "to your friends and colleagues"
      },
      {
        "start": 591.44,
        "duration": 9.8,
        "text": "Etc in this SQL uh um uh app this this"
      },
      {
        "start": 597.519,
        "duration": 7.161,
        "text": "this webinar page we do have a few tabs"
      },
      {
        "start": 601.24,
        "duration": 4.8,
        "text": "on the right hand side we've got chat"
      },
      {
        "start": 604.68,
        "duration": 5.839,
        "text": "and we've got"
      },
      {
        "start": 606.04,
        "duration": 7.32,
        "text": "Q&A uh if you've got questions throw"
      },
      {
        "start": 610.519,
        "duration": 6.361,
        "text": "them into Q&A we're going to hold some"
      },
      {
        "start": 613.36,
        "duration": 4.68,
        "text": "time uh off at the end and answer your"
      },
      {
        "start": 616.88,
        "duration": 3.8,
        "text": "all your"
      },
      {
        "start": 618.04,
        "duration": 5.96,
        "text": "questions if there is something which is"
      },
      {
        "start": 620.68,
        "duration": 8.0,
        "text": "pertinent in the moment uh feel free to"
      },
      {
        "start": 624.0,
        "duration": 8.64,
        "text": "also write something in in the chat um"
      },
      {
        "start": 628.68,
        "duration": 3.96,
        "text": "well we'll pick up on that as we"
      },
      {
        "start": 635.48,
        "duration": 5.56,
        "text": "go okay so jumping into the content uh"
      },
      {
        "start": 640.12,
        "duration": 2.68,
        "text": "this is how we're going to lay it out"
      },
      {
        "start": 641.04,
        "duration": 5.32,
        "text": "we're going to go through some of these"
      },
      {
        "start": 642.8,
        "duration": 6.599,
        "text": "uh hot topics for operators first and"
      },
      {
        "start": 646.36,
        "duration": 5.599,
        "text": "then wrap up with uh what's coming and"
      },
      {
        "start": 649.399,
        "duration": 4.721,
        "text": "what's uh what's hot in the use case and"
      },
      {
        "start": 651.959,
        "duration": 2.161,
        "text": "the"
      },
      {
        "start": 656.12,
        "duration": 7.64,
        "text": "development so first up we've got"
      },
      {
        "start": 660.04,
        "duration": 7.68,
        "text": "hot and wide and large"
      },
      {
        "start": 663.76,
        "duration": 8.28,
        "text": "petitions uh I think we've we've all"
      },
      {
        "start": 667.72,
        "duration": 4.32,
        "text": "done a lot of time on this one haven't"
      },
      {
        "start": 673.72,
        "duration": 12.08,
        "text": "we so before I jump into this uh remind"
      },
      {
        "start": 679.959,
        "duration": 8.88,
        "text": "I'm G to ask you uh uh a an initial"
      },
      {
        "start": 685.8,
        "duration": 5.479,
        "text": "question uh we often like people often"
      },
      {
        "start": 688.839,
        "duration": 4.68,
        "text": "just talk about large petitions and when"
      },
      {
        "start": 691.279,
        "duration": 6.201,
        "text": "we had when we were discussing this"
      },
      {
        "start": 693.519,
        "duration": 7.081,
        "text": "before we we laid this out between hot"
      },
      {
        "start": 697.48,
        "duration": 5.919,
        "text": "petitions and wide petitions and large"
      },
      {
        "start": 700.6,
        "duration": 4.479,
        "text": "petitions do you want to quickly give us"
      },
      {
        "start": 703.399,
        "duration": 4.481,
        "text": "the difference between"
      },
      {
        "start": 705.079,
        "duration": 5.721,
        "text": "them so"
      },
      {
        "start": 707.88,
        "duration": 6.48,
        "text": "the let let's start first with what is a"
      },
      {
        "start": 710.8,
        "duration": 5.96,
        "text": "hot partition the the way the way candre"
      },
      {
        "start": 714.36,
        "duration": 4.96,
        "text": "is design is to allow you to do a"
      },
      {
        "start": 716.76,
        "duration": 7.04,
        "text": "replication factor of three uh for your"
      },
      {
        "start": 719.32,
        "duration": 7.199,
        "text": "data and your driver will come with uh"
      },
      {
        "start": 723.8,
        "duration": 6.36,
        "text": "an agreement as to the consistency level"
      },
      {
        "start": 726.519,
        "duration": 6.961,
        "text": "it needs to reach to uh to receive its"
      },
      {
        "start": 730.16,
        "duration": 6.039,
        "text": "response from the database so in a local"
      },
      {
        "start": 733.48,
        "duration": 8.12,
        "text": "Quorum scenario you will ask two of the"
      },
      {
        "start": 736.199,
        "duration": 8.2,
        "text": "nodes to retrieve the data now"
      },
      {
        "start": 741.6,
        "duration": 5.12,
        "text": "the H partition is basically when you're"
      },
      {
        "start": 744.399,
        "duration": 6.161,
        "text": "always eating the same partition hence"
      },
      {
        "start": 746.72,
        "duration": 6.52,
        "text": "the name and that means that that in the"
      },
      {
        "start": 750.56,
        "duration": 5.6,
        "text": "way gason R is designed you will always"
      },
      {
        "start": 753.24,
        "duration": 7.719,
        "text": "be hitting the same notes"
      },
      {
        "start": 756.16,
        "duration": 7.039,
        "text": "so the a hot partition is something that"
      },
      {
        "start": 760.959,
        "duration": 4.521,
        "text": "however you scale your cluster will"
      },
      {
        "start": 763.199,
        "duration": 5.481,
        "text": "never be sorted by any"
      },
      {
        "start": 765.48,
        "duration": 5.76,
        "text": "scaling because the the way you request"
      },
      {
        "start": 768.68,
        "duration": 5.04,
        "text": "a partition key will hit a specific"
      },
      {
        "start": 771.24,
        "duration": 6.08,
        "text": "token and that token is always on the"
      },
      {
        "start": 773.72,
        "duration": 6.6,
        "text": "same three nodes so that's what defines"
      },
      {
        "start": 777.32,
        "duration": 5.44,
        "text": "a hot partition and"
      },
      {
        "start": 780.32,
        "duration": 6.44,
        "text": "so it doesn't matter if you have uh"
      },
      {
        "start": 782.76,
        "duration": 6.519,
        "text": "three nodes 10 nodes 100 nodes 300 nodes"
      },
      {
        "start": 786.76,
        "duration": 6.96,
        "text": "or more hot partition will always give"
      },
      {
        "start": 789.279,
        "duration": 9.321,
        "text": "you the that problem of uh creating"
      },
      {
        "start": 793.72,
        "duration": 7.52,
        "text": "a a funnel to a specific set of your"
      },
      {
        "start": 798.6,
        "duration": 6.64,
        "text": "environment white partitions"
      },
      {
        "start": 801.24,
        "duration": 6.24,
        "text": "is different in that it's the way uh you"
      },
      {
        "start": 805.24,
        "duration": 4.719,
        "text": "create your data in your partition key"
      },
      {
        "start": 807.48,
        "duration": 4.96,
        "text": "and so if you create a"
      },
      {
        "start": 809.959,
        "duration": 5.0,
        "text": "a partition key or no a primary key"
      },
      {
        "start": 812.44,
        "duration": 4.199,
        "text": "which contains let's say one column as"
      },
      {
        "start": 814.959,
        "duration": 4.081,
        "text": "the partition key and one column as the"
      },
      {
        "start": 816.639,
        "duration": 5.2,
        "text": "clustering column you can have multiple"
      },
      {
        "start": 819.04,
        "duration": 5.88,
        "text": "roles coming in your environment coming"
      },
      {
        "start": 821.839,
        "duration": 5.721,
        "text": "in that partition and the problem you"
      },
      {
        "start": 824.92,
        "duration": 5.719,
        "text": "get is when this the number of rols"
      },
      {
        "start": 827.56,
        "duration": 6.56,
        "text": "within a partition get out of"
      },
      {
        "start": 830.639,
        "duration": 6.081,
        "text": "control which means we say from a"
      },
      {
        "start": 834.12,
        "duration": 5.44,
        "text": "conservative perspective uh that a"
      },
      {
        "start": 836.72,
        "duration": 5.28,
        "text": "partition should avoid being above 100"
      },
      {
        "start": 839.56,
        "duration": 4.399,
        "text": "Meg so from a support perspective we"
      },
      {
        "start": 842.0,
        "duration": 4.24,
        "text": "know it's a conservative value but"
      },
      {
        "start": 843.959,
        "duration": 4.8,
        "text": "that's because we know that when you"
      },
      {
        "start": 846.24,
        "duration": 5.36,
        "text": "start to see this you will end up"
      },
      {
        "start": 848.759,
        "duration": 5.64,
        "text": "potentially seeing these particular"
      },
      {
        "start": 851.6,
        "duration": 4.039,
        "text": "partitions grow bigger and bigger and"
      },
      {
        "start": 854.399,
        "duration": 4.56,
        "text": "the challenges around the white"
      },
      {
        "start": 855.639,
        "duration": 5.601,
        "text": "partitions is that they will create"
      },
      {
        "start": 858.959,
        "duration": 5.0,
        "text": "pressure on your system if they grow"
      },
      {
        "start": 861.24,
        "duration": 6.159,
        "text": "totally out of control they will o your"
      },
      {
        "start": 863.959,
        "duration": 5.761,
        "text": "system uh and so"
      },
      {
        "start": 867.399,
        "duration": 3.841,
        "text": "that's so that's the definition of the"
      },
      {
        "start": 869.72,
        "duration": 3.84,
        "text": "hot and the white partition hot"
      },
      {
        "start": 871.24,
        "duration": 5.44,
        "text": "partition is always eating the same node"
      },
      {
        "start": 873.56,
        "duration": 6.04,
        "text": "white partition white partitions will"
      },
      {
        "start": 876.68,
        "duration": 6.0,
        "text": "also give pressure on some systems but"
      },
      {
        "start": 879.6,
        "duration": 5.56,
        "text": "in a different manner one is just"
      },
      {
        "start": 882.68,
        "duration": 5.92,
        "text": "constantly asking the same data to the"
      },
      {
        "start": 885.16,
        "duration": 5.799,
        "text": "same node and the other is uh due to the"
      },
      {
        "start": 888.6,
        "duration": 5.479,
        "text": "blotting of the amount of data you"
      },
      {
        "start": 890.959,
        "duration": 3.12,
        "text": "request from the"
      },
      {
        "start": 895.12,
        "duration": 6.279,
        "text": "nodes and large large petitions uh which"
      },
      {
        "start": 899.72,
        "duration": 4.96,
        "text": "can also be large"
      },
      {
        "start": 901.399,
        "duration": 6.961,
        "text": "rows yeah they're in fact something"
      },
      {
        "start": 904.68,
        "duration": 6.24,
        "text": "different so a large row is um is when"
      },
      {
        "start": 908.36,
        "duration": 6.88,
        "text": "you try to push too much data within one"
      },
      {
        "start": 910.92,
        "duration": 7.039,
        "text": "specific row so typically uh using blobs"
      },
      {
        "start": 915.24,
        "duration": 4.32,
        "text": "and trying to push a full document"
      },
      {
        "start": 917.959,
        "duration": 4.32,
        "text": "within"
      },
      {
        "start": 919.56,
        "duration": 5.279,
        "text": "uh a push when you try to push a full"
      },
      {
        "start": 922.279,
        "duration": 5.081,
        "text": "document a full binary within the"
      },
      {
        "start": 924.839,
        "duration": 6.401,
        "text": "database so that's again something"
      },
      {
        "start": 927.36,
        "duration": 6.0,
        "text": "different um we do see uh one of one of"
      },
      {
        "start": 931.24,
        "duration": 4.159,
        "text": "the red flags for this is when you start"
      },
      {
        "start": 933.36,
        "duration": 3.839,
        "text": "to see the commit lock segment size"
      },
      {
        "start": 935.399,
        "duration": 4.56,
        "text": "increased in the environment this is not"
      },
      {
        "start": 937.199,
        "duration": 4.56,
        "text": "the only uh giveaway but that's"
      },
      {
        "start": 939.959,
        "duration": 4.721,
        "text": "generally a good indication that you're"
      },
      {
        "start": 941.759,
        "duration": 4.52,
        "text": "trying to push too much uh within the"
      },
      {
        "start": 944.68,
        "duration": 6.32,
        "text": "within the rules of your"
      },
      {
        "start": 946.279,
        "duration": 8.081,
        "text": "database um the issue is go ahead there"
      },
      {
        "start": 951.0,
        "duration": 7.68,
        "text": "are also some alerts in Cassandra right"
      },
      {
        "start": 954.36,
        "duration": 6.959,
        "text": "that prevent you to like you said the"
      },
      {
        "start": 958.68,
        "duration": 2.639,
        "text": "the mid"
      },
      {
        "start": 961.639,
        "duration": 8.12,
        "text": "log when when cander detects that you"
      },
      {
        "start": 964.759,
        "duration": 7.801,
        "text": "are pushing a mutation that is more than"
      },
      {
        "start": 969.759,
        "duration": 5.281,
        "text": "half the size of the commit log it will"
      },
      {
        "start": 972.56,
        "duration": 4.079,
        "text": "stop you right yeah a right which is a"
      },
      {
        "start": 975.04,
        "duration": 4.12,
        "text": "right which is half the size of a commit"
      },
      {
        "start": 976.639,
        "duration": 5.281,
        "text": "loog segment size will be"
      },
      {
        "start": 979.16,
        "duration": 3.96,
        "text": "uh will be refused by the database so"
      },
      {
        "start": 981.92,
        "duration": 3.56,
        "text": "that's why I said the commit loog"
      },
      {
        "start": 983.12,
        "duration": 5.12,
        "text": "segment size is one example where this"
      },
      {
        "start": 985.48,
        "duration": 4.479,
        "text": "can be some form of red flag there's"
      },
      {
        "start": 988.24,
        "duration": 4.159,
        "text": "another thing"
      },
      {
        "start": 989.959,
        "duration": 6.721,
        "text": "take that's why you should increase size"
      },
      {
        "start": 992.399,
        "duration": 6.201,
        "text": "of the commit logs uh well no well you"
      },
      {
        "start": 996.68,
        "duration": 3.8,
        "text": "should reduce the size you should reduce"
      },
      {
        "start": 998.6,
        "duration": 6.08,
        "text": "the size of your Ro I'm tricking you I'm"
      },
      {
        "start": 1000.48,
        "duration": 7.76,
        "text": "tricking you no of course not yes"
      },
      {
        "start": 1004.68,
        "duration": 5.839,
        "text": "so the the problem Associated to the"
      },
      {
        "start": 1008.24,
        "duration": 4.039,
        "text": "sorry the problem Associated to the"
      },
      {
        "start": 1010.519,
        "duration": 5.081,
        "text": "large roles and partition actually is"
      },
      {
        "start": 1012.279,
        "duration": 6.761,
        "text": "inherent to the jvm your uh Young"
      },
      {
        "start": 1015.6,
        "duration": 6.56,
        "text": "Generation is actually split in regions"
      },
      {
        "start": 1019.04,
        "duration": 4.52,
        "text": "uh 1,24 to 248 of them and the problem"
      },
      {
        "start": 1022.16,
        "duration": 3.799,
        "text": "you have is that when you're querying"
      },
      {
        "start": 1023.56,
        "duration": 5.119,
        "text": "that data you generate what you call an"
      },
      {
        "start": 1025.959,
        "duration": 5.08,
        "text": "Among Us humongous allocation which"
      },
      {
        "start": 1028.679,
        "duration": 4.841,
        "text": "means that this data does not go through"
      },
      {
        "start": 1031.039,
        "duration": 5.52,
        "text": "your Young Generation and will actually"
      },
      {
        "start": 1033.52,
        "duration": 6.399,
        "text": "go straight in the old gen of the"
      },
      {
        "start": 1036.559,
        "duration": 5.28,
        "text": "jvm and by doing that what this creates"
      },
      {
        "start": 1039.919,
        "duration": 3.321,
        "text": "if you have a lot of large rows it means"
      },
      {
        "start": 1041.839,
        "duration": 3.24,
        "text": "that your cluster is going to start"
      },
      {
        "start": 1043.24,
        "duration": 3.64,
        "text": "doing stop the worlds GCS what we call"
      },
      {
        "start": 1045.079,
        "duration": 4.201,
        "text": "full"
      },
      {
        "start": 1046.88,
        "duration": 3.96,
        "text": "GCS and that's and and that really"
      },
      {
        "start": 1049.28,
        "duration": 4.2,
        "text": "becomes a problem in your environment"
      },
      {
        "start": 1050.84,
        "duration": 4.44,
        "text": "the advice for this being to actually"
      },
      {
        "start": 1053.48,
        "duration": 4.04,
        "text": "take the documents and store them"
      },
      {
        "start": 1055.28,
        "duration": 6.56,
        "text": "outside of the database in that case or"
      },
      {
        "start": 1057.52,
        "duration": 7.68,
        "text": "to store your blob within a storage U"
      },
      {
        "start": 1061.84,
        "duration": 5.48,
        "text": "that you link within the repository you"
      },
      {
        "start": 1065.2,
        "duration": 4.4,
        "text": "have in your castom"
      },
      {
        "start": 1067.32,
        "duration": 5.359,
        "text": "environment and there are some settings"
      },
      {
        "start": 1069.6,
        "duration": 6.88,
        "text": "that can help uh protect against this"
      },
      {
        "start": 1072.679,
        "duration": 6.201,
        "text": "now in newer versions you can you can"
      },
      {
        "start": 1076.48,
        "duration": 4.559,
        "text": "set the max cell size you can there's a"
      },
      {
        "start": 1078.88,
        "duration": 5.2,
        "text": "few things you can do isn't there yeah"
      },
      {
        "start": 1081.039,
        "duration": 4.64,
        "text": "the guard rails which have been"
      },
      {
        "start": 1084.08,
        "duration": 6.52,
        "text": "implemented is that what you're"
      },
      {
        "start": 1085.679,
        "duration": 8.041,
        "text": "referring to Mick yeah yeah yeah um so"
      },
      {
        "start": 1090.6,
        "duration": 5.24,
        "text": "so going back to hot petitions you did"
      },
      {
        "start": 1093.72,
        "duration": 6.76,
        "text": "kind of mention that what's"
      },
      {
        "start": 1095.84,
        "duration": 8.079,
        "text": "some uh what's some typical ways that"
      },
      {
        "start": 1100.48,
        "duration": 5.48,
        "text": "this is obvious like when when when"
      },
      {
        "start": 1103.919,
        "duration": 3.76,
        "text": "support tickets come in and The"
      },
      {
        "start": 1105.96,
        "duration": 4.8,
        "text": "Operators are like the cluster keeps on"
      },
      {
        "start": 1107.679,
        "duration": 5.441,
        "text": "falling over I don't know what it is and"
      },
      {
        "start": 1110.76,
        "duration": 6.919,
        "text": "you see something and you're like I know"
      },
      {
        "start": 1113.12,
        "duration": 8.039,
        "text": "exactly what it is so I I kind of gave"
      },
      {
        "start": 1117.679,
        "duration": 5.681,
        "text": "the hint by explaining the way uh this"
      },
      {
        "start": 1121.159,
        "duration": 3.921,
        "text": "reflects so because you're always"
      },
      {
        "start": 1123.36,
        "duration": 4.04,
        "text": "pinning your work Cloud on the same"
      },
      {
        "start": 1125.08,
        "duration": 5.44,
        "text": "nodes those nodes will have a totally"
      },
      {
        "start": 1127.4,
        "duration": 6.32,
        "text": "different uh CPU usage compared to the"
      },
      {
        "start": 1130.52,
        "duration": 5.639,
        "text": "others in general uh if 90% of your"
      },
      {
        "start": 1133.72,
        "duration": 4.68,
        "text": "traffic is itting the same partition uh"
      },
      {
        "start": 1136.159,
        "duration": 3.081,
        "text": "those nodes are going to exhibit way"
      },
      {
        "start": 1138.4,
        "duration": 3.08,
        "text": "higher"
      },
      {
        "start": 1139.24,
        "duration": 5.2,
        "text": "CPU level so what you will find in that"
      },
      {
        "start": 1141.48,
        "duration": 4.96,
        "text": "case is three nodes uh or depending on"
      },
      {
        "start": 1144.44,
        "duration": 5.16,
        "text": "your replication factor and your"
      },
      {
        "start": 1146.44,
        "duration": 5.84,
        "text": "consistency level uh those nodes will"
      },
      {
        "start": 1149.6,
        "duration": 4.63,
        "text": "actually be a lot hotter in term of CPU"
      },
      {
        "start": 1152.28,
        "duration": 3.04,
        "text": "usage"
      },
      {
        "start": 1154.23,
        "duration": 4.05,
        "text": "[Music]"
      },
      {
        "start": 1155.32,
        "duration": 4.839,
        "text": "and and one way to look into it further"
      },
      {
        "start": 1158.28,
        "duration": 5.24,
        "text": "is for example using the note Tool top"
      },
      {
        "start": 1160.159,
        "duration": 5.841,
        "text": "partitions tool which is not one we use"
      },
      {
        "start": 1163.52,
        "duration": 5.32,
        "text": "on a daily basis but uh when you start"
      },
      {
        "start": 1166.0,
        "duration": 4.679,
        "text": "seeing that kind of significant CPU"
      },
      {
        "start": 1168.84,
        "duration": 4.04,
        "text": "workload difference between your"
      },
      {
        "start": 1170.679,
        "duration": 4.12,
        "text": "environment that's generally a good one"
      },
      {
        "start": 1172.88,
        "duration": 4.08,
        "text": "to start uh to start looking at to"
      },
      {
        "start": 1174.799,
        "duration": 4.641,
        "text": "sample your environment and"
      },
      {
        "start": 1176.96,
        "duration": 5.36,
        "text": "find a bit the structure of the reads"
      },
      {
        "start": 1179.44,
        "duration": 6.04,
        "text": "and the writes on on specific"
      },
      {
        "start": 1182.32,
        "duration": 7.28,
        "text": "tables okay and and my last question"
      },
      {
        "start": 1185.48,
        "duration": 5.12,
        "text": "before we jump on to the next slide is"
      },
      {
        "start": 1189.6,
        "duration": 4.28,
        "text": "why"
      },
      {
        "start": 1190.6,
        "duration": 6.0,
        "text": "partitions this one can be a little bit"
      },
      {
        "start": 1193.88,
        "duration": 4.88,
        "text": "more tricky sometimes uh because"
      },
      {
        "start": 1196.6,
        "duration": 3.72,
        "text": "sometimes you have"
      },
      {
        "start": 1198.76,
        "duration": 4.52,
        "text": "for example time"
      },
      {
        "start": 1200.32,
        "duration": 6.16,
        "text": "series uh data models like the idea of"
      },
      {
        "start": 1203.28,
        "duration": 7.04,
        "text": "having a million rows in a"
      },
      {
        "start": 1206.48,
        "duration": 8.199,
        "text": "petition sometimes that is legitimate so"
      },
      {
        "start": 1210.32,
        "duration": 6.64,
        "text": "sometimes it can be a a a initially"
      },
      {
        "start": 1214.679,
        "duration": 3.681,
        "text": "poorly designed data model or it may"
      },
      {
        "start": 1216.96,
        "duration": 3.719,
        "text": "have been a data model that was"
      },
      {
        "start": 1218.36,
        "duration": 5.16,
        "text": "originally correct and then something a"
      },
      {
        "start": 1220.679,
        "duration": 5.36,
        "text": "change has happened along the way and"
      },
      {
        "start": 1223.52,
        "duration": 5.159,
        "text": "that breaks it"
      },
      {
        "start": 1226.039,
        "duration": 5.601,
        "text": "um what are the ways that you see it and"
      },
      {
        "start": 1228.679,
        "duration": 8.161,
        "text": "and what are the ways that we can fix"
      },
      {
        "start": 1231.64,
        "duration": 6.919,
        "text": "it so one so one of the thing I mean uh"
      },
      {
        "start": 1236.84,
        "duration": 3.719,
        "text": "one of the thing that's noted here is"
      },
      {
        "start": 1238.559,
        "duration": 4.161,
        "text": "that we have a tool such as not tool CF"
      },
      {
        "start": 1240.559,
        "duration": 5.201,
        "text": "stats would allow you to see potentially"
      },
      {
        "start": 1242.72,
        "duration": 6.88,
        "text": "the largest partition in your own vment"
      },
      {
        "start": 1245.76,
        "duration": 6.159,
        "text": "however this can be misleading the I've"
      },
      {
        "start": 1249.6,
        "duration": 5.559,
        "text": "recently encountered that where looking"
      },
      {
        "start": 1251.919,
        "duration": 5.0,
        "text": "into a hip dump I realized that I had a"
      },
      {
        "start": 1255.159,
        "duration": 3.88,
        "text": "a specific partition being called which"
      },
      {
        "start": 1256.919,
        "duration": 4.521,
        "text": "was more than 4 gig in memory the it was"
      },
      {
        "start": 1259.039,
        "duration": 5.321,
        "text": "a small castra cluster there was not a"
      },
      {
        "start": 1261.44,
        "duration": 5.76,
        "text": "lot of Heap allocated to it and that for"
      },
      {
        "start": 1264.36,
        "duration": 4.52,
        "text": "G query o and the environment and when I"
      },
      {
        "start": 1267.2,
        "duration": 3.24,
        "text": "read in the Heap dump realized that it's"
      },
      {
        "start": 1268.88,
        "duration": 3.159,
        "text": "a single partition rate so I'm not"
      },
      {
        "start": 1270.44,
        "duration": 5.8,
        "text": "talking about a wrench query or anything"
      },
      {
        "start": 1272.039,
        "duration": 5.921,
        "text": "it was really a single a single one"
      },
      {
        "start": 1276.24,
        "duration": 3.84,
        "text": "however when I looked into the CF stats"
      },
      {
        "start": 1277.96,
        "duration": 4.599,
        "text": "I realized that you have only one gig uh"
      },
      {
        "start": 1280.08,
        "duration": 4.079,
        "text": "shown here so I think one of the key"
      },
      {
        "start": 1282.559,
        "duration": 4.48,
        "text": "takeway I want to give here is that the"
      },
      {
        "start": 1284.159,
        "duration": 7.241,
        "text": "CF stats give you only the largest"
      },
      {
        "start": 1287.039,
        "duration": 7.161,
        "text": "partition Within an SS table uh across"
      },
      {
        "start": 1291.4,
        "duration": 5.84,
        "text": "all the SS tables it doesn't give you uh"
      },
      {
        "start": 1294.2,
        "duration": 6.2,
        "text": "this across all the different SS table"
      },
      {
        "start": 1297.24,
        "duration": 5.76,
        "text": "so the there's no aggregation of the SS"
      },
      {
        "start": 1300.4,
        "duration": 3.96,
        "text": "table metadata in the CF stats showing"
      },
      {
        "start": 1303.0,
        "duration": 3.88,
        "text": "you that so what you have to understand"
      },
      {
        "start": 1304.36,
        "duration": 4.559,
        "text": "is that when we tell you we see a 100"
      },
      {
        "start": 1306.88,
        "duration": 3.679,
        "text": "Meg partition if that partition exist"
      },
      {
        "start": 1308.919,
        "duration": 4.081,
        "text": "across multiple SS table in your"
      },
      {
        "start": 1310.559,
        "duration": 5.48,
        "text": "environment it can be actually quite"
      },
      {
        "start": 1313.0,
        "duration": 5.64,
        "text": "significantly bigger a way to identify"
      },
      {
        "start": 1316.039,
        "duration": 4.64,
        "text": "this is looking into the logs uh if you"
      },
      {
        "start": 1318.64,
        "duration": 3.639,
        "text": "see the white partitions are notified"
      },
      {
        "start": 1320.679,
        "duration": 3.24,
        "text": "and depending on the versions you use"
      },
      {
        "start": 1322.279,
        "duration": 4.481,
        "text": "some of them will actually even tell you"
      },
      {
        "start": 1323.919,
        "duration": 4.401,
        "text": "which partition key within a table is"
      },
      {
        "start": 1326.76,
        "duration": 6.08,
        "text": "actually giving you"
      },
      {
        "start": 1328.32,
        "duration": 6.8,
        "text": "trouble this is not uh always working"
      },
      {
        "start": 1332.84,
        "duration": 5.88,
        "text": "because I just mentioned that the"
      },
      {
        "start": 1335.12,
        "duration": 5.679,
        "text": "partition can be uh can be spread across"
      },
      {
        "start": 1338.72,
        "duration": 3.36,
        "text": "multiple SS tabl so in a Time series"
      },
      {
        "start": 1340.799,
        "duration": 5.76,
        "text": "this wouldn't"
      },
      {
        "start": 1342.08,
        "duration": 7.4,
        "text": "occur uh but in yeah in stcs or in stcs"
      },
      {
        "start": 1346.559,
        "duration": 4.6,
        "text": "you could end up with that LC LCS you"
      },
      {
        "start": 1349.48,
        "duration": 4.48,
        "text": "would obviously see it based on the way"
      },
      {
        "start": 1351.159,
        "duration": 5.241,
        "text": "LCS is designed on competion it should"
      },
      {
        "start": 1353.96,
        "duration": 4.44,
        "text": "really uh flag it"
      },
      {
        "start": 1356.4,
        "duration": 5.48,
        "text": "up I'm going to jump on I'm going to"
      },
      {
        "start": 1358.4,
        "duration": 5.759,
        "text": "move on uh just with an eye on time um"
      },
      {
        "start": 1361.88,
        "duration": 4.0,
        "text": "but I do want to say that that the both"
      },
      {
        "start": 1364.159,
        "duration": 4.321,
        "text": "the you know that that that carier with"
      },
      {
        "start": 1365.88,
        "duration": 5.6,
        "text": "the CF stats that was something that"
      },
      {
        "start": 1368.48,
        "duration": 5.96,
        "text": "that you taught me uh when you mentioned"
      },
      {
        "start": 1371.48,
        "duration": 5.92,
        "text": "it I was like really uh and and I"
      },
      {
        "start": 1374.44,
        "duration": 5.8,
        "text": "thought about it I'm like yeah that I"
      },
      {
        "start": 1377.4,
        "duration": 5.56,
        "text": "like from code I that makes sense I'm"
      },
      {
        "start": 1380.24,
        "duration": 5.439,
        "text": "like and that's kind of embarrassing why"
      },
      {
        "start": 1382.96,
        "duration": 6.04,
        "text": "why haven't we fixed that yet um that's"
      },
      {
        "start": 1385.679,
        "duration": 5.36,
        "text": "that's the way things are so thank you I"
      },
      {
        "start": 1389.0,
        "duration": 4.159,
        "text": "I had the same uh I had the same as you"
      },
      {
        "start": 1391.039,
        "duration": 5.12,
        "text": "when I was looking at that him DP with"
      },
      {
        "start": 1393.159,
        "duration": 4.841,
        "text": "four gig of data on a single R partition"
      },
      {
        "start": 1396.159,
        "duration": 3.561,
        "text": "and I was looking at my CM stat on the"
      },
      {
        "start": 1398.0,
        "duration": 4.559,
        "text": "other side and I had only one gig"
      },
      {
        "start": 1399.72,
        "duration": 5.839,
        "text": "showing in the CF stat I I did have to"
      },
      {
        "start": 1402.559,
        "duration": 4.761,
        "text": "go and dig uh and discuss around to"
      },
      {
        "start": 1405.559,
        "duration": 4.0,
        "text": "understand where that was coming from so"
      },
      {
        "start": 1407.32,
        "duration": 4.4,
        "text": "yeah he he come to surpris as you as it"
      },
      {
        "start": 1409.559,
        "duration": 6.72,
        "text": "came to"
      },
      {
        "start": 1411.72,
        "duration": 6.48,
        "text": "me one of those little hidden gems okay"
      },
      {
        "start": 1416.279,
        "duration": 6.081,
        "text": "moving on to"
      },
      {
        "start": 1418.2,
        "duration": 10.359,
        "text": "repairs uh Francesco you're our expert"
      },
      {
        "start": 1422.36,
        "duration": 9.439,
        "text": "here um do you w to uh do you w to tell"
      },
      {
        "start": 1428.559,
        "duration": 5.961,
        "text": "me about uh the problems that we hit"
      },
      {
        "start": 1431.799,
        "duration": 5.081,
        "text": "with large petitions with tombstones"
      },
      {
        "start": 1434.52,
        "duration": 4.48,
        "text": "with v noes with tables with dense nodes"
      },
      {
        "start": 1436.88,
        "duration": 6.08,
        "text": "at first line"
      },
      {
        "start": 1439.0,
        "duration": 7.32,
        "text": "um you can you can introduce repairs too"
      },
      {
        "start": 1442.96,
        "duration": 7.36,
        "text": "but I think we can jump into yes you can"
      },
      {
        "start": 1446.32,
        "duration": 5.32,
        "text": "go so repair repair is a is an amazing"
      },
      {
        "start": 1450.32,
        "duration": 5.08,
        "text": "background"
      },
      {
        "start": 1451.64,
        "duration": 5.56,
        "text": "service that it's a it's a misnomer"
      },
      {
        "start": 1455.4,
        "duration": 4.24,
        "text": "because it shouldn't be called repair"
      },
      {
        "start": 1457.2,
        "duration": 4.88,
        "text": "because repair gives a a wrong idea"
      },
      {
        "start": 1459.64,
        "duration": 6.039,
        "text": "about what it what it is it's not it's"
      },
      {
        "start": 1462.08,
        "duration": 6.76,
        "text": "not repairing anything is it's ensuring"
      },
      {
        "start": 1465.679,
        "duration": 5.521,
        "text": "that data is consistent in addition bued"
      },
      {
        "start": 1468.84,
        "duration": 4.199,
        "text": "environment like"
      },
      {
        "start": 1471.2,
        "duration": 5.64,
        "text": "capja"
      },
      {
        "start": 1473.039,
        "duration": 7.0,
        "text": "um cap and all the all the"
      },
      {
        "start": 1476.84,
        "duration": 8.12,
        "text": "rest we are"
      },
      {
        "start": 1480.039,
        "duration": 8.681,
        "text": "trading immediate consistency for for"
      },
      {
        "start": 1484.96,
        "duration": 7.839,
        "text": "Speed and availability"
      },
      {
        "start": 1488.72,
        "duration": 7.439,
        "text": "so in Canda we want the database to be"
      },
      {
        "start": 1492.799,
        "duration": 6.88,
        "text": "extremely fast and we've got some"
      },
      {
        "start": 1496.159,
        "duration": 7.201,
        "text": "amazing examples or very recent example"
      },
      {
        "start": 1499.679,
        "duration": 8.841,
        "text": "of our customers using big cundra"
      },
      {
        "start": 1503.36,
        "duration": 10.08,
        "text": "cluster reaching levels of four four"
      },
      {
        "start": 1508.52,
        "duration": 6.44,
        "text": "Millions request per second 4.5 million"
      },
      {
        "start": 1513.44,
        "duration": 4.8,
        "text": "request per second"
      },
      {
        "start": 1514.96,
        "duration": 6.64,
        "text": "consistently so K is super fast when it"
      },
      {
        "start": 1518.24,
        "duration": 5.439,
        "text": "comes to to speed is super fast when it"
      },
      {
        "start": 1521.6,
        "duration": 5.4,
        "text": "comes to availability it is available"
      },
      {
        "start": 1523.679,
        "duration": 6.48,
        "text": "always available but when it comes to"
      },
      {
        "start": 1527.0,
        "duration": 6.72,
        "text": "consistency it will not be immediately"
      },
      {
        "start": 1530.159,
        "duration": 6.041,
        "text": "consistent so that's what repair is"
      },
      {
        "start": 1533.72,
        "duration": 4.0,
        "text": "repair is a is a is a background process"
      },
      {
        "start": 1536.2,
        "duration": 6.32,
        "text": "that will"
      },
      {
        "start": 1537.72,
        "duration": 6.76,
        "text": "ensure consistency what what it does"
      },
      {
        "start": 1542.52,
        "duration": 6.48,
        "text": "is it"
      },
      {
        "start": 1544.48,
        "duration": 8.319,
        "text": "will go Partition by partition node by"
      },
      {
        "start": 1549.0,
        "duration": 7.36,
        "text": "node and will the nodes what's your"
      },
      {
        "start": 1552.799,
        "duration": 5.281,
        "text": "what's your version of the of this piece"
      },
      {
        "start": 1556.36,
        "duration": 3.439,
        "text": "of data what's your version what's your"
      },
      {
        "start": 1558.08,
        "duration": 4.28,
        "text": "and then it will put them together and"
      },
      {
        "start": 1559.799,
        "duration": 4.921,
        "text": "stream and and"
      },
      {
        "start": 1562.36,
        "duration": 4.08,
        "text": "compare the data of one node and the"
      },
      {
        "start": 1564.72,
        "duration": 4.16,
        "text": "other no and all the three replicas and"
      },
      {
        "start": 1566.44,
        "duration": 3.52,
        "text": "then stream the data across so now it's"
      },
      {
        "start": 1568.88,
        "duration": 6.08,
        "text": "a"
      },
      {
        "start": 1569.96,
        "duration": 9.719,
        "text": "very a crucial service but it does a lot"
      },
      {
        "start": 1574.96,
        "duration": 7.719,
        "text": "of work and it's got it has got some"
      },
      {
        "start": 1579.679,
        "duration": 8.841,
        "text": "impact and is very"
      },
      {
        "start": 1582.679,
        "duration": 8.321,
        "text": "sensitive yeah we often say customers uh"
      },
      {
        "start": 1588.52,
        "duration": 5.879,
        "text": "um we often see customers either not"
      },
      {
        "start": 1591.0,
        "duration": 5.96,
        "text": "doing repairs at all um or they come in"
      },
      {
        "start": 1594.399,
        "duration": 5.961,
        "text": "and they do repairs they don't quite"
      },
      {
        "start": 1596.96,
        "duration": 6.0,
        "text": "understand the the nuances what the"
      },
      {
        "start": 1600.36,
        "duration": 3.919,
        "text": "mechanism is is really about and so"
      },
      {
        "start": 1602.96,
        "duration": 4.839,
        "text": "they're just trying to repair everything"
      },
      {
        "start": 1604.279,
        "duration": 5.361,
        "text": "in a kind of uh without tuning it and"
      },
      {
        "start": 1607.799,
        "duration": 5.041,
        "text": "they can't get the repairs done in a"
      },
      {
        "start": 1609.64,
        "duration": 5.639,
        "text": "week as you exactly what you say it's"
      },
      {
        "start": 1612.84,
        "duration": 4.64,
        "text": "it's a heavy background process that's"
      },
      {
        "start": 1615.279,
        "duration": 4.12,
        "text": "true we we've seen everything in in"
      },
      {
        "start": 1617.48,
        "duration": 5.04,
        "text": "support about about repair and repair"
      },
      {
        "start": 1619.399,
        "duration": 7.201,
        "text": "has been there for since the beginning"
      },
      {
        "start": 1622.52,
        "duration": 8.84,
        "text": "of Cassandra so and in a way it"
      },
      {
        "start": 1626.6,
        "duration": 7.199,
        "text": "hasn't evolved a lot in the past up"
      },
      {
        "start": 1631.36,
        "duration": 6.24,
        "text": "until version"
      },
      {
        "start": 1633.799,
        "duration": 6.76,
        "text": "four Invasion for the clear improvements"
      },
      {
        "start": 1637.6,
        "duration": 4.679,
        "text": "we will talk about them later so repair"
      },
      {
        "start": 1640.559,
        "duration": 4.801,
        "text": "is very sensitive"
      },
      {
        "start": 1642.279,
        "duration": 5.601,
        "text": "to different things it's very sensitive"
      },
      {
        "start": 1645.36,
        "duration": 5.199,
        "text": "to the amount of vodes you're using the"
      },
      {
        "start": 1647.88,
        "duration": 4.84,
        "text": "amount of the large partitions that we"
      },
      {
        "start": 1650.559,
        "duration": 6.6,
        "text": "were talking about before it's it's very"
      },
      {
        "start": 1652.72,
        "duration": 7.72,
        "text": "sensitive to the total amount of data um"
      },
      {
        "start": 1657.159,
        "duration": 5.561,
        "text": "in the cluster on the nodes"
      },
      {
        "start": 1660.44,
        "duration": 4.88,
        "text": "so the"
      },
      {
        "start": 1662.72,
        "duration": 5.079,
        "text": "old what what they were called anti"
      },
      {
        "start": 1665.32,
        "duration": 5.839,
        "text": "patterns Cassandra anti patterns they"
      },
      {
        "start": 1667.799,
        "duration": 7.521,
        "text": "are very significant they are very"
      },
      {
        "start": 1671.159,
        "duration": 7.441,
        "text": "impactful for repair and repair needs to"
      },
      {
        "start": 1675.32,
        "duration": 6.239,
        "text": "run quickly it needs to be consistent it"
      },
      {
        "start": 1678.6,
        "duration": 4.919,
        "text": "needs to be predictable so it's very"
      },
      {
        "start": 1681.559,
        "duration": 6.321,
        "text": "important because otherwise there there"
      },
      {
        "start": 1683.519,
        "duration": 6.241,
        "text": "are other issues in in the data so the"
      },
      {
        "start": 1687.88,
        "duration": 6.08,
        "text": "data will not be consistent eventually"
      },
      {
        "start": 1689.76,
        "duration": 8.639,
        "text": "so if repair needs to be guaranteed a"
      },
      {
        "start": 1693.96,
        "duration": 7.4,
        "text": "nice and easy life but there are many"
      },
      {
        "start": 1698.399,
        "duration": 5.721,
        "text": "different things that so V nodes a"
      },
      {
        "start": 1701.36,
        "duration": 4.72,
        "text": "cluster a Cassandra cluster with a very"
      },
      {
        "start": 1704.12,
        "duration": 6.24,
        "text": "high number of vodes"
      },
      {
        "start": 1706.08,
        "duration": 7.439,
        "text": "will will make sub range"
      },
      {
        "start": 1710.36,
        "duration": 6.799,
        "text": "repair generate an amazing amount of"
      },
      {
        "start": 1713.519,
        "duration": 8.601,
        "text": "tasks and sub ranges that eventually it"
      },
      {
        "start": 1717.159,
        "duration": 7.921,
        "text": "can become a problem uh large partitions"
      },
      {
        "start": 1722.12,
        "duration": 6.64,
        "text": "tombstones are"
      },
      {
        "start": 1725.08,
        "duration": 5.56,
        "text": "all kind of roadblocks for repair"
      },
      {
        "start": 1728.76,
        "duration": 4.799,
        "text": "because they they are they fall in the"
      },
      {
        "start": 1730.64,
        "duration": 6.68,
        "text": "read path and and repair also uses the"
      },
      {
        "start": 1733.559,
        "duration": 6.521,
        "text": "read path So reading a a very large"
      },
      {
        "start": 1737.32,
        "duration": 6.92,
        "text": "partition with a lot of De and Deb"
      },
      {
        "start": 1740.08,
        "duration": 10.199,
        "text": "partition it will will slow down repair"
      },
      {
        "start": 1744.24,
        "duration": 10.36,
        "text": "tombstones will slow down repair um uh a"
      },
      {
        "start": 1750.279,
        "duration": 6.961,
        "text": "lot of data will make repair go above"
      },
      {
        "start": 1754.6,
        "duration": 7.079,
        "text": "again slow down repair and go above the"
      },
      {
        "start": 1757.24,
        "duration": 7.84,
        "text": "threshold of the the very famous a very"
      },
      {
        "start": 1761.679,
        "duration": 4.6,
        "text": "important GC grace period GC Grace"
      },
      {
        "start": 1765.08,
        "duration": 5.76,
        "text": "second"
      },
      {
        "start": 1766.279,
        "duration": 5.841,
        "text": "so repair is a very tricky therapist"
      },
      {
        "start": 1770.84,
        "duration": 3.36,
        "text": "very"
      },
      {
        "start": 1772.12,
        "duration": 6.039,
        "text": "crucial"
      },
      {
        "start": 1774.2,
        "duration": 6.56,
        "text": "and the the user needs to be to to"
      },
      {
        "start": 1778.159,
        "duration": 7.321,
        "text": "monitor that needs to ensure that repair"
      },
      {
        "start": 1780.76,
        "duration": 6.56,
        "text": "has got an easy life within the time of"
      },
      {
        "start": 1785.48,
        "duration": 6.6,
        "text": "GC gra"
      },
      {
        "start": 1787.32,
        "duration": 7.239,
        "text": "seconds so and so with uh what changed"
      },
      {
        "start": 1792.08,
        "duration": 6.199,
        "text": "in Cassandra 4 there's a few things that"
      },
      {
        "start": 1794.559,
        "duration": 7.0,
        "text": "changed in Cassandra 4 and uh we're"
      },
      {
        "start": 1798.279,
        "duration": 6.76,
        "text": "seeing people kind of having to learn"
      },
      {
        "start": 1801.559,
        "duration": 5.081,
        "text": "new things and trip up yes uh on newer"
      },
      {
        "start": 1805.039,
        "duration": 4.24,
        "text": "versions of"
      },
      {
        "start": 1806.64,
        "duration": 6.08,
        "text": "Cassandra yes pentra"
      },
      {
        "start": 1809.279,
        "duration": 8.921,
        "text": "4 brings back an"
      },
      {
        "start": 1812.72,
        "duration": 6.6,
        "text": "old an old uh um kind of an old project"
      },
      {
        "start": 1818.2,
        "duration": 3.199,
        "text": "but an old"
      },
      {
        "start": 1819.32,
        "duration": 7.32,
        "text": "repair repair"
      },
      {
        "start": 1821.399,
        "duration": 9.76,
        "text": "was um was born for to to repair the the"
      },
      {
        "start": 1826.64,
        "duration": 7.32,
        "text": "entire amount of dat in the cluster and"
      },
      {
        "start": 1831.159,
        "duration": 7.281,
        "text": "initially in version two and version"
      },
      {
        "start": 1833.96,
        "duration": 9.199,
        "text": "three incremental repair was brought in"
      },
      {
        "start": 1838.44,
        "duration": 7.2,
        "text": "and it was set of default as well now"
      },
      {
        "start": 1843.159,
        "duration": 6.321,
        "text": "increment so incremental repair as"
      },
      {
        "start": 1845.64,
        "duration": 6.24,
        "text": "opposed to uh range or sub range"
      },
      {
        "start": 1849.48,
        "duration": 8.52,
        "text": "repair"
      },
      {
        "start": 1851.88,
        "duration": 7.519,
        "text": "will try will different will Mark SS"
      },
      {
        "start": 1858.0,
        "duration": 4.2,
        "text": "tabl will Mark"
      },
      {
        "start": 1859.399,
        "duration": 5.801,
        "text": "partitions as repair SS table as"
      },
      {
        "start": 1862.2,
        "duration": 5.959,
        "text": "repaired so when you run incremental"
      },
      {
        "start": 1865.2,
        "duration": 4.959,
        "text": "repair it will break down the assess"
      },
      {
        "start": 1868.159,
        "duration": 4.801,
        "text": "tables will anti- compact the asss"
      },
      {
        "start": 1870.159,
        "duration": 6.081,
        "text": "tables repair a number of partition and"
      },
      {
        "start": 1872.96,
        "duration": 6.24,
        "text": "the number of ranges and create two"
      },
      {
        "start": 1876.24,
        "duration": 6.24,
        "text": "different SS tables one with repaired"
      },
      {
        "start": 1879.2,
        "duration": 7.64,
        "text": "data and one with unrepaired data and"
      },
      {
        "start": 1882.48,
        "duration": 7.24,
        "text": "mark the repair data with a repaired app"
      },
      {
        "start": 1886.84,
        "duration": 5.839,
        "text": "property in the in"
      },
      {
        "start": 1889.72,
        "duration": 6.24,
        "text": "their yeah so the is"
      },
      {
        "start": 1892.679,
        "duration": 5.201,
        "text": "good yeah I think one of the challenges"
      },
      {
        "start": 1895.96,
        "duration": 5.04,
        "text": "that people have with it"
      },
      {
        "start": 1897.88,
        "duration": 5.48,
        "text": "is you know how these two what what you"
      },
      {
        "start": 1901.0,
        "duration": 4.519,
        "text": "said you have these two sets of of ss"
      },
      {
        "start": 1903.36,
        "duration": 6.4,
        "text": "tables now from through through the lens"
      },
      {
        "start": 1905.519,
        "duration": 6.721,
        "text": "of repair and unrepaired and a repaired"
      },
      {
        "start": 1909.76,
        "duration": 5.68,
        "text": "and uh so what repairs are actually"
      },
      {
        "start": 1912.24,
        "duration": 8.08,
        "text": "doing is they're doing twice as much"
      },
      {
        "start": 1915.44,
        "duration": 7.44,
        "text": "work um and they be a a a amplification"
      },
      {
        "start": 1920.32,
        "duration": 5.12,
        "text": "that's happening there but the"
      },
      {
        "start": 1922.88,
        "duration": 7.36,
        "text": "incremental approach means that you can"
      },
      {
        "start": 1925.44,
        "duration": 7.959,
        "text": "kind of just do the Deltas so the idea"
      },
      {
        "start": 1930.24,
        "duration": 5.52,
        "text": "is that you even if the cost is double"
      },
      {
        "start": 1933.399,
        "duration": 4.12,
        "text": "you chunking it down into smaller sizes"
      },
      {
        "start": 1935.76,
        "duration": 4.879,
        "text": "and not having to worry about older"
      },
      {
        "start": 1937.519,
        "duration": 6.04,
        "text": "stuff the idea works well it doesn't"
      },
      {
        "start": 1940.639,
        "duration": 5.961,
        "text": "always work in practice it depends on a"
      },
      {
        "start": 1943.559,
        "duration": 4.801,
        "text": "few things so understanding that is is"
      },
      {
        "start": 1946.6,
        "duration": 5.559,
        "text": "important um"
      },
      {
        "start": 1948.36,
        "duration": 6.36,
        "text": "yes Francisco yeah if I may in a in a in"
      },
      {
        "start": 1952.159,
        "duration": 5.64,
        "text": "an Ideal World you wouldn't need repair"
      },
      {
        "start": 1954.72,
        "duration": 5.24,
        "text": "but the the the way Cassandra works is"
      },
      {
        "start": 1957.799,
        "duration": 4.561,
        "text": "try to give you just High availability"
      },
      {
        "start": 1959.96,
        "duration": 5.0,
        "text": "and by doing so will sometimes allow a"
      },
      {
        "start": 1962.36,
        "duration": 4.159,
        "text": "right to not make it to let's say the"
      },
      {
        "start": 1964.96,
        "duration": 4.4,
        "text": "third node in your environment because"
      },
      {
        "start": 1966.519,
        "duration": 4.841,
        "text": "you requested a right at local Quorum it"
      },
      {
        "start": 1969.36,
        "duration": 4.12,
        "text": "would normally do this so the three"
      },
      {
        "start": 1971.36,
        "duration": 4.199,
        "text": "nodes would receive the rights uh even"
      },
      {
        "start": 1973.48,
        "duration": 3.84,
        "text": "if you request a local Quorum so in an"
      },
      {
        "start": 1975.559,
        "duration": 2.96,
        "text": "Ideal World if your cluster is always"
      },
      {
        "start": 1977.32,
        "duration": 4.12,
        "text": "healthy"
      },
      {
        "start": 1978.519,
        "duration": 6.361,
        "text": "there will not be a need to run repair"
      },
      {
        "start": 1981.44,
        "duration": 5.52,
        "text": "but the issue is it it will happen at"
      },
      {
        "start": 1984.88,
        "duration": 3.88,
        "text": "one point or another that there's for a"
      },
      {
        "start": 1986.96,
        "duration": 3.4,
        "text": "reason a system that goes down some"
      },
      {
        "start": 1988.76,
        "duration": 3.639,
        "text": "nodes which are actually not responding"
      },
      {
        "start": 1990.36,
        "duration": 4.679,
        "text": "in time because we mentioned some"
      },
      {
        "start": 1992.399,
        "duration": 4.24,
        "text": "scenarios where this can happen and and"
      },
      {
        "start": 1995.039,
        "duration": 4.281,
        "text": "when that happen you want to make sure"
      },
      {
        "start": 1996.639,
        "duration": 5.801,
        "text": "the data gets resynchronized across the"
      },
      {
        "start": 1999.32,
        "duration": 5.4,
        "text": "across the board and that's where so I"
      },
      {
        "start": 2002.44,
        "duration": 5.079,
        "text": "am with that it should be called"
      },
      {
        "start": 2004.72,
        "duration": 5.0,
        "text": "resynchronization rather than repair"
      },
      {
        "start": 2007.519,
        "duration": 5.52,
        "text": "it's help with read"
      },
      {
        "start": 2009.72,
        "duration": 8.48,
        "text": "latency because normally you would you"
      },
      {
        "start": 2013.039,
        "duration": 7.88,
        "text": "want to uh uh you inforce uh consistency"
      },
      {
        "start": 2018.2,
        "duration": 5.959,
        "text": "at request level anyway so it it could"
      },
      {
        "start": 2020.919,
        "duration": 6.921,
        "text": "be right right level consistency or read"
      },
      {
        "start": 2024.159,
        "duration": 6.841,
        "text": "level consisten so the data should be"
      },
      {
        "start": 2027.84,
        "duration": 6.319,
        "text": "fairly fairly consistent already but"
      },
      {
        "start": 2031.0,
        "duration": 7.48,
        "text": "this depends really on the type of the"
      },
      {
        "start": 2034.159,
        "duration": 8.4,
        "text": "use case for very fast rise very mean"
      },
      {
        "start": 2038.48,
        "duration": 5.28,
        "text": "repair is always there for for ensuring"
      },
      {
        "start": 2042.559,
        "duration": 4.24,
        "text": "a"
      },
      {
        "start": 2043.76,
        "duration": 6.48,
        "text": "consistency in the end but yes uh like"
      },
      {
        "start": 2046.799,
        "duration": 6.04,
        "text": "you were saying Mick um incremental"
      },
      {
        "start": 2050.24,
        "duration": 7.119,
        "text": "repair was a great invention because it"
      },
      {
        "start": 2052.839,
        "duration": 8.8,
        "text": "it allows to to do repair"
      },
      {
        "start": 2057.359,
        "duration": 8.361,
        "text": "only um Deltas or increment increments"
      },
      {
        "start": 2061.639,
        "duration": 6.2,
        "text": "but yes the implementation then um"
      },
      {
        "start": 2065.72,
        "duration": 6.24,
        "text": "didn't"
      },
      {
        "start": 2067.839,
        "duration": 6.161,
        "text": "didn't provide the the expected uh"
      },
      {
        "start": 2071.96,
        "duration": 5.32,
        "text": "performance in"
      },
      {
        "start": 2074.0,
        "duration": 5.04,
        "text": "particular because"
      },
      {
        "start": 2077.28,
        "duration": 3.48,
        "text": "um"
      },
      {
        "start": 2079.04,
        "duration": 7.319,
        "text": "with"
      },
      {
        "start": 2080.76,
        "duration": 7.04,
        "text": "um there there's a risk of some some SS"
      },
      {
        "start": 2086.359,
        "duration": 6.401,
        "text": "Stables not"
      },
      {
        "start": 2087.8,
        "duration": 8.279,
        "text": "being included into the repair uh if"
      },
      {
        "start": 2092.76,
        "duration": 5.96,
        "text": "they uh happen if the the streaming"
      },
      {
        "start": 2096.079,
        "duration": 7.04,
        "text": "happens after the medle tree has been"
      },
      {
        "start": 2098.72,
        "duration": 9.32,
        "text": "built so there are some some edge cases"
      },
      {
        "start": 2103.119,
        "duration": 7.561,
        "text": "some scenarios where you can in fact"
      },
      {
        "start": 2108.04,
        "duration": 3.799,
        "text": "uh remain with some data that is not"
      },
      {
        "start": 2110.68,
        "duration": 8.2,
        "text": "completely"
      },
      {
        "start": 2111.839,
        "duration": 11.561,
        "text": "repaired uh the version four Cassandra 4"
      },
      {
        "start": 2118.88,
        "duration": 6.32,
        "text": "has has completely completely reworked"
      },
      {
        "start": 2123.4,
        "duration": 4.04,
        "text": "the incremental"
      },
      {
        "start": 2125.2,
        "duration": 6.04,
        "text": "repair fixed and fixed a lot of those"
      },
      {
        "start": 2127.44,
        "duration": 8.32,
        "text": "bugs yes fixed a lot of bugs"
      },
      {
        "start": 2131.24,
        "duration": 6.92,
        "text": "and changed the way the Merle tree and"
      },
      {
        "start": 2135.76,
        "duration": 7.28,
        "text": "the time the tree is"
      },
      {
        "start": 2138.16,
        "duration": 7.919,
        "text": "created um in this way it would the the"
      },
      {
        "start": 2143.04,
        "duration": 7.2,
        "text": "um because the antic compaction was was"
      },
      {
        "start": 2146.079,
        "duration": 7.361,
        "text": "also a major uh overhead for incremental"
      },
      {
        "start": 2150.24,
        "duration": 6.48,
        "text": "repair compacting an anti- compacting"
      },
      {
        "start": 2153.44,
        "duration": 7.08,
        "text": "all the times and for low systems"
      },
      {
        "start": 2156.72,
        "duration": 8.2,
        "text": "systems with L discs that would already"
      },
      {
        "start": 2160.52,
        "duration": 6.72,
        "text": "be uh impactful so"
      },
      {
        "start": 2164.92,
        "duration": 5.76,
        "text": "um in this"
      },
      {
        "start": 2167.24,
        "duration": 6.64,
        "text": "way uh anti compaction in Cassandra 4"
      },
      {
        "start": 2170.68,
        "duration": 7.439,
        "text": "anti compaction is done before the meal"
      },
      {
        "start": 2173.88,
        "duration": 8.239,
        "text": "tree is created so in this way the Merle"
      },
      {
        "start": 2178.119,
        "duration": 7.041,
        "text": "tree will contain the data from all the"
      },
      {
        "start": 2182.119,
        "duration": 6.321,
        "text": "assess tables that are not prepared and"
      },
      {
        "start": 2185.16,
        "duration": 6.24,
        "text": "then the m is exchanged and compar"
      },
      {
        "start": 2188.44,
        "duration": 6.28,
        "text": "amongst nodes and eventually the"
      },
      {
        "start": 2191.4,
        "duration": 7.0,
        "text": "streaming happens so candra 4 brings"
      },
      {
        "start": 2194.72,
        "duration": 6.84,
        "text": "back incremental repair it's the default"
      },
      {
        "start": 2198.4,
        "duration": 4.76,
        "text": "and it has completely changed with"
      },
      {
        "start": 2201.56,
        "duration": 6.6,
        "text": "respect to version three so it's"
      },
      {
        "start": 2203.16,
        "duration": 7.919,
        "text": "probably the best way to to do um repair"
      },
      {
        "start": 2208.16,
        "duration": 6.0,
        "text": "now and also the entire just to finish"
      },
      {
        "start": 2211.079,
        "duration": 7.961,
        "text": "the entire repair"
      },
      {
        "start": 2214.16,
        "duration": 8.76,
        "text": "task so the each repair task is enclosed"
      },
      {
        "start": 2219.04,
        "duration": 6.319,
        "text": "is wrapped into a a paxos"
      },
      {
        "start": 2222.92,
        "duration": 4.84,
        "text": "transaction paxos also has been"
      },
      {
        "start": 2225.359,
        "duration": 3.76,
        "text": "completely reworked version four we're"
      },
      {
        "start": 2227.76,
        "duration": 4.96,
        "text": "not going to talk about that because"
      },
      {
        "start": 2229.119,
        "duration": 6.881,
        "text": "it's it's a big thing but the"
      },
      {
        "start": 2232.72,
        "duration": 6.8,
        "text": "entire repair task is a transaction now"
      },
      {
        "start": 2236.0,
        "duration": 5.839,
        "text": "so it will if it doesn't complete"
      },
      {
        "start": 2239.52,
        "duration": 5.36,
        "text": "completely it will fail it will roll"
      },
      {
        "start": 2241.839,
        "duration": 8.0,
        "text": "back it's a it's it's able to roll back"
      },
      {
        "start": 2244.88,
        "duration": 7.68,
        "text": "so yeah increment repair is is pretty"
      },
      {
        "start": 2249.839,
        "duration": 4.881,
        "text": "good I think we're g to keep moving uh"
      },
      {
        "start": 2252.56,
        "duration": 5.72,
        "text": "we do got some questions Landing U one"
      },
      {
        "start": 2254.72,
        "duration": 5.92,
        "text": "of the questions uh is about repairs but"
      },
      {
        "start": 2258.28,
        "duration": 5.68,
        "text": "we'll tackle that at the end and uh"
      },
      {
        "start": 2260.64,
        "duration": 8.12,
        "text": "there's another question about uh"
      },
      {
        "start": 2263.96,
        "duration": 6.96,
        "text": "oltp uh and olap systems uh that will"
      },
      {
        "start": 2268.76,
        "duration": 4.24,
        "text": "touch on some of this as well so we see"
      },
      {
        "start": 2270.92,
        "duration": 7.32,
        "text": "the questions we we'll get we'll get to"
      },
      {
        "start": 2273.0,
        "duration": 9.24,
        "text": "you uh jumping on to the next topic jbod"
      },
      {
        "start": 2278.24,
        "duration": 7.76,
        "text": "and lvm I'm really happy Roman that you"
      },
      {
        "start": 2282.24,
        "duration": 6.24,
        "text": "brought this one up because uh it's also"
      },
      {
        "start": 2286.0,
        "duration": 5.24,
        "text": "one of my favorite uh topics having"
      },
      {
        "start": 2288.48,
        "duration": 6.56,
        "text": "worked at the consultant the last pickle"
      },
      {
        "start": 2291.24,
        "duration": 5.4,
        "text": "for five years um you know jbod was one"
      },
      {
        "start": 2295.04,
        "duration": 4.96,
        "text": "of those things we just wanted to throw"
      },
      {
        "start": 2296.64,
        "duration": 4.8,
        "text": "throw out the window um so so tell us a"
      },
      {
        "start": 2300.0,
        "duration": 6.44,
        "text": "bit more about"
      },
      {
        "start": 2301.44,
        "duration": 8.28,
        "text": "this um yeah so the thing is that jbud"
      },
      {
        "start": 2306.44,
        "duration": 5.52,
        "text": "uh JB look looks great on the paper when"
      },
      {
        "start": 2309.72,
        "duration": 5.0,
        "text": "you when you first look at it and and"
      },
      {
        "start": 2311.96,
        "duration": 5.84,
        "text": "I've I've had my share of uh my share of"
      },
      {
        "start": 2314.72,
        "duration": 5.04,
        "text": "trouble with it as well um so looks"
      },
      {
        "start": 2317.8,
        "duration": 4.039,
        "text": "great on this just just quickly describe"
      },
      {
        "start": 2319.76,
        "duration": 3.359,
        "text": "because it may not everyone or Cassandra"
      },
      {
        "start": 2321.839,
        "duration": 3.201,
        "text": "operators may not understand what we"
      },
      {
        "start": 2323.119,
        "duration": 4.321,
        "text": "mean by of course of course JB but just"
      },
      {
        "start": 2325.04,
        "duration": 4.64,
        "text": "a bunch of dis uh is actually a simple"
      },
      {
        "start": 2327.44,
        "duration": 4.44,
        "text": "way to set up your environment and say I"
      },
      {
        "start": 2329.68,
        "duration": 4.8,
        "text": "have six discs and I'm going to create"
      },
      {
        "start": 2331.88,
        "duration": 5.68,
        "text": "six directories and in the data file"
      },
      {
        "start": 2334.48,
        "duration": 5.839,
        "text": "directories of gas and Dro you just set"
      },
      {
        "start": 2337.56,
        "duration": 5.2,
        "text": "up those six discs to be used by your"
      },
      {
        "start": 2340.319,
        "duration": 5.28,
        "text": "environment okay so so the Cassandra"
      },
      {
        "start": 2342.76,
        "duration": 5.599,
        "text": "configuration the yam is actually"
      },
      {
        "start": 2345.599,
        "duration": 4.601,
        "text": "explicitly pointing to each separate dis"
      },
      {
        "start": 2348.359,
        "duration": 4.441,
        "text": "absolutely each volume is separately"
      },
      {
        "start": 2350.2,
        "duration": 5.04,
        "text": "notified and so you can you can just"
      },
      {
        "start": 2352.8,
        "duration": 5.64,
        "text": "when you add a new dis put a put the"
      },
      {
        "start": 2355.24,
        "duration": 6.8,
        "text": "seven pass in there so on paper it looks"
      },
      {
        "start": 2358.44,
        "duration": 7.12,
        "text": "easy to administer uh but the reality of"
      },
      {
        "start": 2362.04,
        "duration": 8.0,
        "text": "it is quite more complex than that first"
      },
      {
        "start": 2365.56,
        "duration": 7.48,
        "text": "of all jbod act as a kind of subvod so"
      },
      {
        "start": 2370.04,
        "duration": 6.0,
        "text": "it it will split your ranges into"
      },
      {
        "start": 2373.04,
        "duration": 6.559,
        "text": "specific discs so you end up multiplying"
      },
      {
        "start": 2376.04,
        "duration": 5.16,
        "text": "that there's another uh nasty effect"
      },
      {
        "start": 2379.599,
        "duration": 3.24,
        "text": "I've I've realized over the course of"
      },
      {
        "start": 2381.2,
        "duration": 3.6,
        "text": "the years when it comes to it is I"
      },
      {
        "start": 2382.839,
        "duration": 5.081,
        "text": "mentioned the file descriptors so the"
      },
      {
        "start": 2384.8,
        "duration": 5.799,
        "text": "thing is that when you normally write"
      },
      {
        "start": 2387.92,
        "duration": 4.919,
        "text": "let's say uh your data into your"
      },
      {
        "start": 2390.599,
        "duration": 5.0,
        "text": "environment you would write you would"
      },
      {
        "start": 2392.839,
        "duration": 6.401,
        "text": "flush one ss table except that when you"
      },
      {
        "start": 2395.599,
        "duration": 6.121,
        "text": "introduce jbod uh I me six discs uh so"
      },
      {
        "start": 2399.24,
        "duration": 6.0,
        "text": "if you have six discs at that point you"
      },
      {
        "start": 2401.72,
        "duration": 5.04,
        "text": "will flush six SS tables instead and so"
      },
      {
        "start": 2405.24,
        "duration": 3.48,
        "text": "you're multiplying the number of file"
      },
      {
        "start": 2406.76,
        "duration": 5.839,
        "text": "descriptors in your environment by the"
      },
      {
        "start": 2408.72,
        "duration": 6.119,
        "text": "number of jbot discs you have uh you"
      },
      {
        "start": 2412.599,
        "duration": 4.081,
        "text": "will hit the no files limit of your"
      },
      {
        "start": 2414.839,
        "duration": 5.0,
        "text": "kernel if you haven't implemented the"
      },
      {
        "start": 2416.68,
        "duration": 6.0,
        "text": "best practices the default kernel no"
      },
      {
        "start": 2419.839,
        "duration": 5.161,
        "text": "files is generally at 100,000 rest"
      },
      {
        "start": 2422.68,
        "duration": 5.399,
        "text": "assured that if you write a lot in your"
      },
      {
        "start": 2425.0,
        "duration": 6.319,
        "text": "environment you will blow that number up"
      },
      {
        "start": 2428.079,
        "duration": 5.641,
        "text": "at some point with a jbod implementation"
      },
      {
        "start": 2431.319,
        "duration": 4.241,
        "text": "because as I said it acts as a direct"
      },
      {
        "start": 2433.72,
        "duration": 4.2,
        "text": "multiplicator on the number of five"
      },
      {
        "start": 2435.56,
        "duration": 5.36,
        "text": "descriptors you have in to connect to"
      },
      {
        "start": 2437.92,
        "duration": 5.08,
        "text": "the SS tables because Cassandra has uh"
      },
      {
        "start": 2440.92,
        "duration": 4.48,
        "text": "will have a f descriptors open for every"
      },
      {
        "start": 2443.0,
        "duration": 5.16,
        "text": "SS table in your data"
      },
      {
        "start": 2445.4,
        "duration": 5.32,
        "text": "directory so this is this is quite often"
      },
      {
        "start": 2448.16,
        "duration": 5.36,
        "text": "there's the breaking point cluster crash"
      },
      {
        "start": 2450.72,
        "duration": 5.639,
        "text": "because or a node crash because the no"
      },
      {
        "start": 2453.52,
        "duration": 4.36,
        "text": "files limit has been hit so that we"
      },
      {
        "start": 2456.359,
        "duration": 3.601,
        "text": "we'll push that as possible of the best"
      },
      {
        "start": 2457.88,
        "duration": 4.959,
        "text": "practices information it's already"
      },
      {
        "start": 2459.96,
        "duration": 5.359,
        "text": "documented in a fair few places but"
      },
      {
        "start": 2462.839,
        "duration": 4.801,
        "text": "something to keep in mind um the other"
      },
      {
        "start": 2465.319,
        "duration": 4.28,
        "text": "is quite it is quite tricky I'm going to"
      },
      {
        "start": 2467.64,
        "duration": 5.92,
        "text": "ask you to to to"
      },
      {
        "start": 2469.599,
        "duration": 8.161,
        "text": "to keep moving yeah absolutely so so why"
      },
      {
        "start": 2473.56,
        "duration": 5.92,
        "text": "lvm and and how do we do it um so lvm so"
      },
      {
        "start": 2477.76,
        "duration": 4.68,
        "text": "logical volume management is one of the"
      },
      {
        "start": 2479.48,
        "duration": 6.639,
        "text": "Linux solution uh which allows you to"
      },
      {
        "start": 2482.44,
        "duration": 5.52,
        "text": "group to bundle your discs together so"
      },
      {
        "start": 2486.119,
        "duration": 5.081,
        "text": "you just create one partition which"
      },
      {
        "start": 2487.96,
        "duration": 6.32,
        "text": "actually spreads across several dis the"
      },
      {
        "start": 2491.2,
        "duration": 5.8,
        "text": "con uh the con of it is just that you"
      },
      {
        "start": 2494.28,
        "duration": 6.12,
        "text": "lose a dis you lose your lvm but in the"
      },
      {
        "start": 2497.0,
        "duration": 5.599,
        "text": "reality of it while it looks bad you get"
      },
      {
        "start": 2500.4,
        "duration": 5.36,
        "text": "jbot gives you that false sense of I"
      },
      {
        "start": 2502.599,
        "duration": 4.921,
        "text": "still have my data but bringing the disc"
      },
      {
        "start": 2505.76,
        "duration": 4.68,
        "text": "back bringing a disc back in after you"
      },
      {
        "start": 2507.52,
        "duration": 4.4,
        "text": "lost a disc in jbod is a bit painful to"
      },
      {
        "start": 2510.44,
        "duration": 3.56,
        "text": "deal with you need to run around of"
      },
      {
        "start": 2511.92,
        "duration": 4.48,
        "text": "repair quite frankly there's a"
      },
      {
        "start": 2514.0,
        "duration": 4.56,
        "text": "replication factor in Cassandra for"
      },
      {
        "start": 2516.4,
        "duration": 4.88,
        "text": "higher availability fail over if you"
      },
      {
        "start": 2518.56,
        "duration": 5.039,
        "text": "lose a disc you lose the node it's not a"
      },
      {
        "start": 2521.28,
        "duration": 4.88,
        "text": "problem you get a new disc you rebuild"
      },
      {
        "start": 2523.599,
        "duration": 5.0,
        "text": "an lvm you re you reboot strap the n in"
      },
      {
        "start": 2526.16,
        "duration": 6.159,
        "text": "the cluster I I think that's a really"
      },
      {
        "start": 2528.599,
        "duration": 6.401,
        "text": "good point I think like it's a typical"
      },
      {
        "start": 2532.319,
        "duration": 5.04,
        "text": "when we say you just use lvm it makes"
      },
      {
        "start": 2535.0,
        "duration": 5.2,
        "text": "life simple people go oh but like I'm"
      },
      {
        "start": 2537.359,
        "duration": 6.2,
        "text": "not setting up a like a ra redundancy in"
      },
      {
        "start": 2540.2,
        "duration": 5.919,
        "text": "the lvm and if I lose a disc I lose that"
      },
      {
        "start": 2543.559,
        "duration": 4.28,
        "text": "whole thing um and they think that JB's"
      },
      {
        "start": 2546.119,
        "duration": 3.72,
        "text": "a better solution"
      },
      {
        "start": 2547.839,
        "duration": 5.48,
        "text": "and you know the the conversation always"
      },
      {
        "start": 2549.839,
        "duration": 7.48,
        "text": "comes back to but Cassandra was designed"
      },
      {
        "start": 2553.319,
        "duration": 6.161,
        "text": "for this you've got distributed replicas"
      },
      {
        "start": 2557.319,
        "duration": 6.121,
        "text": "the it's actually life is much easier"
      },
      {
        "start": 2559.48,
        "duration": 6.24,
        "text": "with just one whole entity going down uh"
      },
      {
        "start": 2563.44,
        "duration": 5.72,
        "text": "and being able to replace"
      },
      {
        "start": 2565.72,
        "duration": 5.839,
        "text": "it and The Balancing Act in jbod becomes"
      },
      {
        "start": 2569.16,
        "duration": 4.64,
        "text": "is sometimes extremely painful you can"
      },
      {
        "start": 2571.559,
        "duration": 5.04,
        "text": "end up with an imbalance of the discs"
      },
      {
        "start": 2573.8,
        "duration": 5.36,
        "text": "and fixing that is sometimes quite"
      },
      {
        "start": 2576.599,
        "duration": 5.52,
        "text": "tricky the the tool which is used for"
      },
      {
        "start": 2579.16,
        "duration": 5.04,
        "text": "that uh there's the not to relocate as a"
      },
      {
        "start": 2582.119,
        "duration": 6.601,
        "text": "tables doesn't always yield the results"
      },
      {
        "start": 2584.2,
        "duration": 6.56,
        "text": "you want so I would it's yeah it it"
      },
      {
        "start": 2588.72,
        "duration": 5.119,
        "text": "looks nice on paper but the reality is"
      },
      {
        "start": 2590.76,
        "duration": 4.88,
        "text": "an lvm is better and with a if you want"
      },
      {
        "start": 2593.839,
        "duration": 4.28,
        "text": "to keep the performance that's where we"
      },
      {
        "start": 2595.64,
        "duration": 4.679,
        "text": "recommend to use a striped lvm which"
      },
      {
        "start": 2598.119,
        "duration": 4.041,
        "text": "means your volume will actually split in"
      },
      {
        "start": 2600.319,
        "duration": 4.401,
        "text": "in small chunks across the different"
      },
      {
        "start": 2602.16,
        "duration": 5.04,
        "text": "discs so if you have six discs you will"
      },
      {
        "start": 2604.72,
        "duration": 4.44,
        "text": "have the performance of the disc"
      },
      {
        "start": 2607.2,
        "duration": 3.76,
        "text": "multiplied by six when you're actually"
      },
      {
        "start": 2609.16,
        "duration": 4.52,
        "text": "writing into the"
      },
      {
        "start": 2610.96,
        "duration": 4.44,
        "text": "discs as opposed to L one which fills"
      },
      {
        "start": 2613.68,
        "duration": 4.639,
        "text": "the first disc and then the second and"
      },
      {
        "start": 2615.4,
        "duration": 5.719,
        "text": "then the S yeah and that that can be"
      },
      {
        "start": 2618.319,
        "duration": 6.401,
        "text": "incredibly important uh if you are"
      },
      {
        "start": 2621.119,
        "duration": 6.521,
        "text": "running dense nodes so good advice I"
      },
      {
        "start": 2624.72,
        "duration": 5.92,
        "text": "love that we included this slide thank"
      },
      {
        "start": 2627.64,
        "duration": 6.64,
        "text": "you next up major"
      },
      {
        "start": 2630.64,
        "duration": 6.56,
        "text": "upgrades um so this is a slide that I'm"
      },
      {
        "start": 2634.28,
        "duration": 7.6,
        "text": "going to talk to but we've all done our"
      },
      {
        "start": 2637.2,
        "duration": 8.28,
        "text": "time uh around this uh the best bugs"
      },
      {
        "start": 2641.88,
        "duration": 7.0,
        "text": "that we hit are um halfway through"
      },
      {
        "start": 2645.48,
        "duration": 5.92,
        "text": "upgrades um or post upgrades that"
      },
      {
        "start": 2648.88,
        "duration": 5.439,
        "text": "haven't been done well the first thing"
      },
      {
        "start": 2651.4,
        "duration": 4.64,
        "text": "that I want to uh speak to"
      },
      {
        "start": 2654.319,
        "duration": 4.76,
        "text": "is"
      },
      {
        "start": 2656.04,
        "duration": 5.319,
        "text": "um uh what are the upgrade paths that we"
      },
      {
        "start": 2659.079,
        "duration": 7.081,
        "text": "want you to"
      },
      {
        "start": 2661.359,
        "duration": 7.0,
        "text": "do and and that we support So when"
      },
      {
        "start": 2666.16,
        "duration": 7.439,
        "text": "you're doing an upgrade"
      },
      {
        "start": 2668.359,
        "duration": 7.48,
        "text": "always first upgrade to the latest patch"
      },
      {
        "start": 2673.599,
        "duration": 5.081,
        "text": "version on that major version that"
      },
      {
        "start": 2675.839,
        "duration": 5.0,
        "text": "you're on so if you're on 311 upgrade to"
      },
      {
        "start": 2678.68,
        "duration": 6.28,
        "text": "the latest 311"
      },
      {
        "start": 2680.839,
        "duration": 8.441,
        "text": "first uh it's this is because and you"
      },
      {
        "start": 2684.96,
        "duration": 7.96,
        "text": "know hopefully it's it's obvious it's"
      },
      {
        "start": 2689.28,
        "duration": 8.24,
        "text": "because with some uh of the bugs that we"
      },
      {
        "start": 2692.92,
        "duration": 6.88,
        "text": "find we can only fix those bugs in the"
      },
      {
        "start": 2697.52,
        "duration": 4.24,
        "text": "in the source version like in the"
      },
      {
        "start": 2699.8,
        "duration": 4.68,
        "text": "version that you're upgrading from it"
      },
      {
        "start": 2701.76,
        "duration": 6.079,
        "text": "doesn't happen too often but it happens"
      },
      {
        "start": 2704.48,
        "duration": 5.359,
        "text": "inantly so that that this advice is is a"
      },
      {
        "start": 2707.839,
        "duration": 6.801,
        "text": "wise"
      },
      {
        "start": 2709.839,
        "duration": 9.0,
        "text": "one otherwise the since Cassandra 4 the"
      },
      {
        "start": 2714.64,
        "duration": 7.8,
        "text": "the Cassandra Community has been uh more"
      },
      {
        "start": 2718.839,
        "duration": 5.561,
        "text": "clear in that for online upgrades"
      },
      {
        "start": 2722.44,
        "duration": 5.28,
        "text": "upgrades where you're doing a a rolling"
      },
      {
        "start": 2724.4,
        "duration": 6.959,
        "text": "upgrade of each node while maintaining"
      },
      {
        "start": 2727.72,
        "duration": 6.639,
        "text": "uh full availability and and online your"
      },
      {
        "start": 2731.359,
        "duration": 7.161,
        "text": "services online we recommend that you"
      },
      {
        "start": 2734.359,
        "duration": 6.72,
        "text": "only upgrade between adjacent"
      },
      {
        "start": 2738.52,
        "duration": 6.24,
        "text": "major uh"
      },
      {
        "start": 2741.079,
        "duration": 8.601,
        "text": "releases so what we mean by that is like"
      },
      {
        "start": 2744.76,
        "duration": 7.72,
        "text": "the uh sorry adjacent major versions so"
      },
      {
        "start": 2749.68,
        "duration": 5.399,
        "text": "you can upgrade from any three version"
      },
      {
        "start": 2752.48,
        "duration": 5.96,
        "text": "to any four version but you can't"
      },
      {
        "start": 2755.079,
        "duration": 8.24,
        "text": "upgrade from 3 to five and skip"
      },
      {
        "start": 2758.44,
        "duration": 9.96,
        "text": "for um for offline we're getting better"
      },
      {
        "start": 2763.319,
        "duration": 8.361,
        "text": "that you can uh upgrade SS tables from"
      },
      {
        "start": 2768.4,
        "duration": 5.439,
        "text": "any uh version this is particularly"
      },
      {
        "start": 2771.68,
        "duration": 4.2,
        "text": "important if you start to think about"
      },
      {
        "start": 2773.839,
        "duration": 4.841,
        "text": "backups you want to pull out your"
      },
      {
        "start": 2775.88,
        "duration": 5.12,
        "text": "backups and they be may be quite old and"
      },
      {
        "start": 2778.68,
        "duration": 5.919,
        "text": "you still need to read them so we're at"
      },
      {
        "start": 2781.0,
        "duration": 4.8,
        "text": "the the uh SS table format level we're"
      },
      {
        "start": 2784.599,
        "duration": 3.161,
        "text": "trying to maintain"
      },
      {
        "start": 2785.8,
        "duration": 5.319,
        "text": "compatibility um but when it comes to"
      },
      {
        "start": 2787.76,
        "duration": 5.68,
        "text": "online upgrades there's just too much to"
      },
      {
        "start": 2791.119,
        "duration": 5.24,
        "text": "test and worry about and the community"
      },
      {
        "start": 2793.44,
        "duration": 5.52,
        "text": "only has so many resources to work with"
      },
      {
        "start": 2796.359,
        "duration": 5.041,
        "text": "so this constraint makes life easier and"
      },
      {
        "start": 2798.96,
        "duration": 5.84,
        "text": "safer for everybody a little text on"
      },
      {
        "start": 2801.4,
        "duration": 6.4,
        "text": "this Slide the the summary of it is"
      },
      {
        "start": 2804.8,
        "duration": 5.72,
        "text": "there really is a lot of homework you"
      },
      {
        "start": 2807.8,
        "duration": 5.44,
        "text": "got to do with major upgrades there are"
      },
      {
        "start": 2810.52,
        "duration": 5.24,
        "text": "prequisites that you should do there's"
      },
      {
        "start": 2813.24,
        "duration": 5.119,
        "text": "preparation steps that you should do and"
      },
      {
        "start": 2815.76,
        "duration": 4.599,
        "text": "the actual action"
      },
      {
        "start": 2818.359,
        "duration": 5.161,
        "text": "um sequence of actions that you should"
      },
      {
        "start": 2820.359,
        "duration": 4.0,
        "text": "take are all super important reach out"
      },
      {
        "start": 2823.52,
        "duration": 5.4,
        "text": "to"
      },
      {
        "start": 2824.359,
        "duration": 7.121,
        "text": "support uh and get help um don't"
      },
      {
        "start": 2828.92,
        "duration": 5.84,
        "text": "underestimate any of this"
      },
      {
        "start": 2831.48,
        "duration": 9.0,
        "text": "stuff you want a smooth"
      },
      {
        "start": 2834.76,
        "duration": 5.72,
        "text": "experience okay jumping on uh system"
      },
      {
        "start": 2841.16,
        "duration": 6.36,
        "text": "changes so Roman you had you brought"
      },
      {
        "start": 2844.319,
        "duration": 6.481,
        "text": "this up um uh"
      },
      {
        "start": 2847.52,
        "duration": 6.079,
        "text": "well so I think"
      },
      {
        "start": 2850.8,
        "duration": 4.6,
        "text": "um yeah I think one one of the important"
      },
      {
        "start": 2853.599,
        "duration": 4.321,
        "text": "Point here"
      },
      {
        "start": 2855.4,
        "duration": 4.199,
        "text": "uh as part of as part of the discussion"
      },
      {
        "start": 2857.92,
        "duration": 4.12,
        "text": "we had preparing this session is to"
      },
      {
        "start": 2859.599,
        "duration": 4.76,
        "text": "mention that uh it's it's important to"
      },
      {
        "start": 2862.04,
        "duration": 6.039,
        "text": "know your data model so it's just like"
      },
      {
        "start": 2864.359,
        "duration": 8.521,
        "text": "nothing can replace that uh I regularly"
      },
      {
        "start": 2868.079,
        "duration": 7.401,
        "text": "get uh CAU by uh request types that I"
      },
      {
        "start": 2872.88,
        "duration": 4.679,
        "text": "didn't think uh"
      },
      {
        "start": 2875.48,
        "duration": 4.72,
        "text": "existed um"
      },
      {
        "start": 2877.559,
        "duration": 5.441,
        "text": "mentioning uh mentioning for example the"
      },
      {
        "start": 2880.2,
        "duration": 5.159,
        "text": "case a case I had some quite a long time"
      },
      {
        "start": 2883.0,
        "duration": 4.96,
        "text": "ago but secondary index uh based on a"
      },
      {
        "start": 2885.359,
        "duration": 4.121,
        "text": "bullan value and so you you need to know"
      },
      {
        "start": 2887.96,
        "duration": 3.84,
        "text": "your data model because in that case"
      },
      {
        "start": 2889.48,
        "duration": 4.599,
        "text": "what happen is that a secondary index"
      },
      {
        "start": 2891.8,
        "duration": 4.48,
        "text": "with a Boolean value means that if you"
      },
      {
        "start": 2894.079,
        "duration": 5.0,
        "text": "have five million RS and they're spread"
      },
      {
        "start": 2896.28,
        "duration": 5.68,
        "text": "evenly your cardinality of two because"
      },
      {
        "start": 2899.079,
        "duration": 5.161,
        "text": "your bullan is z or one will mean you"
      },
      {
        "start": 2901.96,
        "duration": 4.879,
        "text": "will bring 2.5 million document whether"
      },
      {
        "start": 2904.24,
        "duration": 3.879,
        "text": "you're querying the zero or the one so"
      },
      {
        "start": 2906.839,
        "duration": 3.801,
        "text": "those are those are important"
      },
      {
        "start": 2908.119,
        "duration": 3.841,
        "text": "consideration you need to know uh what"
      },
      {
        "start": 2910.64,
        "duration": 3.04,
        "text": "you have and how you're actually"
      },
      {
        "start": 2911.96,
        "duration": 4.159,
        "text": "querying it and have some idea of how"
      },
      {
        "start": 2913.68,
        "duration": 6.919,
        "text": "it's going to make the computer behave"
      },
      {
        "start": 2916.119,
        "duration": 6.121,
        "text": "uh as a result so it's um I've I"
      },
      {
        "start": 2920.599,
        "duration": 4.281,
        "text": "regularly have challenges around that"
      },
      {
        "start": 2922.24,
        "duration": 4.879,
        "text": "front uh because this is something that"
      },
      {
        "start": 2924.88,
        "duration": 6.52,
        "text": "Cassandra will not necessarily show"
      },
      {
        "start": 2927.119,
        "duration": 7.921,
        "text": "you yeah and and and you know it's it's"
      },
      {
        "start": 2931.4,
        "duration": 5.919,
        "text": "databases depend a lot on the underlying"
      },
      {
        "start": 2935.04,
        "duration": 4.92,
        "text": "system and"
      },
      {
        "start": 2937.319,
        "duration": 6.641,
        "text": "especially like a lot of the Linux"
      },
      {
        "start": 2939.96,
        "duration": 6.599,
        "text": "distributions their default setup is not"
      },
      {
        "start": 2943.96,
        "duration": 4.639,
        "text": "suitable for Gander"
      },
      {
        "start": 2946.559,
        "duration": 3.481,
        "text": "technology um and so a lot of people"
      },
      {
        "start": 2948.599,
        "duration": 4.2,
        "text": "come in and they just got start running"
      },
      {
        "start": 2950.04,
        "duration": 6.36,
        "text": "and there's some baseline advice that"
      },
      {
        "start": 2952.799,
        "duration": 6.56,
        "text": "we have to offer it's recommended check"
      },
      {
        "start": 2956.4,
        "duration": 6.959,
        "text": "it out um but also I think one of the"
      },
      {
        "start": 2959.359,
        "duration": 6.841,
        "text": "other things is that once they once the"
      },
      {
        "start": 2963.359,
        "duration": 5.48,
        "text": "penny drops on that front and then they"
      },
      {
        "start": 2966.2,
        "duration": 5.24,
        "text": "start investigating and learning all of"
      },
      {
        "start": 2968.839,
        "duration": 4.641,
        "text": "the the the the Linux and underlying"
      },
      {
        "start": 2971.44,
        "duration": 3.919,
        "text": "configuration changes that they can make"
      },
      {
        "start": 2973.48,
        "duration": 3.8,
        "text": "they jump in and they change everything"
      },
      {
        "start": 2975.359,
        "duration": 4.2,
        "text": "at"
      },
      {
        "start": 2977.28,
        "duration": 4.76,
        "text": "once"
      },
      {
        "start": 2979.559,
        "duration": 4.04,
        "text": "uh yeah that's what you mentioned here"
      },
      {
        "start": 2982.04,
        "duration": 3.68,
        "text": "uh in the in the bullet point about"
      },
      {
        "start": 2983.599,
        "duration": 4.601,
        "text": "doing the changes sequentially is just"
      },
      {
        "start": 2985.72,
        "duration": 4.24,
        "text": "like one one step at a time is generally"
      },
      {
        "start": 2988.2,
        "duration": 5.0,
        "text": "a better one at a"
      },
      {
        "start": 2989.96,
        "duration": 5.32,
        "text": "time I'll make I'll make an ex I'll make"
      },
      {
        "start": 2993.2,
        "duration": 5.08,
        "text": "an exception to the best practices which"
      },
      {
        "start": 2995.28,
        "duration": 5.76,
        "text": "are generally a good of them to get your"
      },
      {
        "start": 2998.28,
        "duration": 4.0,
        "text": "environment going setting assuring you"
      },
      {
        "start": 3001.04,
        "duration": 3.72,
        "text": "that you have your reader heads"
      },
      {
        "start": 3002.28,
        "duration": 4.48,
        "text": "correctly said that you know the values"
      },
      {
        "start": 3004.76,
        "duration": 4.4,
        "text": "kernel parameters I mentioned no files"
      },
      {
        "start": 3006.76,
        "duration": 4.599,
        "text": "earlier which is one that you can that"
      },
      {
        "start": 3009.16,
        "duration": 3.76,
        "text": "you can hit and and when it when it hits"
      },
      {
        "start": 3011.359,
        "duration": 3.641,
        "text": "you the problem is that the kernel will"
      },
      {
        "start": 3012.92,
        "duration": 4.72,
        "text": "just stop your process so you end up"
      },
      {
        "start": 3015.0,
        "duration": 5.839,
        "text": "with a not down so some of the the best"
      },
      {
        "start": 3017.64,
        "duration": 7.32,
        "text": "practices really should be in place um"
      },
      {
        "start": 3020.839,
        "duration": 6.681,
        "text": "on the as a general rule absolutely and"
      },
      {
        "start": 3024.96,
        "duration": 5.44,
        "text": "and and and those things what like a"
      },
      {
        "start": 3027.52,
        "duration": 6.599,
        "text": "what I've seen all too often is that"
      },
      {
        "start": 3030.4,
        "duration": 6.919,
        "text": "they'll they'll they'll hit multiple"
      },
      {
        "start": 3034.119,
        "duration": 7.601,
        "text": "nodes in a"
      },
      {
        "start": 3037.319,
        "duration": 6.0,
        "text": "surprisingly uh similar point in time uh"
      },
      {
        "start": 3041.72,
        "duration": 3.24,
        "text": "they like all of a sudden the cluster is"
      },
      {
        "start": 3043.319,
        "duration": 3.601,
        "text": "just falling over node after node after"
      },
      {
        "start": 3044.96,
        "duration": 5.04,
        "text": "node that's the last thing you want and"
      },
      {
        "start": 3046.92,
        "duration": 5.28,
        "text": "Sandra set up properly uh it's"
      },
      {
        "start": 3050.0,
        "duration": 5.2,
        "text": "incredibly resilient it's supposed to be"
      },
      {
        "start": 3052.2,
        "duration": 4.56,
        "text": "an extremely resilient system that just"
      },
      {
        "start": 3055.2,
        "duration": 4.04,
        "text": "keeps on working no matter what happens"
      },
      {
        "start": 3056.76,
        "duration": 4.24,
        "text": "happens this is one of the gotes on that"
      },
      {
        "start": 3059.24,
        "duration": 4.2,
        "text": "front I'm really happy that you"
      },
      {
        "start": 3061.0,
        "duration": 6.48,
        "text": "mentioned read ahead readhead I think"
      },
      {
        "start": 3063.44,
        "duration": 7.0,
        "text": "readhead and the GC tuning are your two"
      },
      {
        "start": 3067.48,
        "duration": 4.839,
        "text": "loow hanging fruits to get huge"
      },
      {
        "start": 3070.44,
        "duration": 4.399,
        "text": "improvements on"
      },
      {
        "start": 3072.319,
        "duration": 6.401,
        "text": "systems uh so so thank you for that we"
      },
      {
        "start": 3074.839,
        "duration": 5.76,
        "text": "are really pushing for time um so I'm"
      },
      {
        "start": 3078.72,
        "duration": 6.04,
        "text": "gonna I'm going to jump"
      },
      {
        "start": 3080.599,
        "duration": 8.561,
        "text": "over uh the tooling and the operational"
      },
      {
        "start": 3084.76,
        "duration": 6.599,
        "text": "stop I'm going to jump um there is a"
      },
      {
        "start": 3089.16,
        "duration": 5.6,
        "text": "core to action for this page which I"
      },
      {
        "start": 3091.359,
        "duration": 4.801,
        "text": "think is is useful there's documentation"
      },
      {
        "start": 3094.76,
        "duration": 3.559,
        "text": "on the"
      },
      {
        "start": 3096.16,
        "duration": 5.56,
        "text": "Cassandra uh"
      },
      {
        "start": 3098.319,
        "duration": 7.721,
        "text": "website that's really rich uh and it was"
      },
      {
        "start": 3101.72,
        "duration": 6.16,
        "text": "it's recently new um go check that out"
      },
      {
        "start": 3106.04,
        "duration": 7.279,
        "text": "it goes through a lot of the tools that"
      },
      {
        "start": 3107.88,
        "duration": 7.32,
        "text": "we've mentioned here um and uh and how"
      },
      {
        "start": 3113.319,
        "duration": 6.361,
        "text": "to use them and when to use them there"
      },
      {
        "start": 3115.2,
        "duration": 6.399,
        "text": "was an item here Roman that uh it's"
      },
      {
        "start": 3119.68,
        "duration": 5.72,
        "text": "really important for us to talk about"
      },
      {
        "start": 3121.599,
        "duration": 5.401,
        "text": "the new Mac tell us about it so yeah"
      },
      {
        "start": 3125.4,
        "duration": 5.24,
        "text": "I'll try I'll try to go quickly because"
      },
      {
        "start": 3127.0,
        "duration": 10.559,
        "text": "I realize we're shot on time here"
      },
      {
        "start": 3130.64,
        "duration": 10.679,
        "text": "um data Stacks uh and cundra have uh"
      },
      {
        "start": 3137.559,
        "duration": 5.76,
        "text": "some some Metric toolings available but"
      },
      {
        "start": 3141.319,
        "duration": 3.921,
        "text": "uh there's there's quite it's quite a"
      },
      {
        "start": 3143.319,
        "duration": 3.8,
        "text": "bit let's let's put it that way it's the"
      },
      {
        "start": 3145.24,
        "duration": 3.4,
        "text": "jungle out there when it comes to all"
      },
      {
        "start": 3147.119,
        "duration": 5.081,
        "text": "the different tools available the"
      },
      {
        "start": 3148.64,
        "duration": 5.679,
        "text": "Cassandra exporter and uh and we have"
      },
      {
        "start": 3152.2,
        "duration": 5.52,
        "text": "designed ourselves mcak and we have the"
      },
      {
        "start": 3154.319,
        "duration": 5.401,
        "text": "DS metric collector for data Stacks um"
      },
      {
        "start": 3157.72,
        "duration": 4.24,
        "text": "and all of those are their pros and cons"
      },
      {
        "start": 3159.72,
        "duration": 4.8,
        "text": "and we know that uh there's been a lot"
      },
      {
        "start": 3161.96,
        "duration": 4.96,
        "text": "of there's been some issues with notably"
      },
      {
        "start": 3164.52,
        "duration": 5.559,
        "text": "enak and the and the DSC metrics"
      },
      {
        "start": 3166.92,
        "duration": 5.399,
        "text": "collector and those have there's genuine"
      },
      {
        "start": 3170.079,
        "duration": 3.881,
        "text": "design flows in them uh which means"
      },
      {
        "start": 3172.319,
        "duration": 5.0,
        "text": "they're really hard to"
      },
      {
        "start": 3173.96,
        "duration": 6.359,
        "text": "fix uh we've worked with with uh our"
      },
      {
        "start": 3177.319,
        "duration": 5.52,
        "text": "development team and uh with one of our"
      },
      {
        "start": 3180.319,
        "duration": 5.441,
        "text": "colleagues in support uh and he came up"
      },
      {
        "start": 3182.839,
        "duration": 6.561,
        "text": "with a full documentation on uh how to"
      },
      {
        "start": 3185.76,
        "duration": 6.079,
        "text": "use MAAC on premise the idea being uh"
      },
      {
        "start": 3189.4,
        "duration": 4.6,
        "text": "MAAC is a is actually part of the Kate"
      },
      {
        "start": 3191.839,
        "duration": 7.201,
        "text": "Sandra operator"
      },
      {
        "start": 3194.0,
        "duration": 6.72,
        "text": "uh uh offering so when you if I take it"
      },
      {
        "start": 3199.04,
        "duration": 3.96,
        "text": "people have heard of Kate Sandra which"
      },
      {
        "start": 3200.72,
        "duration": 4.119,
        "text": "is the one of the data stack open source"
      },
      {
        "start": 3203.0,
        "duration": 3.48,
        "text": "solution to deploy your environment on"
      },
      {
        "start": 3204.839,
        "duration": 3.96,
        "text": "kubernets and it has its own metric"
      },
      {
        "start": 3206.48,
        "duration": 3.879,
        "text": "system which is different from the"
      },
      {
        "start": 3208.799,
        "duration": 4.241,
        "text": "previous ones I"
      },
      {
        "start": 3210.359,
        "duration": 4.681,
        "text": "mentioned but it's limited to kubernets"
      },
      {
        "start": 3213.04,
        "duration": 4.68,
        "text": "and what we've done is extract all the"
      },
      {
        "start": 3215.04,
        "duration": 5.44,
        "text": "information and extract the agent from"
      },
      {
        "start": 3217.72,
        "duration": 2.76,
        "text": "the existing"
      },
      {
        "start": 3221.0,
        "duration": 4.72,
        "text": "kubernetes originally provided in the"
      },
      {
        "start": 3223.28,
        "duration": 5.24,
        "text": "code of uh of KRA"
      },
      {
        "start": 3225.72,
        "duration": 5.639,
        "text": "operator uh and we have created a"
      },
      {
        "start": 3228.52,
        "duration": 6.92,
        "text": "knowledge based article which will allow"
      },
      {
        "start": 3231.359,
        "duration": 8.881,
        "text": "you to use MAAC for on Prime deployment"
      },
      {
        "start": 3235.44,
        "duration": 5.919,
        "text": "now at this time the KB is designed"
      },
      {
        "start": 3240.24,
        "duration": 4.319,
        "text": "toward"
      },
      {
        "start": 3241.359,
        "duration": 6.401,
        "text": "DSC but the idea is that this is valid"
      },
      {
        "start": 3244.559,
        "duration": 6.721,
        "text": "as well for Cassandra 4 Cassandra 41 and"
      },
      {
        "start": 3247.76,
        "duration": 5.68,
        "text": "Cassandra 5 because I mentioned mcak"
      },
      {
        "start": 3251.28,
        "duration": 5.0,
        "text": "earlier mcak doesn't go further than"
      },
      {
        "start": 3253.44,
        "duration": 4.679,
        "text": "Cassandra 4.0 I believe we have a 41"
      },
      {
        "start": 3256.28,
        "duration": 4.92,
        "text": "Alpha which was released but never"
      },
      {
        "start": 3258.119,
        "duration": 5.401,
        "text": "passed the alpha level so I think it's"
      },
      {
        "start": 3261.2,
        "duration": 4.72,
        "text": "um a good takeaway as part of this"
      },
      {
        "start": 3263.52,
        "duration": 5.52,
        "text": "discussion for people to be aware that"
      },
      {
        "start": 3265.92,
        "duration": 5.48,
        "text": "we have been working on making this"
      },
      {
        "start": 3269.04,
        "duration": 5.079,
        "text": "available even outside of katundra"
      },
      {
        "start": 3271.4,
        "duration": 6.32,
        "text": "operator and we have our first released"
      },
      {
        "start": 3274.119,
        "duration": 6.68,
        "text": "which came out literally two hours ago"
      },
      {
        "start": 3277.72,
        "duration": 6.359,
        "text": "for the article nice one so so everyone"
      },
      {
        "start": 3280.799,
        "duration": 4.56,
        "text": "who's struggling with enabling all your"
      },
      {
        "start": 3284.079,
        "duration": 4.24,
        "text": "metrics"
      },
      {
        "start": 3285.359,
        "duration": 6.121,
        "text": "monitoring uh on your Cassandra"
      },
      {
        "start": 3288.319,
        "duration": 6.721,
        "text": "clusters uh and if anyone who's using"
      },
      {
        "start": 3291.48,
        "duration": 5.28,
        "text": "mcac pay attention to this uh it's it's"
      },
      {
        "start": 3295.04,
        "duration": 3.96,
        "text": "coming out thank you Peter for writing"
      },
      {
        "start": 3296.76,
        "duration": 4.24,
        "text": "that blog post and I'm thank you I'm"
      },
      {
        "start": 3299.0,
        "duration": 4.24,
        "text": "going to thank Michael Berman who I"
      },
      {
        "start": 3301.0,
        "duration": 4.96,
        "text": "think is one of the engineers um who did"
      },
      {
        "start": 3303.24,
        "duration": 5.64,
        "text": "a lot of this work um which has been"
      },
      {
        "start": 3305.96,
        "duration": 4.639,
        "text": "asked for for a lot of people I'm GNA"
      },
      {
        "start": 3308.88,
        "duration": 3.64,
        "text": "jump forward I'm gonna I'm going to run"
      },
      {
        "start": 3310.599,
        "duration": 4.24,
        "text": "quickly over a minute or two to get to"
      },
      {
        "start": 3312.52,
        "duration": 5.68,
        "text": "the questions that we"
      },
      {
        "start": 3314.839,
        "duration": 5.681,
        "text": "have um so having a just a quick look"
      },
      {
        "start": 3318.2,
        "duration": 5.2,
        "text": "Cassandra 5 what do we got coming the"
      },
      {
        "start": 3320.52,
        "duration": 5.24,
        "text": "headliner has to be the Accord"
      },
      {
        "start": 3323.4,
        "duration": 6.919,
        "text": "transactions this is going to make"
      },
      {
        "start": 3325.76,
        "duration": 7.52,
        "text": "cassand ra an asset compliant system in"
      },
      {
        "start": 3330.319,
        "duration": 8.0,
        "text": "fact it because it implements strictly"
      },
      {
        "start": 3333.28,
        "duration": 8.12,
        "text": "serializable isolation it makes it uh"
      },
      {
        "start": 3338.319,
        "duration": 5.28,
        "text": "the most asset compliance uh database"
      },
      {
        "start": 3341.4,
        "duration": 4.439,
        "text": "out there on power with"
      },
      {
        "start": 3343.599,
        "duration": 3.401,
        "text": "spanner um this is big stuff but"
      },
      {
        "start": 3345.839,
        "duration": 2.96,
        "text": "probably more interesting to most people"
      },
      {
        "start": 3347.0,
        "duration": 4.28,
        "text": "is what you can actually do with it on"
      },
      {
        "start": 3348.799,
        "duration": 5.841,
        "text": "this slide there's an example syntax of"
      },
      {
        "start": 3351.28,
        "duration": 5.96,
        "text": "the begin transaction um and how you can"
      },
      {
        "start": 3354.64,
        "duration": 5.199,
        "text": "you can grab variables and then write a"
      },
      {
        "start": 3357.24,
        "duration": 5.4,
        "text": "condition and then do something in that"
      },
      {
        "start": 3359.839,
        "duration": 5.52,
        "text": "condition this means one of the the the"
      },
      {
        "start": 3362.64,
        "duration": 5.199,
        "text": "big wins with this will be you can now"
      },
      {
        "start": 3365.359,
        "duration": 5.641,
        "text": "do referential Integrity in your data"
      },
      {
        "start": 3367.839,
        "duration": 7.161,
        "text": "model um this will be huge uh it's about"
      },
      {
        "start": 3371.0,
        "duration": 5.88,
        "text": "to get merged to trunk okay so uh the"
      },
      {
        "start": 3375.0,
        "duration": 6.079,
        "text": "next section I'm going to jump to one"
      },
      {
        "start": 3376.88,
        "duration": 10.6,
        "text": "key slide we all know geni is massive at"
      },
      {
        "start": 3381.079,
        "duration": 11.28,
        "text": "the moment and uh and and data Stacks is"
      },
      {
        "start": 3387.48,
        "duration": 7.48,
        "text": "pumping out products around gen and Rag"
      },
      {
        "start": 3392.359,
        "duration": 5.281,
        "text": "and Vector search because we're just"
      },
      {
        "start": 3394.96,
        "duration": 5.399,
        "text": "we're seeing such huge adoption around"
      },
      {
        "start": 3397.64,
        "duration": 5.04,
        "text": "this and the potential of what you can"
      },
      {
        "start": 3400.359,
        "duration": 6.121,
        "text": "solve is"
      },
      {
        "start": 3402.68,
        "duration": 5.48,
        "text": "is is everywhere and and it's brand new"
      },
      {
        "start": 3406.48,
        "duration": 4.48,
        "text": "what you can"
      },
      {
        "start": 3408.16,
        "duration": 5.12,
        "text": "do the one place where this really"
      },
      {
        "start": 3410.96,
        "duration": 4.839,
        "text": "touches on Cassandra is the vector"
      },
      {
        "start": 3413.28,
        "duration": 5.12,
        "text": "search Cassandra is a database which you"
      },
      {
        "start": 3415.799,
        "duration": 5.56,
        "text": "know where seeing this brand new traffic"
      },
      {
        "start": 3418.4,
        "duration": 5.439,
        "text": "shape and load uh land on the"
      },
      {
        "start": 3421.359,
        "duration": 5.68,
        "text": "transactional database so it's perfectly"
      },
      {
        "start": 3423.839,
        "duration": 5.28,
        "text": "positioned uh for this U being that"
      },
      {
        "start": 3427.039,
        "duration": 5.0,
        "text": "scalable database that can just take"
      },
      {
        "start": 3429.119,
        "duration": 4.96,
        "text": "anything and stay super fast on top of"
      },
      {
        "start": 3432.039,
        "duration": 4.28,
        "text": "that we have the vector search the"
      },
      {
        "start": 3434.079,
        "duration": 5.24,
        "text": "vector search has been implemented on"
      },
      {
        "start": 3436.319,
        "duration": 6.0,
        "text": "top of the SII indexes in Cassandra and"
      },
      {
        "start": 3439.319,
        "duration": 5.921,
        "text": "allows you to you do Vector search"
      },
      {
        "start": 3442.319,
        "duration": 6.401,
        "text": "through the aut by clause and you can do"
      },
      {
        "start": 3445.24,
        "duration": 6.599,
        "text": "like Global vector to search um in a"
      },
      {
        "start": 3448.72,
        "duration": 5.44,
        "text": "table you can do it within a petition"
      },
      {
        "start": 3451.839,
        "duration": 4.681,
        "text": "and you can also do hybrid against other"
      },
      {
        "start": 3454.16,
        "duration": 4.8,
        "text": "indexes it's a very powerful feature"
      },
      {
        "start": 3456.52,
        "duration": 4.44,
        "text": "it's put on top of the J Vector Library"
      },
      {
        "start": 3458.96,
        "duration": 3.639,
        "text": "uh which is best in class if you have a"
      },
      {
        "start": 3460.96,
        "duration": 3.839,
        "text": "look at the version three of that J"
      },
      {
        "start": 3462.599,
        "duration": 4.281,
        "text": "Vector Library uh and how it's"
      },
      {
        "start": 3464.799,
        "duration": 6.641,
        "text": "implemented dis and and a number of"
      },
      {
        "start": 3466.88,
        "duration": 7.04,
        "text": "other industry-leading um Solutions it's"
      },
      {
        "start": 3471.44,
        "duration": 5.679,
        "text": "it's blowing all the competitors uh and"
      },
      {
        "start": 3473.92,
        "duration": 5.96,
        "text": "other open source Solutions the"
      },
      {
        "start": 3477.119,
        "duration": 5.68,
        "text": "water I mentioned all the products that"
      },
      {
        "start": 3479.88,
        "duration": 4.479,
        "text": "we're pumping out um so data sex has"
      },
      {
        "start": 3482.799,
        "duration": 5.32,
        "text": "this"
      },
      {
        "start": 3484.359,
        "duration": 7.24,
        "text": "AI uh pass solution so there's heaps"
      },
      {
        "start": 3488.119,
        "duration": 6.641,
        "text": "here jump in um people talking in your"
      },
      {
        "start": 3491.599,
        "duration": 6.801,
        "text": "company you got colleagues talking about"
      },
      {
        "start": 3494.76,
        "duration": 6.0,
        "text": "J uh mention this to them um you know"
      },
      {
        "start": 3498.4,
        "duration": 3.76,
        "text": "there's there's in every hype Circle"
      },
      {
        "start": 3500.76,
        "duration": 4.68,
        "text": "there's so much noise it's a little bit"
      },
      {
        "start": 3502.16,
        "duration": 5.08,
        "text": "hard to kind of grab hold of what's real"
      },
      {
        "start": 3505.44,
        "duration": 4.52,
        "text": "what's happening this is one of those"
      },
      {
        "start": 3507.24,
        "duration": 3.839,
        "text": "things and it's a really good opener so"
      },
      {
        "start": 3509.96,
        "duration": 3.119,
        "text": "this is something you can just kind of"
      },
      {
        "start": 3511.079,
        "duration": 3.881,
        "text": "show hey look here's a whole list of"
      },
      {
        "start": 3513.079,
        "duration": 5.641,
        "text": "things for you to go check out and see"
      },
      {
        "start": 3514.96,
        "duration": 5.96,
        "text": "if they match uh what you're currently"
      },
      {
        "start": 3518.72,
        "duration": 3.92,
        "text": "exploring to"
      },
      {
        "start": 3520.92,
        "duration": 5.399,
        "text": "do"
      },
      {
        "start": 3522.64,
        "duration": 3.679,
        "text": "okay let's get to the"
      },
      {
        "start": 3528.68,
        "duration": 5.0,
        "text": "questions yeah got a couple"
      },
      {
        "start": 3531.68,
        "duration": 4.04,
        "text": "questions yeah there was a question in"
      },
      {
        "start": 3533.68,
        "duration": 4.2,
        "text": "the chat as well which we can jump back"
      },
      {
        "start": 3535.72,
        "duration": 4.96,
        "text": "to the first question I think I think"
      },
      {
        "start": 3537.88,
        "duration": 5.719,
        "text": "there are I I I think the question in"
      },
      {
        "start": 3540.68,
        "duration": 4.96,
        "text": "the chat was then put in the Q&A as well"
      },
      {
        "start": 3543.599,
        "duration": 3.081,
        "text": "because I think that's the same one from"
      },
      {
        "start": 3545.64,
        "duration": 5.8,
        "text": "aod"
      },
      {
        "start": 3546.68,
        "duration": 9.359,
        "text": "Deli AI okay yeah a deli is asking about"
      },
      {
        "start": 3551.44,
        "duration": 10.679,
        "text": "um incre changing parameters in yaml"
      },
      {
        "start": 3556.039,
        "duration": 10.08,
        "text": "file for uh to Street between oltp and"
      },
      {
        "start": 3562.119,
        "duration": 4.0,
        "text": "Olaf now"
      },
      {
        "start": 3566.48,
        "duration": 6.04,
        "text": "should we should we each should we each"
      },
      {
        "start": 3569.039,
        "duration": 5.441,
        "text": "give 10 seconds on this one uh what"
      },
      {
        "start": 3572.52,
        "duration": 8.319,
        "text": "would be your your"
      },
      {
        "start": 3574.48,
        "duration": 9.359,
        "text": "highlight yeah you can do analytics work"
      },
      {
        "start": 3580.839,
        "duration": 4.881,
        "text": "using other tools like spark or other"
      },
      {
        "start": 3583.839,
        "duration": 4.361,
        "text": "tools outside of a"
      },
      {
        "start": 3585.72,
        "duration": 4.079,
        "text": "Cassandra but Cassandra"
      },
      {
        "start": 3588.2,
        "duration": 6.32,
        "text": "itself"
      },
      {
        "start": 3589.799,
        "duration": 10.32,
        "text": "is is a is a is a ntp type"
      },
      {
        "start": 3594.52,
        "duration": 5.599,
        "text": "of small short um"
      },
      {
        "start": 3600.4,
        "duration": 4.52,
        "text": "operations not really analytics"
      },
      {
        "start": 3603.039,
        "duration": 3.921,
        "text": "operations so this is my 10"
      },
      {
        "start": 3604.92,
        "duration": 4.919,
        "text": "seconds that's right that's that's"
      },
      {
        "start": 3606.96,
        "duration": 4.92,
        "text": "perfect was a good introduction Roman"
      },
      {
        "start": 3609.839,
        "duration": 3.881,
        "text": "I'll give a try to it I'd say depending"
      },
      {
        "start": 3611.88,
        "duration": 3.919,
        "text": "on the number of so it's all about the"
      },
      {
        "start": 3613.72,
        "duration": 3.56,
        "text": "sizing of your box you can potentially"
      },
      {
        "start": 3615.799,
        "duration": 4.441,
        "text": "increase the concurrent read and"
      },
      {
        "start": 3617.28,
        "duration": 4.16,
        "text": "concurrent rights uh but you also have"
      },
      {
        "start": 3620.24,
        "duration": 3.119,
        "text": "to make sure you monitor your"
      },
      {
        "start": 3621.44,
        "duration": 3.359,
        "text": "environment accordingly going back to"
      },
      {
        "start": 3623.359,
        "duration": 3.24,
        "text": "what we mentioned originally best"
      },
      {
        "start": 3624.799,
        "duration": 3.721,
        "text": "practices should be in place rhead is"
      },
      {
        "start": 3626.599,
        "duration": 6.321,
        "text": "something on any red flavor that will"
      },
      {
        "start": 3628.52,
        "duration": 7.96,
        "text": "bite you uh when you so yeah it also"
      },
      {
        "start": 3632.92,
        "duration": 5.8,
        "text": "depends on what what type of uh worklow"
      },
      {
        "start": 3636.48,
        "duration": 5.52,
        "text": "do you have reads wres so if you have a"
      },
      {
        "start": 3638.72,
        "duration": 6.839,
        "text": "right oriented workflow use LCS and no"
      },
      {
        "start": 3642.0,
        "duration": 6.76,
        "text": "sorry use use sdcs if if you if you have"
      },
      {
        "start": 3645.559,
        "duration": 6.8,
        "text": "a read oriented workflow use LCS so 10"
      },
      {
        "start": 3648.76,
        "duration": 6.64,
        "text": "to1 ratio for LCS that's my 10 seconds"
      },
      {
        "start": 3652.359,
        "duration": 5.321,
        "text": "version but also check CPU and check the"
      },
      {
        "start": 3655.4,
        "duration": 5.12,
        "text": "iio on your uh on your systems when you"
      },
      {
        "start": 3657.68,
        "duration": 4.8,
        "text": "start doing this so use use no SQL bench"
      },
      {
        "start": 3660.52,
        "duration": 3.76,
        "text": "DS bulk or some other tools to generate"
      },
      {
        "start": 3662.48,
        "duration": 6.359,
        "text": "some stress on the"
      },
      {
        "start": 3664.28,
        "duration": 6.48,
        "text": "environment see see how it it yeah it's"
      },
      {
        "start": 3668.839,
        "duration": 4.081,
        "text": "a good question because it it it has a"
      },
      {
        "start": 3670.76,
        "duration": 3.92,
        "text": "whole lot of challenges I mean you're"
      },
      {
        "start": 3672.92,
        "duration": 3.679,
        "text": "you're going from transaction database"
      },
      {
        "start": 3674.68,
        "duration": 3.28,
        "text": "to analytics database you're going from"
      },
      {
        "start": 3676.599,
        "duration": 6.0,
        "text": "tuning"
      },
      {
        "start": 3677.96,
        "duration": 8.119,
        "text": "for uh latency and removing uh long"
      },
      {
        "start": 3682.599,
        "duration": 6.921,
        "text": "tails in the latency uh you're going for"
      },
      {
        "start": 3686.079,
        "duration": 7.361,
        "text": "from probably a CP system to an AP"
      },
      {
        "start": 3689.52,
        "duration": 6.839,
        "text": "system and in analytics you know you you"
      },
      {
        "start": 3693.44,
        "duration": 6.52,
        "text": "don't you often don't care uh you can"
      },
      {
        "start": 3696.359,
        "duration": 5.601,
        "text": "just restart a job um if if it"
      },
      {
        "start": 3699.96,
        "duration": 3.599,
        "text": "crashes uh and still completes within in"
      },
      {
        "start": 3701.96,
        "duration": 4.159,
        "text": "a certain window so you care more about"
      },
      {
        "start": 3703.559,
        "duration": 6.8,
        "text": "throughput than you do latency and so"
      },
      {
        "start": 3706.119,
        "duration": 6.801,
        "text": "you do end up with quite different uh y"
      },
      {
        "start": 3710.359,
        "duration": 4.561,
        "text": "more files and quite different read"
      },
      {
        "start": 3712.92,
        "duration": 5.359,
        "text": "ahead settings it goes all the way down"
      },
      {
        "start": 3714.92,
        "duration": 6.04,
        "text": "the stack and so we often see people"
      },
      {
        "start": 3718.279,
        "duration": 5.441,
        "text": "creating a separate Cassandra data"
      },
      {
        "start": 3720.96,
        "duration": 5.8,
        "text": "center uh and having their clients do"
      },
      {
        "start": 3723.72,
        "duration": 5.96,
        "text": "the analytics workload of that second"
      },
      {
        "start": 3726.76,
        "duration": 4.72,
        "text": "data center where it can create as much"
      },
      {
        "start": 3729.68,
        "duration": 4.119,
        "text": "instability as it wants and everything"
      },
      {
        "start": 3731.48,
        "duration": 3.119,
        "text": "has been Che for throughput rather than"
      },
      {
        "start": 3733.799,
        "duration": 5.28,
        "text": "low"
      },
      {
        "start": 3734.599,
        "duration": 8.76,
        "text": "latency um so that's my takeaway um as"
      },
      {
        "start": 3739.079,
        "duration": 7.921,
        "text": "far as individual yaml settings go uh"
      },
      {
        "start": 3743.359,
        "duration": 8.24,
        "text": "there's too many to touch to give you a"
      },
      {
        "start": 3747.0,
        "duration": 7.0,
        "text": "good answer Adela um you can ask us that"
      },
      {
        "start": 3751.599,
        "duration": 6.161,
        "text": "uh offline if you like we'll be happy to"
      },
      {
        "start": 3754.0,
        "duration": 6.039,
        "text": "help next question in Cassandra 4 are"
      },
      {
        "start": 3757.76,
        "duration": 4.12,
        "text": "both unrepaired and repaired tables"
      },
      {
        "start": 3760.039,
        "duration": 6.0,
        "text": "looked at for"
      },
      {
        "start": 3761.88,
        "duration": 6.08,
        "text": "compaction or is only repaired tables"
      },
      {
        "start": 3766.039,
        "duration": 4.76,
        "text": "compacted Francisco do you want to"
      },
      {
        "start": 3767.96,
        "duration": 5.72,
        "text": "answer that one yeah well so if we're"
      },
      {
        "start": 3770.799,
        "duration": 5.161,
        "text": "talking about Minor compaction all SS"
      },
      {
        "start": 3773.68,
        "duration": 6.399,
        "text": "tables will be compacted that depends on"
      },
      {
        "start": 3775.96,
        "duration": 8.0,
        "text": "the um compaction strategy that's not"
      },
      {
        "start": 3780.079,
        "duration": 7.2,
        "text": "driven by by repair so if you talking"
      },
      {
        "start": 3783.96,
        "duration": 6.92,
        "text": "about anti compaction in in in"
      },
      {
        "start": 3787.279,
        "duration": 6.241,
        "text": "incremental repair then"
      },
      {
        "start": 3790.88,
        "duration": 6.12,
        "text": "again all SS Stables will be anti-"
      },
      {
        "start": 3793.52,
        "duration": 6.039,
        "text": "compacted because it could happen that"
      },
      {
        "start": 3797.0,
        "duration": 7.2,
        "text": "SS Stables"
      },
      {
        "start": 3799.559,
        "duration": 9.321,
        "text": "become can mix not not doing um become"
      },
      {
        "start": 3804.2,
        "duration": 4.68,
        "text": "mixed uh with repaired and un repaired"
      },
      {
        "start": 3809.319,
        "duration": 4.641,
        "text": "table so I would assume the question"
      },
      {
        "start": 3811.559,
        "duration": 6.641,
        "text": "here is just about how normal"
      },
      {
        "start": 3813.96,
        "duration": 7.399,
        "text": "compactions not the antic compaction so"
      },
      {
        "start": 3818.2,
        "duration": 4.68,
        "text": "all yeah and and and you're right and I"
      },
      {
        "start": 3821.359,
        "duration": 4.081,
        "text": "think"
      },
      {
        "start": 3822.88,
        "duration": 6.84,
        "text": "it's we spoken before that there's now"
      },
      {
        "start": 3825.44,
        "duration": 7.44,
        "text": "two different sets of ss tables there's"
      },
      {
        "start": 3829.72,
        "duration": 7.399,
        "text": "unrepaired and there's"
      },
      {
        "start": 3832.88,
        "duration": 6.919,
        "text": "repaired and the the compaction now kind"
      },
      {
        "start": 3837.119,
        "duration": 5.881,
        "text": "of splits those sets separately so it"
      },
      {
        "start": 3839.799,
        "duration": 6.121,
        "text": "will only compact the unrepaired SS"
      },
      {
        "start": 3843.0,
        "duration": 5.119,
        "text": "tables together and it will only compact"
      },
      {
        "start": 3845.92,
        "duration": 3.96,
        "text": "the repaired SS tables together so it"
      },
      {
        "start": 3848.119,
        "duration": 4.641,
        "text": "does both but it has to do them"
      },
      {
        "start": 3849.88,
        "duration": 5.04,
        "text": "separately yeah I'm with Nick on that"
      },
      {
        "start": 3852.76,
        "duration": 5.96,
        "text": "one there's there's two streams of of"
      },
      {
        "start": 3854.92,
        "duration": 7.52,
        "text": "compaction occurring separate so they"
      },
      {
        "start": 3858.72,
        "duration": 4.96,
        "text": "the answer is go ahead sorry Francisco"
      },
      {
        "start": 3862.44,
        "duration": 5.119,
        "text": "no no please go"
      },
      {
        "start": 3863.68,
        "duration": 6.04,
        "text": "ahead um so are they compacted the"
      },
      {
        "start": 3867.559,
        "duration": 4.841,
        "text": "answer is repair disc compacted and"
      },
      {
        "start": 3869.72,
        "duration": 5.119,
        "text": "unrepaired disc compacted but"
      },
      {
        "start": 3872.4,
        "duration": 6.52,
        "text": "separately and it can bring its share of"
      },
      {
        "start": 3874.839,
        "duration": 7.841,
        "text": "surprises as well so we had a recent"
      },
      {
        "start": 3878.92,
        "duration": 6.639,
        "text": "event where uh someone tried a major"
      },
      {
        "start": 3882.68,
        "duration": 5.08,
        "text": "compaction on their environment and they"
      },
      {
        "start": 3885.559,
        "duration": 4.441,
        "text": "were expecting to shave a lot of data"
      },
      {
        "start": 3887.76,
        "duration": 5.279,
        "text": "with it and it didn't happen and the"
      },
      {
        "start": 3890.0,
        "duration": 7.72,
        "text": "reason was that because they had a a rep"
      },
      {
        "start": 3893.039,
        "duration": 6.881,
        "text": "table uh which dated back from 2022 and"
      },
      {
        "start": 3897.72,
        "duration": 4.48,
        "text": "that couldn't compact with the"
      },
      {
        "start": 3899.92,
        "duration": 4.879,
        "text": "unrepaired"
      },
      {
        "start": 3902.2,
        "duration": 4.359,
        "text": "table uh and there was a tomb there were"
      },
      {
        "start": 3904.799,
        "duration": 4.921,
        "text": "some tombstones in the repair table"
      },
      {
        "start": 3906.559,
        "duration": 6.56,
        "text": "blocking the blocking the deletion of"
      },
      {
        "start": 3909.72,
        "duration": 5.28,
        "text": "tombstones so this is really this"
      },
      {
        "start": 3913.119,
        "duration": 4.081,
        "text": "happens a lot with time R compaction"
      },
      {
        "start": 3915.0,
        "duration": 4.44,
        "text": "strategy as well and I think it's"
      },
      {
        "start": 3917.2,
        "duration": 5.68,
        "text": "probably a really great answer for this"
      },
      {
        "start": 3919.44,
        "duration": 5.2,
        "text": "question because if if you are seeing"
      },
      {
        "start": 3922.88,
        "duration": 5.84,
        "text": "something that indicates this is not"
      },
      {
        "start": 3924.64,
        "duration": 5.959,
        "text": "happening there are reasons for that"
      },
      {
        "start": 3928.72,
        "duration": 4.68,
        "text": "worth investigating isn't that what we"
      },
      {
        "start": 3930.599,
        "duration": 5.081,
        "text": "call Shadow tombstones uh"
      },
      {
        "start": 3933.4,
        "duration": 3.76,
        "text": "or yes and there's a setting like for"
      },
      {
        "start": 3935.68,
        "duration": 6.04,
        "text": "time winner competion strategy there's a"
      },
      {
        "start": 3937.16,
        "duration": 8.08,
        "text": "setting called uh unsafe unsafe"
      },
      {
        "start": 3941.72,
        "duration": 5.399,
        "text": "aggressive yeah um I think I was partly"
      },
      {
        "start": 3945.24,
        "duration": 4.48,
        "text": "responsible for that naming um let's"
      },
      {
        "start": 3947.119,
        "duration": 5.841,
        "text": "make it as scary as possible um but if"
      },
      {
        "start": 3949.72,
        "duration": 6.399,
        "text": "that righty it's a it's a very effective"
      },
      {
        "start": 3952.96,
        "duration": 5.24,
        "text": "a very effective option to turn on but"
      },
      {
        "start": 3956.119,
        "duration": 4.48,
        "text": "you need you do need to turn it on both"
      },
      {
        "start": 3958.2,
        "duration": 5.599,
        "text": "in the Amo and in its assistant property"
      },
      {
        "start": 3960.599,
        "duration": 9.081,
        "text": "yes repairing repairing uh"
      },
      {
        "start": 3963.799,
        "duration": 10.441,
        "text": "tables with with twcs or or LCS is not"
      },
      {
        "start": 3969.68,
        "duration": 6.48,
        "text": "necessarily a great thing I mean uh they"
      },
      {
        "start": 3974.24,
        "duration": 5.799,
        "text": "tables with"
      },
      {
        "start": 3976.16,
        "duration": 8.28,
        "text": "uh it could mix up the the time the time"
      },
      {
        "start": 3980.039,
        "duration": 7.52,
        "text": "series and and the time Windows of the"
      },
      {
        "start": 3984.44,
        "duration": 3.119,
        "text": "of the S table so"
      },
      {
        "start": 3987.76,
        "duration": 4.839,
        "text": "it's still that's one aspect I would I'm"
      },
      {
        "start": 3990.839,
        "duration": 7.361,
        "text": "not sure what the challenge is with the"
      },
      {
        "start": 3992.599,
        "duration": 7.76,
        "text": "LCS SP but I do agree we know twcs twcs"
      },
      {
        "start": 3998.2,
        "duration": 4.96,
        "text": "is definitely something that that is"
      },
      {
        "start": 4000.359,
        "duration": 5.0,
        "text": "challenging to repair I agree with that"
      },
      {
        "start": 4003.16,
        "duration": 3.399,
        "text": "and this problem can happen uh like like"
      },
      {
        "start": 4005.359,
        "duration": 3.281,
        "text": "the example you gave can happen"
      },
      {
        "start": 4006.559,
        "duration": 5.081,
        "text": "elsewhere too well over time we're"
      },
      {
        "start": 4008.64,
        "duration": 5.08,
        "text": "starting to to lose some people uh thank"
      },
      {
        "start": 4011.64,
        "duration": 4.479,
        "text": "you for everyone who's hung around to"
      },
      {
        "start": 4013.72,
        "duration": 4.44,
        "text": "the end um"
      },
      {
        "start": 4016.119,
        "duration": 6.96,
        "text": "thank you ran and franccesco for doing"
      },
      {
        "start": 4018.16,
        "duration": 7.199,
        "text": "this together it's been a pleasure um"
      },
      {
        "start": 4023.079,
        "duration": 4.321,
        "text": "and as I said before I'm the one who"
      },
      {
        "start": 4025.359,
        "duration": 7.041,
        "text": "brought Christmas to the group uh as you"
      },
      {
        "start": 4027.4,
        "duration": 7.76,
        "text": "can see in my background uh so uh I hope"
      },
      {
        "start": 4032.4,
        "duration": 4.719,
        "text": "you all have a white Christmas uh and a"
      },
      {
        "start": 4035.16,
        "duration": 5.24,
        "text": "good holiday"
      },
      {
        "start": 4037.119,
        "duration": 6.841,
        "text": "season thanks franisco and everyone for"
      },
      {
        "start": 4040.4,
        "duration": 5.959,
        "text": "uh for being there goodbye thank you"
      },
      {
        "start": 4043.96,
        "duration": 4.599,
        "text": "very Brad"
      },
      {
        "start": 4046.359,
        "duration": 2.2,
        "text": "to"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-10T23:49:19.387060+00:00"
}