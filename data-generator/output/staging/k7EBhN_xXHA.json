{
  "video_id": "k7EBhN_xXHA",
  "title": "3 Steps to GenAI Success for Cassandra Teams",
  "description": "Teams are being pressured to bring AI features to existing applications but it’s all very overwhelming. You know how to work with application data but making the jump to AI? Is this you? Come learn how to unlock the potential of Generative AI (GenAI) with your Apache Cassandra data! Join us for a workshop designed specifically for teams currently utilizing Apache Cassandra, DataStax Astra and DataStax Enterprise. \n\nIn this session, you'll learn:\n--The Basics of Generative AI: This session will help you understand the parts of Generative AI and why it's becoming an indispensable feature in the tech landscape. We'll demystify GenAI technologies and discuss their relevance to data-driven organizations. Retrieval-Augmented Generation(RAG). Vector Search. Large Language Models. LangChain. LlamaIndex. Embeddings. A lot of words but we’ll help you get up to speed. \n--How Cassandra Fits: Learn how recent changes to Cassandra have made this even more accessible. Even with older versions of Cassandra, there are interesting use cases to be had. \n--Three Steps to Integration Success:\nStep 1: Identifying Opportunities - Finding use cases and narrowing them down to your first POC. \nStep 2: Implementing Solutions - From the simple to the more complex, you will be surprised how simple it is to start and show results. A large volume of code and libraries exist to get you from idea to production quickly. \nStep 3: Measuring Impact and Iterating - Learn how to effectively measure the success of your GenAI projects and use these insights to iterate and improve continuously. We'll discuss KPIs, feedback loops, and strategies for scaling GenAI initiatives.\n--Live Q&A Session: Have your questions answered by our experts in real time. Whether you're curious about technical details, implementation strategies, or potential challenges, we're here to help. We’ll also give you ways to keep the conversation going after the fact. It’s never a one-and-done! \n\nIf your boss is asking about when AI wi",
  "published_at": "2024-04-24T06:01:28Z",
  "thumbnail": "https://i.ytimg.com/vi/k7EBhN_xXHA/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "search",
    "datastax",
    "apache_cassandra",
    "vector",
    "astra",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=k7EBhN_xXHA",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "hello everyone we're here for another Workshop I hope there's folks here if not I'm just going to talk to the recording I I started just a smidge early just to let everyone find a chair you know there's some in the back I see okay cool um yeah so today we're going to talk about some geni stuff but I really hope this is you're a Cassandra person because this is geared towards you if not you're going to get something out of this uh that's not going to be a problem there's a there's a lot to talk about gen AI isn't there and if you're if you're here shout out on the old chat I appreciate it all right okay let's get moving here because I got a lot of slides and a lot of demo going to do and let's let's just get to it um so today we're going to talk about those geni success problems when it comes to building building with Cassandra and this is for Cassandra teams so just a little bit about the I'm g i i put this slide up a lot and I think it's important for you to see this is you maybe um are you feeling you know stressed out feeling behind that boss that like hey we need an AI if that's you you're in the right place today and I I'm GNA this is my safe Safe Harbor comment and uh by that I mean we're all kind of feeling this let's just be uh helpful and work with each other and you know this is a community of users right um communities we we stick together and we help each other and we make things work together and it's always been my experience to technology especially when technology growes through these ripples like they are now that we all depend on each other to get through without you know getting them scarred and beat up and destroyed and this is no different from any other time so um it's not a place for someone to say that they are more Superior than someone else or that you know people are falling behind and we leave them behind if you see someone falling behind ask them can I help and if you're falling behind hey ask for help we're always here for you and of course let's do this together so that being said let's move on to the Contex so today's agenda I'm going to do a oneone level of generative Ai and if this is something that's new to you hopefully you'll get something out of it it's something not new to you hopefully you'll pick up some nuggets along the way I've been working with generative AI for quite a while now for about over a year now and especially around Cassandra I've learned some things but I've also figured out the code it's not as crazy as you think and I think it's important that we have these discussions around this idea of like like okay um here's the tool here's all the different parts and this is how it works for you it's not magical it's not mystical it's just another tool and then I'll go into how cassander fits into this whole process and it does and you being a cander team orander person you've got an opportunity in front of you and I want to help you with that and then finally we're going to go through some more practical stuff like those that three steps production and what I want to do is go through uh there's some Hands-On some practical information along the way it by no means are we going to build anything today we we have an hour but um at the end of this whole thing I wanna I'm gonna I have a little poll that I W to ask everyone so just keep in mind when you're watching this what other topics I want to do one of these maybe every couple weeks and dig dig into each topic but I want to do the ones that you need I've done this for years and I think this is the bright way to go I can come up with topics trust me but as you're going along through this content if you say hey can you dig into that more great um I want to hear about it and I want to do it so just keep that in mind as we go along so let's begin with the basics of generative Ai and that that conversation will not start with just Ai and I think this is a this has been a problem that we've had for years is we have many flavors of AI and it's kind of gets Blended together and mushed into a ball and they are very different uh AI is not new if you know if you w to you want to get a opinionation from an AI person they're GNA be like it's been around for a million years well it has I mean AI was a as soon as computers became a thing everyone started talking about AI that was a long long time ago in a galaxy far far away but there are definely there are definitely like some buckets for that so starting with decision support and this has been around for a long time it's actually one of the first things around AI was like Hey how do we have a a smart decision machine and it's more than just to say a state machine where this input makes it do that that input makes it do this you know that sort of thing it it's putting some sort of intelligence around decision- making um the the ultimate thing would be like autonomous driving which you know if you're driving a Tesla right now you probably got it for free right now yeah but you know where it is right now it's not it's not 100% I would not trust my car to drive me around it's cool for cruise control but that's the kind of that decision support system the AI it's not intelligent as so much as it's making good decisions or it's trying to make good decisions and that's been around for a while now recognition like facial recognition name recognition all these different recognitions um those are uh those have also been around for quite a while too in various forms uh facial recog nition has been around for quite a while um ask any government agency but the recognition part um has been kind of a foundation for what's happening in generative Ai and I'll talk about that in a minute but that's been pretty well established now the one that we've probably all been exposed to the most in the past few years is predictive and predictive AI is the what we would normally call like machine learning like how do I get you to buy one more thing um and that's the inference in that sort of thing like you take this training data you figure out what everyone's buying and you try to create a model and then you put it out and then you when someone goes into their shopping cart you're like but wait have you thought about this that's predictive Ai and it is cornered around a lot of that ml the mlops world um ml ml ML and predictive has definitely been the dominant version of AI for years and years and years and it's because it actually makes money that's that's important right and or at least in some places it has I would say that Google is Google because of predictive AI they they know how to put an ad in front of you really quickly and that makes them bazillions of dollars so here we are the next generation is Generation generative AI um not it's not new but it has a step function generative AI has been something about and of course that means means creating new content creating new data um and that has been around for a while but it just hasn't been super useful or very effective and that uh that has just kept it pretty much in the back room or you know an Academia and it took good old open Ai and chat GPT to really blow us away and it happened fast I think everyone remembers that's in this industry can remember like when chat G came online it just changed everything everyone just lost their mind because you could go in and have this conversation with this thing and it knows stuff and it just does stuff and and we got a little carried away we're like oh that's it it's the end of the universe we're done AI is g to take over everything yeah not really but it is really really helpful and useful for a lot of applications and that also includes not just text but also like do um Dolly DOL uh the building images and if you use mid Journey or something like that stable diffusion this those are really cool because that's creating an image and now that's also creating meltdowns like the Sora videos of creating you like from taking text and making a movie out of it there's a lot of people having a little existential crisis but that's all generative and it's based on this idea of a model and the model building the model this is what we all talk about like when we talk about you know hey what about this model gp4 blah blah blah those models are all built for a couple of different there's a couple of different flavors of models there's these text based models which we are familiar with in chat and it takes an input like hey how would you describe a cat and it outputs something like hey a cat is a small domestic animal blah blah blah and it's it's great as a demonstration for sure but it also gives us an ability that we haven't had before which is we can do like natural language communication with a computer and if you view something like Siri or something like that um you know that or Alexa it's like it it picks up a few words not all of them and it's great I can like I can make a mistake I would I would say how you describe a kitty cat or you know use a different language or whatever and it's figures it out um so that's that's a big step function in in what we can do with these models and then for images a different kind of model um these are um these are built on this idea of building up images but it's still it's still in that kind of a generative AI world and but it's a little different text um images video audio they're all in this U same class of of generative models um where they build up this the information but they take one thing and the the basis of this is taking an instru instruction so and I'm going to use instructions um instead of saying um text prompt I'm going to say instruction they they take instructions and what they do is they encode it so they uh they break it down into pieces and then you hear this word tokenized tokenizing isn't like just every word it's it's like phenomes and there's just a whole branch of tokenization but like what what what that is um not important for today but I want you to see this as like okay this is a this is a machine a black box type thing but it's not too mystical there's no magic going on here it takes instructions encodes those instructions and then passes it through a really complicated bit of uh Matrix math decodes it ways and does predictions and then it gives you output that that process is really intense um it takes a this is what you all see now is nvidia's stock is through the roof because it takes the biggest Nvidia cards they have to do this sort of thing and it's not just and it's not just running the llm it's the training and that that's the next slide but um just understanding that this is more or less a machine of some sort is you takes instruction does some work produces output that's what all these are the the gp4s Llama 3s um Mist draws they all do this in some way and some most of them in their own way but the killer the Killer app is this which is the training and so that's where you see like what's the difference between between 3.5 and four and potentially five mostly it's around how much training they do now we won't go into like how it's being trained I what I would like to talk about is what it's being trained with and this is what makes it a useful machine for us is that the LMS are being trained with information like um you know like how people interact with each other when they help like stack Overflow um you know the information from an article and this is a really important Point how people in specific roles respond and we'll get to that in a minute but that because there could be different responses from different roles and that's that's something you can encode and important and then you know code examples is another thing um it pours a lot of information but also a lot of the the navigation of information into these models and these huge neural Nets and what we get out of it is a thing that could do not only uh intelligent is responses I say is because there's moments but also some reasoning like how things come to where they are um there's some really interesting videos out there um from some of the founders of open AI that talk about how they're actually encoding a person's role or how they respond to something into these llms which gives it a flavor that reasoning like if you were to ask yourself hey how would you solve this problem you go through these steps well that got encoded into this LM and we could use that to our advantage that's kind of cool and we will in a minute and this is uh a great slide for you to keep in mind all the time I've talked to lots and lots of cassander folks lots and lots of it folks and there there is a of magical thinking going on around llms they are not magical um this is not C3PO you know it's not going to just doesn't have ascensions and it's not universally intelligent um you know you hear a lot of that that talk going on like is it is it intelligent no it's AGI um and artificial general intelligence not even close but the things that you don't want to do are the things I'm listing here like it's not a replacement for spark uh you spark has this use cases that LMS will never be able to do at least not in the current iteration and then probably the thing that's most important to understand is precise numerical communication numerical calculations um adding up a bunch of numbers or doing a math problem is super hard for an LM and it's just based on the fact that it's giving you probabilities not it it's not like writing a Python program to add numbers it's not that um as a matter of fact l can write a program to do that and will if given the choice because that's that's that's just not something that's inherently built into an LM how can do um I put some other things in here like these realtime high stakes decision- making just because it's based on probabilities um you do not want to make a life for decision and at a minimum do not trust it with your money right now that's just not a good idea um and you know the software tools you know it's never going to um these are built to replace things like a cad soft or something like that and then finally this long-term data storage there are they do encode a lot of information but they're not a replacement for a database and uh maybe in the future they will be in a lot of ways who knows lots of speculation but the thing is it's really expensive to run an llm and that's probably one of the reasons all these things are really not a good idea is because an llm is probably one of the more expensive things you're going to use in your infrastructure at this point because um when you when you have that parsing of tokens and then it does the work and then it outputs the tokens all of that takes a lot of GPU of course GPU is most expensive thing we could have in our data centers at this point um and it's just not efficient um so it's best to think about an llm as a tool that can do a certain thing that you want to minimize the amount of of traffic to and just use it the best way possible it's not an everything tool it's not the hammer for everything so there you go all right moving on now digging into the using the tool and this is this is been a thing that I've been pouring over tons of YouTube videos tons of Articles like what is this and you probably have heard this about a billion times and it's just annoying um this whole thing of prompt engineering or prompting um it's it's a thing it's a thing but the reason why is because whenever you have think of them is um if you're using a function like a like a Java function or a python function or something and it has parameters you use the parameters to get the output you want and prompting is that and I um I paraphrase several comments but this is the best way I can describe what prompting is steering a language model towards the desired output without altering its internal weights so you're you're influencing the model to go a certain direction um there's a big Aster on there you know the aster is uh changing the internal weights or altering the internal weights is this thing called fine tuning different topic different different whole different thing but um that is something that people can do and it it it what it does it just tweaks it a little bit so fine-tuning tweaks things so it goes a certain direction but let's say without fine-tuning and fine-tuning is is um is a hard thing to do so what if you just want to use the model right out of the box well cool there's a plan and it's formula for how you create the prompt so uh I think this is and this goes into like how it was built so you have this Persona context task an example if you can and a format if you can and this formula I put into like an actual let's let's walk through this so I start out with a prompt I say you are an Apache Cassandra expert okay so now what we've done is we immediately put it in the mode of like okay I'm going to wait things based on this role that I have so instead of giving you answers for like postgress or something else it's like nope I'm Apache consider expert um and then this context is you are Consulting with teams provide advice so oh I'm a consultant okay this is making it and these are actually changing the way the output works you can play around with these and see how it works and it's fascinating how it does really make that difference and then the task and the task is what do you want it to do your job is to create a runbook for decommissioning a node and a cluster okay and that's something it could actually do uh we'll look at it this in a bit but um gp4 could do this because that information is encoded in somewhere but we can make it better and and then finally you know I I provide an example like this is what I want it to look like um you know they have the precondition checking each step instructions checks each step and then postd commission checks and then I I say I want it in markdown that's the format you can say I wanted do Json or whatever and mostly G like any LM will comply with all those things it'll try to do that it'll um what's funny is things like um like uh the some llms prefer XML over Json okay but in general this is how it works right now prompting has gotten a booster and this is you've heard this enough too rag like how does rag fit into this whole prompting thing I'm glad you asked and it's really just creating context um real-time context so So when you say hey what are compaction settings for cander 311 that's really specific and uh that that question it's a good question to get an exact answer on because if you're using say cassander 2 those are different and so uh you'll see like in rag that they bring out these things like you see the the vector databases being used in that way um you don't have to but we'll get to that um but using a vector database what it does is it tries to find text a similarity search oh cassander 3.1 311 compaction and a properly vectorized data set like the whole docs for Cassandra would say oh I know where that is and it would retrieve the document page for compaction settings for caser 311 and then use that as a context so whenever you say our previous uh prompt where we say hey you're an AP pachic sander exor here's the context which is a whole context of is page of docs on um compaction settings for 311 how do I do this you know how do I do compaction or what what are the compaction settings and we send that to the llm so this big block of text and then lo and behold we get a response which is really nice nicely formatted I maybe I wanted in markdown or something like that like a table of values um but it's not making anything up at that point is taking all that information from the database and uh inserting it into our prompt so this is the retrieval that's the r the retrieval is um augmenting uh into the llm and then the response is based on that that is a really useful tool and we'll find out lots of cool ways to hack on that um it doesn't always have to be one thing you could do quite a bit with this but really it's about making sure the LM has enough information to give you a reasonable response and the one of final topics on llms which I personally this is one of my personal things I love these is using tools now you may see them called functions um the the industry started out with um open AI they released a GPT functions or function calls uh last summer with one of their models with gp4 and and 3.5 so those slowly got morphed into tools tools and now most models support what they call tools and um The Lang chain L index they they they call them tools so we're just going to use that word and so what does that mean um well whenever you make a uh a call to LM you can also embed a list of tools that it has access to so for instance I if I want to add two numbers together this is really simple like I said an llm is not super good at math and you don't want it to have to go off and do math but I know if I wrote a program to do it then it would be good that's the right way to do it so what I do is I say when I ask the question I also pass it a list hey here's the tools and what you say is I have a function called add numbers here's a description takes a list of numbers and adds them together the LM can make a reasoning decision say oh you want me to add numbers I have a tool for that and the response will include a tool call and the tool call says hey look I need to do this thing here's um call this OD numbers function here's a list of all the the parameters these 20 and 40 and then inside your like uh Lang chain has support Linex has support um this is really widely supported it will make a call to that program that you've you've added and it's a little complicated I would love to do some content if you want me to do more on this um I I I have more to talk about but in a basic thing it just runs whatever arbitrary code you want return that value now you can either return it directly to the user or you could pass it back through the llm so it spruces it up a little bit makes it look nice says the answer is 60 and you could trust that because I went through a tool with it's deterministic instead of the LM just making up a response it probably will say 60 because that's a pretty easy one but if it's something more complicated oh like accessing a database um maybe a tool would be better choice so that's that on LMS and the basics of geni I went through this pretty quickly although I did burn through 24 minutes I feel like it was important that we get kind of a baseline here and again uh keep in mind if you want me to dig into any of these topics I would love to do that in a in a future Workshop so how about Cassandra our favorite database how does it fit in here and that's a that's a challenging question because Cassandra's been around for a while and has definitely proven itself I mean here this right um Cassandra is not disputed as a scale leader and how does it fit into this new world of gen it uh it is a database and it has data and just thinking about just more openly like that's if you're going to work with uh say Rag and things like that llms are great for communication but they just don't know a lot of things and we've put a lot of our important data in Cassandra if you're listening to me now and you're using you have a Cassandra cluster somewhere chances are that data is pretty critical to your business so how could we use it well you have choices and those choices are and I think this is a good thing for everyone to understand is that we've gone down the path of like oh you to do the new thing you have to upgrade and I'm here to tell you no you don't you can there are things to do so if you want to stay where you are if you're using cassander 4.1 or less and you want to do some gen stuff cool we can do that and uh same with dsse less than 6.9 and that's just because there's going to be gen stuff built into that afterwards and of course Astra if you use use regular classic astroid that's just standard service um it doesn't have Vector support or anything gen on it you're still cool you can still do stuff and actually it's pretty useful now if you do want to go down the path of an upgrade really what you're asking to do is do I want to run a vector I need to do Vector search and that's going to be uh conser five or greater uh greater than DSC 6.9 and then um uh asra Vector for that and of course if you have a new project that'd be a great choice you just want to start doing it but you do have a choice and I think this is the the thing that I wanted to make sure that everybody realizes you don't have to have Vector to do rack and I'm going to say that again you do not have to have Vector to do rag that is not a requirement okay that opens up you to a ton of use cases for geni that means that you can use that that retrieval part of it a little more natively with whatever standard installation you have with the data you have to do some really cool things I use an example here chatbot let's say that you have uh you're in e-commerce you need to do a chatbot uh with your customers and you know that's classic that's a hello world for Jen eyes chatbot so what if a customer says what did I buy last week that does not take a complicated bit of data engineering to make work what you need to do is just have a query that already you probably already have where I go into the custom orders table look up the user by ID and I do a range check on their order dates and I return back all the orders now I build a I build a prompt that says um you know you are a helpful customer bot and um the the customer asked for when know like when the customer asks for their their orders here's the data and then provide this back as a useful table in a chatbot and you kick that through the llm and it gives you a response back in the chatbot and it'll probably give you some super wordy response like oh you bought three things last week um it won't just say that it'll say a lot but at no point did I use Vector for that and it's just a matter of retrieving data and giving it you're you're wrapping this data which is just a hard number probably or a part number or something like that around you're wrapping an llm around it to make it more humid and it's a really cool use case think of the things you could do with that um I use I use llms for that purpose I want to be able to communicate and in a way that's a little vague but also I want to be able to get information back that kind of gleans out meaning from it and LM are good for that the key is data and we'll get into how we can make that work so the upgrade uh conser 5 DC um data sex Enterprise data sex Astro that is doing Vector here's what's happening it's using J vector and J Vector is a really cool project um it's Jonathan Ellis has it in his GitHub I have the link there um I didn't put it in the links I will I'll put it in there um J Vector is getting a bit of love out there not a bit a lot of love it's probably one of the the top Java it is the top Java trending project in GitHub it's being used by multiple projects now it is getting a lot of press a lot of pickup from um researchers it it's really the best one of the best java libr liaries out there for this type of thing but it's also one of the fastest libraries period for Vector search and yeah it's not written in Rust sorry it's using a lot of really cool techniques that are built into Java now modern Java has some really neat things when it comes to Vector math and working with simd and that sort of thing j Vector uses all of it and as a result uh it's made uh Vector search in Cassandra all flavors very interesting and um it's cool because once again we're leading the charge and this is going to work at highly distributed value uh highly distributed systems that we that we use the the core uh part of J Vector is scale uh Cassandra is about scale pedabytes terabytes whatever J Vector Jonathan when he was started going down this path realized that the libraries out there that that existed really weren't built for scale they were built for a laptop or very small use cases you know a thousand a million vectors that sort of thing what happens when you have a billion vectors a trillion vectors that sort of thing um that's the cassander problem this is a problem we've been solving for a long time nothing new and here we go so it's pretty exciting I I am excited about it and what it does in a nutshell is it it adds to the data model for Cassandra so it adds a new type called vector and uh a vector is just to float with some dimensions and the dimensions now this is a five-dimensional beddings I really doubt you'll ever find a dimensional embedding at that size most of them are in their hundreds some are in the thousands vectors are again a different topic I would love to cover it if you want me to let me know I have a surve a little poll at the end there you can tell me what you like but vectors are are beasts they're big numbers that's why you need a specialized um indexer but in this case use five and then you create an index this is all built into the way Cassandra does indexing already it's using a storage attached index already um and then you could do things like uh you could do this a&n which is approximate nearest neighbor searches on you could give it a like here's a vector what's close to this Vector space stuff is really interesting um you know that's how you find like a is this a hot dog or not a hot dog um pictures that sort of thing um but whatever you embed into it in your vector you could figure out what's similar or close to it words pictures audio whatever and um really interesting search algorithm and it's funny because there's a lot of use cases that have nothing to do with Gen too but this is what it does and when you couple this with some actual like hard data so for instance with my my document you can um find a vector embedding model that will that will give you semantics of the document and then you can also store the document alongside it in Cassandra because it's a cassander database think about like in this case um give me everything from the product table um yeah thanks Ain Ain I Aaron PL he's already telling me he's like you use the hot dog example and I told him I never would again and he's telling me on chat that I did it but it's such a great example if you don't know what I'm talking about go watch silic Valley so anyhow um using Vector search in a non geni thing is also a use case might be worth the talk too so that's something you're interested didn't let me know and now with a vector now we can this is that thing that I talked about this is what makes it happen and building hybrid data is cool because a lot of times you need to find something out there right and you don't know where it is and you have like this is that fuzzy search it's not an exact search so you're not saying you know where where picture equals hot dog it's like kind of looks like that and you could use that with your existing data say like product tables I mean like your product data or um metadata around a picture or maybe even documents um that's the classic use case right now is go find My Documents but it's going to give you fast access to context that you can put into your prompt drop it into LM and that's real-time data too the other thing that's really cool about using CER in this use case is real time right it's say a customer is in the middle of filling a shopping cart and they ask a question about their shopping cart no point is that in an LM the LM is a unaware so you need to say oh dump the contents of the you know you are a helpful shopping bot here's the contents of the shopping cart the customer has questions about this and here's some here's some context great and that's real time right and that's probably a use case you could do quickly and easily yay team the thing that makes it work in this case is that real time stuff and this is where Jor here's here's the the slide that everyone's going to like oh architecture no this is some this you should just look at this as like wow J Vector is fast well yeah because that's the way it was built so it it says Astro DB Vector that's what's running um that's what's running this particular one yep super fast and J Vector is built to do this these really um fast uh search lookups now if you'll notice these numbers aren't like single digits we're not there yet no Vector search is a heavy thing and so it it takes some work but we're getting faster and faster and faster and this is something that is being worked on continuously all day great discussions hopefully uh I think we're going to see more talks around J Vector out there about how we're actually making it faster we'll get there but um the thing to to remember is if you're using like a specialty Vector database it's just not differentiated as a matter of fact is probably slower than anything that Cassandra can do so yeah go team okay we're at the point where we now I got to play around with my keyboard let's dig into like how we can actually do this all right you want to do this you wanna you want some steps like give me some practical stuff here dude I'm here so first thing is identifying opportunities and this is something that I've worked with a lot of users out there on and I think is the first step of the crisis geni it's like what do we even do man I'm not open AI I get it um it's not that complicated so let's just take a breath together it's not that complicated we can do this and you're going to pick your path first things first do you want to use your existing data in Cassandra or are you looking for do new things with Vector that's important like where you want to start and staying where you are is probably the right choice to get going you you know I've worked as a it consultant for years and years and years which means I gota go in and tell you how to do it and I walk away great job by the way but the thing that I always see is there's always way more excitement and hype about doing the thing and the plans get ridiculous and scope creep immediately and then it never gets built start with something simple really simple and get your success get your feet under yourself ji this is that that Panic is going to be better you're going to understand so much by just doing something simple like how it llm works I'll tell you the biggest one of the biggest um hurdles you're gonna go over is your legal team when you say we're gonna put some customer data near an llm I'm just gonna tell you right now that's a fun conversation but one that needs to be had and um there are of course plenty of ways to mitigate that but um and I would love to talk about that if you want but you know this is I I'm going to say stay where you are is probably a good first choice but what are you gonna do like how do you want to do this now because I know this is a problem I actually built a project um and I have this uh I'm gonna see if I can share the link on here hold on go into my little calls to action now I put the link up uh and hopefully it's super simple for you but it's the the thing that um this does and so I'm gonna do some handson here in a second um so what this does is uh it's a Python program that will uh go use open AI so you put in your open AI key if you use local Patrick Cassandra you just put in the IP address of the of the nodes it's like one of the nodes if you're using dsse same thing if if you need to usern your password use that um but what it does is it goes through it queries the entire schema um sends it to an LM like open AI you can use many you can use any LM you can use a local one if you want but it does some prompting and pulls out a report and says oh here's here's what you have in here I it's not magical it's not anything revolutionary but it sure gets you started and um as a matter of fact Let me let me show you the project here so I'm gonna share that screen here's a Hands-On part all right I want to do a tab there we go now this is this is my GitHub project um and there's a few things in here that are helpful for you so first of all there's the getting started but you know it walks you through it's just a simple python app um the the report and this is um this is generated by gp4 I used my killer video schema which some of you are probably familiar with and I just had it evaluate and it gave me lots of really cool stuff now I put a lot of context in it and that's kind of important to get good um good output but what I did was I had it I said yeah tell me the use cases and it nailed it said yeah here's video content analytics blah blah blah but then I said give me some generative AI features and data model enhancements and it did it uh it it said hey you know what what would be cool is content summarization or video recommendations and it gives you the like the table breakdowns and everything and I'll show you how I did this it's not that crazy but it did take some good prompting um content sentiment analysis um tag generation for videos which I thought was really clever and it's because each video has a tag on it for like the different kind of content it is like what if you could just have the AI look like look at the video description or something and then create some tags automatically great so that is what it does now um I've I've gone down this path a few times of just using something like chat TBT to do some work and it's pretty cool how that actually happens so I'm gonna I'm gon to show you how well here's a few things you should see in here there's if we go into the test directory there's a schema you could play around there's the entire killer video schema in here um if you want to play around with somebody else's schema great here you go um this is a really this is a really good application schema if you want to play with it and um it works on 311 all the way up um and I don't think I have any SEI things in here yet so it's pretty basic it doesn't need to be complicated you just you just need a schema for it to look at and chew on um I also have some some prompts in here too these and there helper docks and uh as a matter of fact I I had to do some work with this like adding a node with no context or with context um but I have a prompt question in here that this is what I actually use to build that um that schema or output for that schema it takes the docs for Vector on the consenter website adds that as context and then it has this thing your job is to create a report for end users here's the template blah blah blah um does all that for you all right I'm GNA show you how that would actually work so let's stop sharing here and I'm G to go over here and I'm going to share my I'm gonna share some chat GPT actually what I think I'm gonna do I'm just gonna share the whole window all right now I'm going to take uh that schema let's go let's take a look at the schema here now this is stuff that you could do immediately so I'm just going to copy all of it I'm gonna go over to chat GPT and say all right you are a eron let me know if you can see this okay or if I need to zoom in more because I know you're watching um Apache Cassandra expert okay so now we've created um created the the role okay sorry and aache I love grammarly it just tells me how bad I am in English um you are uh all right thanks a you are to build reports for schemas oops you know it's live when I make those kind of mistakes all right here is one to evaluate break down use cases okay and then what I do is I just paste in that whole entire schema and watch it work and it's really cool because I don't know if you've done this before if you haven't used chat GPT for this sort of thing it does work really really well so I'm using gp4 um and it's been updated since I think September so there's enough information in here to be dangerous with Cassandra um I can add context and I will for Vector because what's funny is when we say watch what happens when I add it to add more use cases um it's gonna it's going to show the limits of its knowledge but we can fix that right and um what I this is also probably a good thing to look see how slow this is going that's the LM experience that's why I want to minimize in an application how much interaction I have with an llm and I want to minimize the amount of information it has to create to respond back to the user and these are just slow now there are companies like grock that built a chip that are so much faster but as it sits right now uh any data that goes through an llm is going to be like the old dial at modem speed um but it is doing a good job it it broke down every one of the the the tables in the system gave it purpose a use case I mean that's pretty cool right um and I've used this a few times to sort through now again before you go and just do this make sure it's legally cool with your company you do not want to get in trouble for this and there I have some other ways to do that um for instance for data Stacks we're uh we can use Azure and um use GPT and Azure for sensitive things because we have our own version of a GPT in there that's something our legal department vetted out and said this is cool you can do this one thing and and we still have to ask for certain things I I'm giving you this as an example because you should be doing the same thing you do not want to get in trouble because there's just a lot of fud people are afraid of llms and don't get in trouble but anyway here we are and I'm gonna say can you Rec recommend I can't even spell but this this is great I could completely misspell it and it still knows it yeah it's like I have no idea what you're talking about so I would do this like that now we have some context to give it so what is that context uh go over to helper dock here's some context this prompt question and right here um you include the use of con Stander Vector data support now what I did is I just copy the docs from the cander website and use that in my prompt so I'm like okay that's enough stop stop so I'm just going to put this in and what what I what I'm telling you is like okay I want you to use castander Vector here's the docs here's the task and I'm giving it context that's something that is similar to if I use a like a a search or like a rag I'm just doing manual rag M rag but the this rag I just like the results that I would get would go into the context now I'm giving it context I'm like okay here's everything about a vector database go use that Watch What Happens it's like aha now it's like this is my favorite feature of Chad gbt now it's like oh I I knew that given the comprehensive details about using Vector data support Cassandra so it's like almost like thankful but now it's not only giving me the answers it's giving me code it's giving me an alter table that's cool and it works I've tested this it it does work but I think the takeaway here is when you're in the mode of trying to figure out an implementation and you're using your existing data um you can go down this path if I want to use a vector database or I want to add Vector you know C Stander 5 stuff or ESC or as ra or not and you can give it to schema and do some ideation inside here one of the things that LMS are really good at is being concise about taking tons of information and answering a concise question um you're like what about non Vector rag use cases so it's going to think about that and it knows what rag is which is cool um this is why I use CH you know chat GPT 4 is the best model out there right now um there are many and gp4 still kind of kills them all and that's okay um you know it's it's competition is good although I was gonna have uh we could have some fun I also have a Gemini 1.5 and Gemini 1.5 is pretty slick right now too it's uh it does things different differently and it has different context inside of its own search engine history because you know Gemini was trained off with Google data but um it could do some really cool things too so we'll do that real quick and have fun with it but anyway you could see what it's trying to do here is like hey you know you could do this and here's here's how you can set it up I can even have it write some of the basic code as well but we'll get to that in a minute but um actually right now let's let's switch gear let's go back to what we're trying to do stop sharing and go back to sharing your window all right back to this all right so we did some stuff here we did some Hands-On so exploring use cases my my encouragement to you is go out there and try this and you will be pleasantly surprised at how it does work work and it can give you some good ideas you're using gen to create gen and you know it's it's good to you because it gets you in the game you're getting your hands on an llm you're understanding how LMS work um that's important that's very important okay so what about implementations now okay we have our idea how do we want to do this now I've you can use Lang chain and LW Index right now as they are and they are those are the two libraries that are are predominantly for building agents in gen linkchain and L index have don't completely overlap it's worth a look at both of these and I'd be I would love to do a deep dive into both um they are very different in a lot of ways uh and more importantly is that da Stacks we have our own Library called rag stack and it's uh a tested Enterprise version of both L chain and LOM index we work with both of those teams pretty extensive L rag Stacks open source you can go pip install rag stack IO and it works um it's sorry rack. but uh the link I'll put the link up in here change that one so there's the rag stack link that I put in the page um and it should be in the description as well it's a great little tool um I you know what it does is it gives you confidence to work with one of these fast moving Li and that's probably the thing you have to understand is L chain align index are moving fast because this industry is moving fast and if you want something that's a little more stable that you can rely on the on it being tested that's what we did with rag stacks and um like I said we're doing this in conjunction with L chain alignment Index this is not as as a um we're not doing it avac serially at all and it's it's good for them because they can move fast and we can provide a lot of help with that and we use it inside our product quite a bit and when it when we work with Cassandra that's the other thing is it works really seamlessly with Cassandra workflows in Astra and DSC and in open source Cassandra so I I mentioned this uh like how do I get um how do I get some code in this and building some prototypes let's go back and try this real quick I know we're coming up on the end of the hour but we'll do this quick so I'm gonna go ahead and share my screen once again um and we're going to see what let's let's play with Gemini let's see what Gemini will come up with let love this you never know right so all right I'm gonna go in here I'm gonna grab this page all right so I'm going to go back to my schema I'm gonna grab this and I'm going to pull the schema into Gemini 1.5 and I'm say here's schema so Gemini is interesting because it has a massive context window like a million right um you could pretty much upload the entire code base for Cassandra in there it might be expensive and slow so a don't but here we go but you are a uh Pache Cassandra developer in Python there we go so we just created that um you are building building a rag application using Cassandra data here is the schema one what would you build first now I'm just going to give it some code here you go uh oh I just gave it the vector search oops I gave it the wrong thing you know this is you know it's live but it is kicking out a lot of stuff huh I say stop all right here's the schema no that's not what I want why is it not copying that fine for some reason I can't copy schema anymore I have to do this copy paste there we go pH we're doing it live now it's going to go through there give me the embeddings it's going to say use embeddings but okay fine regardless what I wanted to do and I'm going to stop it real quick because I don't want to completely do this can you I'm I have no idea if this is gonna work so I'm G python Lane chain for sentiment video sentiment I have no idea if this is going to work but as most LMS go they're very confidently going to give you the answer and look at that it's creating all the code for that so that worked now I've done this a lot and I found that it is pretty good about coming up with prototype code not all of it works the way you want it to but it's easy to tweak it's a good way to get started it also opens the door for something you haven't done before like maybe I've never done sentiment analysis with a video well it gives me all the the starter like here use all this stuff and it may not be exactly what I want but it sure gets me down the road and I can import this into vs code or something and copy into vs code and make some changes or I and the best thing that it can do is I can go back and say oh no no no I want to use this library or I want to make this change to it whatever and then it'll re rewrite the code so hopefully that's helpful for you as well now let me wrap things up here a little bit because we're at the top of the hour things go fast I always enjoy my time with you so uh we're gonna switch over to my other screen and all right so we've we did that now the final thing that I'm going to cover is this measuring impact and the measurements are important so from measurement standpoints the fundamentals are still there you want to latency through put errors you need to be monitoring for that you need to be looking at those things and those are just fundamentals I mentioned that because we're going to add something to it and those new metrics are around using uh say rag um nlm something like that and those new ones are the consistency like do you get the same answer every time relevance is just the answer that I was expecting this Precision has to do with Rag and that's actually really measurement when it comes to using a vector databases um if I ask for compaction documentation does it also give me everything about SS tables and bulk loader that's a lot of noise I want just signal so how how precise are those answers and a lot of that comes into like how you do your vectors that sort of thing but Precision is important because it reduces the amount of junk that it has to go through um and then recall uh what is it actually getting what I need if I say I need something about compaction and it gives me something about something else like streaming I I don't I didn't want that well that's the recall is poor but those are things you can measure and the measurement there's some cool really interesting Cool Tools out there uh Lang Lang chain uh the library of the company has Lang Smith which actually has the name of the company but they do tracing and this is probably what you were looking for in an Enterprise application is is tracing what happened in that call and there's a great Library out there open source Library called ragas which uh integrates with lsmith a few others uh link fuse Phoenix rise and open layer that it it does in the library does all the measurements you needed to do and that um that is what's going to save you in the long run now what's next so I'm coming up at the top of the hour easy I need you to give me your feedback so I have a little content pull out there and uh this is super simple um it's one thing uh it was just here's a list of topics and if you don't see it on there then tell me what you want and I'm going to create a lot of content and I want to help you so you if you've seen my content before you know that I love to dig into topics and I would love to dig you know get deep dive put on the scuba gear and go for it um you hopefully um it's something that you your need and that I can provide so go for it get crazy and um there's nothing on here there's no like collecting email addresses or anything like that so it's pretty Anonymous just a bunch of checkboxes and then other if there's something not on that list let me know and with that I think I'm good to go I don't know if I have a lot of questions out there probably not a ton because I just poured information into you but um you all know where to find me um I'm on LinkedIn I love love inmail as long as it's not a salesperson um use that all the time on the SF slack I'm there all the time and uh of course on Twitter Patrick mcfaden and connect with me on LinkedIn I would love for you to let me know what you're doing and if you have questions I would get on a zoom call with you whatever you want I'm I'm here to help and I want to learn more about what you're up to so thank you very much I appreciate your time here with me today and take my PLL I'd love to see what you're thinking all right everyone have a great day and go make something really really cool",
    "segments": [
      {
        "start": 5.4,
        "duration": 5.84,
        "text": "hello everyone we're here for another"
      },
      {
        "start": 8.599,
        "duration": 5.04,
        "text": "Workshop I hope there's folks here if"
      },
      {
        "start": 11.24,
        "duration": 5.16,
        "text": "not I'm just going to talk to the"
      },
      {
        "start": 13.639,
        "duration": 5.201,
        "text": "recording I I started just a smidge"
      },
      {
        "start": 16.4,
        "duration": 4.48,
        "text": "early just to let everyone find a chair"
      },
      {
        "start": 18.84,
        "duration": 5.24,
        "text": "you know there's some in the back I see"
      },
      {
        "start": 20.88,
        "duration": 5.76,
        "text": "okay cool um yeah so today we're going"
      },
      {
        "start": 24.08,
        "duration": 4.88,
        "text": "to talk about some geni stuff but I"
      },
      {
        "start": 26.64,
        "duration": 3.799,
        "text": "really hope this is you're a Cassandra"
      },
      {
        "start": 28.96,
        "duration": 3.08,
        "text": "person because this is geared towards"
      },
      {
        "start": 30.439,
        "duration": 4.8,
        "text": "you if not you're going to get something"
      },
      {
        "start": 32.04,
        "duration": 5.679,
        "text": "out of this uh that's not going to be a"
      },
      {
        "start": 35.239,
        "duration": 5.681,
        "text": "problem there's a there's a lot to talk"
      },
      {
        "start": 37.719,
        "duration": 5.881,
        "text": "about gen AI isn't there and if you're"
      },
      {
        "start": 40.92,
        "duration": 5.28,
        "text": "if you're here shout out on the old chat"
      },
      {
        "start": 43.6,
        "duration": 2.6,
        "text": "I appreciate"
      },
      {
        "start": 48.84,
        "duration": 3.28,
        "text": "it all"
      },
      {
        "start": 54.68,
        "duration": 5.32,
        "text": "right okay let's get moving here because"
      },
      {
        "start": 58.079,
        "duration": 4.44,
        "text": "I got a lot of slides and a lot of demo"
      },
      {
        "start": 60.0,
        "duration": 5.519,
        "text": "going to do and let's let's just get to"
      },
      {
        "start": 62.519,
        "duration": 5.201,
        "text": "it um so today we're going to talk about"
      },
      {
        "start": 65.519,
        "duration": 3.521,
        "text": "those geni success problems when it"
      },
      {
        "start": 67.72,
        "duration": 3.64,
        "text": "comes to"
      },
      {
        "start": 69.04,
        "duration": 4.719,
        "text": "building building with Cassandra and"
      },
      {
        "start": 71.36,
        "duration": 4.92,
        "text": "this is for Cassandra teams so just a"
      },
      {
        "start": 73.759,
        "duration": 4.081,
        "text": "little bit about the I'm g i i put this"
      },
      {
        "start": 76.28,
        "duration": 4.879,
        "text": "slide up a lot and I think it's"
      },
      {
        "start": 77.84,
        "duration": 5.84,
        "text": "important for you to see this is you"
      },
      {
        "start": 81.159,
        "duration": 5.401,
        "text": "maybe um are you feeling you know"
      },
      {
        "start": 83.68,
        "duration": 5.92,
        "text": "stressed out feeling behind that boss"
      },
      {
        "start": 86.56,
        "duration": 5.72,
        "text": "that like hey we need an AI if that's"
      },
      {
        "start": 89.6,
        "duration": 5.559,
        "text": "you you're in the right place today and"
      },
      {
        "start": 92.28,
        "duration": 6.479,
        "text": "I I'm GNA this is my safe Safe Harbor"
      },
      {
        "start": 95.159,
        "duration": 5.761,
        "text": "comment and uh by that I mean we're all"
      },
      {
        "start": 98.759,
        "duration": 4.72,
        "text": "kind of feeling this let's just be uh"
      },
      {
        "start": 100.92,
        "duration": 4.96,
        "text": "helpful and work with each other and you"
      },
      {
        "start": 103.479,
        "duration": 5.68,
        "text": "know this is a community of users"
      },
      {
        "start": 105.88,
        "duration": 5.0,
        "text": "right um communities we we stick"
      },
      {
        "start": 109.159,
        "duration": 4.0,
        "text": "together and we help each other and we"
      },
      {
        "start": 110.88,
        "duration": 4.239,
        "text": "make things work together and it's"
      },
      {
        "start": 113.159,
        "duration": 3.801,
        "text": "always been my experience to technology"
      },
      {
        "start": 115.119,
        "duration": 4.28,
        "text": "especially when technology growes"
      },
      {
        "start": 116.96,
        "duration": 4.479,
        "text": "through these ripples like they are now"
      },
      {
        "start": 119.399,
        "duration": 4.68,
        "text": "that we all depend on each other to get"
      },
      {
        "start": 121.439,
        "duration": 5.28,
        "text": "through without you know getting them"
      },
      {
        "start": 124.079,
        "duration": 4.96,
        "text": "scarred and beat up and destroyed and"
      },
      {
        "start": 126.719,
        "duration": 4.88,
        "text": "this is no different from any other time"
      },
      {
        "start": 129.039,
        "duration": 4.881,
        "text": "so um it's not a place for someone to"
      },
      {
        "start": 131.599,
        "duration": 5.081,
        "text": "say that they are more Superior than"
      },
      {
        "start": 133.92,
        "duration": 4.599,
        "text": "someone else or that you know people are"
      },
      {
        "start": 136.68,
        "duration": 3.48,
        "text": "falling behind and we leave them behind"
      },
      {
        "start": 138.519,
        "duration": 3.201,
        "text": "if you see someone falling behind ask"
      },
      {
        "start": 140.16,
        "duration": 4.079,
        "text": "them can I help and if you're falling"
      },
      {
        "start": 141.72,
        "duration": 5.48,
        "text": "behind hey ask for help we're always"
      },
      {
        "start": 144.239,
        "duration": 4.881,
        "text": "here for you and of course let's do this"
      },
      {
        "start": 147.2,
        "duration": 4.16,
        "text": "together so that being said let's move"
      },
      {
        "start": 149.12,
        "duration": 6.16,
        "text": "on to the Contex so today's"
      },
      {
        "start": 151.36,
        "duration": 6.64,
        "text": "agenda I'm going to do a oneone level of"
      },
      {
        "start": 155.28,
        "duration": 4.239,
        "text": "generative Ai and if this is something"
      },
      {
        "start": 158.0,
        "duration": 2.92,
        "text": "that's new to you hopefully you'll get"
      },
      {
        "start": 159.519,
        "duration": 2.8,
        "text": "something out of it it's something not"
      },
      {
        "start": 160.92,
        "duration": 3.72,
        "text": "new to you hopefully you'll pick up some"
      },
      {
        "start": 162.319,
        "duration": 4.081,
        "text": "nuggets along the way I've been working"
      },
      {
        "start": 164.64,
        "duration": 3.959,
        "text": "with generative AI for quite a while now"
      },
      {
        "start": 166.4,
        "duration": 4.16,
        "text": "for about over a year now and especially"
      },
      {
        "start": 168.599,
        "duration": 4.761,
        "text": "around Cassandra I've learned some"
      },
      {
        "start": 170.56,
        "duration": 6.36,
        "text": "things but I've also figured out the"
      },
      {
        "start": 173.36,
        "duration": 4.92,
        "text": "code it's not as crazy as you think and"
      },
      {
        "start": 176.92,
        "duration": 2.8,
        "text": "I think it's important that we have"
      },
      {
        "start": 178.28,
        "duration": 4.239,
        "text": "these discussions around this idea of"
      },
      {
        "start": 179.72,
        "duration": 4.239,
        "text": "like like okay um here's the tool here's"
      },
      {
        "start": 182.519,
        "duration": 3.08,
        "text": "all the different parts and this is how"
      },
      {
        "start": 183.959,
        "duration": 5.161,
        "text": "it works for you it's not magical it's"
      },
      {
        "start": 185.599,
        "duration": 5.28,
        "text": "not mystical it's just another tool and"
      },
      {
        "start": 189.12,
        "duration": 5.32,
        "text": "then I'll go into how cassander fits"
      },
      {
        "start": 190.879,
        "duration": 6.961,
        "text": "into this whole process and it does and"
      },
      {
        "start": 194.44,
        "duration": 4.84,
        "text": "you being a cander team orander person"
      },
      {
        "start": 197.84,
        "duration": 3.28,
        "text": "you've got an opportunity in front of"
      },
      {
        "start": 199.28,
        "duration": 3.039,
        "text": "you and I want to help you with that and"
      },
      {
        "start": 201.12,
        "duration": 2.92,
        "text": "then finally we're going to go through"
      },
      {
        "start": 202.319,
        "duration": 3.2,
        "text": "some more practical stuff like those"
      },
      {
        "start": 204.04,
        "duration": 3.68,
        "text": "that three steps"
      },
      {
        "start": 205.519,
        "duration": 4.321,
        "text": "production and what I want to do is go"
      },
      {
        "start": 207.72,
        "duration": 4.32,
        "text": "through uh there's some Hands-On some"
      },
      {
        "start": 209.84,
        "duration": 3.52,
        "text": "practical information along the way it"
      },
      {
        "start": 212.04,
        "duration": 4.04,
        "text": "by no means are we going to build"
      },
      {
        "start": 213.36,
        "duration": 5.079,
        "text": "anything today we we have an hour but um"
      },
      {
        "start": 216.08,
        "duration": 4.32,
        "text": "at the end of this whole thing I wanna"
      },
      {
        "start": 218.439,
        "duration": 4.281,
        "text": "I'm gonna I have a little poll that I W"
      },
      {
        "start": 220.4,
        "duration": 4.559,
        "text": "to ask everyone so just keep in mind"
      },
      {
        "start": 222.72,
        "duration": 4.04,
        "text": "when you're watching this what other"
      },
      {
        "start": 224.959,
        "duration": 4.801,
        "text": "topics I want to do one of these maybe"
      },
      {
        "start": 226.76,
        "duration": 4.64,
        "text": "every couple weeks and dig dig into each"
      },
      {
        "start": 229.76,
        "duration": 4.52,
        "text": "topic but I want to do the ones that you"
      },
      {
        "start": 231.4,
        "duration": 6.16,
        "text": "need I've done this for years and I"
      },
      {
        "start": 234.28,
        "duration": 5.84,
        "text": "think this is the bright way to go I can"
      },
      {
        "start": 237.56,
        "duration": 4.399,
        "text": "come up with topics trust me but as"
      },
      {
        "start": 240.12,
        "duration": 3.52,
        "text": "you're going along through this content"
      },
      {
        "start": 241.959,
        "duration": 4.92,
        "text": "if you say hey can you dig into that"
      },
      {
        "start": 243.64,
        "duration": 5.319,
        "text": "more great um I want to hear about it"
      },
      {
        "start": 246.879,
        "duration": 3.761,
        "text": "and I want to do it so just keep that in"
      },
      {
        "start": 248.959,
        "duration": 4.56,
        "text": "mind as we go"
      },
      {
        "start": 250.64,
        "duration": 4.239,
        "text": "along so let's begin with the basics of"
      },
      {
        "start": 253.519,
        "duration": 4.201,
        "text": "generative"
      },
      {
        "start": 254.879,
        "duration": 7.521,
        "text": "Ai and that that conversation will not"
      },
      {
        "start": 257.72,
        "duration": 6.479,
        "text": "start with just Ai and I think this is a"
      },
      {
        "start": 262.4,
        "duration": 4.2,
        "text": "this has been a problem that we've had"
      },
      {
        "start": 264.199,
        "duration": 3.801,
        "text": "for years is we have many flavors of AI"
      },
      {
        "start": 266.6,
        "duration": 4.319,
        "text": "and it's kind of gets Blended together"
      },
      {
        "start": 268.0,
        "duration": 5.6,
        "text": "and mushed into a ball and they are very"
      },
      {
        "start": 270.919,
        "duration": 5.321,
        "text": "different uh AI is not new if you know"
      },
      {
        "start": 273.6,
        "duration": 4.879,
        "text": "if you w to you want to get a"
      },
      {
        "start": 276.24,
        "duration": 3.679,
        "text": "opinionation from an AI person they're"
      },
      {
        "start": 278.479,
        "duration": 3.881,
        "text": "GNA be like it's been around for a"
      },
      {
        "start": 279.919,
        "duration": 4.56,
        "text": "million years well it has I mean AI was"
      },
      {
        "start": 282.36,
        "duration": 4.24,
        "text": "a as soon as computers became a thing"
      },
      {
        "start": 284.479,
        "duration": 4.16,
        "text": "everyone started talking about AI that"
      },
      {
        "start": 286.6,
        "duration": 5.36,
        "text": "was a long long time ago in a galaxy far"
      },
      {
        "start": 288.639,
        "duration": 5.0,
        "text": "far away but there are definely there"
      },
      {
        "start": 291.96,
        "duration": 5.56,
        "text": "are definitely like some buckets for"
      },
      {
        "start": 293.639,
        "duration": 7.201,
        "text": "that so starting with decision support"
      },
      {
        "start": 297.52,
        "duration": 4.76,
        "text": "and this has been around for a long time"
      },
      {
        "start": 300.84,
        "duration": 4.12,
        "text": "it's actually one of the first things"
      },
      {
        "start": 302.28,
        "duration": 5.12,
        "text": "around AI was like Hey how do we have a"
      },
      {
        "start": 304.96,
        "duration": 4.84,
        "text": "a smart decision machine and it's more"
      },
      {
        "start": 307.4,
        "duration": 4.68,
        "text": "than just to say a state machine where"
      },
      {
        "start": 309.8,
        "duration": 4.0,
        "text": "this input makes it do that that input"
      },
      {
        "start": 312.08,
        "duration": 3.8,
        "text": "makes it do this you know that sort of"
      },
      {
        "start": 313.8,
        "duration": 4.36,
        "text": "thing it it's putting some sort of"
      },
      {
        "start": 315.88,
        "duration": 5.84,
        "text": "intelligence around decision-"
      },
      {
        "start": 318.16,
        "duration": 5.96,
        "text": "making um the the ultimate thing would"
      },
      {
        "start": 321.72,
        "duration": 4.08,
        "text": "be like autonomous driving which you"
      },
      {
        "start": 324.12,
        "duration": 3.76,
        "text": "know if you're driving a Tesla right now"
      },
      {
        "start": 325.8,
        "duration": 3.48,
        "text": "you probably got it for free right now"
      },
      {
        "start": 327.88,
        "duration": 2.68,
        "text": "yeah but you know where it is right now"
      },
      {
        "start": 329.28,
        "duration": 3.24,
        "text": "it's not"
      },
      {
        "start": 330.56,
        "duration": 4.68,
        "text": "it's not 100% I would not trust my car"
      },
      {
        "start": 332.52,
        "duration": 4.88,
        "text": "to drive me around it's cool for cruise"
      },
      {
        "start": 335.24,
        "duration": 4.44,
        "text": "control but that's the kind of that"
      },
      {
        "start": 337.4,
        "duration": 4.799,
        "text": "decision support system the AI it's not"
      },
      {
        "start": 339.68,
        "duration": 4.04,
        "text": "intelligent as so much as it's making"
      },
      {
        "start": 342.199,
        "duration": 3.361,
        "text": "good decisions or it's trying to make"
      },
      {
        "start": 343.72,
        "duration": 4.8,
        "text": "good decisions and that's been around"
      },
      {
        "start": 345.56,
        "duration": 5.16,
        "text": "for a while now recognition like facial"
      },
      {
        "start": 348.52,
        "duration": 4.399,
        "text": "recognition name recognition all these"
      },
      {
        "start": 350.72,
        "duration": 5.52,
        "text": "different recognitions"
      },
      {
        "start": 352.919,
        "duration": 5.361,
        "text": "um those are uh those have also been"
      },
      {
        "start": 356.24,
        "duration": 4.04,
        "text": "around for quite a while too in various"
      },
      {
        "start": 358.28,
        "duration": 3.96,
        "text": "forms uh facial recog nition has been"
      },
      {
        "start": 360.28,
        "duration": 5.44,
        "text": "around for quite a while um ask any"
      },
      {
        "start": 362.24,
        "duration": 6.44,
        "text": "government agency but the recognition"
      },
      {
        "start": 365.72,
        "duration": 4.44,
        "text": "part um has been kind of a foundation"
      },
      {
        "start": 368.68,
        "duration": 2.72,
        "text": "for what's happening in generative Ai"
      },
      {
        "start": 370.16,
        "duration": 3.8,
        "text": "and I'll talk about that in a minute but"
      },
      {
        "start": 371.4,
        "duration": 4.32,
        "text": "that's been pretty well established now"
      },
      {
        "start": 373.96,
        "duration": 3.799,
        "text": "the one that we've probably all been"
      },
      {
        "start": 375.72,
        "duration": 4.599,
        "text": "exposed to the most in the past few"
      },
      {
        "start": 377.759,
        "duration": 4.481,
        "text": "years is predictive and predictive AI is"
      },
      {
        "start": 380.319,
        "duration": 3.72,
        "text": "the what we would normally call like"
      },
      {
        "start": 382.24,
        "duration": 6.72,
        "text": "machine learning like how do I get you"
      },
      {
        "start": 384.039,
        "duration": 6.921,
        "text": "to buy one more thing um and that's the"
      },
      {
        "start": 388.96,
        "duration": 3.6,
        "text": "inference in that sort of thing like you"
      },
      {
        "start": 390.96,
        "duration": 3.16,
        "text": "take this training data you figure out"
      },
      {
        "start": 392.56,
        "duration": 3.28,
        "text": "what everyone's buying and you try to"
      },
      {
        "start": 394.12,
        "duration": 3.44,
        "text": "create a model and then you put it out"
      },
      {
        "start": 395.84,
        "duration": 3.919,
        "text": "and then you when someone goes into"
      },
      {
        "start": 397.56,
        "duration": 3.84,
        "text": "their shopping cart you're like but wait"
      },
      {
        "start": 399.759,
        "duration": 3.72,
        "text": "have you thought about this that's"
      },
      {
        "start": 401.4,
        "duration": 5.84,
        "text": "predictive Ai and it is cornered around"
      },
      {
        "start": 403.479,
        "duration": 7.641,
        "text": "a lot of that ml the mlops world um ml"
      },
      {
        "start": 407.24,
        "duration": 6.72,
        "text": "ml ML and predictive has definitely been"
      },
      {
        "start": 411.12,
        "duration": 4.96,
        "text": "the dominant version of AI for years and"
      },
      {
        "start": 413.96,
        "duration": 3.28,
        "text": "years and years and it's because it"
      },
      {
        "start": 416.08,
        "duration": 4.76,
        "text": "actually makes"
      },
      {
        "start": 417.24,
        "duration": 6.399,
        "text": "money that's that's important right and"
      },
      {
        "start": 420.84,
        "duration": 5.759,
        "text": "or at least in some places it has I"
      },
      {
        "start": 423.639,
        "duration": 5.321,
        "text": "would say that Google is Google because"
      },
      {
        "start": 426.599,
        "duration": 4.32,
        "text": "of predictive AI they they know how to"
      },
      {
        "start": 428.96,
        "duration": 4.919,
        "text": "put an ad in front of you really quickly"
      },
      {
        "start": 430.919,
        "duration": 6.081,
        "text": "and that makes them bazillions of"
      },
      {
        "start": 433.879,
        "duration": 7.121,
        "text": "dollars so here we are the next"
      },
      {
        "start": 437.0,
        "duration": 8.759,
        "text": "generation is Generation generative AI"
      },
      {
        "start": 441.0,
        "duration": 6.8,
        "text": "um not it's not new but it has a step"
      },
      {
        "start": 445.759,
        "duration": 3.761,
        "text": "function generative AI has been"
      },
      {
        "start": 447.8,
        "duration": 4.48,
        "text": "something about and of course that means"
      },
      {
        "start": 449.52,
        "duration": 6.32,
        "text": "means creating new content creating new"
      },
      {
        "start": 452.28,
        "duration": 4.919,
        "text": "data um and that has been around for a"
      },
      {
        "start": 455.84,
        "duration": 5.96,
        "text": "while but it just hasn't been super"
      },
      {
        "start": 457.199,
        "duration": 6.361,
        "text": "useful or very effective and that uh"
      },
      {
        "start": 461.8,
        "duration": 5.359,
        "text": "that has just kept it pretty much in the"
      },
      {
        "start": 463.56,
        "duration": 8.16,
        "text": "back room or you know an Academia and it"
      },
      {
        "start": 467.159,
        "duration": 8.241,
        "text": "took good old open Ai and chat GPT to"
      },
      {
        "start": 471.72,
        "duration": 5.919,
        "text": "really blow us away and it happened fast"
      },
      {
        "start": 475.4,
        "duration": 3.6,
        "text": "I think everyone remembers that's in"
      },
      {
        "start": 477.639,
        "duration": 3.361,
        "text": "this industry can remember like when"
      },
      {
        "start": 479.0,
        "duration": 4.08,
        "text": "chat G came online it just changed"
      },
      {
        "start": 481.0,
        "duration": 3.199,
        "text": "everything everyone just lost their mind"
      },
      {
        "start": 483.08,
        "duration": 2.36,
        "text": "because you could go in and have this"
      },
      {
        "start": 484.199,
        "duration": 3.361,
        "text": "conversation with this thing and it"
      },
      {
        "start": 485.44,
        "duration": 3.56,
        "text": "knows stuff and it just does stuff and"
      },
      {
        "start": 487.56,
        "duration": 3.079,
        "text": "and we got a little carried away we're"
      },
      {
        "start": 489.0,
        "duration": 4.319,
        "text": "like oh that's it it's the end of the"
      },
      {
        "start": 490.639,
        "duration": 4.921,
        "text": "universe we're done AI is g to take over"
      },
      {
        "start": 493.319,
        "duration": 3.801,
        "text": "everything yeah not really but it is"
      },
      {
        "start": 495.56,
        "duration": 3.44,
        "text": "really really helpful and useful for a"
      },
      {
        "start": 497.12,
        "duration": 4.96,
        "text": "lot of applications and that also"
      },
      {
        "start": 499.0,
        "duration": 6.84,
        "text": "includes not just text but also like do"
      },
      {
        "start": 502.08,
        "duration": 5.48,
        "text": "um Dolly DOL uh the building images and"
      },
      {
        "start": 505.84,
        "duration": 4.039,
        "text": "if you use mid Journey or something like"
      },
      {
        "start": 507.56,
        "duration": 4.359,
        "text": "that stable diffusion this those are"
      },
      {
        "start": 509.879,
        "duration": 3.441,
        "text": "really cool because that's creating an"
      },
      {
        "start": 511.919,
        "duration": 5.401,
        "text": "image and now that's also creating"
      },
      {
        "start": 513.32,
        "duration": 6.279,
        "text": "meltdowns like the Sora videos of"
      },
      {
        "start": 517.32,
        "duration": 4.839,
        "text": "creating you like from taking text and"
      },
      {
        "start": 519.599,
        "duration": 4.001,
        "text": "making a movie out of it there's a lot"
      },
      {
        "start": 522.159,
        "duration": 4.841,
        "text": "of people having a little existential"
      },
      {
        "start": 523.6,
        "duration": 8.2,
        "text": "crisis but that's all generative and"
      },
      {
        "start": 527.0,
        "duration": 7.399,
        "text": "it's based on this idea of a model and"
      },
      {
        "start": 531.8,
        "duration": 4.039,
        "text": "the model building the model this is"
      },
      {
        "start": 534.399,
        "duration": 3.361,
        "text": "what we all talk about like when we talk"
      },
      {
        "start": 535.839,
        "duration": 4.801,
        "text": "about you know hey what about this model"
      },
      {
        "start": 537.76,
        "duration": 4.72,
        "text": "gp4 blah blah blah those models are all"
      },
      {
        "start": 540.64,
        "duration": 4.68,
        "text": "built for a couple of different there's"
      },
      {
        "start": 542.48,
        "duration": 4.96,
        "text": "a couple of different flavors of models"
      },
      {
        "start": 545.32,
        "duration": 5.28,
        "text": "there's these text based models which we"
      },
      {
        "start": 547.44,
        "duration": 5.32,
        "text": "are familiar with in chat and it takes"
      },
      {
        "start": 550.6,
        "duration": 5.08,
        "text": "an input like hey how would you describe"
      },
      {
        "start": 552.76,
        "duration": 4.84,
        "text": "a cat and it outputs something like hey"
      },
      {
        "start": 555.68,
        "duration": 5.2,
        "text": "a cat is a small domestic animal blah"
      },
      {
        "start": 557.6,
        "duration": 5.56,
        "text": "blah blah and it's it's great as a"
      },
      {
        "start": 560.88,
        "duration": 5.28,
        "text": "demonstration for sure but it also gives"
      },
      {
        "start": 563.16,
        "duration": 4.88,
        "text": "us an ability that we haven't had before"
      },
      {
        "start": 566.16,
        "duration": 4.16,
        "text": "which is we can do like natural language"
      },
      {
        "start": 568.04,
        "duration": 4.64,
        "text": "communication with a computer"
      },
      {
        "start": 570.32,
        "duration": 5.84,
        "text": "and if you view something like Siri or"
      },
      {
        "start": 572.68,
        "duration": 5.839,
        "text": "something like that um you know that or"
      },
      {
        "start": 576.16,
        "duration": 5.96,
        "text": "Alexa it's like it it picks up a few"
      },
      {
        "start": 578.519,
        "duration": 6.921,
        "text": "words not all of them and it's great I"
      },
      {
        "start": 582.12,
        "duration": 5.24,
        "text": "can like I can make a mistake I would I"
      },
      {
        "start": 585.44,
        "duration": 4.76,
        "text": "would say how you describe a kitty cat"
      },
      {
        "start": 587.36,
        "duration": 6.32,
        "text": "or you know use a different language or"
      },
      {
        "start": 590.2,
        "duration": 6.6,
        "text": "whatever and it's figures it out um so"
      },
      {
        "start": 593.68,
        "duration": 5.32,
        "text": "that's that's a big step function in in"
      },
      {
        "start": 596.8,
        "duration": 3.159,
        "text": "what we can do with these models and"
      },
      {
        "start": 599.0,
        "duration": 3.68,
        "text": "then for"
      },
      {
        "start": 599.959,
        "duration": 5.721,
        "text": "images a different kind of model um"
      },
      {
        "start": 602.68,
        "duration": 4.68,
        "text": "these are um these are built on this"
      },
      {
        "start": 605.68,
        "duration": 4.08,
        "text": "idea of building up images but it's"
      },
      {
        "start": 607.36,
        "duration": 4.039,
        "text": "still it's still in that kind of a"
      },
      {
        "start": 609.76,
        "duration": 4.48,
        "text": "generative AI world and but it's a"
      },
      {
        "start": 611.399,
        "duration": 6.56,
        "text": "little different text um images video"
      },
      {
        "start": 614.24,
        "duration": 6.159,
        "text": "audio they're all in this U same class"
      },
      {
        "start": 617.959,
        "duration": 5.12,
        "text": "of of generative models um where they"
      },
      {
        "start": 620.399,
        "duration": 3.961,
        "text": "build up this the information but they"
      },
      {
        "start": 623.079,
        "duration": 5.481,
        "text": "take one"
      },
      {
        "start": 624.36,
        "duration": 6.88,
        "text": "thing and the the basis of this is"
      },
      {
        "start": 628.56,
        "duration": 5.32,
        "text": "taking an instru instruction so and I'm"
      },
      {
        "start": 631.24,
        "duration": 5.279,
        "text": "going to use instructions um instead of"
      },
      {
        "start": 633.88,
        "duration": 5.24,
        "text": "saying um text prompt I'm going to say"
      },
      {
        "start": 636.519,
        "duration": 4.601,
        "text": "instruction they they take instructions"
      },
      {
        "start": 639.12,
        "duration": 4.8,
        "text": "and what they do is they encode it so"
      },
      {
        "start": 641.12,
        "duration": 4.32,
        "text": "they uh they break it down into pieces"
      },
      {
        "start": 643.92,
        "duration": 4.479,
        "text": "and then you hear this word tokenized"
      },
      {
        "start": 645.44,
        "duration": 4.88,
        "text": "tokenizing isn't like just every word"
      },
      {
        "start": 648.399,
        "duration": 4.721,
        "text": "it's it's like phenomes and there's just"
      },
      {
        "start": 650.32,
        "duration": 5.24,
        "text": "a whole branch of tokenization but like"
      },
      {
        "start": 653.12,
        "duration": 5.64,
        "text": "what what what that is um not important"
      },
      {
        "start": 655.56,
        "duration": 6.32,
        "text": "for today but I want you to see this as"
      },
      {
        "start": 658.76,
        "duration": 5.319,
        "text": "like okay this is a this is a machine a"
      },
      {
        "start": 661.88,
        "duration": 4.639,
        "text": "black box type thing but it's not too"
      },
      {
        "start": 664.079,
        "duration": 4.641,
        "text": "mystical there's no magic going on here"
      },
      {
        "start": 666.519,
        "duration": 4.081,
        "text": "it takes instructions encodes those"
      },
      {
        "start": 668.72,
        "duration": 4.64,
        "text": "instructions and then passes it through"
      },
      {
        "start": 670.6,
        "duration": 4.919,
        "text": "a really complicated bit of uh Matrix"
      },
      {
        "start": 673.36,
        "duration": 4.719,
        "text": "math decodes it ways and does"
      },
      {
        "start": 675.519,
        "duration": 5.721,
        "text": "predictions and then it gives you output"
      },
      {
        "start": 678.079,
        "duration": 6.041,
        "text": "that that process is really intense um"
      },
      {
        "start": 681.24,
        "duration": 5.44,
        "text": "it takes a this is what you all see now"
      },
      {
        "start": 684.12,
        "duration": 4.64,
        "text": "is nvidia's stock is through the roof"
      },
      {
        "start": 686.68,
        "duration": 5.599,
        "text": "because it takes the biggest Nvidia"
      },
      {
        "start": 688.76,
        "duration": 5.92,
        "text": "cards they have to do this sort of thing"
      },
      {
        "start": 692.279,
        "duration": 5.201,
        "text": "and it's not just and it's not just"
      },
      {
        "start": 694.68,
        "duration": 6.159,
        "text": "running the llm it's the training and"
      },
      {
        "start": 697.48,
        "duration": 4.84,
        "text": "that that's the next slide but um just"
      },
      {
        "start": 700.839,
        "duration": 3.881,
        "text": "understanding that this is more or less"
      },
      {
        "start": 702.32,
        "duration": 4.68,
        "text": "a machine of some sort is you takes"
      },
      {
        "start": 704.72,
        "duration": 4.679,
        "text": "instruction does some work produces"
      },
      {
        "start": 707.0,
        "duration": 6.6,
        "text": "output that's what all these are the the"
      },
      {
        "start": 709.399,
        "duration": 6.481,
        "text": "gp4s Llama 3s um Mist draws they all do"
      },
      {
        "start": 713.6,
        "duration": 4.2,
        "text": "this in some way and some most of them"
      },
      {
        "start": 715.88,
        "duration": 6.759,
        "text": "in their own way but the"
      },
      {
        "start": 717.8,
        "duration": 7.92,
        "text": "killer the Killer app is this which is"
      },
      {
        "start": 722.639,
        "duration": 4.2,
        "text": "the training and so that's where you see"
      },
      {
        "start": 725.72,
        "duration": 4.2,
        "text": "like what's the difference between"
      },
      {
        "start": 726.839,
        "duration": 5.601,
        "text": "between 3.5 and four and potentially"
      },
      {
        "start": 729.92,
        "duration": 5.479,
        "text": "five mostly it's around how much"
      },
      {
        "start": 732.44,
        "duration": 5.199,
        "text": "training they do now we won't go into"
      },
      {
        "start": 735.399,
        "duration": 4.481,
        "text": "like how it's being trained I what I"
      },
      {
        "start": 737.639,
        "duration": 3.921,
        "text": "would like to talk about is what it's"
      },
      {
        "start": 739.88,
        "duration": 5.44,
        "text": "being trained with and this is what"
      },
      {
        "start": 741.56,
        "duration": 5.959,
        "text": "makes it a useful machine for us is that"
      },
      {
        "start": 745.32,
        "duration": 5.199,
        "text": "the LMS are being trained with"
      },
      {
        "start": 747.519,
        "duration": 4.32,
        "text": "information like um you know like how"
      },
      {
        "start": 750.519,
        "duration": 3.44,
        "text": "people interact with each other when"
      },
      {
        "start": 751.839,
        "duration": 4.841,
        "text": "they help like stack"
      },
      {
        "start": 753.959,
        "duration": 5.041,
        "text": "Overflow um you know the information"
      },
      {
        "start": 756.68,
        "duration": 5.08,
        "text": "from an article and this is a really"
      },
      {
        "start": 759.0,
        "duration": 5.6,
        "text": "important Point how people in specific"
      },
      {
        "start": 761.76,
        "duration": 5.6,
        "text": "roles respond and we'll get to that in a"
      },
      {
        "start": 764.6,
        "duration": 4.96,
        "text": "minute but that because there could be"
      },
      {
        "start": 767.36,
        "duration": 3.64,
        "text": "different responses from different roles"
      },
      {
        "start": 769.56,
        "duration": 4.079,
        "text": "and that's that's something you can"
      },
      {
        "start": 771.0,
        "duration": 5.8,
        "text": "encode and important and then you know"
      },
      {
        "start": 773.639,
        "duration": 5.281,
        "text": "code examples is another thing um it"
      },
      {
        "start": 776.8,
        "duration": 5.279,
        "text": "pours a lot of information but also a"
      },
      {
        "start": 778.92,
        "duration": 5.719,
        "text": "lot of the the navigation of information"
      },
      {
        "start": 782.079,
        "duration": 5.681,
        "text": "into these models and these huge neural"
      },
      {
        "start": 784.639,
        "duration": 6.2,
        "text": "Nets and what we get out of it is a"
      },
      {
        "start": 787.76,
        "duration": 5.84,
        "text": "thing that could do not only uh"
      },
      {
        "start": 790.839,
        "duration": 5.0,
        "text": "intelligent is responses I say is"
      },
      {
        "start": 793.6,
        "duration": 4.76,
        "text": "because there's moments but also some"
      },
      {
        "start": 795.839,
        "duration": 4.36,
        "text": "reasoning like how things come to where"
      },
      {
        "start": 798.36,
        "duration": 4.919,
        "text": "they are um there's some really"
      },
      {
        "start": 800.199,
        "duration": 4.76,
        "text": "interesting videos out there um from"
      },
      {
        "start": 803.279,
        "duration": 3.92,
        "text": "some of the founders of open AI that"
      },
      {
        "start": 804.959,
        "duration": 5.481,
        "text": "talk about how they're actually encoding"
      },
      {
        "start": 807.199,
        "duration": 6.08,
        "text": "a person's role or how they respond to"
      },
      {
        "start": 810.44,
        "duration": 5.399,
        "text": "something into these llms which gives it"
      },
      {
        "start": 813.279,
        "duration": 4.761,
        "text": "a flavor that reasoning like if you were"
      },
      {
        "start": 815.839,
        "duration": 3.961,
        "text": "to ask yourself hey how would you solve"
      },
      {
        "start": 818.04,
        "duration": 4.479,
        "text": "this problem you go through these steps"
      },
      {
        "start": 819.8,
        "duration": 4.2,
        "text": "well that got encoded into this LM and"
      },
      {
        "start": 822.519,
        "duration": 4.241,
        "text": "we could use that to our advantage"
      },
      {
        "start": 824.0,
        "duration": 6.639,
        "text": "that's kind of cool and we will in a"
      },
      {
        "start": 826.76,
        "duration": 6.319,
        "text": "minute and this is uh a great slide for"
      },
      {
        "start": 830.639,
        "duration": 4.681,
        "text": "you to keep in mind all the time I've"
      },
      {
        "start": 833.079,
        "duration": 4.56,
        "text": "talked to lots and lots of cassander"
      },
      {
        "start": 835.32,
        "duration": 4.8,
        "text": "folks lots and lots of it folks and"
      },
      {
        "start": 837.639,
        "duration": 4.44,
        "text": "there there is a of magical thinking"
      },
      {
        "start": 840.12,
        "duration": 5.839,
        "text": "going on around llms they are not"
      },
      {
        "start": 842.079,
        "duration": 5.401,
        "text": "magical um this is not C3PO you know"
      },
      {
        "start": 845.959,
        "duration": 3.921,
        "text": "it's not going to just doesn't have"
      },
      {
        "start": 847.48,
        "duration": 4.76,
        "text": "ascensions and it's not universally"
      },
      {
        "start": 849.88,
        "duration": 5.04,
        "text": "intelligent um you know you hear a lot"
      },
      {
        "start": 852.24,
        "duration": 6.88,
        "text": "of that that talk going on like is it is"
      },
      {
        "start": 854.92,
        "duration": 6.279,
        "text": "it intelligent no it's AGI um and"
      },
      {
        "start": 859.12,
        "duration": 4.48,
        "text": "artificial general intelligence not even"
      },
      {
        "start": 861.199,
        "duration": 5.32,
        "text": "close but the things that you don't want"
      },
      {
        "start": 863.6,
        "duration": 6.12,
        "text": "to do are the things I'm listing here"
      },
      {
        "start": 866.519,
        "duration": 6.361,
        "text": "like it's not a replacement for spark uh"
      },
      {
        "start": 869.72,
        "duration": 4.96,
        "text": "you spark has this use cases that LMS"
      },
      {
        "start": 872.88,
        "duration": 2.879,
        "text": "will never be able to do at least not in"
      },
      {
        "start": 874.68,
        "duration": 3.24,
        "text": "the current"
      },
      {
        "start": 875.759,
        "duration": 3.681,
        "text": "iteration and then probably the thing"
      },
      {
        "start": 877.92,
        "duration": 3.479,
        "text": "that's most important to understand is"
      },
      {
        "start": 879.44,
        "duration": 3.079,
        "text": "precise numerical communication"
      },
      {
        "start": 881.399,
        "duration": 3.201,
        "text": "numerical"
      },
      {
        "start": 882.519,
        "duration": 4.081,
        "text": "calculations um adding up a bunch of"
      },
      {
        "start": 884.6,
        "duration": 3.919,
        "text": "numbers or doing a math problem is super"
      },
      {
        "start": 886.6,
        "duration": 2.919,
        "text": "hard for an LM and it's just based on"
      },
      {
        "start": 888.519,
        "duration": 3.921,
        "text": "the fact that it's giving you"
      },
      {
        "start": 889.519,
        "duration": 5.161,
        "text": "probabilities not it it's not like"
      },
      {
        "start": 892.44,
        "duration": 5.28,
        "text": "writing a Python program to add numbers"
      },
      {
        "start": 894.68,
        "duration": 6.2,
        "text": "it's not that um as a matter of"
      },
      {
        "start": 897.72,
        "duration": 5.799,
        "text": "fact l can write a program to do that"
      },
      {
        "start": 900.88,
        "duration": 4.24,
        "text": "and will if given the choice because"
      },
      {
        "start": 903.519,
        "duration": 3.361,
        "text": "that's that's that's just not something"
      },
      {
        "start": 905.12,
        "duration": 3.92,
        "text": "that's inherently built into an LM how"
      },
      {
        "start": 906.88,
        "duration": 4.24,
        "text": "can do um I put some other things in"
      },
      {
        "start": 909.04,
        "duration": 3.64,
        "text": "here like these realtime high stakes"
      },
      {
        "start": 911.12,
        "duration": 3.68,
        "text": "decision- making just because it's based"
      },
      {
        "start": 912.68,
        "duration": 4.44,
        "text": "on probabilities um you do not want to"
      },
      {
        "start": 914.8,
        "duration": 4.56,
        "text": "make a life for decision and at a"
      },
      {
        "start": 917.12,
        "duration": 4.959,
        "text": "minimum do not trust it with your money"
      },
      {
        "start": 919.36,
        "duration": 5.56,
        "text": "right now that's just not a good idea um"
      },
      {
        "start": 922.079,
        "duration": 4.961,
        "text": "and you know the software tools you know"
      },
      {
        "start": 924.92,
        "duration": 4.359,
        "text": "it's never going to um these are built"
      },
      {
        "start": 927.04,
        "duration": 4.44,
        "text": "to replace things like a cad soft or"
      },
      {
        "start": 929.279,
        "duration": 4.521,
        "text": "something like that and then finally"
      },
      {
        "start": 931.48,
        "duration": 4.799,
        "text": "this long-term data storage there are"
      },
      {
        "start": 933.8,
        "duration": 4.8,
        "text": "they do encode a lot of information but"
      },
      {
        "start": 936.279,
        "duration": 4.8,
        "text": "they're not a replacement for a database"
      },
      {
        "start": 938.6,
        "duration": 4.2,
        "text": "and uh maybe in the future they will be"
      },
      {
        "start": 941.079,
        "duration": 4.161,
        "text": "in a lot of ways who knows lots of"
      },
      {
        "start": 942.8,
        "duration": 4.44,
        "text": "speculation but the thing is it's really"
      },
      {
        "start": 945.24,
        "duration": 4.12,
        "text": "expensive to run an llm and that's"
      },
      {
        "start": 947.24,
        "duration": 4.399,
        "text": "probably one of the reasons all these"
      },
      {
        "start": 949.36,
        "duration": 4.039,
        "text": "things are really not a good idea is"
      },
      {
        "start": 951.639,
        "duration": 2.921,
        "text": "because an llm is probably one of the"
      },
      {
        "start": 953.399,
        "duration": 3.201,
        "text": "more expensive things you're going to"
      },
      {
        "start": 954.56,
        "duration": 4.759,
        "text": "use in your infrastructure at this point"
      },
      {
        "start": 956.6,
        "duration": 4.599,
        "text": "because um when you when you have that"
      },
      {
        "start": 959.319,
        "duration": 3.841,
        "text": "parsing of tokens and then it does the"
      },
      {
        "start": 961.199,
        "duration": 5.12,
        "text": "work and then it outputs the tokens all"
      },
      {
        "start": 963.16,
        "duration": 4.919,
        "text": "of that takes a lot of GPU of course GPU"
      },
      {
        "start": 966.319,
        "duration": 5.2,
        "text": "is most expensive thing we could have in"
      },
      {
        "start": 968.079,
        "duration": 6.44,
        "text": "our data centers at this point um and"
      },
      {
        "start": 971.519,
        "duration": 5.32,
        "text": "it's just not efficient um so it's best"
      },
      {
        "start": 974.519,
        "duration": 4.081,
        "text": "to think about an llm as a tool that can"
      },
      {
        "start": 976.839,
        "duration": 5.601,
        "text": "do a certain thing that you want to"
      },
      {
        "start": 978.6,
        "duration": 5.44,
        "text": "minimize the amount of of traffic to and"
      },
      {
        "start": 982.44,
        "duration": 3.36,
        "text": "just use it the best way possible it's"
      },
      {
        "start": 984.04,
        "duration": 4.44,
        "text": "not an everything tool it's not the"
      },
      {
        "start": 985.8,
        "duration": 4.479,
        "text": "hammer for everything so there you go"
      },
      {
        "start": 988.48,
        "duration": 6.159,
        "text": "all right moving"
      },
      {
        "start": 990.279,
        "duration": 8.081,
        "text": "on now digging into the using the tool"
      },
      {
        "start": 994.639,
        "duration": 6.961,
        "text": "and this is this is been a thing that"
      },
      {
        "start": 998.36,
        "duration": 6.2,
        "text": "I've been pouring over tons of YouTube"
      },
      {
        "start": 1001.6,
        "duration": 4.719,
        "text": "videos tons of Articles like what is"
      },
      {
        "start": 1004.56,
        "duration": 3.32,
        "text": "this and you probably have heard this"
      },
      {
        "start": 1006.319,
        "duration": 3.721,
        "text": "about a billion times and it's just"
      },
      {
        "start": 1007.88,
        "duration": 6.0,
        "text": "annoying um this whole thing of prompt"
      },
      {
        "start": 1010.04,
        "duration": 6.68,
        "text": "engineering or prompting um it's it's a"
      },
      {
        "start": 1013.88,
        "duration": 5.28,
        "text": "thing it's a thing but the reason why is"
      },
      {
        "start": 1016.72,
        "duration": 4.96,
        "text": "because whenever you have think of them"
      },
      {
        "start": 1019.16,
        "duration": 3.799,
        "text": "is um if you're using a function like a"
      },
      {
        "start": 1021.68,
        "duration": 2.519,
        "text": "like a Java function or a python"
      },
      {
        "start": 1022.959,
        "duration": 3.161,
        "text": "function or something and it has"
      },
      {
        "start": 1024.199,
        "duration": 5.441,
        "text": "parameters you use the parameters to get"
      },
      {
        "start": 1026.12,
        "duration": 6.719,
        "text": "the output you want and prompting is"
      },
      {
        "start": 1029.64,
        "duration": 6.039,
        "text": "that and I um I paraphrase several"
      },
      {
        "start": 1032.839,
        "duration": 4.761,
        "text": "comments but this is the best way I can"
      },
      {
        "start": 1035.679,
        "duration": 4.321,
        "text": "describe what prompting is steering a"
      },
      {
        "start": 1037.6,
        "duration": 4.56,
        "text": "language model towards the desired"
      },
      {
        "start": 1040.0,
        "duration": 5.36,
        "text": "output without altering its internal"
      },
      {
        "start": 1042.16,
        "duration": 6.72,
        "text": "weights so you're you're influencing the"
      },
      {
        "start": 1045.36,
        "duration": 5.439,
        "text": "model to go a certain direction um"
      },
      {
        "start": 1048.88,
        "duration": 4.799,
        "text": "there's a big Aster on there you know"
      },
      {
        "start": 1050.799,
        "duration": 4.481,
        "text": "the aster is uh changing the internal"
      },
      {
        "start": 1053.679,
        "duration": 3.281,
        "text": "weights or altering the internal weights"
      },
      {
        "start": 1055.28,
        "duration": 3.56,
        "text": "is this thing called fine tuning"
      },
      {
        "start": 1056.96,
        "duration": 4.24,
        "text": "different topic different different"
      },
      {
        "start": 1058.84,
        "duration": 4.16,
        "text": "whole different thing but um that is"
      },
      {
        "start": 1061.2,
        "duration": 3.12,
        "text": "something that people can do and it it"
      },
      {
        "start": 1063.0,
        "duration": 3.559,
        "text": "it what it does it just tweaks it a"
      },
      {
        "start": 1064.32,
        "duration": 3.96,
        "text": "little bit so fine-tuning tweaks things"
      },
      {
        "start": 1066.559,
        "duration": 3.801,
        "text": "so it goes a certain direction but let's"
      },
      {
        "start": 1068.28,
        "duration": 5.24,
        "text": "say without fine-tuning and fine-tuning"
      },
      {
        "start": 1070.36,
        "duration": 4.36,
        "text": "is is um is a hard thing to do so what"
      },
      {
        "start": 1073.52,
        "duration": 4.76,
        "text": "if you just want to use the model right"
      },
      {
        "start": 1074.72,
        "duration": 6.04,
        "text": "out of the box well cool there's a plan"
      },
      {
        "start": 1078.28,
        "duration": 6.24,
        "text": "and it's formula for how you create the"
      },
      {
        "start": 1080.76,
        "duration": 5.799,
        "text": "prompt so uh I think this is and this"
      },
      {
        "start": 1084.52,
        "duration": 4.76,
        "text": "goes into like how it was built so you"
      },
      {
        "start": 1086.559,
        "duration": 4.921,
        "text": "have this Persona context task an"
      },
      {
        "start": 1089.28,
        "duration": 6.399,
        "text": "example if you can and a format if you"
      },
      {
        "start": 1091.48,
        "duration": 6.92,
        "text": "can and this formula I put into like an"
      },
      {
        "start": 1095.679,
        "duration": 4.721,
        "text": "actual let's let's walk through this so"
      },
      {
        "start": 1098.4,
        "duration": 4.68,
        "text": "I start out with a prompt I say you are"
      },
      {
        "start": 1100.4,
        "duration": 4.24,
        "text": "an Apache Cassandra expert okay so now"
      },
      {
        "start": 1103.08,
        "duration": 3.599,
        "text": "what we've done is we immediately put it"
      },
      {
        "start": 1104.64,
        "duration": 3.96,
        "text": "in the mode of like okay I'm going to"
      },
      {
        "start": 1106.679,
        "duration": 2.641,
        "text": "wait things based on this role that I"
      },
      {
        "start": 1108.6,
        "duration": 2.72,
        "text": "have"
      },
      {
        "start": 1109.32,
        "duration": 4.4,
        "text": "so instead of giving you answers for"
      },
      {
        "start": 1111.32,
        "duration": 4.92,
        "text": "like postgress or something else it's"
      },
      {
        "start": 1113.72,
        "duration": 4.04,
        "text": "like nope I'm Apache consider expert um"
      },
      {
        "start": 1116.24,
        "duration": 3.48,
        "text": "and then this context is you are"
      },
      {
        "start": 1117.76,
        "duration": 3.799,
        "text": "Consulting with teams provide advice so"
      },
      {
        "start": 1119.72,
        "duration": 4.079,
        "text": "oh I'm a consultant okay this is making"
      },
      {
        "start": 1121.559,
        "duration": 3.641,
        "text": "it and these are actually changing the"
      },
      {
        "start": 1123.799,
        "duration": 3.76,
        "text": "way the output works you can play around"
      },
      {
        "start": 1125.2,
        "duration": 4.32,
        "text": "with these and see how it works and it's"
      },
      {
        "start": 1127.559,
        "duration": 4.161,
        "text": "fascinating how it does really make that"
      },
      {
        "start": 1129.52,
        "duration": 3.96,
        "text": "difference and then the task and the"
      },
      {
        "start": 1131.72,
        "duration": 2.92,
        "text": "task is what do you want it to do your"
      },
      {
        "start": 1133.48,
        "duration": 4.0,
        "text": "job is to create a runbook for"
      },
      {
        "start": 1134.64,
        "duration": 5.399,
        "text": "decommissioning a node and a cluster"
      },
      {
        "start": 1137.48,
        "duration": 4.6,
        "text": "okay and"
      },
      {
        "start": 1140.039,
        "duration": 4.401,
        "text": "that's something it could actually do uh"
      },
      {
        "start": 1142.08,
        "duration": 3.959,
        "text": "we'll look at it this in a bit but um"
      },
      {
        "start": 1144.44,
        "duration": 3.68,
        "text": "gp4 could do this because that"
      },
      {
        "start": 1146.039,
        "duration": 4.841,
        "text": "information is encoded in somewhere but"
      },
      {
        "start": 1148.12,
        "duration": 4.919,
        "text": "we can make it better and and then"
      },
      {
        "start": 1150.88,
        "duration": 4.24,
        "text": "finally you know I I provide an example"
      },
      {
        "start": 1153.039,
        "duration": 3.401,
        "text": "like this is what I want it to look like"
      },
      {
        "start": 1155.12,
        "duration": 3.0,
        "text": "um you know they have the precondition"
      },
      {
        "start": 1156.44,
        "duration": 3.479,
        "text": "checking each step instructions checks"
      },
      {
        "start": 1158.12,
        "duration": 4.08,
        "text": "each step and then postd commission"
      },
      {
        "start": 1159.919,
        "duration": 4.281,
        "text": "checks and then I I say I want it in"
      },
      {
        "start": 1162.2,
        "duration": 6.04,
        "text": "markdown that's the format you can say I"
      },
      {
        "start": 1164.2,
        "duration": 6.28,
        "text": "wanted do Json or whatever and mostly G"
      },
      {
        "start": 1168.24,
        "duration": 4.559,
        "text": "like any LM will comply with all those"
      },
      {
        "start": 1170.48,
        "duration": 5.679,
        "text": "things it'll try to do that it'll um"
      },
      {
        "start": 1172.799,
        "duration": 7.641,
        "text": "what's funny is things like um like uh"
      },
      {
        "start": 1176.159,
        "duration": 6.601,
        "text": "the some llms prefer XML over Json okay"
      },
      {
        "start": 1180.44,
        "duration": 7.28,
        "text": "but in general this is how it works"
      },
      {
        "start": 1182.76,
        "duration": 7.44,
        "text": "right now prompting has gotten a booster"
      },
      {
        "start": 1187.72,
        "duration": 4.6,
        "text": "and this is you've heard this enough too"
      },
      {
        "start": 1190.2,
        "duration": 4.56,
        "text": "rag like how does rag fit into this"
      },
      {
        "start": 1192.32,
        "duration": 4.96,
        "text": "whole prompting thing I'm glad you asked"
      },
      {
        "start": 1194.76,
        "duration": 5.6,
        "text": "and it's really just creating context um"
      },
      {
        "start": 1197.28,
        "duration": 5.36,
        "text": "real-time context so So when you say hey"
      },
      {
        "start": 1200.36,
        "duration": 7.12,
        "text": "what are compaction settings for cander"
      },
      {
        "start": 1202.64,
        "duration": 6.84,
        "text": "311 that's really specific and uh that"
      },
      {
        "start": 1207.48,
        "duration": 3.76,
        "text": "that question it's a good question to"
      },
      {
        "start": 1209.48,
        "duration": 4.199,
        "text": "get an exact answer on because if you're"
      },
      {
        "start": 1211.24,
        "duration": 5.84,
        "text": "using say cassander 2 those are"
      },
      {
        "start": 1213.679,
        "duration": 5.48,
        "text": "different and so uh you'll see like in"
      },
      {
        "start": 1217.08,
        "duration": 4.479,
        "text": "rag that they bring out these things"
      },
      {
        "start": 1219.159,
        "duration": 4.481,
        "text": "like you see the the vector databases"
      },
      {
        "start": 1221.559,
        "duration": 4.761,
        "text": "being used in that way um you don't have"
      },
      {
        "start": 1223.64,
        "duration": 4.36,
        "text": "to but we'll get to that um but using a"
      },
      {
        "start": 1226.32,
        "duration": 4.2,
        "text": "vector database what it does is it tries"
      },
      {
        "start": 1228.0,
        "duration": 6.039,
        "text": "to find text a similarity search oh"
      },
      {
        "start": 1230.52,
        "duration": 7.48,
        "text": "cassander 3.1 311"
      },
      {
        "start": 1234.039,
        "duration": 5.64,
        "text": "compaction and a properly vectorized"
      },
      {
        "start": 1238.0,
        "duration": 3.919,
        "text": "data set like the whole docs for"
      },
      {
        "start": 1239.679,
        "duration": 4.401,
        "text": "Cassandra would say oh I know where that"
      },
      {
        "start": 1241.919,
        "duration": 4.481,
        "text": "is and it would retrieve the document"
      },
      {
        "start": 1244.08,
        "duration": 5.16,
        "text": "page for compaction settings for caser"
      },
      {
        "start": 1246.4,
        "duration": 5.96,
        "text": "311 and then use that as a context so"
      },
      {
        "start": 1249.24,
        "duration": 4.36,
        "text": "whenever you say our previous uh prompt"
      },
      {
        "start": 1252.36,
        "duration": 3.92,
        "text": "where we say hey you're an AP pachic"
      },
      {
        "start": 1253.6,
        "duration": 7.24,
        "text": "sander exor here's the context which is"
      },
      {
        "start": 1256.28,
        "duration": 7.24,
        "text": "a whole context of is page of docs on um"
      },
      {
        "start": 1260.84,
        "duration": 6.0,
        "text": "compaction settings for 311 how do I do"
      },
      {
        "start": 1263.52,
        "duration": 4.76,
        "text": "this you know how do I do compaction or"
      },
      {
        "start": 1266.84,
        "duration": 3.6,
        "text": "what what are the compaction settings"
      },
      {
        "start": 1268.28,
        "duration": 4.759,
        "text": "and we send that to the llm so this big"
      },
      {
        "start": 1270.44,
        "duration": 4.28,
        "text": "block of text and then lo and behold we"
      },
      {
        "start": 1273.039,
        "duration": 3.441,
        "text": "get a response which is really nice"
      },
      {
        "start": 1274.72,
        "duration": 3.199,
        "text": "nicely formatted I maybe I wanted in"
      },
      {
        "start": 1276.48,
        "duration": 5.079,
        "text": "markdown or something like that like a"
      },
      {
        "start": 1277.919,
        "duration": 6.24,
        "text": "table of values um but it's not making"
      },
      {
        "start": 1281.559,
        "duration": 6.521,
        "text": "anything up at that point is taking all"
      },
      {
        "start": 1284.159,
        "duration": 4.88,
        "text": "that information from the database and"
      },
      {
        "start": 1288.08,
        "duration": 3.44,
        "text": "uh"
      },
      {
        "start": 1289.039,
        "duration": 5.441,
        "text": "inserting it into our prompt so this is"
      },
      {
        "start": 1291.52,
        "duration": 4.36,
        "text": "the retrieval that's the r the retrieval"
      },
      {
        "start": 1294.48,
        "duration": 3.96,
        "text": "is um"
      },
      {
        "start": 1295.88,
        "duration": 5.679,
        "text": "augmenting uh into the llm and then the"
      },
      {
        "start": 1298.44,
        "duration": 5.719,
        "text": "response is based on that that is a"
      },
      {
        "start": 1301.559,
        "duration": 5.6,
        "text": "really useful tool and we'll find out"
      },
      {
        "start": 1304.159,
        "duration": 4.961,
        "text": "lots of cool ways to hack on that um it"
      },
      {
        "start": 1307.159,
        "duration": 3.52,
        "text": "doesn't always have to be one thing you"
      },
      {
        "start": 1309.12,
        "duration": 3.6,
        "text": "could do quite a bit with this but"
      },
      {
        "start": 1310.679,
        "duration": 3.24,
        "text": "really it's about making sure the LM has"
      },
      {
        "start": 1312.72,
        "duration": 2.72,
        "text": "enough information to give you a"
      },
      {
        "start": 1313.919,
        "duration": 5.561,
        "text": "reasonable"
      },
      {
        "start": 1315.44,
        "duration": 8.04,
        "text": "response and the one of final topics on"
      },
      {
        "start": 1319.48,
        "duration": 6.96,
        "text": "llms which I personally this is one of"
      },
      {
        "start": 1323.48,
        "duration": 5.92,
        "text": "my personal things I love these is using"
      },
      {
        "start": 1326.44,
        "duration": 6.359,
        "text": "tools now you may see them called"
      },
      {
        "start": 1329.4,
        "duration": 7.519,
        "text": "functions um the the industry started"
      },
      {
        "start": 1332.799,
        "duration": 7.0,
        "text": "out with um open AI they released a GPT"
      },
      {
        "start": 1336.919,
        "duration": 6.36,
        "text": "functions or function calls uh last"
      },
      {
        "start": 1339.799,
        "duration": 8.281,
        "text": "summer with one of their models with gp4"
      },
      {
        "start": 1343.279,
        "duration": 7.041,
        "text": "and and 3.5 so those slowly got morphed"
      },
      {
        "start": 1348.08,
        "duration": 5.079,
        "text": "into tools tools and now most models"
      },
      {
        "start": 1350.32,
        "duration": 5.0,
        "text": "support what they call tools and um The"
      },
      {
        "start": 1353.159,
        "duration": 3.561,
        "text": "Lang chain L index they they they call"
      },
      {
        "start": 1355.32,
        "duration": 4.599,
        "text": "them tools so we're just going to use"
      },
      {
        "start": 1356.72,
        "duration": 8.079,
        "text": "that word and so what does that mean um"
      },
      {
        "start": 1359.919,
        "duration": 7.401,
        "text": "well whenever you make a uh a call to LM"
      },
      {
        "start": 1364.799,
        "duration": 6.161,
        "text": "you can also embed a list of tools that"
      },
      {
        "start": 1367.32,
        "duration": 5.64,
        "text": "it has access to so for instance I if I"
      },
      {
        "start": 1370.96,
        "duration": 4.92,
        "text": "want to add two numbers together this is"
      },
      {
        "start": 1372.96,
        "duration": 5.88,
        "text": "really simple like I said an llm is not"
      },
      {
        "start": 1375.88,
        "duration": 5.72,
        "text": "super good at math and you don't want it"
      },
      {
        "start": 1378.84,
        "duration": 4.76,
        "text": "to have to go off and do math but I know"
      },
      {
        "start": 1381.6,
        "duration": 3.679,
        "text": "if I wrote a program to do it then it"
      },
      {
        "start": 1383.6,
        "duration": 4.559,
        "text": "would be good that's the right way to do"
      },
      {
        "start": 1385.279,
        "duration": 4.64,
        "text": "it so what I do is I say when I ask the"
      },
      {
        "start": 1388.159,
        "duration": 3.441,
        "text": "question I also pass it a list hey"
      },
      {
        "start": 1389.919,
        "duration": 3.441,
        "text": "here's the tools and what you say is I"
      },
      {
        "start": 1391.6,
        "duration": 3.559,
        "text": "have a function called add numbers"
      },
      {
        "start": 1393.36,
        "duration": 4.08,
        "text": "here's a description takes a list of"
      },
      {
        "start": 1395.159,
        "duration": 4.241,
        "text": "numbers and adds them together the LM"
      },
      {
        "start": 1397.44,
        "duration": 4.04,
        "text": "can make a reasoning decision say oh you"
      },
      {
        "start": 1399.4,
        "duration": 5.08,
        "text": "want me to add numbers I have a tool for"
      },
      {
        "start": 1401.48,
        "duration": 5.6,
        "text": "that and the response will include a"
      },
      {
        "start": 1404.48,
        "duration": 6.04,
        "text": "tool call and the tool call says hey"
      },
      {
        "start": 1407.08,
        "duration": 5.76,
        "text": "look I need to do this thing here's um"
      },
      {
        "start": 1410.52,
        "duration": 4.96,
        "text": "call this OD numbers function here's a"
      },
      {
        "start": 1412.84,
        "duration": 6.48,
        "text": "list of all the the parameters these 20"
      },
      {
        "start": 1415.48,
        "duration": 6.24,
        "text": "and 40 and then inside your like uh Lang"
      },
      {
        "start": 1419.32,
        "duration": 4.599,
        "text": "chain has support Linex has support um"
      },
      {
        "start": 1421.72,
        "duration": 4.079,
        "text": "this is really widely supported it will"
      },
      {
        "start": 1423.919,
        "duration": 3.321,
        "text": "make a call to that program that you've"
      },
      {
        "start": 1425.799,
        "duration": 3.401,
        "text": "you've added and it's a little"
      },
      {
        "start": 1427.24,
        "duration": 3.48,
        "text": "complicated I would love to do some"
      },
      {
        "start": 1429.2,
        "duration": 4.359,
        "text": "content if you want me to do more on"
      },
      {
        "start": 1430.72,
        "duration": 5.199,
        "text": "this um I I I have more to talk about"
      },
      {
        "start": 1433.559,
        "duration": 5.041,
        "text": "but in a basic thing it just runs"
      },
      {
        "start": 1435.919,
        "duration": 4.041,
        "text": "whatever arbitrary code you want return"
      },
      {
        "start": 1438.6,
        "duration": 3.48,
        "text": "that value now you can either return it"
      },
      {
        "start": 1439.96,
        "duration": 4.839,
        "text": "directly to the user or you could pass"
      },
      {
        "start": 1442.08,
        "duration": 4.8,
        "text": "it back through the llm so it spruces it"
      },
      {
        "start": 1444.799,
        "duration": 4.681,
        "text": "up a little bit makes it look nice says"
      },
      {
        "start": 1446.88,
        "duration": 4.36,
        "text": "the answer is 60 and you could trust"
      },
      {
        "start": 1449.48,
        "duration": 3.96,
        "text": "that because I went through a tool with"
      },
      {
        "start": 1451.24,
        "duration": 4.039,
        "text": "it's deterministic instead of the LM"
      },
      {
        "start": 1453.44,
        "duration": 3.52,
        "text": "just making up a response it probably"
      },
      {
        "start": 1455.279,
        "duration": 3.561,
        "text": "will say 60 because that's a pretty easy"
      },
      {
        "start": 1456.96,
        "duration": 4.76,
        "text": "one but if it's something more"
      },
      {
        "start": 1458.84,
        "duration": 6.16,
        "text": "complicated oh like accessing a database"
      },
      {
        "start": 1461.72,
        "duration": 7.679,
        "text": "um maybe a tool would be better choice"
      },
      {
        "start": 1465.0,
        "duration": 7.039,
        "text": "so that's that on LMS and the basics of"
      },
      {
        "start": 1469.399,
        "duration": 5.081,
        "text": "geni I went through this pretty quickly"
      },
      {
        "start": 1472.039,
        "duration": 3.88,
        "text": "although I did burn through 24 minutes I"
      },
      {
        "start": 1474.48,
        "duration": 4.4,
        "text": "feel like it was important that we get"
      },
      {
        "start": 1475.919,
        "duration": 4.36,
        "text": "kind of a baseline here and again uh"
      },
      {
        "start": 1478.88,
        "duration": 3.44,
        "text": "keep in mind if you want me to dig into"
      },
      {
        "start": 1480.279,
        "duration": 4.0,
        "text": "any of these topics I would love to do"
      },
      {
        "start": 1482.32,
        "duration": 4.8,
        "text": "that in a in a future"
      },
      {
        "start": 1484.279,
        "duration": 4.921,
        "text": "Workshop so how about Cassandra our"
      },
      {
        "start": 1487.12,
        "duration": 4.52,
        "text": "favorite database how does it fit in"
      },
      {
        "start": 1489.2,
        "duration": 4.359,
        "text": "here and that's a that's a challenging"
      },
      {
        "start": 1491.64,
        "duration": 5.36,
        "text": "question because Cassandra's been around"
      },
      {
        "start": 1493.559,
        "duration": 7.24,
        "text": "for a while and has definitely proven"
      },
      {
        "start": 1497.0,
        "duration": 5.96,
        "text": "itself I mean here this right um"
      },
      {
        "start": 1500.799,
        "duration": 4.201,
        "text": "Cassandra is not disputed as a scale"
      },
      {
        "start": 1502.96,
        "duration": 5.68,
        "text": "leader and how does it fit into this new"
      },
      {
        "start": 1505.0,
        "duration": 7.0,
        "text": "world of gen it uh it is a database and"
      },
      {
        "start": 1508.64,
        "duration": 6.159,
        "text": "it has data and just thinking about just"
      },
      {
        "start": 1512.0,
        "duration": 5.679,
        "text": "more openly like that's if you're going"
      },
      {
        "start": 1514.799,
        "duration": 5.521,
        "text": "to work with uh say Rag and things like"
      },
      {
        "start": 1517.679,
        "duration": 4.961,
        "text": "that llms are great for communication"
      },
      {
        "start": 1520.32,
        "duration": 4.04,
        "text": "but they just don't know a lot of things"
      },
      {
        "start": 1522.64,
        "duration": 2.919,
        "text": "and we've put a lot of our important"
      },
      {
        "start": 1524.36,
        "duration": 3.36,
        "text": "data in"
      },
      {
        "start": 1525.559,
        "duration": 3.441,
        "text": "Cassandra if you're listening to me now"
      },
      {
        "start": 1527.72,
        "duration": 4.0,
        "text": "and you're using you have a Cassandra"
      },
      {
        "start": 1529.0,
        "duration": 5.12,
        "text": "cluster somewhere chances are that data"
      },
      {
        "start": 1531.72,
        "duration": 3.679,
        "text": "is pretty critical to your business so"
      },
      {
        "start": 1534.12,
        "duration": 3.84,
        "text": "how could we use"
      },
      {
        "start": 1535.399,
        "duration": 4.561,
        "text": "it well you have choices and those"
      },
      {
        "start": 1537.96,
        "duration": 5.48,
        "text": "choices are and I think this is a good"
      },
      {
        "start": 1539.96,
        "duration": 6.64,
        "text": "thing for everyone to understand is"
      },
      {
        "start": 1543.44,
        "duration": 5.44,
        "text": "that we've gone down the path of like oh"
      },
      {
        "start": 1546.6,
        "duration": 4.88,
        "text": "you to do the new thing you have to"
      },
      {
        "start": 1548.88,
        "duration": 5.76,
        "text": "upgrade and I'm here to tell you no you"
      },
      {
        "start": 1551.48,
        "duration": 5.439,
        "text": "don't you can there are things to do so"
      },
      {
        "start": 1554.64,
        "duration": 5.32,
        "text": "if you want to stay where you are if"
      },
      {
        "start": 1556.919,
        "duration": 5.961,
        "text": "you're using cassander 4.1 or less and"
      },
      {
        "start": 1559.96,
        "duration": 6.52,
        "text": "you want to do some gen stuff cool we"
      },
      {
        "start": 1562.88,
        "duration": 5.799,
        "text": "can do that and uh same with dsse less"
      },
      {
        "start": 1566.48,
        "duration": 4.4,
        "text": "than 6.9 and that's just because there's"
      },
      {
        "start": 1568.679,
        "duration": 4.48,
        "text": "going to be gen stuff built into that"
      },
      {
        "start": 1570.88,
        "duration": 4.56,
        "text": "afterwards and of course Astra if you"
      },
      {
        "start": 1573.159,
        "duration": 4.921,
        "text": "use use regular classic astroid that's"
      },
      {
        "start": 1575.44,
        "duration": 5.8,
        "text": "just standard service um it doesn't have"
      },
      {
        "start": 1578.08,
        "duration": 5.4,
        "text": "Vector support or anything gen on it"
      },
      {
        "start": 1581.24,
        "duration": 5.159,
        "text": "you're still cool you can still do stuff"
      },
      {
        "start": 1583.48,
        "duration": 4.36,
        "text": "and actually it's pretty useful now if"
      },
      {
        "start": 1586.399,
        "duration": 3.361,
        "text": "you do want to go down the path of an"
      },
      {
        "start": 1587.84,
        "duration": 4.079,
        "text": "upgrade really what you're asking to do"
      },
      {
        "start": 1589.76,
        "duration": 5.039,
        "text": "is do I want to run a vector I need to"
      },
      {
        "start": 1591.919,
        "duration": 5.601,
        "text": "do Vector search and that's going to be"
      },
      {
        "start": 1594.799,
        "duration": 6.521,
        "text": "uh conser five or greater uh greater"
      },
      {
        "start": 1597.52,
        "duration": 6.32,
        "text": "than DSC 6.9 and then um uh asra Vector"
      },
      {
        "start": 1601.32,
        "duration": 3.88,
        "text": "for that and of course if you have a new"
      },
      {
        "start": 1603.84,
        "duration": 3.319,
        "text": "project that'd be a great choice you"
      },
      {
        "start": 1605.2,
        "duration": 3.4,
        "text": "just want to start doing it but you do"
      },
      {
        "start": 1607.159,
        "duration": 4.321,
        "text": "have a"
      },
      {
        "start": 1608.6,
        "duration": 4.16,
        "text": "choice and I think this is the the thing"
      },
      {
        "start": 1611.48,
        "duration": 2.319,
        "text": "that I wanted to make sure that"
      },
      {
        "start": 1612.76,
        "duration": 3.68,
        "text": "everybody"
      },
      {
        "start": 1613.799,
        "duration": 4.721,
        "text": "realizes you don't have to have Vector"
      },
      {
        "start": 1616.44,
        "duration": 4.119,
        "text": "to do rack"
      },
      {
        "start": 1618.52,
        "duration": 5.36,
        "text": "and I'm going to say that again you do"
      },
      {
        "start": 1620.559,
        "duration": 4.761,
        "text": "not have to have Vector to do rag that"
      },
      {
        "start": 1623.88,
        "duration": 5.24,
        "text": "is not a"
      },
      {
        "start": 1625.32,
        "duration": 6.2,
        "text": "requirement okay that opens up you to a"
      },
      {
        "start": 1629.12,
        "duration": 5.159,
        "text": "ton of use cases for"
      },
      {
        "start": 1631.52,
        "duration": 5.32,
        "text": "geni that means that you can use that"
      },
      {
        "start": 1634.279,
        "duration": 4.081,
        "text": "that retrieval part of it a little more"
      },
      {
        "start": 1636.84,
        "duration": 3.36,
        "text": "natively with whatever standard"
      },
      {
        "start": 1638.36,
        "duration": 4.24,
        "text": "installation you have with the data you"
      },
      {
        "start": 1640.2,
        "duration": 3.599,
        "text": "have to do some really cool things I use"
      },
      {
        "start": 1642.6,
        "duration": 4.36,
        "text": "an example"
      },
      {
        "start": 1643.799,
        "duration": 5.401,
        "text": "here chatbot let's say that you have uh"
      },
      {
        "start": 1646.96,
        "duration": 4.56,
        "text": "you're in e-commerce you need to do a"
      },
      {
        "start": 1649.2,
        "duration": 4.44,
        "text": "chatbot uh with your customers and you"
      },
      {
        "start": 1651.52,
        "duration": 5.0,
        "text": "know that's classic that's a hello world"
      },
      {
        "start": 1653.64,
        "duration": 5.519,
        "text": "for Jen eyes chatbot so what if a"
      },
      {
        "start": 1656.52,
        "duration": 5.32,
        "text": "customer says what did I buy last week"
      },
      {
        "start": 1659.159,
        "duration": 5.281,
        "text": "that does not take a complicated bit of"
      },
      {
        "start": 1661.84,
        "duration": 4.92,
        "text": "data engineering to make work what you"
      },
      {
        "start": 1664.44,
        "duration": 4.52,
        "text": "need to do is just have a query that"
      },
      {
        "start": 1666.76,
        "duration": 4.08,
        "text": "already you probably already have where"
      },
      {
        "start": 1668.96,
        "duration": 5.8,
        "text": "I go into the custom orders table look"
      },
      {
        "start": 1670.84,
        "duration": 6.24,
        "text": "up the user by ID and I do a range check"
      },
      {
        "start": 1674.76,
        "duration": 4.44,
        "text": "on their order dates and I return back"
      },
      {
        "start": 1677.08,
        "duration": 4.959,
        "text": "all the orders now"
      },
      {
        "start": 1679.2,
        "duration": 6.079,
        "text": "I build a I build a prompt that says um"
      },
      {
        "start": 1682.039,
        "duration": 6.961,
        "text": "you know you are a helpful customer bot"
      },
      {
        "start": 1685.279,
        "duration": 6.0,
        "text": "and um the the customer asked for when"
      },
      {
        "start": 1689.0,
        "duration": 5.679,
        "text": "know like when the customer asks for"
      },
      {
        "start": 1691.279,
        "duration": 6.12,
        "text": "their their orders here's the data and"
      },
      {
        "start": 1694.679,
        "duration": 4.921,
        "text": "then provide this back as a useful table"
      },
      {
        "start": 1697.399,
        "duration": 4.16,
        "text": "in a chatbot and you kick that through"
      },
      {
        "start": 1699.6,
        "duration": 4.72,
        "text": "the llm and it gives you a response back"
      },
      {
        "start": 1701.559,
        "duration": 4.48,
        "text": "in the chatbot and it'll probably give"
      },
      {
        "start": 1704.32,
        "duration": 4.0,
        "text": "you some super wordy response like oh"
      },
      {
        "start": 1706.039,
        "duration": 5.12,
        "text": "you bought three things last week um it"
      },
      {
        "start": 1708.32,
        "duration": 5.719,
        "text": "won't just say that it'll say a lot but"
      },
      {
        "start": 1711.159,
        "duration": 4.921,
        "text": "at no point did I use Vector for that"
      },
      {
        "start": 1714.039,
        "duration": 4.801,
        "text": "and it's just a matter of retrieving"
      },
      {
        "start": 1716.08,
        "duration": 5.16,
        "text": "data and giving it you're you're"
      },
      {
        "start": 1718.84,
        "duration": 4.6,
        "text": "wrapping this data which is just a hard"
      },
      {
        "start": 1721.24,
        "duration": 4.4,
        "text": "number probably or a part number or"
      },
      {
        "start": 1723.44,
        "duration": 3.88,
        "text": "something like that around you're"
      },
      {
        "start": 1725.64,
        "duration": 4.6,
        "text": "wrapping an llm around it to make it"
      },
      {
        "start": 1727.32,
        "duration": 5.239,
        "text": "more humid and it's a really cool use"
      },
      {
        "start": 1730.24,
        "duration": 6.159,
        "text": "case think of the things you could do"
      },
      {
        "start": 1732.559,
        "duration": 6.041,
        "text": "with that um I use I use llms for that"
      },
      {
        "start": 1736.399,
        "duration": 5.12,
        "text": "purpose I want to be able to"
      },
      {
        "start": 1738.6,
        "duration": 5.76,
        "text": "communicate and in a way that's a little"
      },
      {
        "start": 1741.519,
        "duration": 5.04,
        "text": "vague but also I want to be able to get"
      },
      {
        "start": 1744.36,
        "duration": 4.72,
        "text": "information back that kind of gleans out"
      },
      {
        "start": 1746.559,
        "duration": 4.72,
        "text": "meaning from it and LM are good for that"
      },
      {
        "start": 1749.08,
        "duration": 3.68,
        "text": "the key is data and we'll get into how"
      },
      {
        "start": 1751.279,
        "duration": 3.561,
        "text": "we can make that"
      },
      {
        "start": 1752.76,
        "duration": 6.12,
        "text": "work so the"
      },
      {
        "start": 1754.84,
        "duration": 7.079,
        "text": "upgrade uh conser 5 DC um data sex"
      },
      {
        "start": 1758.88,
        "duration": 5.6,
        "text": "Enterprise data sex Astro that is doing"
      },
      {
        "start": 1761.919,
        "duration": 4.801,
        "text": "Vector here's what's happening it's"
      },
      {
        "start": 1764.48,
        "duration": 4.52,
        "text": "using J vector and J Vector is a really"
      },
      {
        "start": 1766.72,
        "duration": 5.199,
        "text": "cool project um"
      },
      {
        "start": 1769.0,
        "duration": 5.24,
        "text": "it's Jonathan Ellis has it in his GitHub"
      },
      {
        "start": 1771.919,
        "duration": 4.401,
        "text": "I have the link there um I didn't put it"
      },
      {
        "start": 1774.24,
        "duration": 4.439,
        "text": "in the links I will I'll put it in there"
      },
      {
        "start": 1776.32,
        "duration": 4.52,
        "text": "um J Vector is getting a bit of love out"
      },
      {
        "start": 1778.679,
        "duration": 4.641,
        "text": "there not a bit a lot of love it's"
      },
      {
        "start": 1780.84,
        "duration": 6.079,
        "text": "probably one of the the top Java it is"
      },
      {
        "start": 1783.32,
        "duration": 6.16,
        "text": "the top Java trending project in GitHub"
      },
      {
        "start": 1786.919,
        "duration": 4.721,
        "text": "it's being used by multiple projects now"
      },
      {
        "start": 1789.48,
        "duration": 4.48,
        "text": "it is getting a lot of press a lot of"
      },
      {
        "start": 1791.64,
        "duration": 5.159,
        "text": "pickup from um"
      },
      {
        "start": 1793.96,
        "duration": 5.4,
        "text": "researchers it it's really the best one"
      },
      {
        "start": 1796.799,
        "duration": 3.921,
        "text": "of the best java libr liaries out there"
      },
      {
        "start": 1799.36,
        "duration": 3.439,
        "text": "for this type of thing but it's also one"
      },
      {
        "start": 1800.72,
        "duration": 5.679,
        "text": "of the fastest libraries period for"
      },
      {
        "start": 1802.799,
        "duration": 7.041,
        "text": "Vector search and yeah it's not written"
      },
      {
        "start": 1806.399,
        "duration": 5.081,
        "text": "in Rust sorry it's using a lot of really"
      },
      {
        "start": 1809.84,
        "duration": 3.76,
        "text": "cool techniques that are built into Java"
      },
      {
        "start": 1811.48,
        "duration": 3.679,
        "text": "now modern Java has some really neat"
      },
      {
        "start": 1813.6,
        "duration": 4.079,
        "text": "things when it comes to Vector math and"
      },
      {
        "start": 1815.159,
        "duration": 6.36,
        "text": "working with simd and that sort of thing"
      },
      {
        "start": 1817.679,
        "duration": 6.921,
        "text": "j Vector uses all of it and as a result"
      },
      {
        "start": 1821.519,
        "duration": 6.201,
        "text": "uh it's made uh Vector search in"
      },
      {
        "start": 1824.6,
        "duration": 5.88,
        "text": "Cassandra all flavors very interesting"
      },
      {
        "start": 1827.72,
        "duration": 5.72,
        "text": "and um it's cool because once again"
      },
      {
        "start": 1830.48,
        "duration": 5.4,
        "text": "we're leading the charge and this is"
      },
      {
        "start": 1833.44,
        "duration": 5.239,
        "text": "going to work at highly distributed"
      },
      {
        "start": 1835.88,
        "duration": 4.72,
        "text": "value uh highly distributed systems that"
      },
      {
        "start": 1838.679,
        "duration": 7.081,
        "text": "we that we"
      },
      {
        "start": 1840.6,
        "duration": 7.48,
        "text": "use the the core uh part of J Vector is"
      },
      {
        "start": 1845.76,
        "duration": 5.639,
        "text": "scale uh Cassandra is about scale"
      },
      {
        "start": 1848.08,
        "duration": 5.68,
        "text": "pedabytes terabytes whatever J Vector"
      },
      {
        "start": 1851.399,
        "duration": 4.4,
        "text": "Jonathan when he was started going down"
      },
      {
        "start": 1853.76,
        "duration": 4.399,
        "text": "this path realized that the libraries"
      },
      {
        "start": 1855.799,
        "duration": 4.0,
        "text": "out there that that existed really"
      },
      {
        "start": 1858.159,
        "duration": 4.921,
        "text": "weren't built for scale they were built"
      },
      {
        "start": 1859.799,
        "duration": 5.641,
        "text": "for a laptop or very small use cases you"
      },
      {
        "start": 1863.08,
        "duration": 4.079,
        "text": "know a thousand a million vectors that"
      },
      {
        "start": 1865.44,
        "duration": 3.32,
        "text": "sort of thing what happens when you have"
      },
      {
        "start": 1867.159,
        "duration": 3.76,
        "text": "a billion vectors a trillion vectors"
      },
      {
        "start": 1868.76,
        "duration": 3.639,
        "text": "that sort of thing um that's the"
      },
      {
        "start": 1870.919,
        "duration": 3.321,
        "text": "cassander problem this is a problem"
      },
      {
        "start": 1872.399,
        "duration": 3.961,
        "text": "we've been solving for a long time"
      },
      {
        "start": 1874.24,
        "duration": 4.919,
        "text": "nothing new and here we go so it's"
      },
      {
        "start": 1876.36,
        "duration": 5.799,
        "text": "pretty exciting I I am excited about it"
      },
      {
        "start": 1879.159,
        "duration": 5.801,
        "text": "and what it does in a nutshell is it it"
      },
      {
        "start": 1882.159,
        "duration": 6.321,
        "text": "adds to the data model for Cassandra so"
      },
      {
        "start": 1884.96,
        "duration": 5.079,
        "text": "it adds a new type called vector and uh"
      },
      {
        "start": 1888.48,
        "duration": 3.96,
        "text": "a vector is just to float with some"
      },
      {
        "start": 1890.039,
        "duration": 5.081,
        "text": "dimensions and the dimensions now this"
      },
      {
        "start": 1892.44,
        "duration": 4.16,
        "text": "is a five-dimensional beddings I really"
      },
      {
        "start": 1895.12,
        "duration": 3.48,
        "text": "doubt you'll ever find a dimensional"
      },
      {
        "start": 1896.6,
        "duration": 4.199,
        "text": "embedding at that size most of them are"
      },
      {
        "start": 1898.6,
        "duration": 5.6,
        "text": "in their hundreds some are in the"
      },
      {
        "start": 1900.799,
        "duration": 4.76,
        "text": "thousands vectors are again a different"
      },
      {
        "start": 1904.2,
        "duration": 3.439,
        "text": "topic I would love to cover it if you"
      },
      {
        "start": 1905.559,
        "duration": 3.321,
        "text": "want me to let me know I have a surve a"
      },
      {
        "start": 1907.639,
        "duration": 4.361,
        "text": "little poll at the end there you can"
      },
      {
        "start": 1908.88,
        "duration": 5.159,
        "text": "tell me what you like but vectors are"
      },
      {
        "start": 1912.0,
        "duration": 4.84,
        "text": "are beasts they're big numbers that's"
      },
      {
        "start": 1914.039,
        "duration": 5.201,
        "text": "why you need a specialized um indexer"
      },
      {
        "start": 1916.84,
        "duration": 4.559,
        "text": "but in this case use five and then you"
      },
      {
        "start": 1919.24,
        "duration": 3.96,
        "text": "create an index this is all built into"
      },
      {
        "start": 1921.399,
        "duration": 3.76,
        "text": "the way Cassandra does indexing already"
      },
      {
        "start": 1923.2,
        "duration": 4.92,
        "text": "it's using a storage attached index"
      },
      {
        "start": 1925.159,
        "duration": 5.841,
        "text": "already um and then you could do things"
      },
      {
        "start": 1928.12,
        "duration": 5.76,
        "text": "like uh you could do this a&n which is"
      },
      {
        "start": 1931.0,
        "duration": 4.88,
        "text": "approximate nearest neighbor searches on"
      },
      {
        "start": 1933.88,
        "duration": 4.639,
        "text": "you could give it a like here's a vector"
      },
      {
        "start": 1935.88,
        "duration": 4.96,
        "text": "what's close to this Vector space stuff"
      },
      {
        "start": 1938.519,
        "duration": 4.441,
        "text": "is really interesting um you know that's"
      },
      {
        "start": 1940.84,
        "duration": 4.28,
        "text": "how you find like a is this a hot dog or"
      },
      {
        "start": 1942.96,
        "duration": 4.04,
        "text": "not a hot dog um pictures that sort of"
      },
      {
        "start": 1945.12,
        "duration": 3.76,
        "text": "thing um but whatever you embed into it"
      },
      {
        "start": 1947.0,
        "duration": 4.08,
        "text": "in your vector you could figure out"
      },
      {
        "start": 1948.88,
        "duration": 5.72,
        "text": "what's similar or close to it words"
      },
      {
        "start": 1951.08,
        "duration": 5.28,
        "text": "pictures audio whatever and um really"
      },
      {
        "start": 1954.6,
        "duration": 3.319,
        "text": "interesting search algorithm and it's"
      },
      {
        "start": 1956.36,
        "duration": 4.6,
        "text": "funny because there's a lot of use cases"
      },
      {
        "start": 1957.919,
        "duration": 6.281,
        "text": "that have nothing to do with Gen too but"
      },
      {
        "start": 1960.96,
        "duration": 5.599,
        "text": "this is what it does and when you couple"
      },
      {
        "start": 1964.2,
        "duration": 5.359,
        "text": "this with some actual like hard data so"
      },
      {
        "start": 1966.559,
        "duration": 5.0,
        "text": "for instance with my my document you can"
      },
      {
        "start": 1969.559,
        "duration": 4.96,
        "text": "um find a vector embedding model that"
      },
      {
        "start": 1971.559,
        "duration": 5.041,
        "text": "will that will give you semantics of the"
      },
      {
        "start": 1974.519,
        "duration": 4.16,
        "text": "document and then you can also store the"
      },
      {
        "start": 1976.6,
        "duration": 4.52,
        "text": "document alongside it in Cassandra"
      },
      {
        "start": 1978.679,
        "duration": 4.72,
        "text": "because it's a cassander database think"
      },
      {
        "start": 1981.12,
        "duration": 4.679,
        "text": "about like in this case um give me"
      },
      {
        "start": 1983.399,
        "duration": 5.561,
        "text": "everything from the product table um"
      },
      {
        "start": 1985.799,
        "duration": 4.521,
        "text": "yeah thanks Ain Ain I Aaron PL he's"
      },
      {
        "start": 1988.96,
        "duration": 2.88,
        "text": "already telling me he's like you use the"
      },
      {
        "start": 1990.32,
        "duration": 3.4,
        "text": "hot dog example and I told him I never"
      },
      {
        "start": 1991.84,
        "duration": 3.839,
        "text": "would again and he's telling me on chat"
      },
      {
        "start": 1993.72,
        "duration": 3.0,
        "text": "that I did it but it's such a great"
      },
      {
        "start": 1995.679,
        "duration": 3.041,
        "text": "example if you don't know what I'm"
      },
      {
        "start": 1996.72,
        "duration": 5.839,
        "text": "talking about go watch silic"
      },
      {
        "start": 1998.72,
        "duration": 6.36,
        "text": "Valley so anyhow um using Vector search"
      },
      {
        "start": 2002.559,
        "duration": 4.6,
        "text": "in a non geni thing is also a use case"
      },
      {
        "start": 2005.08,
        "duration": 3.079,
        "text": "might be worth the talk too so that's"
      },
      {
        "start": 2007.159,
        "duration": 2.64,
        "text": "something you're interested didn't let"
      },
      {
        "start": 2008.159,
        "duration": 6.441,
        "text": "me"
      },
      {
        "start": 2009.799,
        "duration": 6.681,
        "text": "know and now with a vector now we can"
      },
      {
        "start": 2014.6,
        "duration": 4.16,
        "text": "this is that thing that I talked about"
      },
      {
        "start": 2016.48,
        "duration": 6.039,
        "text": "this is what makes it happen and"
      },
      {
        "start": 2018.76,
        "duration": 5.639,
        "text": "building hybrid data is cool because a"
      },
      {
        "start": 2022.519,
        "duration": 3.961,
        "text": "lot of times you need to find something"
      },
      {
        "start": 2024.399,
        "duration": 5.24,
        "text": "out there right and you don't know where"
      },
      {
        "start": 2026.48,
        "duration": 5.28,
        "text": "it is and you have like this is that"
      },
      {
        "start": 2029.639,
        "duration": 4.241,
        "text": "fuzzy search it's not an exact search so"
      },
      {
        "start": 2031.76,
        "duration": 4.48,
        "text": "you're not saying you know where where"
      },
      {
        "start": 2033.88,
        "duration": 4.36,
        "text": "picture equals hot dog it's like kind of"
      },
      {
        "start": 2036.24,
        "duration": 4.24,
        "text": "looks like that"
      },
      {
        "start": 2038.24,
        "duration": 4.72,
        "text": "and you could use that with your"
      },
      {
        "start": 2040.48,
        "duration": 5.319,
        "text": "existing data say like product tables I"
      },
      {
        "start": 2042.96,
        "duration": 4.959,
        "text": "mean like your product data or um"
      },
      {
        "start": 2045.799,
        "duration": 4.36,
        "text": "metadata around a picture or maybe even"
      },
      {
        "start": 2047.919,
        "duration": 4.68,
        "text": "documents um that's the classic use case"
      },
      {
        "start": 2050.159,
        "duration": 5.401,
        "text": "right now is go find My Documents but"
      },
      {
        "start": 2052.599,
        "duration": 4.28,
        "text": "it's going to give you fast access to"
      },
      {
        "start": 2055.56,
        "duration": 3.68,
        "text": "context that you can put into your"
      },
      {
        "start": 2056.879,
        "duration": 4.881,
        "text": "prompt drop it into LM and that's"
      },
      {
        "start": 2059.24,
        "duration": 4.24,
        "text": "real-time data too the other thing"
      },
      {
        "start": 2061.76,
        "duration": 4.319,
        "text": "that's really cool about using CER in"
      },
      {
        "start": 2063.48,
        "duration": 4.879,
        "text": "this use case is real time right it's"
      },
      {
        "start": 2066.079,
        "duration": 4.201,
        "text": "say a customer is in the middle of"
      },
      {
        "start": 2068.359,
        "duration": 4.72,
        "text": "filling a shopping cart and they ask a"
      },
      {
        "start": 2070.28,
        "duration": 5.48,
        "text": "question about their shopping cart no"
      },
      {
        "start": 2073.079,
        "duration": 5.52,
        "text": "point is that in an LM the LM is a"
      },
      {
        "start": 2075.76,
        "duration": 5.319,
        "text": "unaware so you need to say oh dump the"
      },
      {
        "start": 2078.599,
        "duration": 4.8,
        "text": "contents of the you know you are a"
      },
      {
        "start": 2081.079,
        "duration": 4.6,
        "text": "helpful shopping bot here's the contents"
      },
      {
        "start": 2083.399,
        "duration": 3.96,
        "text": "of the shopping cart the customer has"
      },
      {
        "start": 2085.679,
        "duration": 2.801,
        "text": "questions about this and here's some"
      },
      {
        "start": 2087.359,
        "duration": 4.601,
        "text": "here's some"
      },
      {
        "start": 2088.48,
        "duration": 5.359,
        "text": "context great and that's real time right"
      },
      {
        "start": 2091.96,
        "duration": 5.96,
        "text": "and that's probably a use case you could"
      },
      {
        "start": 2093.839,
        "duration": 5.76,
        "text": "do quickly and easily yay team the thing"
      },
      {
        "start": 2097.92,
        "duration": 3.76,
        "text": "that makes it work in this case is that"
      },
      {
        "start": 2099.599,
        "duration": 3.601,
        "text": "real time stuff and this is where Jor"
      },
      {
        "start": 2101.68,
        "duration": 3.24,
        "text": "here's here's the the slide that"
      },
      {
        "start": 2103.2,
        "duration": 2.919,
        "text": "everyone's going to like oh architecture"
      },
      {
        "start": 2104.92,
        "duration": 3.52,
        "text": "no this is"
      },
      {
        "start": 2106.119,
        "duration": 5.681,
        "text": "some this you should just look at this"
      },
      {
        "start": 2108.44,
        "duration": 4.76,
        "text": "as like wow J Vector is fast well yeah"
      },
      {
        "start": 2111.8,
        "duration": 3.319,
        "text": "because that's the way it was built so"
      },
      {
        "start": 2113.2,
        "duration": 3.6,
        "text": "it it says Astro DB Vector that's what's"
      },
      {
        "start": 2115.119,
        "duration": 5.0,
        "text": "running um that's what's running this"
      },
      {
        "start": 2116.8,
        "duration": 6.4,
        "text": "particular one yep super fast and J"
      },
      {
        "start": 2120.119,
        "duration": 6.72,
        "text": "Vector is built to do this these really"
      },
      {
        "start": 2123.2,
        "duration": 5.2,
        "text": "um fast uh search lookups now if you'll"
      },
      {
        "start": 2126.839,
        "duration": 3.801,
        "text": "notice these numbers aren't like single"
      },
      {
        "start": 2128.4,
        "duration": 4.84,
        "text": "digits we're not there yet no Vector"
      },
      {
        "start": 2130.64,
        "duration": 5.199,
        "text": "search is a heavy thing and so it it"
      },
      {
        "start": 2133.24,
        "duration": 4.32,
        "text": "takes some work but we're getting faster"
      },
      {
        "start": 2135.839,
        "duration": 3.401,
        "text": "and faster and faster and this is"
      },
      {
        "start": 2137.56,
        "duration": 4.48,
        "text": "something that is being worked on"
      },
      {
        "start": 2139.24,
        "duration": 4.56,
        "text": "continuously all day great discussions"
      },
      {
        "start": 2142.04,
        "duration": 3.36,
        "text": "hopefully uh I think we're going to see"
      },
      {
        "start": 2143.8,
        "duration": 2.799,
        "text": "more talks around J Vector out there"
      },
      {
        "start": 2145.4,
        "duration": 4.88,
        "text": "about how we're actually making it"
      },
      {
        "start": 2146.599,
        "duration": 7.0,
        "text": "faster we'll get there but um the thing"
      },
      {
        "start": 2150.28,
        "duration": 6.12,
        "text": "to to remember is if you're using like a"
      },
      {
        "start": 2153.599,
        "duration": 4.281,
        "text": "specialty Vector database it's just not"
      },
      {
        "start": 2156.4,
        "duration": 2.84,
        "text": "differentiated as a matter of fact is"
      },
      {
        "start": 2157.88,
        "duration": 6.64,
        "text": "probably slower than anything that"
      },
      {
        "start": 2159.24,
        "duration": 8.64,
        "text": "Cassandra can do so yeah go"
      },
      {
        "start": 2164.52,
        "duration": 4.88,
        "text": "team okay we're at the point where we"
      },
      {
        "start": 2167.88,
        "duration": 4.16,
        "text": "now I got to play around with my"
      },
      {
        "start": 2169.4,
        "duration": 5.12,
        "text": "keyboard let's dig into like how we can"
      },
      {
        "start": 2172.04,
        "duration": 4.6,
        "text": "actually do this all right you want to"
      },
      {
        "start": 2174.52,
        "duration": 3.559,
        "text": "do this you wanna you want some steps"
      },
      {
        "start": 2176.64,
        "duration": 4.28,
        "text": "like give me some practical stuff here"
      },
      {
        "start": 2178.079,
        "duration": 4.481,
        "text": "dude I'm here so first thing is"
      },
      {
        "start": 2180.92,
        "duration": 4.36,
        "text": "identifying opportunities and this is"
      },
      {
        "start": 2182.56,
        "duration": 5.44,
        "text": "something that I've worked with a lot of"
      },
      {
        "start": 2185.28,
        "duration": 5.76,
        "text": "users out there on and I think is the"
      },
      {
        "start": 2188.0,
        "duration": 6.16,
        "text": "first step of the crisis geni it's like"
      },
      {
        "start": 2191.04,
        "duration": 7.039,
        "text": "what do we even do man I'm not open AI I"
      },
      {
        "start": 2194.16,
        "duration": 6.84,
        "text": "get it um it's not that complicated so"
      },
      {
        "start": 2198.079,
        "duration": 5.601,
        "text": "let's just take a breath together it's"
      },
      {
        "start": 2201.0,
        "duration": 4.04,
        "text": "not that complicated we can do this and"
      },
      {
        "start": 2203.68,
        "duration": 3.24,
        "text": "you're going to pick your path first"
      },
      {
        "start": 2205.04,
        "duration": 4.72,
        "text": "things first do you want to use your"
      },
      {
        "start": 2206.92,
        "duration": 6.08,
        "text": "existing data in Cassandra or are you"
      },
      {
        "start": 2209.76,
        "duration": 5.52,
        "text": "looking for do new things with"
      },
      {
        "start": 2213.0,
        "duration": 5.24,
        "text": "Vector that's important like where you"
      },
      {
        "start": 2215.28,
        "duration": 4.68,
        "text": "want to start and"
      },
      {
        "start": 2218.24,
        "duration": 5.28,
        "text": "staying where you are is probably the"
      },
      {
        "start": 2219.96,
        "duration": 5.639,
        "text": "right choice to get going you you know"
      },
      {
        "start": 2223.52,
        "duration": 4.0,
        "text": "I've worked as a it consultant for years"
      },
      {
        "start": 2225.599,
        "duration": 3.081,
        "text": "and years and years which means I gota"
      },
      {
        "start": 2227.52,
        "duration": 4.44,
        "text": "go in and tell you how to do it and I"
      },
      {
        "start": 2228.68,
        "duration": 6.56,
        "text": "walk away great job by the way but the"
      },
      {
        "start": 2231.96,
        "duration": 6.24,
        "text": "thing that I always see is there's"
      },
      {
        "start": 2235.24,
        "duration": 5.599,
        "text": "always way more excitement and hype"
      },
      {
        "start": 2238.2,
        "duration": 5.52,
        "text": "about doing the thing and the plans get"
      },
      {
        "start": 2240.839,
        "duration": 4.641,
        "text": "ridiculous and scope creep immediately"
      },
      {
        "start": 2243.72,
        "duration": 4.399,
        "text": "and then it never gets built start with"
      },
      {
        "start": 2245.48,
        "duration": 4.28,
        "text": "something simple really simple and get"
      },
      {
        "start": 2248.119,
        "duration": 5.121,
        "text": "your success get your feet under"
      },
      {
        "start": 2249.76,
        "duration": 5.44,
        "text": "yourself ji this is that that Panic is"
      },
      {
        "start": 2253.24,
        "duration": 3.52,
        "text": "going to be better you're going to"
      },
      {
        "start": 2255.2,
        "duration": 4.28,
        "text": "understand so much by just doing"
      },
      {
        "start": 2256.76,
        "duration": 4.44,
        "text": "something simple like how it llm works"
      },
      {
        "start": 2259.48,
        "duration": 3.599,
        "text": "I'll tell you the biggest one of the"
      },
      {
        "start": 2261.2,
        "duration": 3.24,
        "text": "biggest um hurdles you're gonna go over"
      },
      {
        "start": 2263.079,
        "duration": 4.52,
        "text": "is your legal"
      },
      {
        "start": 2264.44,
        "duration": 4.84,
        "text": "team when you say we're gonna put some"
      },
      {
        "start": 2267.599,
        "duration": 3.681,
        "text": "customer data near an"
      },
      {
        "start": 2269.28,
        "duration": 4.24,
        "text": "llm I'm just gonna tell you right now"
      },
      {
        "start": 2271.28,
        "duration": 4.76,
        "text": "that's a fun conversation but one that"
      },
      {
        "start": 2273.52,
        "duration": 4.72,
        "text": "needs to be had and um there are of"
      },
      {
        "start": 2276.04,
        "duration": 3.76,
        "text": "course plenty of ways to mitigate that"
      },
      {
        "start": 2278.24,
        "duration": 4.4,
        "text": "but um and I would love to talk about"
      },
      {
        "start": 2279.8,
        "duration": 4.039,
        "text": "that if you want but you know this is I"
      },
      {
        "start": 2282.64,
        "duration": 3.0,
        "text": "I'm going to say stay where you are is"
      },
      {
        "start": 2283.839,
        "duration": 3.24,
        "text": "probably a good first choice but what"
      },
      {
        "start": 2285.64,
        "duration": 5.0,
        "text": "are you gonna do like how do you want to"
      },
      {
        "start": 2287.079,
        "duration": 6.04,
        "text": "do this now because I know this is a"
      },
      {
        "start": 2290.64,
        "duration": 4.959,
        "text": "problem I actually built a project um"
      },
      {
        "start": 2293.119,
        "duration": 4.441,
        "text": "and I have this uh I'm gonna see if I"
      },
      {
        "start": 2295.599,
        "duration": 5.0,
        "text": "can share the link on"
      },
      {
        "start": 2297.56,
        "duration": 7.72,
        "text": "here hold"
      },
      {
        "start": 2300.599,
        "duration": 4.681,
        "text": "on go into my little calls to"
      },
      {
        "start": 2305.48,
        "duration": 5.92,
        "text": "action now I put the link up"
      },
      {
        "start": 2308.44,
        "duration": 6.919,
        "text": "uh and hopefully it's super simple for"
      },
      {
        "start": 2311.4,
        "duration": 5.84,
        "text": "you but it's the the thing that um this"
      },
      {
        "start": 2315.359,
        "duration": 6.561,
        "text": "does and so I'm gonna do some handson"
      },
      {
        "start": 2317.24,
        "duration": 8.48,
        "text": "here in a second um so what this does is"
      },
      {
        "start": 2321.92,
        "duration": 6.28,
        "text": "uh it's a Python program that will uh go"
      },
      {
        "start": 2325.72,
        "duration": 6.0,
        "text": "use open AI so you put in your open AI"
      },
      {
        "start": 2328.2,
        "duration": 5.159,
        "text": "key if you use local Patrick Cassandra"
      },
      {
        "start": 2331.72,
        "duration": 3.639,
        "text": "you just put in the IP address of the of"
      },
      {
        "start": 2333.359,
        "duration": 4.161,
        "text": "the nodes it's like one of the nodes if"
      },
      {
        "start": 2335.359,
        "duration": 4.72,
        "text": "you're using dsse same thing if if you"
      },
      {
        "start": 2337.52,
        "duration": 4.52,
        "text": "need to usern your password use that um"
      },
      {
        "start": 2340.079,
        "duration": 5.561,
        "text": "but what it does is it goes through it"
      },
      {
        "start": 2342.04,
        "duration": 5.96,
        "text": "queries the entire schema um sends it to"
      },
      {
        "start": 2345.64,
        "duration": 4.52,
        "text": "an LM like open AI you can use many you"
      },
      {
        "start": 2348.0,
        "duration": 5.48,
        "text": "can use any LM you can use a local one"
      },
      {
        "start": 2350.16,
        "duration": 6.04,
        "text": "if you want but it does some prompting"
      },
      {
        "start": 2353.48,
        "duration": 6.28,
        "text": "and pulls out a report and says oh"
      },
      {
        "start": 2356.2,
        "duration": 5.32,
        "text": "here's here's what you have in here I"
      },
      {
        "start": 2359.76,
        "duration": 3.319,
        "text": "it's not magical it's not anything"
      },
      {
        "start": 2361.52,
        "duration": 5.2,
        "text": "revolutionary but it sure gets you"
      },
      {
        "start": 2363.079,
        "duration": 5.28,
        "text": "started and um as a matter of fact Let"
      },
      {
        "start": 2366.72,
        "duration": 4.639,
        "text": "me let me show you the project here so"
      },
      {
        "start": 2368.359,
        "duration": 4.76,
        "text": "I'm gonna share that screen here's a"
      },
      {
        "start": 2371.359,
        "duration": 5.201,
        "text": "Hands-On"
      },
      {
        "start": 2373.119,
        "duration": 5.401,
        "text": "part all right I want to"
      },
      {
        "start": 2376.56,
        "duration": 4.0,
        "text": "do a"
      },
      {
        "start": 2378.52,
        "duration": 5.799,
        "text": "tab there we"
      },
      {
        "start": 2380.56,
        "duration": 6.559,
        "text": "go now this is this is my GitHub project"
      },
      {
        "start": 2384.319,
        "duration": 5.441,
        "text": "um and there's a few things in here that"
      },
      {
        "start": 2387.119,
        "duration": 5.72,
        "text": "are helpful for you so first of all"
      },
      {
        "start": 2389.76,
        "duration": 4.48,
        "text": "there's the getting started but you know"
      },
      {
        "start": 2392.839,
        "duration": 5.441,
        "text": "it walks you through it's just a simple"
      },
      {
        "start": 2394.24,
        "duration": 7.079,
        "text": "python app um the the report and this is"
      },
      {
        "start": 2398.28,
        "duration": 5.0,
        "text": "um this is generated by gp4 I used my"
      },
      {
        "start": 2401.319,
        "duration": 3.76,
        "text": "killer video schema which some of you"
      },
      {
        "start": 2403.28,
        "duration": 4.68,
        "text": "are probably familiar with and I just"
      },
      {
        "start": 2405.079,
        "duration": 4.76,
        "text": "had it evaluate and it gave me lots of"
      },
      {
        "start": 2407.96,
        "duration": 4.52,
        "text": "really cool stuff now I put a lot of"
      },
      {
        "start": 2409.839,
        "duration": 5.681,
        "text": "context in it and that's kind of"
      },
      {
        "start": 2412.48,
        "duration": 4.879,
        "text": "important to get good um good output but"
      },
      {
        "start": 2415.52,
        "duration": 4.319,
        "text": "what I did was I had it I said yeah tell"
      },
      {
        "start": 2417.359,
        "duration": 4.321,
        "text": "me the use cases and it nailed it said"
      },
      {
        "start": 2419.839,
        "duration": 3.561,
        "text": "yeah here's video content analytics blah"
      },
      {
        "start": 2421.68,
        "duration": 3.76,
        "text": "blah blah but then I said give me some"
      },
      {
        "start": 2423.4,
        "duration": 5.52,
        "text": "generative AI features and data model"
      },
      {
        "start": 2425.44,
        "duration": 5.24,
        "text": "enhancements and it did it uh it it said"
      },
      {
        "start": 2428.92,
        "duration": 4.199,
        "text": "hey you know what what would be cool is"
      },
      {
        "start": 2430.68,
        "duration": 4.399,
        "text": "content summarization or video"
      },
      {
        "start": 2433.119,
        "duration": 3.801,
        "text": "recommendations and it gives you the"
      },
      {
        "start": 2435.079,
        "duration": 3.801,
        "text": "like the table breakdowns and everything"
      },
      {
        "start": 2436.92,
        "duration": 5.199,
        "text": "and I'll show you how I did this it's"
      },
      {
        "start": 2438.88,
        "duration": 7.04,
        "text": "not that crazy but it did take some good"
      },
      {
        "start": 2442.119,
        "duration": 5.641,
        "text": "prompting um content sentiment analysis"
      },
      {
        "start": 2445.92,
        "duration": 4.08,
        "text": "um tag generation for videos which I"
      },
      {
        "start": 2447.76,
        "duration": 4.16,
        "text": "thought was really clever and it's"
      },
      {
        "start": 2450.0,
        "duration": 3.56,
        "text": "because each video has a tag on it for"
      },
      {
        "start": 2451.92,
        "duration": 4.24,
        "text": "like the different kind of content it is"
      },
      {
        "start": 2453.56,
        "duration": 4.4,
        "text": "like what if you could just have the AI"
      },
      {
        "start": 2456.16,
        "duration": 3.24,
        "text": "look like look at the video description"
      },
      {
        "start": 2457.96,
        "duration": 5.119,
        "text": "or something and then create some tags"
      },
      {
        "start": 2459.4,
        "duration": 8.04,
        "text": "automatically great so that is what it"
      },
      {
        "start": 2463.079,
        "duration": 7.161,
        "text": "does now um I've I've gone down this"
      },
      {
        "start": 2467.44,
        "duration": 7.12,
        "text": "path a few times of just using something"
      },
      {
        "start": 2470.24,
        "duration": 7.0,
        "text": "like chat TBT to do some work and it's"
      },
      {
        "start": 2474.56,
        "duration": 4.68,
        "text": "pretty cool how that actually happens so"
      },
      {
        "start": 2477.24,
        "duration": 3.48,
        "text": "I'm gonna I'm gon to show you how well"
      },
      {
        "start": 2479.24,
        "duration": 4.04,
        "text": "here's a few things you should see in"
      },
      {
        "start": 2480.72,
        "duration": 4.639,
        "text": "here there's if we go into the test"
      },
      {
        "start": 2483.28,
        "duration": 4.039,
        "text": "directory there's a schema you could"
      },
      {
        "start": 2485.359,
        "duration": 5.041,
        "text": "play around there's the entire killer"
      },
      {
        "start": 2487.319,
        "duration": 5.241,
        "text": "video schema in here um if you want to"
      },
      {
        "start": 2490.4,
        "duration": 4.199,
        "text": "play around with somebody else's schema"
      },
      {
        "start": 2492.56,
        "duration": 3.6,
        "text": "great here you go um this is a really"
      },
      {
        "start": 2494.599,
        "duration": 3.921,
        "text": "this is a really good application schema"
      },
      {
        "start": 2496.16,
        "duration": 6.0,
        "text": "if you want to play with it and um it"
      },
      {
        "start": 2498.52,
        "duration": 5.28,
        "text": "works on 311 all the way up um and I"
      },
      {
        "start": 2502.16,
        "duration": 4.56,
        "text": "don't think I have any SEI things in"
      },
      {
        "start": 2503.8,
        "duration": 4.559,
        "text": "here yet so it's pretty basic it doesn't"
      },
      {
        "start": 2506.72,
        "duration": 3.48,
        "text": "need to be complicated you just you just"
      },
      {
        "start": 2508.359,
        "duration": 5.361,
        "text": "need a schema for it to look at and chew"
      },
      {
        "start": 2510.2,
        "duration": 6.04,
        "text": "on um I also have some some prompts in"
      },
      {
        "start": 2513.72,
        "duration": 6.04,
        "text": "here too these and there helper docks"
      },
      {
        "start": 2516.24,
        "duration": 5.32,
        "text": "and uh as a matter of fact I I had to do"
      },
      {
        "start": 2519.76,
        "duration": 5.44,
        "text": "some work with this like adding a node"
      },
      {
        "start": 2521.56,
        "duration": 6.68,
        "text": "with no context or with context um but I"
      },
      {
        "start": 2525.2,
        "duration": 7.8,
        "text": "have a prompt question in here that this"
      },
      {
        "start": 2528.24,
        "duration": 7.16,
        "text": "is what I actually use to build that um"
      },
      {
        "start": 2533.0,
        "duration": 4.52,
        "text": "that schema or output for that schema it"
      },
      {
        "start": 2535.4,
        "duration": 4.88,
        "text": "takes the docs for Vector on the"
      },
      {
        "start": 2537.52,
        "duration": 4.039,
        "text": "consenter website adds that as context"
      },
      {
        "start": 2540.28,
        "duration": 3.0,
        "text": "and then it has this thing your job is"
      },
      {
        "start": 2541.559,
        "duration": 4.641,
        "text": "to create a report for end users here's"
      },
      {
        "start": 2543.28,
        "duration": 4.52,
        "text": "the template blah blah blah um does all"
      },
      {
        "start": 2546.2,
        "duration": 3.0,
        "text": "that for you"
      },
      {
        "start": 2547.8,
        "duration": 3.559,
        "text": "all right I'm GNA show you how that"
      },
      {
        "start": 2549.2,
        "duration": 3.639,
        "text": "would actually work so let's stop"
      },
      {
        "start": 2551.359,
        "duration": 3.641,
        "text": "sharing"
      },
      {
        "start": 2552.839,
        "duration": 4.041,
        "text": "here and I'm G to go over here and I'm"
      },
      {
        "start": 2555.0,
        "duration": 4.16,
        "text": "going to share my I'm gonna share some"
      },
      {
        "start": 2556.88,
        "duration": 2.28,
        "text": "chat"
      },
      {
        "start": 2563.119,
        "duration": 7.681,
        "text": "GPT actually what I think I'm gonna"
      },
      {
        "start": 2566.319,
        "duration": 4.481,
        "text": "do I'm just gonna share the whole"
      },
      {
        "start": 2571.24,
        "duration": 5.2,
        "text": "window all"
      },
      {
        "start": 2573.64,
        "duration": 4.04,
        "text": "right now I'm going to take uh that"
      },
      {
        "start": 2576.44,
        "duration": 4.56,
        "text": "schema let's go"
      },
      {
        "start": 2577.68,
        "duration": 3.32,
        "text": "let's take a look at the schema"
      },
      {
        "start": 2583.96,
        "duration": 4.72,
        "text": "here now this is stuff that you could do"
      },
      {
        "start": 2586.68,
        "duration": 4.24,
        "text": "immediately so I'm just going to copy"
      },
      {
        "start": 2588.68,
        "duration": 6.2,
        "text": "all of it I'm gonna go over to chat GPT"
      },
      {
        "start": 2590.92,
        "duration": 5.439,
        "text": "and say all right you are a eron let me"
      },
      {
        "start": 2594.88,
        "duration": 3.76,
        "text": "know if you can see this"
      },
      {
        "start": 2596.359,
        "duration": 5.801,
        "text": "okay or if I need to zoom in more"
      },
      {
        "start": 2598.64,
        "duration": 6.56,
        "text": "because I know you're watching um"
      },
      {
        "start": 2602.16,
        "duration": 7.399,
        "text": "Apache Cassandra expert okay so now"
      },
      {
        "start": 2605.2,
        "duration": 8.119,
        "text": "we've created um created the the role"
      },
      {
        "start": 2609.559,
        "duration": 6.881,
        "text": "okay sorry and aache I love grammarly it"
      },
      {
        "start": 2613.319,
        "duration": 4.961,
        "text": "just tells me how bad I am in English um"
      },
      {
        "start": 2616.44,
        "duration": 4.72,
        "text": "you"
      },
      {
        "start": 2618.28,
        "duration": 6.2,
        "text": "are"
      },
      {
        "start": 2621.16,
        "duration": 8.08,
        "text": "uh all right thanks a you are to"
      },
      {
        "start": 2624.48,
        "duration": 6.8,
        "text": "build reports for"
      },
      {
        "start": 2629.24,
        "duration": 4.4,
        "text": "schemas"
      },
      {
        "start": 2631.28,
        "duration": 4.44,
        "text": "oops you know it's live when I make"
      },
      {
        "start": 2633.64,
        "duration": 7.04,
        "text": "those kind of mistakes all"
      },
      {
        "start": 2635.72,
        "duration": 4.96,
        "text": "right here is one to"
      },
      {
        "start": 2640.8,
        "duration": 7.36,
        "text": "evaluate break down use"
      },
      {
        "start": 2645.04,
        "duration": 6.2,
        "text": "cases okay and then what I do is I just"
      },
      {
        "start": 2648.16,
        "duration": 7.04,
        "text": "paste in that whole entire schema and"
      },
      {
        "start": 2651.24,
        "duration": 5.839,
        "text": "watch it work and it's really cool"
      },
      {
        "start": 2655.2,
        "duration": 4.48,
        "text": "because I don't know if you've done this"
      },
      {
        "start": 2657.079,
        "duration": 4.881,
        "text": "before if you haven't used chat GPT for"
      },
      {
        "start": 2659.68,
        "duration": 4.52,
        "text": "this sort of thing it does work really"
      },
      {
        "start": 2661.96,
        "duration": 5.2,
        "text": "really well so I'm using"
      },
      {
        "start": 2664.2,
        "duration": 5.159,
        "text": "gp4 um and it's been updated since I"
      },
      {
        "start": 2667.16,
        "duration": 4.6,
        "text": "think September so there's enough"
      },
      {
        "start": 2669.359,
        "duration": 5.361,
        "text": "information in here to be dangerous with"
      },
      {
        "start": 2671.76,
        "duration": 4.88,
        "text": "Cassandra um I can add context and I"
      },
      {
        "start": 2674.72,
        "duration": 4.48,
        "text": "will for Vector because what's funny is"
      },
      {
        "start": 2676.64,
        "duration": 6.24,
        "text": "when we say watch what happens when I"
      },
      {
        "start": 2679.2,
        "duration": 5.2,
        "text": "add it to add more use cases um it's"
      },
      {
        "start": 2682.88,
        "duration": 4.479,
        "text": "gonna it's going to show the limits of"
      },
      {
        "start": 2684.4,
        "duration": 6.36,
        "text": "its knowledge but we can fix that right"
      },
      {
        "start": 2687.359,
        "duration": 5.441,
        "text": "and um what I this is also probably a"
      },
      {
        "start": 2690.76,
        "duration": 4.4,
        "text": "good thing to look see how slow this is"
      },
      {
        "start": 2692.8,
        "duration": 4.759,
        "text": "going that's the LM experience that's"
      },
      {
        "start": 2695.16,
        "duration": 4.64,
        "text": "why I want to minimize in an application"
      },
      {
        "start": 2697.559,
        "duration": 3.841,
        "text": "how much interaction I have with an llm"
      },
      {
        "start": 2699.8,
        "duration": 3.96,
        "text": "and I want to minimize the amount of"
      },
      {
        "start": 2701.4,
        "duration": 5.919,
        "text": "information it has to create to respond"
      },
      {
        "start": 2703.76,
        "duration": 5.24,
        "text": "back to the user and these are just slow"
      },
      {
        "start": 2707.319,
        "duration": 4.201,
        "text": "now there are companies like grock that"
      },
      {
        "start": 2709.0,
        "duration": 5.04,
        "text": "built a chip that are so much faster but"
      },
      {
        "start": 2711.52,
        "duration": 5.799,
        "text": "as it sits right now uh any data that"
      },
      {
        "start": 2714.04,
        "duration": 7.16,
        "text": "goes through an llm is going to be like"
      },
      {
        "start": 2717.319,
        "duration": 6.441,
        "text": "the old dial at modem speed um but it is"
      },
      {
        "start": 2721.2,
        "duration": 5.28,
        "text": "doing a good job it it broke down every"
      },
      {
        "start": 2723.76,
        "duration": 4.839,
        "text": "one of the the the tables in the system"
      },
      {
        "start": 2726.48,
        "duration": 4.44,
        "text": "gave it purpose a use case I mean that's"
      },
      {
        "start": 2728.599,
        "duration": 6.48,
        "text": "pretty cool right um and I've used this"
      },
      {
        "start": 2730.92,
        "duration": 6.08,
        "text": "a few times to sort through now again"
      },
      {
        "start": 2735.079,
        "duration": 3.961,
        "text": "before you go and just do this make sure"
      },
      {
        "start": 2737.0,
        "duration": 3.8,
        "text": "it's legally cool with your company you"
      },
      {
        "start": 2739.04,
        "duration": 4.039,
        "text": "do not want to get in trouble for this"
      },
      {
        "start": 2740.8,
        "duration": 5.4,
        "text": "and there I have some other ways to do"
      },
      {
        "start": 2743.079,
        "duration": 7.121,
        "text": "that um for instance for data Stacks"
      },
      {
        "start": 2746.2,
        "duration": 6.84,
        "text": "we're uh we can use Azure and um use GPT"
      },
      {
        "start": 2750.2,
        "duration": 4.919,
        "text": "and Azure for sensitive things because"
      },
      {
        "start": 2753.04,
        "duration": 3.48,
        "text": "we have our own version of a GPT in"
      },
      {
        "start": 2755.119,
        "duration": 3.121,
        "text": "there that's something our legal"
      },
      {
        "start": 2756.52,
        "duration": 4.96,
        "text": "department vetted out and said this is"
      },
      {
        "start": 2758.24,
        "duration": 4.92,
        "text": "cool you can do this one thing and and"
      },
      {
        "start": 2761.48,
        "duration": 3.72,
        "text": "we still have to ask for certain things"
      },
      {
        "start": 2763.16,
        "duration": 3.36,
        "text": "I I'm giving you this as an example"
      },
      {
        "start": 2765.2,
        "duration": 2.879,
        "text": "because you should be doing the same"
      },
      {
        "start": 2766.52,
        "duration": 3.72,
        "text": "thing you do not want to get in trouble"
      },
      {
        "start": 2768.079,
        "duration": 5.04,
        "text": "because there's just a lot of fud people"
      },
      {
        "start": 2770.24,
        "duration": 5.72,
        "text": "are afraid of llms and don't get in"
      },
      {
        "start": 2773.119,
        "duration": 6.0,
        "text": "trouble but anyway here we are and I'm"
      },
      {
        "start": 2775.96,
        "duration": 6.159,
        "text": "gonna say can you"
      },
      {
        "start": 2779.119,
        "duration": 3.0,
        "text": "Rec"
      },
      {
        "start": 2783.76,
        "duration": 5.16,
        "text": "recommend I can't even spell but this"
      },
      {
        "start": 2785.72,
        "duration": 3.2,
        "text": "this is great"
      },
      {
        "start": 2790.0,
        "duration": 3.04,
        "text": "I could completely misspell it and it"
      },
      {
        "start": 2791.359,
        "duration": 5.121,
        "text": "still knows"
      },
      {
        "start": 2793.04,
        "duration": 5.88,
        "text": "it yeah it's like I have no idea what"
      },
      {
        "start": 2796.48,
        "duration": 2.44,
        "text": "you're talking"
      },
      {
        "start": 2801.079,
        "duration": 7.721,
        "text": "about so I would do this like that now"
      },
      {
        "start": 2804.48,
        "duration": 4.32,
        "text": "we have some context to give"
      },
      {
        "start": 2809.559,
        "duration": 6.28,
        "text": "it so what is that context uh go over to"
      },
      {
        "start": 2812.92,
        "duration": 5.919,
        "text": "helper dock here's some context this"
      },
      {
        "start": 2815.839,
        "duration": 3.0,
        "text": "prompt question"
      },
      {
        "start": 2819.64,
        "duration": 4.719,
        "text": "and right here um you include the use of"
      },
      {
        "start": 2823.04,
        "duration": 3.24,
        "text": "con Stander Vector data support now what"
      },
      {
        "start": 2824.359,
        "duration": 4.921,
        "text": "I did is I just copy the docs from the"
      },
      {
        "start": 2826.28,
        "duration": 6.88,
        "text": "cander website and use that in my prompt"
      },
      {
        "start": 2829.28,
        "duration": 5.799,
        "text": "so I'm like okay that's enough stop stop"
      },
      {
        "start": 2833.16,
        "duration": 3.8,
        "text": "so I'm just going to put this in and"
      },
      {
        "start": 2835.079,
        "duration": 4.801,
        "text": "what what I what I'm telling you is like"
      },
      {
        "start": 2836.96,
        "duration": 5.84,
        "text": "okay I want you to use castander Vector"
      },
      {
        "start": 2839.88,
        "duration": 5.8,
        "text": "here's the docs here's the task and I'm"
      },
      {
        "start": 2842.8,
        "duration": 5.72,
        "text": "giving it context that's something that"
      },
      {
        "start": 2845.68,
        "duration": 6.12,
        "text": "is similar to if I use a like a a search"
      },
      {
        "start": 2848.52,
        "duration": 7.24,
        "text": "or like a rag I'm just doing manual rag"
      },
      {
        "start": 2851.8,
        "duration": 5.799,
        "text": "M rag but the this rag I just like the"
      },
      {
        "start": 2855.76,
        "duration": 3.599,
        "text": "results that I would get would go into"
      },
      {
        "start": 2857.599,
        "duration": 3.681,
        "text": "the context now I'm giving it context"
      },
      {
        "start": 2859.359,
        "duration": 4.641,
        "text": "I'm like okay here's everything about a"
      },
      {
        "start": 2861.28,
        "duration": 4.92,
        "text": "vector database go use that Watch What"
      },
      {
        "start": 2864.0,
        "duration": 4.96,
        "text": "Happens it's like"
      },
      {
        "start": 2866.2,
        "duration": 5.52,
        "text": "aha now it's like this is my favorite"
      },
      {
        "start": 2868.96,
        "duration": 5.28,
        "text": "feature of Chad gbt now it's like oh I I"
      },
      {
        "start": 2871.72,
        "duration": 4.28,
        "text": "knew that given the comprehensive"
      },
      {
        "start": 2874.24,
        "duration": 4.0,
        "text": "details about using Vector data support"
      },
      {
        "start": 2876.0,
        "duration": 5.359,
        "text": "Cassandra so it's like almost like"
      },
      {
        "start": 2878.24,
        "duration": 5.24,
        "text": "thankful but now it's not only giving me"
      },
      {
        "start": 2881.359,
        "duration": 3.681,
        "text": "the answers it's giving me code it's"
      },
      {
        "start": 2883.48,
        "duration": 5.48,
        "text": "giving me an alter"
      },
      {
        "start": 2885.04,
        "duration": 7.0,
        "text": "table that's cool and it works I've"
      },
      {
        "start": 2888.96,
        "duration": 5.639,
        "text": "tested this it it does work but I think"
      },
      {
        "start": 2892.04,
        "duration": 4.6,
        "text": "the takeaway here is when you're in the"
      },
      {
        "start": 2894.599,
        "duration": 3.641,
        "text": "mode of trying to figure out an"
      },
      {
        "start": 2896.64,
        "duration": 2.64,
        "text": "implementation and you're using your"
      },
      {
        "start": 2898.24,
        "duration": 3.44,
        "text": "existing"
      },
      {
        "start": 2899.28,
        "duration": 4.24,
        "text": "data um you can go down this path if I"
      },
      {
        "start": 2901.68,
        "duration": 3.96,
        "text": "want to use a vector database or I want"
      },
      {
        "start": 2903.52,
        "duration": 7.72,
        "text": "to add Vector you know C Stander 5 stuff"
      },
      {
        "start": 2905.64,
        "duration": 8.88,
        "text": "or ESC or as ra or not and you can give"
      },
      {
        "start": 2911.24,
        "duration": 5.04,
        "text": "it to schema and do some ideation inside"
      },
      {
        "start": 2914.52,
        "duration": 4.079,
        "text": "here one of the things that LMS are"
      },
      {
        "start": 2916.28,
        "duration": 4.079,
        "text": "really good at is being concise about"
      },
      {
        "start": 2918.599,
        "duration": 5.801,
        "text": "taking tons of information and answering"
      },
      {
        "start": 2920.359,
        "duration": 10.521,
        "text": "a concise question um you're like what"
      },
      {
        "start": 2924.4,
        "duration": 6.48,
        "text": "about non Vector rag use"
      },
      {
        "start": 2931.92,
        "duration": 7.24,
        "text": "cases so it's going to think about that"
      },
      {
        "start": 2936.2,
        "duration": 4.2,
        "text": "and it knows what rag is which is cool"
      },
      {
        "start": 2939.16,
        "duration": 5.6,
        "text": "um this is why I"
      },
      {
        "start": 2940.4,
        "duration": 7.48,
        "text": "use CH you know chat GPT 4 is the best"
      },
      {
        "start": 2944.76,
        "duration": 7.4,
        "text": "model out there right now um there are"
      },
      {
        "start": 2947.88,
        "duration": 8.04,
        "text": "many and gp4 still kind of kills them"
      },
      {
        "start": 2952.16,
        "duration": 6.52,
        "text": "all and that's okay um you know it's"
      },
      {
        "start": 2955.92,
        "duration": 4.84,
        "text": "it's competition is good although I was"
      },
      {
        "start": 2958.68,
        "duration": 4.72,
        "text": "gonna have uh we could have some fun I"
      },
      {
        "start": 2960.76,
        "duration": 5.2,
        "text": "also have a Gemini 1.5 and Gemini 1.5 is"
      },
      {
        "start": 2963.4,
        "duration": 5.159,
        "text": "pretty slick right now too it's uh it"
      },
      {
        "start": 2965.96,
        "duration": 4.68,
        "text": "does things different differently and it"
      },
      {
        "start": 2968.559,
        "duration": 3.881,
        "text": "has different context inside of its own"
      },
      {
        "start": 2970.64,
        "duration": 3.88,
        "text": "search engine history because you know"
      },
      {
        "start": 2972.44,
        "duration": 3.639,
        "text": "Gemini was trained off with Google data"
      },
      {
        "start": 2974.52,
        "duration": 2.76,
        "text": "but um it could do some really cool"
      },
      {
        "start": 2976.079,
        "duration": 3.361,
        "text": "things"
      },
      {
        "start": 2977.28,
        "duration": 4.559,
        "text": "too so we'll do that real quick and have"
      },
      {
        "start": 2979.44,
        "duration": 3.919,
        "text": "fun with it but anyway you could see"
      },
      {
        "start": 2981.839,
        "duration": 3.561,
        "text": "what it's trying to do here is like hey"
      },
      {
        "start": 2983.359,
        "duration": 4.361,
        "text": "you know you could do this and here's"
      },
      {
        "start": 2985.4,
        "duration": 4.32,
        "text": "here's how you can set it up I can even"
      },
      {
        "start": 2987.72,
        "duration": 4.96,
        "text": "have it write some of the basic code as"
      },
      {
        "start": 2989.72,
        "duration": 6.24,
        "text": "well but we'll get to that in a"
      },
      {
        "start": 2992.68,
        "duration": 5.639,
        "text": "minute but um actually right now let's"
      },
      {
        "start": 2995.96,
        "duration": 4.96,
        "text": "let's switch gear let's go back to what"
      },
      {
        "start": 2998.319,
        "duration": 8.04,
        "text": "we're trying to do stop"
      },
      {
        "start": 3000.92,
        "duration": 5.439,
        "text": "sharing and go back to sharing your"
      },
      {
        "start": 3009.28,
        "duration": 4.799,
        "text": "window all right back to this all right"
      },
      {
        "start": 3012.44,
        "duration": 5.8,
        "text": "so we did some stuff here we did some"
      },
      {
        "start": 3014.079,
        "duration": 6.361,
        "text": "Hands-On so exploring use cases my my"
      },
      {
        "start": 3018.24,
        "duration": 5.8,
        "text": "encouragement to you is go out there and"
      },
      {
        "start": 3020.44,
        "duration": 6.879,
        "text": "try this and you will be pleasantly"
      },
      {
        "start": 3024.04,
        "duration": 5.12,
        "text": "surprised at how it does work work and"
      },
      {
        "start": 3027.319,
        "duration": 5.441,
        "text": "it can give you some good ideas you're"
      },
      {
        "start": 3029.16,
        "duration": 5.24,
        "text": "using gen to create gen and you know"
      },
      {
        "start": 3032.76,
        "duration": 3.76,
        "text": "it's it's good to you because it gets"
      },
      {
        "start": 3034.4,
        "duration": 3.919,
        "text": "you in the game you're getting your"
      },
      {
        "start": 3036.52,
        "duration": 5.76,
        "text": "hands on an llm you're understanding how"
      },
      {
        "start": 3038.319,
        "duration": 6.0,
        "text": "LMS work um that's important that's very"
      },
      {
        "start": 3042.28,
        "duration": 5.24,
        "text": "important okay so what about"
      },
      {
        "start": 3044.319,
        "duration": 6.361,
        "text": "implementations now okay we have our"
      },
      {
        "start": 3047.52,
        "duration": 4.88,
        "text": "idea how do we want to do this now I've"
      },
      {
        "start": 3050.68,
        "duration": 4.399,
        "text": "you can use Lang chain and LW Index"
      },
      {
        "start": 3052.4,
        "duration": 4.84,
        "text": "right now as they are and they are those"
      },
      {
        "start": 3055.079,
        "duration": 5.441,
        "text": "are the two libraries that are"
      },
      {
        "start": 3057.24,
        "duration": 7.079,
        "text": "are predominantly for building agents in"
      },
      {
        "start": 3060.52,
        "duration": 6.319,
        "text": "gen linkchain and L index have don't"
      },
      {
        "start": 3064.319,
        "duration": 4.161,
        "text": "completely overlap it's worth a look at"
      },
      {
        "start": 3066.839,
        "duration": 3.641,
        "text": "both of these and I'd be I would love to"
      },
      {
        "start": 3068.48,
        "duration": 5.68,
        "text": "do a deep dive into both um they are"
      },
      {
        "start": 3070.48,
        "duration": 6.079,
        "text": "very different in a lot of ways uh and"
      },
      {
        "start": 3074.16,
        "duration": 5.399,
        "text": "more importantly is that da Stacks we"
      },
      {
        "start": 3076.559,
        "duration": 6.8,
        "text": "have our own Library called rag stack"
      },
      {
        "start": 3079.559,
        "duration": 5.601,
        "text": "and it's uh a tested Enterprise version"
      },
      {
        "start": 3083.359,
        "duration": 2.841,
        "text": "of both L chain and LOM index we work"
      },
      {
        "start": 3085.16,
        "duration": 3.52,
        "text": "with both of those teams pretty"
      },
      {
        "start": 3086.2,
        "duration": 6.72,
        "text": "extensive L rag Stacks open source you"
      },
      {
        "start": 3088.68,
        "duration": 8.56,
        "text": "can go pip install rag stack IO and it"
      },
      {
        "start": 3092.92,
        "duration": 7.159,
        "text": "works um it's sorry rack. but uh the"
      },
      {
        "start": 3097.24,
        "duration": 5.119,
        "text": "link I'll put the link up in here change"
      },
      {
        "start": 3100.079,
        "duration": 2.28,
        "text": "that"
      },
      {
        "start": 3104.28,
        "duration": 4.559,
        "text": "one so there's the rag stack link that I"
      },
      {
        "start": 3107.079,
        "duration": 3.561,
        "text": "put in the page um and it should be in"
      },
      {
        "start": 3108.839,
        "duration": 4.76,
        "text": "the description as well it's a great"
      },
      {
        "start": 3110.64,
        "duration": 4.4,
        "text": "little tool um I you know what it does"
      },
      {
        "start": 3113.599,
        "duration": 3.561,
        "text": "is it gives you confidence to work with"
      },
      {
        "start": 3115.04,
        "duration": 3.24,
        "text": "one of these fast moving Li and that's"
      },
      {
        "start": 3117.16,
        "duration": 4.08,
        "text": "probably the thing you have to"
      },
      {
        "start": 3118.28,
        "duration": 4.76,
        "text": "understand is L chain align index are"
      },
      {
        "start": 3121.24,
        "duration": 3.119,
        "text": "moving fast because this industry is"
      },
      {
        "start": 3123.04,
        "duration": 3.2,
        "text": "moving fast and if you want something"
      },
      {
        "start": 3124.359,
        "duration": 4.72,
        "text": "that's a little more stable that you can"
      },
      {
        "start": 3126.24,
        "duration": 5.48,
        "text": "rely on the on it being tested that's"
      },
      {
        "start": 3129.079,
        "duration": 4.48,
        "text": "what we did with rag stacks and um like"
      },
      {
        "start": 3131.72,
        "duration": 3.96,
        "text": "I said we're doing this in conjunction"
      },
      {
        "start": 3133.559,
        "duration": 4.481,
        "text": "with L chain alignment Index this is not"
      },
      {
        "start": 3135.68,
        "duration": 4.96,
        "text": "as as a um we're not doing it avac"
      },
      {
        "start": 3138.04,
        "duration": 3.96,
        "text": "serially at all and it's it's good for"
      },
      {
        "start": 3140.64,
        "duration": 3.64,
        "text": "them because they can move fast and we"
      },
      {
        "start": 3142.0,
        "duration": 3.92,
        "text": "can provide a lot of help with that and"
      },
      {
        "start": 3144.28,
        "duration": 4.36,
        "text": "we use it inside our product quite a bit"
      },
      {
        "start": 3145.92,
        "duration": 4.6,
        "text": "and when it when we work with Cassandra"
      },
      {
        "start": 3148.64,
        "duration": 3.439,
        "text": "that's the other thing is it works"
      },
      {
        "start": 3150.52,
        "duration": 4.2,
        "text": "really seamlessly with Cassandra"
      },
      {
        "start": 3152.079,
        "duration": 4.72,
        "text": "workflows in Astra and DSC and in open"
      },
      {
        "start": 3154.72,
        "duration": 5.96,
        "text": "source"
      },
      {
        "start": 3156.799,
        "duration": 7.04,
        "text": "Cassandra so I I mentioned this uh like"
      },
      {
        "start": 3160.68,
        "duration": 5.6,
        "text": "how do I get um how do I get some code"
      },
      {
        "start": 3163.839,
        "duration": 4.48,
        "text": "in this and building some prototypes"
      },
      {
        "start": 3166.28,
        "duration": 3.36,
        "text": "let's go back and try this real quick I"
      },
      {
        "start": 3168.319,
        "duration": 4.361,
        "text": "know we're coming up on the end of the"
      },
      {
        "start": 3169.64,
        "duration": 5.36,
        "text": "hour but we'll do this quick"
      },
      {
        "start": 3172.68,
        "duration": 6.0,
        "text": "so I'm gonna go ahead and share my"
      },
      {
        "start": 3175.0,
        "duration": 6.2,
        "text": "screen once again um and we're going to"
      },
      {
        "start": 3178.68,
        "duration": 5.04,
        "text": "see what let's let's play with Gemini"
      },
      {
        "start": 3181.2,
        "duration": 5.359,
        "text": "let's see what Gemini will come up with"
      },
      {
        "start": 3183.72,
        "duration": 5.92,
        "text": "let love this you never know"
      },
      {
        "start": 3186.559,
        "duration": 7.321,
        "text": "right so all right I'm gonna go in here"
      },
      {
        "start": 3189.64,
        "duration": 6.6,
        "text": "I'm gonna grab this page all right so"
      },
      {
        "start": 3193.88,
        "duration": 4.959,
        "text": "I'm going to go back to my schema I'm"
      },
      {
        "start": 3196.24,
        "duration": 2.599,
        "text": "gonna grab"
      },
      {
        "start": 3199.24,
        "duration": 9.2,
        "text": "this and I'm going to pull the schema"
      },
      {
        "start": 3202.319,
        "duration": 8.601,
        "text": "into Gemini 1.5 and I'm say here's"
      },
      {
        "start": 3208.44,
        "duration": 4.359,
        "text": "schema so Gemini is interesting because"
      },
      {
        "start": 3210.92,
        "duration": 4.399,
        "text": "it has a massive context window like a"
      },
      {
        "start": 3212.799,
        "duration": 3.881,
        "text": "million right um you could pretty much"
      },
      {
        "start": 3215.319,
        "duration": 4.921,
        "text": "upload the entire code base for"
      },
      {
        "start": 3216.68,
        "duration": 6.639,
        "text": "Cassandra in there it might be expensive"
      },
      {
        "start": 3220.24,
        "duration": 4.4,
        "text": "and slow so a don't but here we go but"
      },
      {
        "start": 3223.319,
        "duration": 3.401,
        "text": "you are"
      },
      {
        "start": 3224.64,
        "duration": 4.36,
        "text": "a uh"
      },
      {
        "start": 3226.72,
        "duration": 5.96,
        "text": "Pache"
      },
      {
        "start": 3229.0,
        "duration": 6.96,
        "text": "Cassandra developer in Python there we"
      },
      {
        "start": 3232.68,
        "duration": 5.76,
        "text": "go so we just created that um you are"
      },
      {
        "start": 3235.96,
        "duration": 2.48,
        "text": "building"
      },
      {
        "start": 3241.0,
        "duration": 7.079,
        "text": "building a rag application using"
      },
      {
        "start": 3245.28,
        "duration": 5.839,
        "text": "Cassandra"
      },
      {
        "start": 3248.079,
        "duration": 6.04,
        "text": "data here is the"
      },
      {
        "start": 3251.119,
        "duration": 3.0,
        "text": "schema"
      },
      {
        "start": 3255.28,
        "duration": 3.92,
        "text": "one what would you build"
      },
      {
        "start": 3259.4,
        "duration": 3.919,
        "text": "first now I'm just going to give it some"
      },
      {
        "start": 3261.44,
        "duration": 4.359,
        "text": "code here you"
      },
      {
        "start": 3263.319,
        "duration": 5.04,
        "text": "go uh oh I just gave it the vector"
      },
      {
        "start": 3265.799,
        "duration": 2.56,
        "text": "search"
      },
      {
        "start": 3270.72,
        "duration": 4.839,
        "text": "oops I gave it the wrong"
      },
      {
        "start": 3272.76,
        "duration": 5.039,
        "text": "thing you know this is you know it's"
      },
      {
        "start": 3275.559,
        "duration": 4.641,
        "text": "live but it is kicking out a lot of"
      },
      {
        "start": 3277.799,
        "duration": 2.401,
        "text": "stuff"
      },
      {
        "start": 3283.599,
        "duration": 4.681,
        "text": "huh I say stop all"
      },
      {
        "start": 3290.079,
        "duration": 8.24,
        "text": "right here's the schema no that's not"
      },
      {
        "start": 3293.48,
        "duration": 4.839,
        "text": "what I want why is it not copying that"
      },
      {
        "start": 3303.2,
        "duration": 8.28,
        "text": "fine for some reason I can't copy schema"
      },
      {
        "start": 3307.559,
        "duration": 3.921,
        "text": "anymore I have to do this"
      },
      {
        "start": 3312.079,
        "duration": 7.081,
        "text": "copy paste there we"
      },
      {
        "start": 3315.48,
        "duration": 7.44,
        "text": "go pH we're doing it"
      },
      {
        "start": 3319.16,
        "duration": 5.399,
        "text": "live now it's going to go through there"
      },
      {
        "start": 3322.92,
        "duration": 6.24,
        "text": "give me the embeddings it's going to say"
      },
      {
        "start": 3324.559,
        "duration": 6.401,
        "text": "use embeddings but okay fine regardless"
      },
      {
        "start": 3329.16,
        "duration": 3.0,
        "text": "what I wanted to do and I'm going to"
      },
      {
        "start": 3330.96,
        "duration": 5.68,
        "text": "stop it real quick because I don't want"
      },
      {
        "start": 3332.16,
        "duration": 4.48,
        "text": "to completely do this can"
      },
      {
        "start": 3346.96,
        "duration": 5.879,
        "text": "you I'm I have no idea if this is gonna"
      },
      {
        "start": 3349.64,
        "duration": 7.439,
        "text": "work so I'm G python Lane chain for"
      },
      {
        "start": 3352.839,
        "duration": 6.921,
        "text": "sentiment video sentiment"
      },
      {
        "start": 3357.079,
        "duration": 5.641,
        "text": "I have no idea if this is going to work"
      },
      {
        "start": 3359.76,
        "duration": 5.359,
        "text": "but as most LMS go they're very"
      },
      {
        "start": 3362.72,
        "duration": 5.2,
        "text": "confidently going to give you the answer"
      },
      {
        "start": 3365.119,
        "duration": 5.281,
        "text": "and look at that it's creating all the"
      },
      {
        "start": 3367.92,
        "duration": 5.52,
        "text": "code for that so that"
      },
      {
        "start": 3370.4,
        "duration": 5.159,
        "text": "worked now I've done this a lot and I"
      },
      {
        "start": 3373.44,
        "duration": 4.119,
        "text": "found that it is pretty good about"
      },
      {
        "start": 3375.559,
        "duration": 4.28,
        "text": "coming up with prototype"
      },
      {
        "start": 3377.559,
        "duration": 4.161,
        "text": "code not all of it works the way you"
      },
      {
        "start": 3379.839,
        "duration": 4.24,
        "text": "want it to but it's easy to tweak it's a"
      },
      {
        "start": 3381.72,
        "duration": 3.879,
        "text": "good way to get started it also opens"
      },
      {
        "start": 3384.079,
        "duration": 3.601,
        "text": "the door for something you haven't done"
      },
      {
        "start": 3385.599,
        "duration": 5.641,
        "text": "before like maybe I've never done"
      },
      {
        "start": 3387.68,
        "duration": 5.679,
        "text": "sentiment analysis with a video well it"
      },
      {
        "start": 3391.24,
        "duration": 5.599,
        "text": "gives me all the the starter like here"
      },
      {
        "start": 3393.359,
        "duration": 5.521,
        "text": "use all this stuff and it may not be"
      },
      {
        "start": 3396.839,
        "duration": 3.96,
        "text": "exactly what I want but it sure gets me"
      },
      {
        "start": 3398.88,
        "duration": 4.12,
        "text": "down the road and I can import this into"
      },
      {
        "start": 3400.799,
        "duration": 4.361,
        "text": "vs code or something and copy into vs"
      },
      {
        "start": 3403.0,
        "duration": 3.599,
        "text": "code and make some changes or I and the"
      },
      {
        "start": 3405.16,
        "duration": 3.879,
        "text": "best thing that it can do is I can go"
      },
      {
        "start": 3406.599,
        "duration": 3.841,
        "text": "back and say oh no no no I want to use"
      },
      {
        "start": 3409.039,
        "duration": 3.32,
        "text": "this library or I want to make this"
      },
      {
        "start": 3410.44,
        "duration": 4.359,
        "text": "change to it whatever and then it'll re"
      },
      {
        "start": 3412.359,
        "duration": 5.081,
        "text": "rewrite the code so hopefully that's"
      },
      {
        "start": 3414.799,
        "duration": 5.0,
        "text": "helpful for you as well"
      },
      {
        "start": 3417.44,
        "duration": 4.599,
        "text": "now let me wrap things up here a little"
      },
      {
        "start": 3419.799,
        "duration": 4.121,
        "text": "bit because we're at the top of the hour"
      },
      {
        "start": 3422.039,
        "duration": 5.601,
        "text": "things go fast I always enjoy my time"
      },
      {
        "start": 3423.92,
        "duration": 4.84,
        "text": "with you so uh we're gonna switch over"
      },
      {
        "start": 3427.64,
        "duration": 5.0,
        "text": "to my other"
      },
      {
        "start": 3428.76,
        "duration": 7.079,
        "text": "screen and all right so we've we did"
      },
      {
        "start": 3432.64,
        "duration": 5.159,
        "text": "that now the final thing that I'm going"
      },
      {
        "start": 3435.839,
        "duration": 6.321,
        "text": "to cover is this measuring impact and"
      },
      {
        "start": 3437.799,
        "duration": 5.961,
        "text": "the measurements are important so from"
      },
      {
        "start": 3442.16,
        "duration": 3.76,
        "text": "measurement standpoints the fundamentals"
      },
      {
        "start": 3443.76,
        "duration": 4.079,
        "text": "are still there you want to latency"
      },
      {
        "start": 3445.92,
        "duration": 3.439,
        "text": "through put errors you need to be"
      },
      {
        "start": 3447.839,
        "duration": 3.24,
        "text": "monitoring for that you need to be"
      },
      {
        "start": 3449.359,
        "duration": 4.081,
        "text": "looking at those things and those are"
      },
      {
        "start": 3451.079,
        "duration": 4.24,
        "text": "just fundamentals I mentioned that"
      },
      {
        "start": 3453.44,
        "duration": 5.76,
        "text": "because we're going to add something to"
      },
      {
        "start": 3455.319,
        "duration": 7.921,
        "text": "it and those new metrics are around"
      },
      {
        "start": 3459.2,
        "duration": 6.04,
        "text": "using uh say rag um nlm something like"
      },
      {
        "start": 3463.24,
        "duration": 3.839,
        "text": "that and those new ones are the"
      },
      {
        "start": 3465.24,
        "duration": 4.48,
        "text": "consistency like do you get the same"
      },
      {
        "start": 3467.079,
        "duration": 4.681,
        "text": "answer every time relevance is just the"
      },
      {
        "start": 3469.72,
        "duration": 3.879,
        "text": "answer that I was expecting this"
      },
      {
        "start": 3471.76,
        "duration": 3.2,
        "text": "Precision has to do with Rag and that's"
      },
      {
        "start": 3473.599,
        "duration": 4.561,
        "text": "actually really measurement when it"
      },
      {
        "start": 3474.96,
        "duration": 6.159,
        "text": "comes to using a vector databases um if"
      },
      {
        "start": 3478.16,
        "duration": 5.0,
        "text": "I ask for compaction documentation does"
      },
      {
        "start": 3481.119,
        "duration": 4.2,
        "text": "it also give me everything about SS"
      },
      {
        "start": 3483.16,
        "duration": 4.52,
        "text": "tables and bulk loader that's a lot of"
      },
      {
        "start": 3485.319,
        "duration": 3.841,
        "text": "noise I want just signal so how how"
      },
      {
        "start": 3487.68,
        "duration": 2.879,
        "text": "precise are those answers and a lot of"
      },
      {
        "start": 3489.16,
        "duration": 3.04,
        "text": "that comes into like how you do your"
      },
      {
        "start": 3490.559,
        "duration": 3.441,
        "text": "vectors that sort of thing but Precision"
      },
      {
        "start": 3492.2,
        "duration": 4.639,
        "text": "is important because it reduces the"
      },
      {
        "start": 3494.0,
        "duration": 6.92,
        "text": "amount of junk that it has to go through"
      },
      {
        "start": 3496.839,
        "duration": 5.52,
        "text": "um and then recall uh what is it"
      },
      {
        "start": 3500.92,
        "duration": 3.04,
        "text": "actually getting what I need if I say I"
      },
      {
        "start": 3502.359,
        "duration": 3.121,
        "text": "need something about compaction and it"
      },
      {
        "start": 3503.96,
        "duration": 3.32,
        "text": "gives me something about something else"
      },
      {
        "start": 3505.48,
        "duration": 3.96,
        "text": "like streaming I I don't I didn't want"
      },
      {
        "start": 3507.28,
        "duration": 4.759,
        "text": "that well that's the recall is poor but"
      },
      {
        "start": 3509.44,
        "duration": 4.399,
        "text": "those are things you can measure and the"
      },
      {
        "start": 3512.039,
        "duration": 4.481,
        "text": "measurement there's some cool really"
      },
      {
        "start": 3513.839,
        "duration": 5.96,
        "text": "interesting Cool Tools out there uh Lang"
      },
      {
        "start": 3516.52,
        "duration": 4.88,
        "text": "Lang chain uh the library of the company"
      },
      {
        "start": 3519.799,
        "duration": 4.841,
        "text": "has Lang Smith which actually has the"
      },
      {
        "start": 3521.4,
        "duration": 4.959,
        "text": "name of the company but they do tracing"
      },
      {
        "start": 3524.64,
        "duration": 3.56,
        "text": "and this is probably what you were"
      },
      {
        "start": 3526.359,
        "duration": 5.041,
        "text": "looking for in an Enterprise application"
      },
      {
        "start": 3528.2,
        "duration": 4.839,
        "text": "is is tracing what happened in that call"
      },
      {
        "start": 3531.4,
        "duration": 4.36,
        "text": "and there's a great Library out there"
      },
      {
        "start": 3533.039,
        "duration": 5.201,
        "text": "open source Library called ragas which"
      },
      {
        "start": 3535.76,
        "duration": 5.76,
        "text": "uh integrates with lsmith a few others"
      },
      {
        "start": 3538.24,
        "duration": 6.44,
        "text": "uh link fuse Phoenix rise and open layer"
      },
      {
        "start": 3541.52,
        "duration": 5.799,
        "text": "that it it does in the library does all"
      },
      {
        "start": 3544.68,
        "duration": 5.2,
        "text": "the measurements you needed to do and"
      },
      {
        "start": 3547.319,
        "duration": 3.601,
        "text": "that um that is what's going to save you"
      },
      {
        "start": 3549.88,
        "duration": 4.36,
        "text": "in the long"
      },
      {
        "start": 3550.92,
        "duration": 7.24,
        "text": "run now what's next so I'm coming up at"
      },
      {
        "start": 3554.24,
        "duration": 6.559,
        "text": "the top of the hour easy I need you to"
      },
      {
        "start": 3558.16,
        "duration": 6.399,
        "text": "give me your feedback so I have a little"
      },
      {
        "start": 3560.799,
        "duration": 7.56,
        "text": "content pull out there and uh this is"
      },
      {
        "start": 3564.559,
        "duration": 5.601,
        "text": "super simple um it's one thing uh it was"
      },
      {
        "start": 3568.359,
        "duration": 4.72,
        "text": "just here's a list of topics and if you"
      },
      {
        "start": 3570.16,
        "duration": 4.6,
        "text": "don't see it on there then tell me what"
      },
      {
        "start": 3573.079,
        "duration": 4.48,
        "text": "you want and I'm going to create a lot"
      },
      {
        "start": 3574.76,
        "duration": 4.52,
        "text": "of content and I want to help you so you"
      },
      {
        "start": 3577.559,
        "duration": 3.921,
        "text": "if you've seen my content before you"
      },
      {
        "start": 3579.28,
        "duration": 4.16,
        "text": "know that I love to dig into topics and"
      },
      {
        "start": 3581.48,
        "duration": 4.52,
        "text": "I would love to dig you know get deep"
      },
      {
        "start": 3583.44,
        "duration": 5.72,
        "text": "dive put on the scuba gear and go for it"
      },
      {
        "start": 3586.0,
        "duration": 6.079,
        "text": "um you hopefully um it's something that"
      },
      {
        "start": 3589.16,
        "duration": 6.12,
        "text": "you your need and that I can provide so"
      },
      {
        "start": 3592.079,
        "duration": 4.601,
        "text": "go for it get crazy and um there's"
      },
      {
        "start": 3595.28,
        "duration": 2.96,
        "text": "nothing on here there's no like"
      },
      {
        "start": 3596.68,
        "duration": 3.679,
        "text": "collecting email addresses or anything"
      },
      {
        "start": 3598.24,
        "duration": 3.96,
        "text": "like that so it's pretty Anonymous just"
      },
      {
        "start": 3600.359,
        "duration": 3.841,
        "text": "a bunch of checkboxes and then other if"
      },
      {
        "start": 3602.2,
        "duration": 3.2,
        "text": "there's something not on that list let"
      },
      {
        "start": 3604.2,
        "duration": 4.76,
        "text": "me"
      },
      {
        "start": 3605.4,
        "duration": 5.12,
        "text": "know and with that I think I'm good to"
      },
      {
        "start": 3608.96,
        "duration": 3.399,
        "text": "go I don't know if I have a lot of"
      },
      {
        "start": 3610.52,
        "duration": 3.4,
        "text": "questions out there probably not a ton"
      },
      {
        "start": 3612.359,
        "duration": 6.2,
        "text": "because I just poured information into"
      },
      {
        "start": 3613.92,
        "duration": 8.36,
        "text": "you but um you all know where to find me"
      },
      {
        "start": 3618.559,
        "duration": 6.24,
        "text": "um I'm on LinkedIn I love love inmail as"
      },
      {
        "start": 3622.28,
        "duration": 4.64,
        "text": "long as it's not a salesperson um use"
      },
      {
        "start": 3624.799,
        "duration": 5.0,
        "text": "that all the time on the SF slack I'm"
      },
      {
        "start": 3626.92,
        "duration": 5.76,
        "text": "there all the time and uh of course on"
      },
      {
        "start": 3629.799,
        "duration": 4.921,
        "text": "Twitter Patrick mcfaden and connect with"
      },
      {
        "start": 3632.68,
        "duration": 3.399,
        "text": "me on LinkedIn I would love for you to"
      },
      {
        "start": 3634.72,
        "duration": 4.079,
        "text": "let me know what you're doing and if you"
      },
      {
        "start": 3636.079,
        "duration": 4.401,
        "text": "have questions I would get on a zoom"
      },
      {
        "start": 3638.799,
        "duration": 3.24,
        "text": "call with you whatever you want I'm I'm"
      },
      {
        "start": 3640.48,
        "duration": 4.119,
        "text": "here to help and I want to learn more"
      },
      {
        "start": 3642.039,
        "duration": 4.481,
        "text": "about what you're up to so thank you"
      },
      {
        "start": 3644.599,
        "duration": 4.881,
        "text": "very much I appreciate your time here"
      },
      {
        "start": 3646.52,
        "duration": 5.12,
        "text": "with me today and take my PLL I'd love"
      },
      {
        "start": 3649.48,
        "duration": 4.2,
        "text": "to see what you're thinking all right"
      },
      {
        "start": 3651.64,
        "duration": 6.04,
        "text": "everyone have a great day and go make"
      },
      {
        "start": 3653.68,
        "duration": 4.0,
        "text": "something really really cool"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T17:17:29.504944+00:00"
}