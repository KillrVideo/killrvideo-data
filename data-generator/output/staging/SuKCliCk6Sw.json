{
  "video_id": "SuKCliCk6Sw",
  "title": "DS320.02 Introduction: Spark Architecture | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.02 Introduction: Spark Architecture\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:25:04Z",
  "thumbnail": "https://i.ytimg.com/vi/SuKCliCk6Sw/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "introduction",
    "architecture",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=SuKCliCk6Sw",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's take a quick look at the architecture of spark remember fundamentally we're trying to write a program that does some data analysis and that's in this diagram the thing we call the spark client that's a program that maybe we've written in java or scala or python those are the three most common languages that does the actual computation inside the spark client is a component we call the driver which is code that's interacting with the spark context now the spark context is an actual object in one of those languages for example if you're writing in scala or you're writing in java there's a class called spark context with a well-documented interface and you directly program that interface to create pieces of data to import data export data transform it and run computations on it and a lot of learning spark programming involves learning the apis in the spark context and the objects related to it so that driver which is potentially a complex piece of code has to get resources to do the computation it does that by connecting to a special node and spark cluster called the master the master is a fairly lightweight process clients what they do is they request a certain number of cores and a certain amount of memory to do some computation so the master will ask workers according to the number of cores and the amount of memory they have available to allocate executors once those workers have allocated executors the master is in a sense out of the picture those executors then communicate directly with the spark driver and the spark driver gives them their computational work to do importing data exporting transforming running some action or some aggregate computation over data all of that communication happens between driver and executor and when results are returned those go from executor back to the driver the master is not in the loop anymore at that point fundamental to spark computation is a directed acyclic graph of operations we just call that a dag dag for directed acyclic graph now this is again a thing we will dig into in more detail later but to think of it in simple terms you might imagine you start with some source data you perform one or two maybe filtering operations on it and then maybe compute an average of some value in that data you've already defined a sequence of operations where one depends on the next in a complex job that sequence and really that network of operations forms a more interesting graph spark computes that graph and is aware of that graph sometimes as a developer using spark you don't even need to be directly aware that the spark driver is computing that graph for you the driver will break that sequence of computations down into one or more jobs and each job will be composed of stages each stage will get implemented by one or more tasks and it's this task level that the driver finally decomposes the work down to where computation can happen those tasks are then handed off to those executors that we had allocated just a minute ago the work is done by those executors and the results can then be obtained later on by the driver when it needs them you can see how this architecture lends itself pretty well to horizontal scale things get decomposed into these relatively simple tasks from a potentially complex series of operations that we define as developers and those tasks then get parceled out to various nodes in the cluster how many nodes we don't know we could have a small number we could have a large number if we have a lot of work to do the architecture itself it doesn't really care how big the cluster is which is definitely an attribute that we'd like now we're going to assume that you already know the basics of cassandra if you don't there are resources elsewhere here on data stacks academy that you'll be able to learn the fundamentals of the architecture but remember it's a peer-to-peer database with fault tolerance baked in and there is the consistent hashing mechanism that you can see illustrated in the slide that cassandra uses to distribute data among nodes in the ring so all of the data ought to be in a properly designed cassandra schema evenly distributed among the nodes this picture shows three which is kind of a minimum viable cassandra cluster in some sense of course there could be dozens or even hundreds of nodes and all the principles are the same importantly you also have the ability for client applications to connect to arbitrary nodes we saw a minute ago how the spark client connects to the spark cluster in the case of client applications writing data to cassandra and reading it from cassandra they can connect to any node they want and the cluster itself maintains the distribution protocol when spark is running on cassandra as in the case of the dse analytics feature you can see that every node in the cassandra cluster also has the potential to function as a spark node one of them will be elected as the spark master the others will be spark workers now the way this works in practice is sometimes not every node in a cluster is given analytics duties sometimes that's partitioned we'll look later at how that happens but in principle every node is available to do that same work and while spark is electing a master because that's important to sparks architecture remember there's no masternode or no distinguished node of any kind in cassandra and using spark on top of cassandra doesn't change that fact since cassandra has multi-data center functionality baked into it we can use that to partition analytics workloads the actual tuning that the ops people are going to have to do on the servers themselves the ones that run the analytics workload the ones that run the transactional workload those happen to be different sometimes there are trade-offs that work at crossed purposes so you might want to optimize a certain set of nodes in your cluster to handle the transactional workload well and another set to handle the analytics workload well in cassandra we'll set up multiple data centers now they don't have to be actually geographically diverse data centers they can simply be a set of five nodes and a set of ten nodes that are all in the same rack but one of those data centers can be given spark responsibilities and the other purely cassandra data storage responsibilities cassandra will keep the two data centers consistent and they were able to optimize the analytics workload on the nodes that are running spark",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.879,
        "duration": 3.68,
        "text": "let's take a quick"
      },
      {
        "start": 7.759,
        "duration": 3.361,
        "text": "look at the architecture of spark"
      },
      {
        "start": 10.559,
        "duration": 2.08,
        "text": "remember"
      },
      {
        "start": 11.12,
        "duration": 3.84,
        "text": "fundamentally we're trying to write a"
      },
      {
        "start": 12.639,
        "duration": 4.001,
        "text": "program that does some data analysis"
      },
      {
        "start": 14.96,
        "duration": 3.2,
        "text": "and that's in this diagram the thing we"
      },
      {
        "start": 16.64,
        "duration": 2.88,
        "text": "call the spark client"
      },
      {
        "start": 18.16,
        "duration": 3.92,
        "text": "that's a program that maybe we've"
      },
      {
        "start": 19.52,
        "duration": 3.999,
        "text": "written in java or scala or python those"
      },
      {
        "start": 22.08,
        "duration": 4.24,
        "text": "are the three most common languages"
      },
      {
        "start": 23.519,
        "duration": 4.481,
        "text": "that does the actual computation inside"
      },
      {
        "start": 26.32,
        "duration": 2.64,
        "text": "the spark client is a component we call"
      },
      {
        "start": 28.0,
        "duration": 2.4,
        "text": "the driver"
      },
      {
        "start": 28.96,
        "duration": 3.2,
        "text": "which is code that's interacting with"
      },
      {
        "start": 30.4,
        "duration": 3.679,
        "text": "the spark context"
      },
      {
        "start": 32.16,
        "duration": 3.52,
        "text": "now the spark context is an actual"
      },
      {
        "start": 34.079,
        "duration": 3.121,
        "text": "object in one of those languages for"
      },
      {
        "start": 35.68,
        "duration": 3.199,
        "text": "example if you're writing in scala"
      },
      {
        "start": 37.2,
        "duration": 3.359,
        "text": "or you're writing in java there's a"
      },
      {
        "start": 38.879,
        "duration": 3.2,
        "text": "class called spark context with a"
      },
      {
        "start": 40.559,
        "duration": 4.32,
        "text": "well-documented interface"
      },
      {
        "start": 42.079,
        "duration": 3.841,
        "text": "and you directly program that interface"
      },
      {
        "start": 44.879,
        "duration": 3.041,
        "text": "to create"
      },
      {
        "start": 45.92,
        "duration": 3.52,
        "text": "pieces of data to import data export"
      },
      {
        "start": 47.92,
        "duration": 4.0,
        "text": "data transform it"
      },
      {
        "start": 49.44,
        "duration": 3.279,
        "text": "and run computations on it and a lot of"
      },
      {
        "start": 51.92,
        "duration": 3.76,
        "text": "learning"
      },
      {
        "start": 52.719,
        "duration": 4.881,
        "text": "spark programming involves learning the"
      },
      {
        "start": 55.68,
        "duration": 3.76,
        "text": "apis in the spark context and the"
      },
      {
        "start": 57.6,
        "duration": 3.52,
        "text": "objects related to it so that driver"
      },
      {
        "start": 59.44,
        "duration": 2.32,
        "text": "which is potentially a complex piece of"
      },
      {
        "start": 61.12,
        "duration": 2.079,
        "text": "code"
      },
      {
        "start": 61.76,
        "duration": 3.52,
        "text": "has to get resources to do the"
      },
      {
        "start": 63.199,
        "duration": 3.681,
        "text": "computation it does that by connecting"
      },
      {
        "start": 65.28,
        "duration": 2.48,
        "text": "to a special node and spark cluster"
      },
      {
        "start": 66.88,
        "duration": 2.8,
        "text": "called the master"
      },
      {
        "start": 67.76,
        "duration": 3.6,
        "text": "the master is a fairly lightweight"
      },
      {
        "start": 69.68,
        "duration": 3.92,
        "text": "process clients what they do"
      },
      {
        "start": 71.36,
        "duration": 4.399,
        "text": "is they request a certain number of"
      },
      {
        "start": 73.6,
        "duration": 4.32,
        "text": "cores and a certain amount of memory"
      },
      {
        "start": 75.759,
        "duration": 4.321,
        "text": "to do some computation so the master"
      },
      {
        "start": 77.92,
        "duration": 3.68,
        "text": "will ask workers according to the number"
      },
      {
        "start": 80.08,
        "duration": 3.76,
        "text": "of cores and the amount of memory"
      },
      {
        "start": 81.6,
        "duration": 3.36,
        "text": "they have available to allocate"
      },
      {
        "start": 83.84,
        "duration": 2.8,
        "text": "executors"
      },
      {
        "start": 84.96,
        "duration": 3.519,
        "text": "once those workers have allocated"
      },
      {
        "start": 86.64,
        "duration": 2.64,
        "text": "executors the master is in a sense out"
      },
      {
        "start": 88.479,
        "duration": 2.721,
        "text": "of the picture"
      },
      {
        "start": 89.28,
        "duration": 3.44,
        "text": "those executors then communicate"
      },
      {
        "start": 91.2,
        "duration": 3.919,
        "text": "directly with"
      },
      {
        "start": 92.72,
        "duration": 3.28,
        "text": "the spark driver and the spark driver"
      },
      {
        "start": 95.119,
        "duration": 3.201,
        "text": "gives them"
      },
      {
        "start": 96.0,
        "duration": 4.56,
        "text": "their computational work to do importing"
      },
      {
        "start": 98.32,
        "duration": 3.92,
        "text": "data exporting transforming"
      },
      {
        "start": 100.56,
        "duration": 3.36,
        "text": "running some action or some aggregate"
      },
      {
        "start": 102.24,
        "duration": 3.12,
        "text": "computation over data"
      },
      {
        "start": 103.92,
        "duration": 3.519,
        "text": "all of that communication happens"
      },
      {
        "start": 105.36,
        "duration": 5.28,
        "text": "between driver and"
      },
      {
        "start": 107.439,
        "duration": 5.921,
        "text": "executor and when results are returned"
      },
      {
        "start": 110.64,
        "duration": 4.08,
        "text": "those go from executor back to the"
      },
      {
        "start": 113.36,
        "duration": 4.079,
        "text": "driver the master is"
      },
      {
        "start": 114.72,
        "duration": 3.759,
        "text": "not in the loop anymore at that point"
      },
      {
        "start": 117.439,
        "duration": 3.68,
        "text": "fundamental to"
      },
      {
        "start": 118.479,
        "duration": 4.721,
        "text": "spark computation is a directed acyclic"
      },
      {
        "start": 121.119,
        "duration": 2.561,
        "text": "graph of operations we just call that a"
      },
      {
        "start": 123.2,
        "duration": 2.879,
        "text": "dag"
      },
      {
        "start": 123.68,
        "duration": 4.079,
        "text": "dag for directed acyclic graph now this"
      },
      {
        "start": 126.079,
        "duration": 1.921,
        "text": "is again a thing we will dig into in"
      },
      {
        "start": 127.759,
        "duration": 2.161,
        "text": "more"
      },
      {
        "start": 128.0,
        "duration": 3.44,
        "text": "detail later but to think of it in"
      },
      {
        "start": 129.92,
        "duration": 3.44,
        "text": "simple terms you might imagine"
      },
      {
        "start": 131.44,
        "duration": 3.84,
        "text": "you start with some source data you"
      },
      {
        "start": 133.36,
        "duration": 3.12,
        "text": "perform one or two maybe filtering"
      },
      {
        "start": 135.28,
        "duration": 3.44,
        "text": "operations on it"
      },
      {
        "start": 136.48,
        "duration": 3.839,
        "text": "and then maybe compute an average of"
      },
      {
        "start": 138.72,
        "duration": 3.2,
        "text": "some value in that data"
      },
      {
        "start": 140.319,
        "duration": 4.081,
        "text": "you've already defined a sequence of"
      },
      {
        "start": 141.92,
        "duration": 4.56,
        "text": "operations where one depends on the next"
      },
      {
        "start": 144.4,
        "duration": 4.24,
        "text": "in a complex job that sequence and"
      },
      {
        "start": 146.48,
        "duration": 3.28,
        "text": "really that network of operations forms"
      },
      {
        "start": 148.64,
        "duration": 4.08,
        "text": "a more interesting"
      },
      {
        "start": 149.76,
        "duration": 3.92,
        "text": "graph spark computes that graph and is"
      },
      {
        "start": 152.72,
        "duration": 2.96,
        "text": "aware of that graph"
      },
      {
        "start": 153.68,
        "duration": 4.4,
        "text": "sometimes as a developer using spark you"
      },
      {
        "start": 155.68,
        "duration": 4.639,
        "text": "don't even need to be directly aware"
      },
      {
        "start": 158.08,
        "duration": 4.0,
        "text": "that the spark driver is computing that"
      },
      {
        "start": 160.319,
        "duration": 3.681,
        "text": "graph for you the driver will break that"
      },
      {
        "start": 162.08,
        "duration": 2.4,
        "text": "sequence of computations down into one"
      },
      {
        "start": 164.0,
        "duration": 3.599,
        "text": "or more"
      },
      {
        "start": 164.48,
        "duration": 6.96,
        "text": "jobs and each job will be composed of"
      },
      {
        "start": 167.599,
        "duration": 6.64,
        "text": "stages each stage will get implemented"
      },
      {
        "start": 171.44,
        "duration": 4.56,
        "text": "by one or more tasks and it's this task"
      },
      {
        "start": 174.239,
        "duration": 2.72,
        "text": "level that the driver finally decomposes"
      },
      {
        "start": 176.0,
        "duration": 3.76,
        "text": "the work down to"
      },
      {
        "start": 176.959,
        "duration": 3.841,
        "text": "where computation can happen those tasks"
      },
      {
        "start": 179.76,
        "duration": 3.119,
        "text": "are then handed off"
      },
      {
        "start": 180.8,
        "duration": 3.2,
        "text": "to those executors that we had allocated"
      },
      {
        "start": 182.879,
        "duration": 4.561,
        "text": "just a minute ago"
      },
      {
        "start": 184.0,
        "duration": 5.599,
        "text": "the work is done by those executors"
      },
      {
        "start": 187.44,
        "duration": 3.04,
        "text": "and the results can then be obtained"
      },
      {
        "start": 189.599,
        "duration": 2.72,
        "text": "later on"
      },
      {
        "start": 190.48,
        "duration": 3.28,
        "text": "by the driver when it needs them you can"
      },
      {
        "start": 192.319,
        "duration": 3.441,
        "text": "see how this architecture lends itself"
      },
      {
        "start": 193.76,
        "duration": 3.36,
        "text": "pretty well to horizontal scale"
      },
      {
        "start": 195.76,
        "duration": 3.199,
        "text": "things get decomposed into these"
      },
      {
        "start": 197.12,
        "duration": 3.839,
        "text": "relatively simple tasks"
      },
      {
        "start": 198.959,
        "duration": 4.321,
        "text": "from a potentially complex series of"
      },
      {
        "start": 200.959,
        "duration": 3.36,
        "text": "operations that we define as developers"
      },
      {
        "start": 203.28,
        "duration": 3.679,
        "text": "and those tasks"
      },
      {
        "start": 204.319,
        "duration": 4.401,
        "text": "then get parceled out to various nodes"
      },
      {
        "start": 206.959,
        "duration": 2.081,
        "text": "in the cluster how many nodes we don't"
      },
      {
        "start": 208.72,
        "duration": 1.519,
        "text": "know"
      },
      {
        "start": 209.04,
        "duration": 2.64,
        "text": "we could have a small number we could"
      },
      {
        "start": 210.239,
        "duration": 3.441,
        "text": "have a large number if we have a lot of"
      },
      {
        "start": 211.68,
        "duration": 3.839,
        "text": "work to do the architecture itself"
      },
      {
        "start": 213.68,
        "duration": 3.119,
        "text": "it doesn't really care how big the"
      },
      {
        "start": 215.519,
        "duration": 2.401,
        "text": "cluster is which is definitely an"
      },
      {
        "start": 216.799,
        "duration": 2.72,
        "text": "attribute that we'd like"
      },
      {
        "start": 217.92,
        "duration": 3.84,
        "text": "now we're going to assume that you"
      },
      {
        "start": 219.519,
        "duration": 3.681,
        "text": "already know the basics of cassandra if"
      },
      {
        "start": 221.76,
        "duration": 4.16,
        "text": "you don't there are resources"
      },
      {
        "start": 223.2,
        "duration": 3.759,
        "text": "elsewhere here on data stacks academy"
      },
      {
        "start": 225.92,
        "duration": 2.399,
        "text": "that you'll be able to learn the"
      },
      {
        "start": 226.959,
        "duration": 2.081,
        "text": "fundamentals of the architecture but"
      },
      {
        "start": 228.319,
        "duration": 2.961,
        "text": "remember"
      },
      {
        "start": 229.04,
        "duration": 3.759,
        "text": "it's a peer-to-peer database with fault"
      },
      {
        "start": 231.28,
        "duration": 3.36,
        "text": "tolerance baked in"
      },
      {
        "start": 232.799,
        "duration": 3.681,
        "text": "and there is the consistent hashing"
      },
      {
        "start": 234.64,
        "duration": 2.799,
        "text": "mechanism that you can see illustrated"
      },
      {
        "start": 236.48,
        "duration": 3.839,
        "text": "in the slide"
      },
      {
        "start": 237.439,
        "duration": 4.641,
        "text": "that cassandra uses to distribute data"
      },
      {
        "start": 240.319,
        "duration": 3.761,
        "text": "among nodes in the ring"
      },
      {
        "start": 242.08,
        "duration": 3.68,
        "text": "so all of the data ought to be in a"
      },
      {
        "start": 244.08,
        "duration": 4.56,
        "text": "properly designed cassandra schema"
      },
      {
        "start": 245.76,
        "duration": 3.679,
        "text": "evenly distributed among the nodes this"
      },
      {
        "start": 248.64,
        "duration": 3.519,
        "text": "picture shows"
      },
      {
        "start": 249.439,
        "duration": 3.921,
        "text": "three which is kind of a minimum viable"
      },
      {
        "start": 252.159,
        "duration": 2.8,
        "text": "cassandra cluster"
      },
      {
        "start": 253.36,
        "duration": 3.76,
        "text": "in some sense of course there could be"
      },
      {
        "start": 254.959,
        "duration": 3.84,
        "text": "dozens or even hundreds of nodes"
      },
      {
        "start": 257.12,
        "duration": 3.679,
        "text": "and all the principles are the same"
      },
      {
        "start": 258.799,
        "duration": 2.56,
        "text": "importantly you also have the ability"
      },
      {
        "start": 260.799,
        "duration": 2.561,
        "text": "for"
      },
      {
        "start": 261.359,
        "duration": 3.441,
        "text": "client applications to connect to"
      },
      {
        "start": 263.36,
        "duration": 3.44,
        "text": "arbitrary nodes"
      },
      {
        "start": 264.8,
        "duration": 3.36,
        "text": "we saw a minute ago how the spark client"
      },
      {
        "start": 266.8,
        "duration": 3.28,
        "text": "connects to the spark cluster"
      },
      {
        "start": 268.16,
        "duration": 4.0,
        "text": "in the case of client applications"
      },
      {
        "start": 270.08,
        "duration": 3.04,
        "text": "writing data to cassandra and reading it"
      },
      {
        "start": 272.16,
        "duration": 2.64,
        "text": "from cassandra"
      },
      {
        "start": 273.12,
        "duration": 3.04,
        "text": "they can connect to any node they want"
      },
      {
        "start": 274.8,
        "duration": 3.76,
        "text": "and the cluster itself"
      },
      {
        "start": 276.16,
        "duration": 3.52,
        "text": "maintains the distribution protocol when"
      },
      {
        "start": 278.56,
        "duration": 3.68,
        "text": "spark is running"
      },
      {
        "start": 279.68,
        "duration": 3.6,
        "text": "on cassandra as in the case of the dse"
      },
      {
        "start": 282.24,
        "duration": 2.56,
        "text": "analytics feature"
      },
      {
        "start": 283.28,
        "duration": 4.0,
        "text": "you can see that every node in the"
      },
      {
        "start": 284.8,
        "duration": 3.119,
        "text": "cassandra cluster also has the potential"
      },
      {
        "start": 287.28,
        "duration": 3.12,
        "text": "to function"
      },
      {
        "start": 287.919,
        "duration": 3.921,
        "text": "as a spark node one of them will be"
      },
      {
        "start": 290.4,
        "duration": 4.4,
        "text": "elected as the spark"
      },
      {
        "start": 291.84,
        "duration": 4.72,
        "text": "master the others will be spark workers"
      },
      {
        "start": 294.8,
        "duration": 2.48,
        "text": "now the way this works in practice is"
      },
      {
        "start": 296.56,
        "duration": 3.12,
        "text": "sometimes"
      },
      {
        "start": 297.28,
        "duration": 3.84,
        "text": "not every node in a cluster is given"
      },
      {
        "start": 299.68,
        "duration": 2.88,
        "text": "analytics duties sometimes that's"
      },
      {
        "start": 301.12,
        "duration": 3.359,
        "text": "partitioned we'll look later"
      },
      {
        "start": 302.56,
        "duration": 4.24,
        "text": "at how that happens but in principle"
      },
      {
        "start": 304.479,
        "duration": 2.801,
        "text": "every node is available to do that same"
      },
      {
        "start": 306.8,
        "duration": 2.64,
        "text": "work"
      },
      {
        "start": 307.28,
        "duration": 3.44,
        "text": "and while spark is electing a master"
      },
      {
        "start": 309.44,
        "duration": 2.08,
        "text": "because that's important to sparks"
      },
      {
        "start": 310.72,
        "duration": 2.96,
        "text": "architecture"
      },
      {
        "start": 311.52,
        "duration": 3.76,
        "text": "remember there's no masternode or no"
      },
      {
        "start": 313.68,
        "duration": 3.84,
        "text": "distinguished node of any kind"
      },
      {
        "start": 315.28,
        "duration": 3.04,
        "text": "in cassandra and using spark on top of"
      },
      {
        "start": 317.52,
        "duration": 2.88,
        "text": "cassandra"
      },
      {
        "start": 318.32,
        "duration": 3.599,
        "text": "doesn't change that fact since cassandra"
      },
      {
        "start": 320.4,
        "duration": 3.519,
        "text": "has multi-data center"
      },
      {
        "start": 321.919,
        "duration": 4.081,
        "text": "functionality baked into it we can use"
      },
      {
        "start": 323.919,
        "duration": 3.921,
        "text": "that to partition analytics workloads"
      },
      {
        "start": 326.0,
        "duration": 3.039,
        "text": "the actual tuning that the ops people"
      },
      {
        "start": 327.84,
        "duration": 3.12,
        "text": "are going to have to do"
      },
      {
        "start": 329.039,
        "duration": 4.321,
        "text": "on the servers themselves the ones that"
      },
      {
        "start": 330.96,
        "duration": 4.799,
        "text": "run the analytics workload the ones that"
      },
      {
        "start": 333.36,
        "duration": 3.92,
        "text": "run the transactional workload those"
      },
      {
        "start": 335.759,
        "duration": 2.401,
        "text": "happen to be different sometimes there"
      },
      {
        "start": 337.28,
        "duration": 2.72,
        "text": "are trade-offs"
      },
      {
        "start": 338.16,
        "duration": 3.12,
        "text": "that work at crossed purposes so you"
      },
      {
        "start": 340.0,
        "duration": 3.039,
        "text": "might want to optimize"
      },
      {
        "start": 341.28,
        "duration": 3.199,
        "text": "a certain set of nodes in your cluster"
      },
      {
        "start": 343.039,
        "duration": 2.561,
        "text": "to handle the transactional workload"
      },
      {
        "start": 344.479,
        "duration": 3.121,
        "text": "well and another"
      },
      {
        "start": 345.6,
        "duration": 3.52,
        "text": "set to handle the analytics workload"
      },
      {
        "start": 347.6,
        "duration": 3.36,
        "text": "well in cassandra"
      },
      {
        "start": 349.12,
        "duration": 3.04,
        "text": "we'll set up multiple data centers now"
      },
      {
        "start": 350.96,
        "duration": 3.12,
        "text": "they don't have to be"
      },
      {
        "start": 352.16,
        "duration": 3.599,
        "text": "actually geographically diverse data"
      },
      {
        "start": 354.08,
        "duration": 3.52,
        "text": "centers they can simply be"
      },
      {
        "start": 355.759,
        "duration": 3.521,
        "text": "a set of five nodes and a set of ten"
      },
      {
        "start": 357.6,
        "duration": 3.28,
        "text": "nodes that are all in the same rack"
      },
      {
        "start": 359.28,
        "duration": 3.84,
        "text": "but one of those data centers can be"
      },
      {
        "start": 360.88,
        "duration": 2.64,
        "text": "given spark responsibilities and the"
      },
      {
        "start": 363.12,
        "duration": 2.56,
        "text": "other"
      },
      {
        "start": 363.52,
        "duration": 4.16,
        "text": "purely cassandra data storage"
      },
      {
        "start": 365.68,
        "duration": 3.359,
        "text": "responsibilities cassandra will keep the"
      },
      {
        "start": 367.68,
        "duration": 2.639,
        "text": "two data centers consistent"
      },
      {
        "start": 369.039,
        "duration": 3.041,
        "text": "and they were able to optimize the"
      },
      {
        "start": 370.319,
        "duration": 11.521,
        "text": "analytics workload on"
      },
      {
        "start": 372.08,
        "duration": 9.76,
        "text": "the nodes that are running spark"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:56:40.727332+00:00"
}