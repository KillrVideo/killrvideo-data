{
  "video_id": "TCVPxQ2sHC0",
  "title": "Using Docker and Apache Cassandraâ„¢ at TARGET",
  "description": "Hear from Tej Malepati as he discusses how Target use Docker and Cassandra in their stack.",
  "published_at": "2020-11-04T00:45:27Z",
  "thumbnail": "https://i.ytimg.com/vi/TCVPxQ2sHC0/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "apache_cassandra"
  ],
  "url": "https://www.youtube.com/watch?v=TCVPxQ2sHC0",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "try to get someone in the call here real quick who might have the ability to log in perfect all right they can hear us so give me one moment I'll do the intro [Music] [Music] all right well welcome everyone to our first Global Cassandra Meetup and I just realized I've got echko on my end because I totally forgot to mute myself all right there we go sorry for the late start folks we had a couple technical difficulties but we are here now so um I am joined today by T who is going to do a a really cool presentation um as you might have noticed from the the logo let's see right up there um he works for Target Target is a very big open source Cassandra user um they've been longtime supporters of the community they've just been a really really uh good part of the cassander community as well especially up here in Minnesota where I believe we're both located so um with that uh take it away awesome thanks ER so yeah a couple of years back like um when uh this getting started uh when kasanda was open source to the Apache foundation and when it was announced to public many people were skeptical about using it because it's a kind of new database and all so uh eventually like U many um many Industries started using it as a kind of like a backend for uh or the cash or anything then uh like last year when uh when everyone was moving to conization uh uh the databases generally don't go with the conization well because the data uh storage volume point it's a little difficult when the container moves around actually so yeah uh so with this like let's get started so uh I'm Tage malati I work as a Cassandra leader Target it just been 5 years and 1 month uh at Target actually it's pretty nice and uh uh it's pretty cool to work on the new technologies and all that stuff so yeah I co-authored uh two books one is mastering of Pache kasan 3x with Aron FX and Advance my SQL 8 So currently I'm puring my MBA as well at University of Minnesota where I'm currently located and I did my M's uh in computer science from University of New Mexico so that's a brief introduction about me so Target like uh kasanda Target um a Target like uh it was one of the kind of like early adopters of any kind of new technology uh they explore uh not only any kind of new database technology but even the other uh other set of Technologies as well so it was one of the early adopters for migrating from bulky monolith applications to uh microservice architecture because it's a cloud native like we would like to go ahead and that's the future which we're thinking of all going so so when we started exploring um multiple no equal databases which are more Cloud native and kind of distributed because uh any kind of retail industry generally requires uh um High availability so let's say uh you try to order something and uh uh you get back to know that like uh the order is not placed so that's really bad right so so we need something like which is highly available so that's why we started with kasanda and all so kazand is a kind of like a very highly available kind of database with B on the C the so uh with these ones like when we started kasanda uh Journey uh we saw actually like a kind of a two full growth over a period of like 1 to two years uh overall period so uh it it was piing up pretty fast actually so and uh we were getting multiple use cases where it has to be something on perm and some has to be on public cloud and some have to be shuffled between the two uh domains as well or even the third domain when it comes up actually so uh then uh um once uh a company tries to move from one cloud provider to another cloud provider it's easy for applications to just move around because they just uh uh cut the traffic uh um stop the microservices and then move to the other ones but for uh databases it's very hard because you have to join to the part of the cluster then expand the data make sure the data is in sync and then start decommissioning process and the data validation takes really really really long time and if it's a kind of like um from relational database to non- relational databases if you're going ahead with it then definitely it's going to be pain so yeah uh uh Target started adopting in early 2014 um with the SSL part as well and recently we have done the even the stocks part because we have seen even Finance applications coming down uh from their uh my SQL background to kand so yeah so we are proudly uh uh announc that like okay we all our clusters what we manage at Target are on 314 version which is the latest version as of now so 1240 oh yeah 3.4 yeah next is for. for sure so yeah so uh when we start with the complexity to manage um we had close to uh 3 years or four uh 2 years back like it was close close to 30 plus different versions across several clusters and we had like 4,000 plus nodes across all the Clusters actually and uh 30 plus different ways of implementation because uh when we're trying to migrate or anything uh we was looking at like what is compatible to that particular domain where with the kand DAT is getting deployed and then accordingly we used to manage it so it was very difficult to manage like uh if a new uh use cases coming up what kind of implementation we have to use and the best practice on one cluster what we use and propag into other set of clusters was becoming very very hard and uh you everyone knows that uh infra as a service or platform as a service if you have multiple compute layers it would be tough to manage as well and there were like um close to uh 50% just 45% of clusters are actually on SSL and 20% on socks and the rest are actually on non socks and non SS so let's start with the complexity like how Okay uh with a simple sandbox environment you might just have a one data center in a cluster with three nodes so doing any kind of like um rolling restarts or config updates or bumping up the CPU or bumping up the max Heap size uh during your test process or anything so it's easy like just Ren know right you can just log in and then do it so for example Let's do let's scale it out now so same single data center but you have like four times the number of nodes which is like 12 nodes still it is okay but like it's a little little time taking now when we go down to uh two data centers with 12 nodes in each data center it's going to be a little more complicated so when I said like 100 plus clusters we manage it Target so there are few clusters which are like even 300 400 nodes as well within a cluster it spread across like six data centers or eight data centers also so you have like a couple of clusters which are eight data centers because of data availability and like different use cases we have it actually so so it's very complex like when you have to update it across all the uh nodes in a particular cluster with the corresponding configuration and let's say like you bumped up the uh Heap size on one cluster uh but like the same value is being used for the other cluster which is not supposed to be used it's very complicated so yeah so because of f like when everyone is moving to canonization as apps are trying to move and we have seen we have seen couple of uh even other uh teams are trying to leverage like databases on Docker and uh for the PC purposes they were pretty much fine with it but for production purposes it was very tough to manage so uh we are Target like uh we started leveraging this effort like uh last year so we Tred to figure out like what are the technical difficulties we would have it and all that stuff so uh once we try to figure out all that the main blockers were actually kind of like a SSL part like how we got like we have to how we can implement it actually so because Docker is just a kind of like automated like where you can directly whatever is available you use it and pick it up and use it that's all so uh but like for example like why we need the SSL part so you know that like Target is a kind of like a publicly traded company so we do maintain um Pi related information on actually so either business related information also so which is little confidential and we don't want to uh be kind of SEC complaint and all so it's just a kind of like additional layer over the authentication so that it gives better confidence for the application teams to go ahead with it so that was the reason mainly we went ahead with the SSL part integrated to kasanda where open source or even data stack also has provided so now when we come down to SSL uh there is two different approaches one is manual and the automated with the manual approach you have to generate on a particular node or like you can gener it on another node and then SCP to the corresponding with all the public Sears and uh the private key the key store and Trust stores and make sure all the public Sears of the nodes are actually imported into the trust St and all the nodes do have maintain that and then restart the cand so let's say you have a 400 node cluster you have to do this approach is it going to be is it going to be easy no I don't think so we went ahead with uh like automated approach where we have a like a um automatic CT API like where you just make an API call get back toer and then uh you can just directly convert it into the format which database requires it and then proceed with the restart that can be done like that's a little bit at least the Lesser manual interation which is so we developed a kind of chickie which is written in go actually so uh we use this chiot actually at multiple no SQL Technologies not only for kasanda we use it for elastic we use for reders we use it for um even mongod as well so so based on different use cases in the technology the key stor and tros have to be in different jks formats and also so with those uh corresponding formats like this is kind of having the capability of uh passing on which kind of format you needed and then automatically generate those ones and then validate it and then automatically it will be able to renew it as well when it's getting expired you can put a kind of threshold that when it's getting expired 30 days or 45 days or something you can definitely renew it automatically uh and it would have like even the metrics as well which is when it's going to expired and all so it's a pretty cool like uh uh custom build application for for like uh the SSL part where we generate it and push it out so and it's like uh agnostic to any kind of os so we can use it on um uh Linux we can use it on this one uh Windows as well anything so the second blocker was related to socks related applications so Ser socks act like why why we need socks actually so there were certain use cases where we have to handle Finance related information which was enforcing some kind of uh restrictions of auditing purposes or only um uh only a certain kind of like a integration to ldap for authentication purposes rather than kanda's uh authentication mechanism and all so which was little typical which is the requirement for security complant onto the socks because when we have when when we do have like external audits for sure like it's going to be little complicated if it's not going to be met and also so for the purpose like uh we started exploring what else can be integrated to our the cic department or anything so uh the first thing which we wanted is like we have to change the authentication um authenticator so we have we used the insta cluster s authenticated with small code changes related to uh our corresponding U uh L app server and the corresponding authentication mechanism and how it can be and then we have to update data kasanda emsh then the next part which is related to the security complaint is related to auditing purpose so currently like U um kasanda Ro manager would not be able to understand this elap mechanism so we have to use the kind of like another authorizer mechanism or Ro manager as well according to it so uh we would be picking up the role manager we use the ens uhle manager where uh we can pass it on again with the same similar by. and then proceed further so with this way like we can generate even the audit log like uh from where the IP is getting generated and like which which username they're trying to connect and what kind of query they're running and all that stuff so it's kind of tracing with more add features which are related to security purposes so uh other Integrations related to socks are basically like uh schema changes which we use good for maintaining our tracking purposes so any changes for socks mainly has to go through that process so that anything which breaks we know that like where it has broken or anything that's so and uh when we have socks for sure like it's not it shouldn't be directly added by some kind of common key which is shared across the team which can they can log or anything so it has to be integrated to uh kind of like a individual person who is locking in with all that mechanisms so we use sssd to allow the SSH via uh ad group kind of a protocol so and then finally uh the main uh like once we fixed all these ones uh we were actually uh this is a little funny because Linux was uh very conscious about like the case uh when we're entering the kind of username and password Windows doesn't so we going we use Linux for deploying ourand clusters so if your L app server maintains a username in a upper case or lower case or some kind of difference then for sure I'm not going to get through so that way like uh uh we have to be very careful about like when we are integrating to ldap related stuff on to socks now let's say like we have 400 nodes in a cluster and we are trying to uh do security patches and on uh uh the public clouds generally uh we have a limitation of like we can't just pull the Y repos and all so it's a kind of static image which we use right so on so here uh when we have to do kind of redep or anything it was very hard for us because when uh like for upgrading even the jbm version also because we were not able to update it uh in um um on the same node the reason is because it uses a static image and we not be able to accessible any of the external repositories or anything like that so we have to go ahead with some kind of approach where we have to add noes syn the data and then decommission old noes that was think close to 3 to four months actually but it's very hard like and we can't just uh leave the second vulnerabilities as well right so we we use the last pickles the same the data tax related block uh we try to map the old device like and we try to detach the dis from the old device I mean old node and then trigger the shutdown process from the new node and then detach the desk attach it to the new node which has a latest security vulnerabilities and then uh install KAS and then start it back and then uh like it would uh the con folder would be ritten because it even though the data is actually part of uh Olden node when it uh takes a metadata of it and make sure that the IP address is updated then automatically it comes back up normally I have a demo as well you'll be seeing it soon and uh uh it would validate the old id to be gone and then it would join to the cluster this was this reduced our effort from 3 to 4 months to just hardly like a day or 2 days so across a 4,000 nodes we generally do maybe twice or thce a year for this kind of ruling redeploy so it's uh we are pretty much competent to kind of SEC and that stuff so here like we have a um uh two data center cluster with three nodes in each data center one is in Central and one is in East so here you can see I'm doing the cluster redeploys on a central region so so uh you can see that 1049 81 238 has gone down so when it goes down uh as it is only we are doing one node at a time uh you wouldn't have any impact of business because they would be using local kodum unless and until if there are any applications which use all that time we do it generally during off business hour and make sure that uh none of the like all kind of consisten is running because there there would be errors and all so what's happening now behind the scenes is the new node is uh coming up and uh the new node has shut down this particular 1049 81 238 in F A of central it's going down gracefully shutting down all the services on that particular node and then it would detach the disk and then attach it to the new node and then it would automatically come back up so if you just give uh yeah you can see f is back up again on new node which is like 1049 80 and 131 so it would have the same data and then join to the cluster with a new IP and all the rest of the uh cluster members also use that IP and make sure that they remove uh from the gossip of the older IP and then add to the new IP so slowly we can do this uh approach for all the ones now the B1 has called on your central B1 which is like 1049 81 and 240 so this IP has gone down so the same proc process is being repeated so uh generally at Target we maintain our uh regions and like replication would be spread across multiple AES by default so uh even though when there are any region failovers they can cut over to the other region but if there are any uh easy failovers sometimes it does happen I think in the last 3 to four years I think it has happened only once but like uh none of our applications had any issues actually because they were using local Kum and even though one a is not available still it would be able to succeed with all the wrs or reads or anything and uh uh it wasn't for a long time it came back within 3 to 5 minutes and then like uh it would have even though there are any rights happening it would automatically May divert the hints to the corresponding node and it should be back up so you can see that b is already back up now so like this we have like when we have a 400 node cluster uh we can do the entire 400 nodes within maybe 3 to 8 hours before it was taking like 1 month or 1 and a half month which reduced our uh effort from that amount of time to only like U 3 to uh 6 hours or 8 hours it's all that was really nice where uh where we were able to get this one but like uh here the interesting thing is uh when I say like 100 plus clust 100 plus clusters are not of the same kind of a configuration so if you you are like three node clusters few are 10 uh um like 30 node few are like 100 if few are like 200 few are 350 or anything like that but when we talk about like uh we have SSL we have nons socks and non socks as well and it's spread across multiple domains we use only a single Docker image we thought of using the community one but we had a lot of customization so we started from scratch so we use a kind of like a Alpine based uh Docker image with which it would have so you can see that c is also backup So within 3 to 5 minutes all the three nodes have been um migrated to new set of noes with which has the kind of new either new jbm version or even the security vulnerabilities in this approach it was very easy for us to do the redeploys and we trying to compete with application teams for even kind of C Level DLo how it can be done easily so so yeah and the uh one thing when we come down to Docker is um look uh let's say we try to deploy kasan to kubernetes through Helm or anything and then on the back end uh you have a storage component which goes back and forth and all so over here you would not be able to log into the P or log into the Container how do you actually get some information or kind of like Ops activities or run some certain set of commands on on a particular note to get some information so uh again this is a kind of like a custom buildt for our team which is written again in go uh zanu so uh one of our TLP and the chickie was written by Rebecca sin and she was a pretty good engineer and zanadu was written by two of our T so it was like uh everyone was picking up the kind of new uh innovative ideas and all that stuff so so this one has the capability of getting the commands or executing some certain set of commands or even just reading out some like for example kandl and kasanda. so these two are the pretty much main important configuration files we use it on kasanda uh for altering anything because our log logb back. XML is almost pretty much same across all the Clusters so that uh it would be integrated part of the container so it shouldn't have any issues so these three ones if we have you update any of the uh hip then uh the uh these locations we generally use it on and we use even zanor even for healing system of theas sometimes let's say there were Network lws for the uh and your network package are lost and when that's the time when you are updating a password or anything then automatically uh the actual nodes which are having it there might be the system authoriz so uh it does Auto heal it as well as when it tries to check it and then automatically clear it off and it uses a kind of a uh regular mechanism of client token and password or yeah it can be integrated to even ldap so there bunch of options which you can use authentication mechanism so just to give a overall performances what we have done it with Docker is actually like it was amazing actually so we have seen uh small better performance on Docker that we can't explain because the underlying compute and the back end was completely constantly changing evolving uh because those teams were also evolving at all so we had like a 300K reads per second on just the 30 node or 39 node plus uh 39 node data center it was able to handle uh 300K reads per second and the same thing was able to handle 150 K rights per second so and we generally all our clusters are actually multi-data Center so even though one data center is off due to uh infra failure or something then automatically they can switch it over and they have that capability already and the latencies for the similar for the same cluster which had this this amount of reads and rights were just like 1 millisecond to 300 milliseconds on an average basing on like what's the load would change and the right latencies were similar to 1 to 100 milliseconds we know that kasanda is actually a uh uh easy right kind of a database and reads a little costly and all uh it was almost uh for as it was competing and uh we try to make sure we try to properly scale it and not overscale it for getting it done so last like um the use cases what we use for kasanda tablet are basically we use it for pricing like uh uh for digital as well as like for the stores wherever it the pricing information comes from Kasandra and the cart where like uh either through uh the stores the sell checkout or even uh online digital ones those ones also use S recommendations so um any of the uh for you offers uh like whatever you see it on uh previously it was uh uh we used to find the cartwheel and then the red perks and then finally loyalty program there was related information all those ones are also part of uhand which comes in the inventory management which is spread across uh distribution centers as stores so it's basically kasanda has got into the the applications have wide we which is digital as well as uh distribution centers and stores uh in fact the recent one which is like a driver uh 3 years back we started this and uh uh it's been pretty good like uh we have seen how it has grown uh literally like threefold over the last like once the co started and all this so coupons related promotions like during uh it's a Thanksgiving time and so we see it's a kind of PE and we see that promotions and all that stuff and the profile information that's the way like way we use uh uh uh SSL part mainly and even the pricing so majority of the Clusters are actually SSL part and uh which are the internal back office generally those are kind of normal so uh other ones are the TV media where you see the target marketing related uh media so those ones are also part of uh Kanda itself so so over this uh Journey like uh a target for on kasanda it was like uh we learned a lot actually because when you're trying to come from a relational background and non- relational background both are actually completely different and they were pretty much good at their own features actually for example the multi clusters initially we tried to use it like how Oracle hasard like where we try to limit resources for a particular schema but on kand we can't so if you have multiple schemas or used by multiple applications uh if an application tries to uh pull out like tries to run a anti pattern query which uses lot amount of resources then you see other schema other applications also seeing the degraded performance the reason is because it's a kind of shade and we can't put a cap for any kind of schema related on uh for socks approaches uh we have seen that uh the service account which you users uh for authentication on the application side uh like it gets locked out sometimes the reason is because incorrect password or bad cache for the password which is trying to authenticate and if the password has been changed recently then all those ones are those kind of Integrations where a database team as well as uh application teams have to collaborate and do a few of the things for sure like those ones we learn that we have to be more collaborative and also and uh you know that uh uh GMX provides a very good way of metrics so uh for the larger clusters not even for larger clusters so even for three nodes when one particular node goes down all the three nodes would eject the message that okay one node is down one node is down so when you try to set up an alert it's very complicated because you get three kind of alerts and you have to group by on the particular cluster so it was it was becom more complicated so we found effective ways for utilizing them so and then fin the kind of uh uh any VI performance or the like when we have seen it would be either antia query or secondary indexes or materialized use or the kind of overloading or even like udfs or collection data types or anything so it's basically like you have to use cassand as a kind of vanilla one not any kind of additional one then you wouldn't have any kind of uh uh barriers for like for the same amount of 2 so for example like uh the 150k uh application which I was talking about the rights per second so they didn't have any of these ones and with only 30 nodes they were able to survive with the kind of load so a similar one not even 150k it was like only 30 to 50K reads per second or wrs per second they were requiring close to 99 noes which is almost like three times the ones because they were using secondary indexes and they were using collection data types so that is integrated uh to the architecture of kasanda how it is built so for sure like uh you can play it around but if you if it's going to be more and more uh right or through heavy then those ones it's better to avoid it on all the stuff and second indexes we know that it's pretty easy for anyone to just uh create a secondary index but you have to be very cautious on which set of columns you have to do the secondary indexes and all so yeah so finally yeah questions let's see here um no one in chat has asked many we got a couple nice presentations and a couple of congratulations so that's always good um we'll give you a guys a minute here uh feel free to put stuff in YouTube chat we are watching it so um you know any questions you've got uh just feel free yeah it was interesting that that you kind of had to almost go back to the pure Cassandra workload like you couldn't do any of the the extra Frills but the the Cassandra still worked as Sandra works that's yes that's true tic so if we tried to explore like bunch of the other options also uh like it does work perform well but like after a certain limit it breaks yeah for example like uh recently one of our cluster uh entirely broke down because of materialized use so they created and then they dropped it but it was a pretty big one so it jammed up all the resources on all the notes it was it was a big cluster was build of the index or the the yeah it was a kind of like re in the so all that stuff was going down going even though we tried to restart it we were not able to figure so finally we tried to stop everything from the application side but it was a nonpr environment so it was fine with all so yeah yeah that's the place where it's supposed to happen right so the thoughts related ones when we started this year it was uh every time it was a little weird because when we try to provision the new cluster from our end they give us a service account we give it but it doesn't work the reason is because the the case is different on the uh elap server so oh wow oh that that's one of those little pernicious things that you just would never oh man that was probably yeah awesome well um I'm not seeing any uh any questions here we're getting more uh you are great you rock superb so so I think it was a good presentation this is really awesome stuff um really cool that you guys are using open source in such a a business critical way too I'm a huge fan of Open Source I don't know if you know this but I actually uh I worked for a Target store through college so oh nice yeah long before my data stack stays oh nice yeah I don't know telling about you actually so like initially when he came down for the data talks or anything at Austin Tech and data Stacks or hosting and all that stuff yeah all right well if we don't have any other questions here um I think we can give it a a wrap um thank you so much for presenting uh next time we have a couple interesting things in the work stay tuned there will be an announcement going out on Eventbrite um I know some some people at some fairly uh other big Cassandra users out there are are itching to get in here and kind of share what they're doing that's really really cool uh with Cassandra so um with that I will bid you all a uh a good night or good day I suppose we're Global now aren't we y see you thanks guys y bye bye",
    "segments": [
      {
        "start": 0.16,
        "duration": 6.119,
        "text": "try to get someone in the call here real"
      },
      {
        "start": 2.679,
        "duration": 6.961,
        "text": "quick who might have the ability"
      },
      {
        "start": 6.279,
        "duration": 3.361,
        "text": "to log"
      },
      {
        "start": 10.88,
        "duration": 5.82,
        "text": "in perfect all right they can hear us so"
      },
      {
        "start": 14.44,
        "duration": 5.349,
        "text": "give me one moment I'll do the intro"
      },
      {
        "start": 16.7,
        "duration": 3.089,
        "text": "[Music]"
      },
      {
        "start": 26.16,
        "duration": 3.3,
        "text": "[Music]"
      },
      {
        "start": 38.879,
        "duration": 5.401,
        "text": "all right well welcome everyone to our"
      },
      {
        "start": 41.039,
        "duration": 5.121,
        "text": "first Global Cassandra Meetup and I just"
      },
      {
        "start": 44.28,
        "duration": 3.36,
        "text": "realized I've got echko on my end"
      },
      {
        "start": 46.16,
        "duration": 3.079,
        "text": "because I totally forgot to mute myself"
      },
      {
        "start": 47.64,
        "duration": 4.36,
        "text": "all right there we"
      },
      {
        "start": 49.239,
        "duration": 4.401,
        "text": "go sorry for the late start folks we had"
      },
      {
        "start": 52.0,
        "duration": 5.359,
        "text": "a couple technical difficulties but we"
      },
      {
        "start": 53.64,
        "duration": 6.84,
        "text": "are here now so um I am joined today by"
      },
      {
        "start": 57.359,
        "duration": 5.16,
        "text": "T who is going to do a a really cool"
      },
      {
        "start": 60.48,
        "duration": 3.639,
        "text": "presentation um as you might have"
      },
      {
        "start": 62.519,
        "duration": 5.28,
        "text": "noticed from the the logo let's see"
      },
      {
        "start": 64.119,
        "duration": 5.961,
        "text": "right up there um he works for Target"
      },
      {
        "start": 67.799,
        "duration": 4.68,
        "text": "Target is a very big open source"
      },
      {
        "start": 70.08,
        "duration": 4.24,
        "text": "Cassandra user um they've been longtime"
      },
      {
        "start": 72.479,
        "duration": 5.201,
        "text": "supporters of the community they've just"
      },
      {
        "start": 74.32,
        "duration": 4.799,
        "text": "been a really really uh good part of the"
      },
      {
        "start": 77.68,
        "duration": 3.72,
        "text": "cassander community as well especially"
      },
      {
        "start": 79.119,
        "duration": 3.561,
        "text": "up here in Minnesota where I believe"
      },
      {
        "start": 81.4,
        "duration": 6.28,
        "text": "we're both"
      },
      {
        "start": 82.68,
        "duration": 9.799,
        "text": "located so um with that uh take it"
      },
      {
        "start": 87.68,
        "duration": 7.64,
        "text": "away awesome thanks ER so yeah a couple"
      },
      {
        "start": 92.479,
        "duration": 5.481,
        "text": "of years back like um when uh this"
      },
      {
        "start": 95.32,
        "duration": 4.56,
        "text": "getting started uh when kasanda was open"
      },
      {
        "start": 97.96,
        "duration": 4.159,
        "text": "source to the Apache foundation and when"
      },
      {
        "start": 99.88,
        "duration": 3.76,
        "text": "it was announced to public many people"
      },
      {
        "start": 102.119,
        "duration": 4.401,
        "text": "were skeptical about using it because"
      },
      {
        "start": 103.64,
        "duration": 5.6,
        "text": "it's a kind of new database and all so"
      },
      {
        "start": 106.52,
        "duration": 4.48,
        "text": "uh eventually like U many um many"
      },
      {
        "start": 109.24,
        "duration": 4.28,
        "text": "Industries started using it as a kind of"
      },
      {
        "start": 111.0,
        "duration": 6.399,
        "text": "like a backend for uh or the cash or"
      },
      {
        "start": 113.52,
        "duration": 6.199,
        "text": "anything then uh like last year when uh"
      },
      {
        "start": 117.399,
        "duration": 4.881,
        "text": "when everyone was moving to conization"
      },
      {
        "start": 119.719,
        "duration": 4.44,
        "text": "uh uh the databases generally don't go"
      },
      {
        "start": 122.28,
        "duration": 5.6,
        "text": "with the conization well because the"
      },
      {
        "start": 124.159,
        "duration": 5.361,
        "text": "data uh storage volume point it's a"
      },
      {
        "start": 127.88,
        "duration": 6.24,
        "text": "little difficult when the container"
      },
      {
        "start": 129.52,
        "duration": 6.16,
        "text": "moves around actually so yeah uh so with"
      },
      {
        "start": 134.12,
        "duration": 4.56,
        "text": "this like let's get"
      },
      {
        "start": 135.68,
        "duration": 3.0,
        "text": "started"
      },
      {
        "start": 138.76,
        "duration": 5.839,
        "text": "so uh I'm Tage malati I work as a"
      },
      {
        "start": 142.239,
        "duration": 4.841,
        "text": "Cassandra leader Target it just been 5"
      },
      {
        "start": 144.599,
        "duration": 4.72,
        "text": "years and 1 month uh at Target actually"
      },
      {
        "start": 147.08,
        "duration": 4.0,
        "text": "it's pretty nice and uh uh it's pretty"
      },
      {
        "start": 149.319,
        "duration": 4.56,
        "text": "cool to work on the new technologies and"
      },
      {
        "start": 151.08,
        "duration": 4.84,
        "text": "all that stuff so yeah I co-authored uh"
      },
      {
        "start": 153.879,
        "duration": 6.521,
        "text": "two books one is mastering of Pache"
      },
      {
        "start": 155.92,
        "duration": 7.08,
        "text": "kasan 3x with Aron FX and Advance my SQL"
      },
      {
        "start": 160.4,
        "duration": 4.4,
        "text": "8 So currently I'm puring my MBA as well"
      },
      {
        "start": 163.0,
        "duration": 5.12,
        "text": "at University of Minnesota where I'm"
      },
      {
        "start": 164.8,
        "duration": 5.159,
        "text": "currently located and I did my M's uh in"
      },
      {
        "start": 168.12,
        "duration": 5.199,
        "text": "computer science from University of New"
      },
      {
        "start": 169.959,
        "duration": 8.0,
        "text": "Mexico so that's a brief introduction"
      },
      {
        "start": 173.319,
        "duration": 8.64,
        "text": "about me so Target like uh kasanda"
      },
      {
        "start": 177.959,
        "duration": 6.041,
        "text": "Target um a Target like uh it was one of"
      },
      {
        "start": 181.959,
        "duration": 5.0,
        "text": "the kind of like early adopters of any"
      },
      {
        "start": 184.0,
        "duration": 5.12,
        "text": "kind of new technology uh they explore"
      },
      {
        "start": 186.959,
        "duration": 4.321,
        "text": "uh not only any kind of new database"
      },
      {
        "start": 189.12,
        "duration": 4.119,
        "text": "technology but even the other uh other"
      },
      {
        "start": 191.28,
        "duration": 4.679,
        "text": "set of Technologies as well so it was"
      },
      {
        "start": 193.239,
        "duration": 5.881,
        "text": "one of the early adopters for migrating"
      },
      {
        "start": 195.959,
        "duration": 5.241,
        "text": "from bulky monolith applications to uh"
      },
      {
        "start": 199.12,
        "duration": 3.6,
        "text": "microservice architecture because it's a"
      },
      {
        "start": 201.2,
        "duration": 3.28,
        "text": "cloud native like we would like to go"
      },
      {
        "start": 202.72,
        "duration": 4.32,
        "text": "ahead and that's the future which we're"
      },
      {
        "start": 204.48,
        "duration": 5.28,
        "text": "thinking of all going so so when we"
      },
      {
        "start": 207.04,
        "duration": 4.6,
        "text": "started exploring um multiple no equal"
      },
      {
        "start": 209.76,
        "duration": 4.839,
        "text": "databases which are more Cloud native"
      },
      {
        "start": 211.64,
        "duration": 4.56,
        "text": "and kind of distributed because uh any"
      },
      {
        "start": 214.599,
        "duration": 5.761,
        "text": "kind of retail industry generally"
      },
      {
        "start": 216.2,
        "duration": 7.399,
        "text": "requires uh um High availability so"
      },
      {
        "start": 220.36,
        "duration": 5.32,
        "text": "let's say uh you try to order something"
      },
      {
        "start": 223.599,
        "duration": 3.84,
        "text": "and uh uh you get back to know that like"
      },
      {
        "start": 225.68,
        "duration": 4.4,
        "text": "uh the order is not placed so that's"
      },
      {
        "start": 227.439,
        "duration": 4.761,
        "text": "really bad right so so we need something"
      },
      {
        "start": 230.08,
        "duration": 5.04,
        "text": "like which is highly available so that's"
      },
      {
        "start": 232.2,
        "duration": 4.8,
        "text": "why we started with kasanda and all so"
      },
      {
        "start": 235.12,
        "duration": 4.399,
        "text": "kazand is a kind of like a very highly"
      },
      {
        "start": 237.0,
        "duration": 5.68,
        "text": "available kind of database with B on the"
      },
      {
        "start": 239.519,
        "duration": 6.44,
        "text": "C the so uh with these ones like when we"
      },
      {
        "start": 242.68,
        "duration": 5.32,
        "text": "started kasanda uh Journey uh we saw"
      },
      {
        "start": 245.959,
        "duration": 4.401,
        "text": "actually like a kind of a two full"
      },
      {
        "start": 248.0,
        "duration": 6.519,
        "text": "growth over a period of like 1 to two"
      },
      {
        "start": 250.36,
        "duration": 6.999,
        "text": "years uh overall period so uh it it was"
      },
      {
        "start": 254.519,
        "duration": 5.24,
        "text": "piing up pretty fast actually so and uh"
      },
      {
        "start": 257.359,
        "duration": 5.161,
        "text": "we were getting multiple use cases where"
      },
      {
        "start": 259.759,
        "duration": 5.321,
        "text": "it has to be something on perm and some"
      },
      {
        "start": 262.52,
        "duration": 5.04,
        "text": "has to be on public cloud and some have"
      },
      {
        "start": 265.08,
        "duration": 4.44,
        "text": "to be shuffled between the two uh"
      },
      {
        "start": 267.56,
        "duration": 5.88,
        "text": "domains as well or even the third domain"
      },
      {
        "start": 269.52,
        "duration": 6.44,
        "text": "when it comes up actually so uh then uh"
      },
      {
        "start": 273.44,
        "duration": 4.0,
        "text": "um once uh a company tries to move from"
      },
      {
        "start": 275.96,
        "duration": 3.799,
        "text": "one cloud provider to another cloud"
      },
      {
        "start": 277.44,
        "duration": 4.8,
        "text": "provider it's easy for applications to"
      },
      {
        "start": 279.759,
        "duration": 4.801,
        "text": "just move around because they just uh uh"
      },
      {
        "start": 282.24,
        "duration": 3.84,
        "text": "cut the traffic uh um stop the"
      },
      {
        "start": 284.56,
        "duration": 4.76,
        "text": "microservices and then move to the other"
      },
      {
        "start": 286.08,
        "duration": 5.0,
        "text": "ones but for uh databases it's very hard"
      },
      {
        "start": 289.32,
        "duration": 3.879,
        "text": "because you have to join to the part of"
      },
      {
        "start": 291.08,
        "duration": 4.32,
        "text": "the cluster then expand the data make"
      },
      {
        "start": 293.199,
        "duration": 4.201,
        "text": "sure the data is in sync and then start"
      },
      {
        "start": 295.4,
        "duration": 4.359,
        "text": "decommissioning process and the data"
      },
      {
        "start": 297.4,
        "duration": 4.88,
        "text": "validation takes really really really"
      },
      {
        "start": 299.759,
        "duration": 4.521,
        "text": "long time and if it's a kind of like um"
      },
      {
        "start": 302.28,
        "duration": 3.72,
        "text": "from relational database to non-"
      },
      {
        "start": 304.28,
        "duration": 3.16,
        "text": "relational databases if you're going"
      },
      {
        "start": 306.0,
        "duration": 5.16,
        "text": "ahead with it then definitely it's going"
      },
      {
        "start": 307.44,
        "duration": 8.16,
        "text": "to be pain so yeah uh uh Target started"
      },
      {
        "start": 311.16,
        "duration": 6.56,
        "text": "adopting in early 2014 um with the SSL"
      },
      {
        "start": 315.6,
        "duration": 4.24,
        "text": "part as well and recently we have done"
      },
      {
        "start": 317.72,
        "duration": 3.64,
        "text": "the even the stocks part because we have"
      },
      {
        "start": 319.84,
        "duration": 5.56,
        "text": "seen even Finance applications coming"
      },
      {
        "start": 321.36,
        "duration": 9.24,
        "text": "down uh from their uh my SQL background"
      },
      {
        "start": 325.4,
        "duration": 6.96,
        "text": "to kand so yeah so we are proudly uh uh"
      },
      {
        "start": 330.6,
        "duration": 4.2,
        "text": "announc that like okay we all our"
      },
      {
        "start": 332.36,
        "duration": 5.399,
        "text": "clusters what we manage at Target are on"
      },
      {
        "start": 334.8,
        "duration": 4.32,
        "text": "314 version which is the latest version"
      },
      {
        "start": 337.759,
        "duration": 3.56,
        "text": "as of now"
      },
      {
        "start": 339.12,
        "duration": 5.359,
        "text": "so"
      },
      {
        "start": 341.319,
        "duration": 6.241,
        "text": "1240 oh yeah 3.4"
      },
      {
        "start": 344.479,
        "duration": 7.481,
        "text": "yeah next is for. for"
      },
      {
        "start": 347.56,
        "duration": 9.24,
        "text": "sure so yeah so uh when we start with"
      },
      {
        "start": 351.96,
        "duration": 7.12,
        "text": "the complexity to manage um we had close"
      },
      {
        "start": 356.8,
        "duration": 4.679,
        "text": "to uh 3 years or four uh 2 years back"
      },
      {
        "start": 359.08,
        "duration": 5.239,
        "text": "like it was close close to 30 plus"
      },
      {
        "start": 361.479,
        "duration": 5.481,
        "text": "different versions across several"
      },
      {
        "start": 364.319,
        "duration": 5.361,
        "text": "clusters and we had like 4,000 plus"
      },
      {
        "start": 366.96,
        "duration": 5.239,
        "text": "nodes across all the Clusters actually"
      },
      {
        "start": 369.68,
        "duration": 5.12,
        "text": "and uh 30 plus different ways of"
      },
      {
        "start": 372.199,
        "duration": 5.161,
        "text": "implementation because uh when we're"
      },
      {
        "start": 374.8,
        "duration": 4.64,
        "text": "trying to migrate or anything uh we was"
      },
      {
        "start": 377.36,
        "duration": 3.679,
        "text": "looking at like what is compatible to"
      },
      {
        "start": 379.44,
        "duration": 3.96,
        "text": "that particular domain where with the"
      },
      {
        "start": 381.039,
        "duration": 4.401,
        "text": "kand DAT is getting deployed and then"
      },
      {
        "start": 383.4,
        "duration": 5.32,
        "text": "accordingly we used to manage it so it"
      },
      {
        "start": 385.44,
        "duration": 6.24,
        "text": "was very difficult to manage like uh if"
      },
      {
        "start": 388.72,
        "duration": 5.039,
        "text": "a new uh use cases coming up what kind"
      },
      {
        "start": 391.68,
        "duration": 4.6,
        "text": "of implementation we have to use and the"
      },
      {
        "start": 393.759,
        "duration": 4.201,
        "text": "best practice on one cluster what we use"
      },
      {
        "start": 396.28,
        "duration": 4.56,
        "text": "and propag into other set of clusters"
      },
      {
        "start": 397.96,
        "duration": 5.04,
        "text": "was becoming very very hard and uh you"
      },
      {
        "start": 400.84,
        "duration": 4.079,
        "text": "everyone knows that uh infra as a"
      },
      {
        "start": 403.0,
        "duration": 4.44,
        "text": "service or platform as a service if you"
      },
      {
        "start": 404.919,
        "duration": 4.801,
        "text": "have multiple compute layers it would be"
      },
      {
        "start": 407.44,
        "duration": 5.72,
        "text": "tough to manage as well and there were"
      },
      {
        "start": 409.72,
        "duration": 6.28,
        "text": "like um close to uh 50% just 45% of"
      },
      {
        "start": 413.16,
        "duration": 5.479,
        "text": "clusters are actually on SSL and 20% on"
      },
      {
        "start": 416.0,
        "duration": 5.4,
        "text": "socks and the rest are actually on non"
      },
      {
        "start": 418.639,
        "duration": 4.4,
        "text": "socks and non SS"
      },
      {
        "start": 421.4,
        "duration": 4.239,
        "text": "so let's start with the complexity like"
      },
      {
        "start": 423.039,
        "duration": 4.801,
        "text": "how Okay uh with a simple sandbox"
      },
      {
        "start": 425.639,
        "duration": 4.28,
        "text": "environment you might just have a one"
      },
      {
        "start": 427.84,
        "duration": 5.039,
        "text": "data center in a cluster with three"
      },
      {
        "start": 429.919,
        "duration": 6.361,
        "text": "nodes so doing any kind of like um"
      },
      {
        "start": 432.879,
        "duration": 6.561,
        "text": "rolling restarts or config updates or"
      },
      {
        "start": 436.28,
        "duration": 5.88,
        "text": "bumping up the CPU or bumping up the max"
      },
      {
        "start": 439.44,
        "duration": 5.159,
        "text": "Heap size uh during your test process or"
      },
      {
        "start": 442.16,
        "duration": 4.599,
        "text": "anything so it's easy like just Ren know"
      },
      {
        "start": 444.599,
        "duration": 4.361,
        "text": "right you can just log in and then do it"
      },
      {
        "start": 446.759,
        "duration": 4.961,
        "text": "so for example Let's do let's scale it"
      },
      {
        "start": 448.96,
        "duration": 4.28,
        "text": "out now so same single data center but"
      },
      {
        "start": 451.72,
        "duration": 4.52,
        "text": "you have like four times the number of"
      },
      {
        "start": 453.24,
        "duration": 5.72,
        "text": "nodes which is like 12 nodes still it is"
      },
      {
        "start": 456.24,
        "duration": 6.56,
        "text": "okay but like it's a little little time"
      },
      {
        "start": 458.96,
        "duration": 6.44,
        "text": "taking now when we go down to uh two"
      },
      {
        "start": 462.8,
        "duration": 4.239,
        "text": "data centers with 12 nodes in each data"
      },
      {
        "start": 465.4,
        "duration": 4.04,
        "text": "center it's going to be a little more"
      },
      {
        "start": 467.039,
        "duration": 5.6,
        "text": "complicated so when I said like 100 plus"
      },
      {
        "start": 469.44,
        "duration": 5.199,
        "text": "clusters we manage it Target so there"
      },
      {
        "start": 472.639,
        "duration": 4.161,
        "text": "are few clusters which are like even 300"
      },
      {
        "start": 474.639,
        "duration": 4.081,
        "text": "400 nodes as well within a cluster it"
      },
      {
        "start": 476.8,
        "duration": 4.04,
        "text": "spread across like six data centers or"
      },
      {
        "start": 478.72,
        "duration": 3.56,
        "text": "eight data centers also so you have like"
      },
      {
        "start": 480.84,
        "duration": 3.039,
        "text": "a couple of clusters which are eight"
      },
      {
        "start": 482.28,
        "duration": 3.0,
        "text": "data centers because of data"
      },
      {
        "start": 483.879,
        "duration": 3.76,
        "text": "availability and like different use"
      },
      {
        "start": 485.28,
        "duration": 4.039,
        "text": "cases we have it actually so so it's"
      },
      {
        "start": 487.639,
        "duration": 5.161,
        "text": "very complex like when you have to"
      },
      {
        "start": 489.319,
        "duration": 4.72,
        "text": "update it across all the uh nodes in a"
      },
      {
        "start": 492.8,
        "duration": 3.239,
        "text": "particular cluster with the"
      },
      {
        "start": 494.039,
        "duration": 4.961,
        "text": "corresponding configuration and let's"
      },
      {
        "start": 496.039,
        "duration": 6.16,
        "text": "say like you bumped up the uh Heap size"
      },
      {
        "start": 499.0,
        "duration": 5.08,
        "text": "on one cluster uh but like the same"
      },
      {
        "start": 502.199,
        "duration": 3.72,
        "text": "value is being used for the other"
      },
      {
        "start": 504.08,
        "duration": 4.6,
        "text": "cluster which is not supposed to be used"
      },
      {
        "start": 505.919,
        "duration": 4.921,
        "text": "it's very complicated so yeah so because"
      },
      {
        "start": 508.68,
        "duration": 4.52,
        "text": "of f like when everyone is moving to"
      },
      {
        "start": 510.84,
        "duration": 4.36,
        "text": "canonization as apps are trying to move"
      },
      {
        "start": 513.2,
        "duration": 5.759,
        "text": "and we have seen we have seen couple of"
      },
      {
        "start": 515.2,
        "duration": 7.44,
        "text": "uh even other uh teams are trying to"
      },
      {
        "start": 518.959,
        "duration": 5.361,
        "text": "leverage like databases on Docker and uh"
      },
      {
        "start": 522.64,
        "duration": 3.199,
        "text": "for the PC purposes they were pretty"
      },
      {
        "start": 524.32,
        "duration": 5.199,
        "text": "much fine with it but for production"
      },
      {
        "start": 525.839,
        "duration": 6.201,
        "text": "purposes it was very tough to manage so"
      },
      {
        "start": 529.519,
        "duration": 4.521,
        "text": "uh we are Target like uh we started"
      },
      {
        "start": 532.04,
        "duration": 3.84,
        "text": "leveraging this effort like uh last year"
      },
      {
        "start": 534.04,
        "duration": 3.76,
        "text": "so we Tred to figure out like what are"
      },
      {
        "start": 535.88,
        "duration": 4.079,
        "text": "the technical difficulties we would have"
      },
      {
        "start": 537.8,
        "duration": 4.52,
        "text": "it and all that stuff so"
      },
      {
        "start": 539.959,
        "duration": 4.601,
        "text": "uh once we try to figure out all that"
      },
      {
        "start": 542.32,
        "duration": 4.72,
        "text": "the main blockers were actually kind of"
      },
      {
        "start": 544.56,
        "duration": 4.6,
        "text": "like a SSL part like how we got like we"
      },
      {
        "start": 547.04,
        "duration": 4.08,
        "text": "have to how we can implement it actually"
      },
      {
        "start": 549.16,
        "duration": 4.0,
        "text": "so because Docker is just a kind of like"
      },
      {
        "start": 551.12,
        "duration": 3.8,
        "text": "automated like where you can directly"
      },
      {
        "start": 553.16,
        "duration": 4.919,
        "text": "whatever is available you use it and"
      },
      {
        "start": 554.92,
        "duration": 4.96,
        "text": "pick it up and use it that's all so uh"
      },
      {
        "start": 558.079,
        "duration": 4.361,
        "text": "but like for example like why we need"
      },
      {
        "start": 559.88,
        "duration": 3.92,
        "text": "the SSL part so you know that like"
      },
      {
        "start": 562.44,
        "duration": 5.399,
        "text": "Target is a kind of like a publicly"
      },
      {
        "start": 563.8,
        "duration": 7.279,
        "text": "traded company so we do maintain um Pi"
      },
      {
        "start": 567.839,
        "duration": 5.881,
        "text": "related information on actually so"
      },
      {
        "start": 571.079,
        "duration": 4.161,
        "text": "either business related information also"
      },
      {
        "start": 573.72,
        "duration": 4.799,
        "text": "so which is little confidential and we"
      },
      {
        "start": 575.24,
        "duration": 5.44,
        "text": "don't want to uh be kind of SEC"
      },
      {
        "start": 578.519,
        "duration": 3.641,
        "text": "complaint and all so it's just a kind of"
      },
      {
        "start": 580.68,
        "duration": 3.399,
        "text": "like additional layer over the"
      },
      {
        "start": 582.16,
        "duration": 3.679,
        "text": "authentication so that it gives better"
      },
      {
        "start": 584.079,
        "duration": 4.161,
        "text": "confidence for the application teams to"
      },
      {
        "start": 585.839,
        "duration": 5.161,
        "text": "go ahead with it so that was the reason"
      },
      {
        "start": 588.24,
        "duration": 5.039,
        "text": "mainly we went ahead with the SSL part"
      },
      {
        "start": 591.0,
        "duration": 5.839,
        "text": "integrated to kasanda where open source"
      },
      {
        "start": 593.279,
        "duration": 6.841,
        "text": "or even data stack also has provided so"
      },
      {
        "start": 596.839,
        "duration": 4.761,
        "text": "now when we come down to SSL uh there is"
      },
      {
        "start": 600.12,
        "duration": 3.64,
        "text": "two different approaches one is manual"
      },
      {
        "start": 601.6,
        "duration": 4.28,
        "text": "and the automated with the manual"
      },
      {
        "start": 603.76,
        "duration": 4.44,
        "text": "approach you have to generate on a"
      },
      {
        "start": 605.88,
        "duration": 4.6,
        "text": "particular node or like you can gener it"
      },
      {
        "start": 608.2,
        "duration": 4.68,
        "text": "on another node and then SCP to the"
      },
      {
        "start": 610.48,
        "duration": 5.0,
        "text": "corresponding with all the public Sears"
      },
      {
        "start": 612.88,
        "duration": 5.36,
        "text": "and uh the private key the key store and"
      },
      {
        "start": 615.48,
        "duration": 5.28,
        "text": "Trust stores and make sure all the"
      },
      {
        "start": 618.24,
        "duration": 4.56,
        "text": "public Sears of the nodes are actually"
      },
      {
        "start": 620.76,
        "duration": 4.519,
        "text": "imported into the trust St and all the"
      },
      {
        "start": 622.8,
        "duration": 4.76,
        "text": "nodes do have maintain that and then"
      },
      {
        "start": 625.279,
        "duration": 4.601,
        "text": "restart the cand so let's say you have a"
      },
      {
        "start": 627.56,
        "duration": 4.839,
        "text": "400 node cluster you have to do this"
      },
      {
        "start": 629.88,
        "duration": 5.6,
        "text": "approach is it going to be is it going"
      },
      {
        "start": 632.399,
        "duration": 5.321,
        "text": "to be easy no I don't think so we went"
      },
      {
        "start": 635.48,
        "duration": 6.0,
        "text": "ahead with uh like automated approach"
      },
      {
        "start": 637.72,
        "duration": 6.239,
        "text": "where we have a like a um automatic CT"
      },
      {
        "start": 641.48,
        "duration": 5.359,
        "text": "API like where you just make an API call"
      },
      {
        "start": 643.959,
        "duration": 5.401,
        "text": "get back toer and then uh you can just"
      },
      {
        "start": 646.839,
        "duration": 4.961,
        "text": "directly convert it into the format"
      },
      {
        "start": 649.36,
        "duration": 4.24,
        "text": "which database requires it and then"
      },
      {
        "start": 651.8,
        "duration": 4.479,
        "text": "proceed with the restart that can be"
      },
      {
        "start": 653.6,
        "duration": 7.12,
        "text": "done like that's a little bit at least"
      },
      {
        "start": 656.279,
        "duration": 6.8,
        "text": "the Lesser manual interation which is"
      },
      {
        "start": 660.72,
        "duration": 6.04,
        "text": "so we developed a kind of chickie which"
      },
      {
        "start": 663.079,
        "duration": 7.121,
        "text": "is written in go actually so uh we use"
      },
      {
        "start": 666.76,
        "duration": 6.04,
        "text": "this chiot actually at multiple no SQL"
      },
      {
        "start": 670.2,
        "duration": 4.639,
        "text": "Technologies not only for kasanda we use"
      },
      {
        "start": 672.8,
        "duration": 6.52,
        "text": "it for elastic we use for reders we use"
      },
      {
        "start": 674.839,
        "duration": 6.401,
        "text": "it for um even mongod as well so so"
      },
      {
        "start": 679.32,
        "duration": 4.48,
        "text": "based on different use cases in the"
      },
      {
        "start": 681.24,
        "duration": 5.92,
        "text": "technology the key stor and tros have to"
      },
      {
        "start": 683.8,
        "duration": 6.0,
        "text": "be in different jks formats and also so"
      },
      {
        "start": 687.16,
        "duration": 4.6,
        "text": "with those uh corresponding formats like"
      },
      {
        "start": 689.8,
        "duration": 4.32,
        "text": "this is kind of having the capability of"
      },
      {
        "start": 691.76,
        "duration": 5.0,
        "text": "uh passing on which kind of format you"
      },
      {
        "start": 694.12,
        "duration": 5.92,
        "text": "needed and then automatically generate"
      },
      {
        "start": 696.76,
        "duration": 5.44,
        "text": "those ones and then validate it and then"
      },
      {
        "start": 700.04,
        "duration": 4.039,
        "text": "automatically it will be able to renew"
      },
      {
        "start": 702.2,
        "duration": 3.439,
        "text": "it as well when it's getting expired you"
      },
      {
        "start": 704.079,
        "duration": 3.601,
        "text": "can put a kind of threshold that when"
      },
      {
        "start": 705.639,
        "duration": 4.281,
        "text": "it's getting expired 30 days or 45 days"
      },
      {
        "start": 707.68,
        "duration": 4.159,
        "text": "or something you can definitely renew it"
      },
      {
        "start": 709.92,
        "duration": 3.8,
        "text": "automatically uh and it would have like"
      },
      {
        "start": 711.839,
        "duration": 3.8,
        "text": "even the metrics as well which is when"
      },
      {
        "start": 713.72,
        "duration": 4.96,
        "text": "it's going to expired and all so it's a"
      },
      {
        "start": 715.639,
        "duration": 5.161,
        "text": "pretty cool like uh uh custom build"
      },
      {
        "start": 718.68,
        "duration": 4.48,
        "text": "application for for like uh the SSL part"
      },
      {
        "start": 720.8,
        "duration": 5.56,
        "text": "where we generate it and push it out so"
      },
      {
        "start": 723.16,
        "duration": 6.919,
        "text": "and it's like uh agnostic to any kind of"
      },
      {
        "start": 726.36,
        "duration": 5.56,
        "text": "os so we can use it on um uh Linux we"
      },
      {
        "start": 730.079,
        "duration": 4.721,
        "text": "can use it on this one uh Windows as"
      },
      {
        "start": 731.92,
        "duration": 5.8,
        "text": "well anything so the second blocker was"
      },
      {
        "start": 734.8,
        "duration": 7.32,
        "text": "related to socks related"
      },
      {
        "start": 737.72,
        "duration": 6.799,
        "text": "applications so Ser socks act like why"
      },
      {
        "start": 742.12,
        "duration": 4.12,
        "text": "why we need socks actually so there were"
      },
      {
        "start": 744.519,
        "duration": 5.0,
        "text": "certain use cases where we have to"
      },
      {
        "start": 746.24,
        "duration": 5.0,
        "text": "handle Finance related information which"
      },
      {
        "start": 749.519,
        "duration": 4.88,
        "text": "was enforcing some kind of uh"
      },
      {
        "start": 751.24,
        "duration": 8.959,
        "text": "restrictions of auditing purposes or"
      },
      {
        "start": 754.399,
        "duration": 7.641,
        "text": "only um uh only a certain kind of like a"
      },
      {
        "start": 760.199,
        "duration": 4.121,
        "text": "integration to ldap for authentication"
      },
      {
        "start": 762.04,
        "duration": 4.56,
        "text": "purposes rather than kanda's uh"
      },
      {
        "start": 764.32,
        "duration": 4.0,
        "text": "authentication mechanism and all so"
      },
      {
        "start": 766.6,
        "duration": 4.16,
        "text": "which was little typical which is the"
      },
      {
        "start": 768.32,
        "duration": 4.56,
        "text": "requirement for security complant onto"
      },
      {
        "start": 770.76,
        "duration": 4.12,
        "text": "the socks because when we have when when"
      },
      {
        "start": 772.88,
        "duration": 3.92,
        "text": "we do have like external audits for sure"
      },
      {
        "start": 774.88,
        "duration": 3.92,
        "text": "like it's going to be little complicated"
      },
      {
        "start": 776.8,
        "duration": 4.44,
        "text": "if it's not going to be met and also so"
      },
      {
        "start": 778.8,
        "duration": 5.96,
        "text": "for the purpose like uh we started"
      },
      {
        "start": 781.24,
        "duration": 8.2,
        "text": "exploring what else can be integrated to"
      },
      {
        "start": 784.76,
        "duration": 6.079,
        "text": "our the cic department or anything so uh"
      },
      {
        "start": 789.44,
        "duration": 5.199,
        "text": "the first thing which we wanted is like"
      },
      {
        "start": 790.839,
        "duration": 6.12,
        "text": "we have to change the authentication um"
      },
      {
        "start": 794.639,
        "duration": 4.801,
        "text": "authenticator so we have we used the"
      },
      {
        "start": 796.959,
        "duration": 4.921,
        "text": "insta cluster s authenticated with small"
      },
      {
        "start": 799.44,
        "duration": 5.959,
        "text": "code changes related to uh our"
      },
      {
        "start": 801.88,
        "duration": 5.16,
        "text": "corresponding U uh L app server and the"
      },
      {
        "start": 805.399,
        "duration": 3.481,
        "text": "corresponding authentication mechanism"
      },
      {
        "start": 807.04,
        "duration": 4.96,
        "text": "and how it can be and then we have to"
      },
      {
        "start": 808.88,
        "duration": 5.16,
        "text": "update data kasanda emsh then the next"
      },
      {
        "start": 812.0,
        "duration": 4.72,
        "text": "part which is related to the security"
      },
      {
        "start": 814.04,
        "duration": 6.12,
        "text": "complaint is related to auditing purpose"
      },
      {
        "start": 816.72,
        "duration": 5.4,
        "text": "so currently like U um kasanda Ro"
      },
      {
        "start": 820.16,
        "duration": 4.44,
        "text": "manager would not be able to understand"
      },
      {
        "start": 822.12,
        "duration": 5.2,
        "text": "this elap mechanism so we have to use"
      },
      {
        "start": 824.6,
        "duration": 4.76,
        "text": "the kind of like another authorizer"
      },
      {
        "start": 827.32,
        "duration": 4.319,
        "text": "mechanism or Ro manager as well"
      },
      {
        "start": 829.36,
        "duration": 4.399,
        "text": "according to it so uh we would be"
      },
      {
        "start": 831.639,
        "duration": 5.64,
        "text": "picking up the role manager we use the"
      },
      {
        "start": 833.759,
        "duration": 5.921,
        "text": "ens uhle manager where uh we can pass it"
      },
      {
        "start": 837.279,
        "duration": 4.92,
        "text": "on again with the same similar"
      },
      {
        "start": 839.68,
        "duration": 3.839,
        "text": "by. and then proceed further so with"
      },
      {
        "start": 842.199,
        "duration": 3.721,
        "text": "this way like we can generate even the"
      },
      {
        "start": 843.519,
        "duration": 4.281,
        "text": "audit log like uh from where the IP is"
      },
      {
        "start": 845.92,
        "duration": 4.0,
        "text": "getting generated and like which which"
      },
      {
        "start": 847.8,
        "duration": 3.44,
        "text": "username they're trying to connect and"
      },
      {
        "start": 849.92,
        "duration": 2.88,
        "text": "what kind of query they're running and"
      },
      {
        "start": 851.24,
        "duration": 4.12,
        "text": "all that stuff so it's kind of tracing"
      },
      {
        "start": 852.8,
        "duration": 4.12,
        "text": "with more add features which are related"
      },
      {
        "start": 855.36,
        "duration": 4.64,
        "text": "to security"
      },
      {
        "start": 856.92,
        "duration": 6.0,
        "text": "purposes so uh other Integrations"
      },
      {
        "start": 860.0,
        "duration": 5.24,
        "text": "related to socks are basically like uh"
      },
      {
        "start": 862.92,
        "duration": 4.52,
        "text": "schema changes which we use good for"
      },
      {
        "start": 865.24,
        "duration": 4.959,
        "text": "maintaining our tracking purposes so any"
      },
      {
        "start": 867.44,
        "duration": 4.759,
        "text": "changes for socks mainly has to go"
      },
      {
        "start": 870.199,
        "duration": 3.64,
        "text": "through that process so that anything"
      },
      {
        "start": 872.199,
        "duration": 4.681,
        "text": "which breaks we know that like where it"
      },
      {
        "start": 873.839,
        "duration": 5.161,
        "text": "has broken or anything that's so and uh"
      },
      {
        "start": 876.88,
        "duration": 4.519,
        "text": "when we have socks for sure like it's"
      },
      {
        "start": 879.0,
        "duration": 4.519,
        "text": "not it shouldn't be directly added by"
      },
      {
        "start": 881.399,
        "duration": 3.8,
        "text": "some kind of common key which is shared"
      },
      {
        "start": 883.519,
        "duration": 4.281,
        "text": "across the team which can they can log"
      },
      {
        "start": 885.199,
        "duration": 6.121,
        "text": "or anything so it has to be integrated"
      },
      {
        "start": 887.8,
        "duration": 4.76,
        "text": "to uh kind of like a individual person"
      },
      {
        "start": 891.32,
        "duration": 4.0,
        "text": "who is locking in with all that"
      },
      {
        "start": 892.56,
        "duration": 5.8,
        "text": "mechanisms so we use sssd to allow the"
      },
      {
        "start": 895.32,
        "duration": 7.48,
        "text": "SSH via uh ad group kind of a protocol"
      },
      {
        "start": 898.36,
        "duration": 7.599,
        "text": "so and then finally uh the main uh like"
      },
      {
        "start": 902.8,
        "duration": 5.399,
        "text": "once we fixed all these ones uh we were"
      },
      {
        "start": 905.959,
        "duration": 6.56,
        "text": "actually uh this is a little funny"
      },
      {
        "start": 908.199,
        "duration": 6.241,
        "text": "because Linux was uh very conscious"
      },
      {
        "start": 912.519,
        "duration": 3.361,
        "text": "about like the case uh when we're"
      },
      {
        "start": 914.44,
        "duration": 4.04,
        "text": "entering the kind of username and"
      },
      {
        "start": 915.88,
        "duration": 5.399,
        "text": "password Windows doesn't so we going we"
      },
      {
        "start": 918.48,
        "duration": 5.44,
        "text": "use Linux for deploying ourand clusters"
      },
      {
        "start": 921.279,
        "duration": 4.841,
        "text": "so if your L app server maintains a"
      },
      {
        "start": 923.92,
        "duration": 4.12,
        "text": "username in a upper case or lower case"
      },
      {
        "start": 926.12,
        "duration": 3.519,
        "text": "or some kind of difference then for sure"
      },
      {
        "start": 928.04,
        "duration": 4.84,
        "text": "I'm not going to get through"
      },
      {
        "start": 929.639,
        "duration": 4.76,
        "text": "so that way like uh uh we have to be"
      },
      {
        "start": 932.88,
        "duration": 5.319,
        "text": "very careful about like when we are"
      },
      {
        "start": 934.399,
        "duration": 6.961,
        "text": "integrating to ldap related stuff on to"
      },
      {
        "start": 938.199,
        "duration": 6.241,
        "text": "socks now let's say like we have 400"
      },
      {
        "start": 941.36,
        "duration": 6.24,
        "text": "nodes in a cluster and we are trying to"
      },
      {
        "start": 944.44,
        "duration": 5.959,
        "text": "uh do security patches and on uh uh the"
      },
      {
        "start": 947.6,
        "duration": 4.159,
        "text": "public clouds generally uh we have a"
      },
      {
        "start": 950.399,
        "duration": 3.44,
        "text": "limitation of like we can't just pull"
      },
      {
        "start": 951.759,
        "duration": 5.32,
        "text": "the Y repos and all so it's a kind of"
      },
      {
        "start": 953.839,
        "duration": 6.961,
        "text": "static image which we use right so on so"
      },
      {
        "start": 957.079,
        "duration": 5.76,
        "text": "here uh when we have to do kind of redep"
      },
      {
        "start": 960.8,
        "duration": 4.64,
        "text": "or anything it was very hard for us"
      },
      {
        "start": 962.839,
        "duration": 5.081,
        "text": "because when uh like for upgrading even"
      },
      {
        "start": 965.44,
        "duration": 5.92,
        "text": "the jbm version also because we were not"
      },
      {
        "start": 967.92,
        "duration": 5.76,
        "text": "able to update it uh in um um on the"
      },
      {
        "start": 971.36,
        "duration": 4.279,
        "text": "same node the reason is because it uses"
      },
      {
        "start": 973.68,
        "duration": 3.719,
        "text": "a static image and we not be able to"
      },
      {
        "start": 975.639,
        "duration": 4.401,
        "text": "accessible any of the external"
      },
      {
        "start": 977.399,
        "duration": 3.88,
        "text": "repositories or anything like that so we"
      },
      {
        "start": 980.04,
        "duration": 3.76,
        "text": "have to go ahead with some kind of"
      },
      {
        "start": 981.279,
        "duration": 5.12,
        "text": "approach where we have to add noes syn"
      },
      {
        "start": 983.8,
        "duration": 4.44,
        "text": "the data and then decommission old noes"
      },
      {
        "start": 986.399,
        "duration": 4.321,
        "text": "that was think close to 3 to four months"
      },
      {
        "start": 988.24,
        "duration": 4.0,
        "text": "actually but it's very hard like and we"
      },
      {
        "start": 990.72,
        "duration": 4.2,
        "text": "can't just uh leave the second"
      },
      {
        "start": 992.24,
        "duration": 5.2,
        "text": "vulnerabilities as well right so we we"
      },
      {
        "start": 994.92,
        "duration": 5.64,
        "text": "use the last pickles the same the data"
      },
      {
        "start": 997.44,
        "duration": 5.44,
        "text": "tax related block uh we try to map the"
      },
      {
        "start": 1000.56,
        "duration": 5.36,
        "text": "old device like and we try to detach the"
      },
      {
        "start": 1002.88,
        "duration": 4.759,
        "text": "dis from the old device I mean old node"
      },
      {
        "start": 1005.92,
        "duration": 4.479,
        "text": "and then trigger the shutdown process"
      },
      {
        "start": 1007.639,
        "duration": 5.081,
        "text": "from the new node and then detach the"
      },
      {
        "start": 1010.399,
        "duration": 4.0,
        "text": "desk attach it to the new node which has"
      },
      {
        "start": 1012.72,
        "duration": 5.4,
        "text": "a latest security"
      },
      {
        "start": 1014.399,
        "duration": 5.201,
        "text": "vulnerabilities and then uh install KAS"
      },
      {
        "start": 1018.12,
        "duration": 3.44,
        "text": "and then start it back"
      },
      {
        "start": 1019.6,
        "duration": 4.439,
        "text": "and then uh like it would uh the con"
      },
      {
        "start": 1021.56,
        "duration": 4.84,
        "text": "folder would be ritten because it even"
      },
      {
        "start": 1024.039,
        "duration": 5.481,
        "text": "though the data is actually part of uh"
      },
      {
        "start": 1026.4,
        "duration": 5.24,
        "text": "Olden node when it uh takes a metadata"
      },
      {
        "start": 1029.52,
        "duration": 4.12,
        "text": "of it and make sure that the IP address"
      },
      {
        "start": 1031.64,
        "duration": 4.12,
        "text": "is updated then automatically it comes"
      },
      {
        "start": 1033.64,
        "duration": 5.6,
        "text": "back up normally I have a demo as well"
      },
      {
        "start": 1035.76,
        "duration": 6.039,
        "text": "you'll be seeing it soon and uh uh it"
      },
      {
        "start": 1039.24,
        "duration": 4.92,
        "text": "would validate the old id to be gone and"
      },
      {
        "start": 1041.799,
        "duration": 4.961,
        "text": "then it would join to the cluster this"
      },
      {
        "start": 1044.16,
        "duration": 4.519,
        "text": "was this reduced our effort from 3 to 4"
      },
      {
        "start": 1046.76,
        "duration": 2.88,
        "text": "months to just hardly like a day or 2"
      },
      {
        "start": 1048.679,
        "duration": 3.401,
        "text": "days"
      },
      {
        "start": 1049.64,
        "duration": 4.96,
        "text": "so across a 4,000 nodes we generally do"
      },
      {
        "start": 1052.08,
        "duration": 5.64,
        "text": "maybe twice or thce a year for this kind"
      },
      {
        "start": 1054.6,
        "duration": 5.36,
        "text": "of ruling redeploy so it's uh we are"
      },
      {
        "start": 1057.72,
        "duration": 3.0,
        "text": "pretty much competent to kind of SEC and"
      },
      {
        "start": 1059.96,
        "duration": 5.0,
        "text": "that"
      },
      {
        "start": 1060.72,
        "duration": 7.44,
        "text": "stuff so here like we have a"
      },
      {
        "start": 1064.96,
        "duration": 5.32,
        "text": "um uh two data center cluster with three"
      },
      {
        "start": 1068.16,
        "duration": 4.48,
        "text": "nodes in each data center one is in"
      },
      {
        "start": 1070.28,
        "duration": 5.279,
        "text": "Central and one is in East so here you"
      },
      {
        "start": 1072.64,
        "duration": 8.24,
        "text": "can see I'm doing the cluster redeploys"
      },
      {
        "start": 1075.559,
        "duration": 9.841,
        "text": "on a central region so so uh you can see"
      },
      {
        "start": 1080.88,
        "duration": 7.4,
        "text": "that 1049 81 238 has gone down so when"
      },
      {
        "start": 1085.4,
        "duration": 5.759,
        "text": "it goes down uh as it is only we are"
      },
      {
        "start": 1088.28,
        "duration": 4.759,
        "text": "doing one node at a time uh you wouldn't"
      },
      {
        "start": 1091.159,
        "duration": 3.721,
        "text": "have any impact of business because they"
      },
      {
        "start": 1093.039,
        "duration": 3.481,
        "text": "would be using local kodum unless and"
      },
      {
        "start": 1094.88,
        "duration": 4.039,
        "text": "until if there are any applications"
      },
      {
        "start": 1096.52,
        "duration": 4.159,
        "text": "which use all that time we do it"
      },
      {
        "start": 1098.919,
        "duration": 4.0,
        "text": "generally during off business hour and"
      },
      {
        "start": 1100.679,
        "duration": 4.401,
        "text": "make sure that uh none of the like all"
      },
      {
        "start": 1102.919,
        "duration": 4.321,
        "text": "kind of consisten is running because"
      },
      {
        "start": 1105.08,
        "duration": 3.839,
        "text": "there there would be errors and all so"
      },
      {
        "start": 1107.24,
        "duration": 5.72,
        "text": "what's happening now behind the scenes"
      },
      {
        "start": 1108.919,
        "duration": 5.601,
        "text": "is the new node is uh coming up and uh"
      },
      {
        "start": 1112.96,
        "duration": 6.4,
        "text": "the new node has shut down this"
      },
      {
        "start": 1114.52,
        "duration": 6.68,
        "text": "particular 1049 81 238 in F A of central"
      },
      {
        "start": 1119.36,
        "duration": 4.199,
        "text": "it's going down gracefully shutting down"
      },
      {
        "start": 1121.2,
        "duration": 4.76,
        "text": "all the services on that particular node"
      },
      {
        "start": 1123.559,
        "duration": 5.081,
        "text": "and then it would detach the disk and"
      },
      {
        "start": 1125.96,
        "duration": 5.0,
        "text": "then attach it to the new node and then"
      },
      {
        "start": 1128.64,
        "duration": 4.48,
        "text": "it would automatically come back up so"
      },
      {
        "start": 1130.96,
        "duration": 4.4,
        "text": "if you just give uh yeah you can see f"
      },
      {
        "start": 1133.12,
        "duration": 6.0,
        "text": "is back up again on new node which is"
      },
      {
        "start": 1135.36,
        "duration": 6.679,
        "text": "like 1049 80 and 131 so it would have"
      },
      {
        "start": 1139.12,
        "duration": 6.0,
        "text": "the same data and then join to the"
      },
      {
        "start": 1142.039,
        "duration": 5.961,
        "text": "cluster with a new IP and all the rest"
      },
      {
        "start": 1145.12,
        "duration": 5.24,
        "text": "of the uh cluster members also use that"
      },
      {
        "start": 1148.0,
        "duration": 4.96,
        "text": "IP and make sure that they remove uh"
      },
      {
        "start": 1150.36,
        "duration": 5.52,
        "text": "from the gossip of the older IP and then"
      },
      {
        "start": 1152.96,
        "duration": 5.12,
        "text": "add to the new IP so slowly we can do"
      },
      {
        "start": 1155.88,
        "duration": 6.039,
        "text": "this uh approach for all the ones now"
      },
      {
        "start": 1158.08,
        "duration": 8.36,
        "text": "the B1 has called on your central B1"
      },
      {
        "start": 1161.919,
        "duration": 7.041,
        "text": "which is like 1049 81 and 240 so this IP"
      },
      {
        "start": 1166.44,
        "duration": 6.0,
        "text": "has gone down so the same proc process"
      },
      {
        "start": 1168.96,
        "duration": 8.32,
        "text": "is being repeated so uh generally at"
      },
      {
        "start": 1172.44,
        "duration": 7.4,
        "text": "Target we maintain our uh regions and"
      },
      {
        "start": 1177.28,
        "duration": 6.399,
        "text": "like replication would be spread across"
      },
      {
        "start": 1179.84,
        "duration": 5.12,
        "text": "multiple AES by default so uh even"
      },
      {
        "start": 1183.679,
        "duration": 3.441,
        "text": "though when there are any region"
      },
      {
        "start": 1184.96,
        "duration": 4.76,
        "text": "failovers they can cut over to the other"
      },
      {
        "start": 1187.12,
        "duration": 4.6,
        "text": "region but if there are any uh easy"
      },
      {
        "start": 1189.72,
        "duration": 3.6,
        "text": "failovers sometimes it does happen I"
      },
      {
        "start": 1191.72,
        "duration": 4.24,
        "text": "think in the last 3 to four years I"
      },
      {
        "start": 1193.32,
        "duration": 4.12,
        "text": "think it has happened only once but like"
      },
      {
        "start": 1195.96,
        "duration": 3.64,
        "text": "uh none of our applications had any"
      },
      {
        "start": 1197.44,
        "duration": 4.8,
        "text": "issues actually because they were using"
      },
      {
        "start": 1199.6,
        "duration": 4.959,
        "text": "local Kum and even though one a is not"
      },
      {
        "start": 1202.24,
        "duration": 5.36,
        "text": "available still it would be able to"
      },
      {
        "start": 1204.559,
        "duration": 5.561,
        "text": "succeed with all the wrs or reads or"
      },
      {
        "start": 1207.6,
        "duration": 4.4,
        "text": "anything and uh uh it wasn't for a long"
      },
      {
        "start": 1210.12,
        "duration": 3.72,
        "text": "time it came back within 3 to 5 minutes"
      },
      {
        "start": 1212.0,
        "duration": 3.44,
        "text": "and then like uh it would have even"
      },
      {
        "start": 1213.84,
        "duration": 4.0,
        "text": "though there are any rights happening it"
      },
      {
        "start": 1215.44,
        "duration": 4.2,
        "text": "would automatically May divert the hints"
      },
      {
        "start": 1217.84,
        "duration": 3.16,
        "text": "to the corresponding node and it should"
      },
      {
        "start": 1219.64,
        "duration": 4.88,
        "text": "be back up so you can see that b is"
      },
      {
        "start": 1221.0,
        "duration": 6.96,
        "text": "already back up now so like this we have"
      },
      {
        "start": 1224.52,
        "duration": 5.76,
        "text": "like when we have a 400 node cluster uh"
      },
      {
        "start": 1227.96,
        "duration": 4.8,
        "text": "we can do the entire 400 nodes within"
      },
      {
        "start": 1230.28,
        "duration": 4.8,
        "text": "maybe 3 to 8 hours before it was taking"
      },
      {
        "start": 1232.76,
        "duration": 5.64,
        "text": "like 1 month or 1 and a half month which"
      },
      {
        "start": 1235.08,
        "duration": 7.16,
        "text": "reduced our uh effort from that amount"
      },
      {
        "start": 1238.4,
        "duration": 6.36,
        "text": "of time to only like U 3 to uh 6 hours"
      },
      {
        "start": 1242.24,
        "duration": 5.0,
        "text": "or 8 hours it's all that was really nice"
      },
      {
        "start": 1244.76,
        "duration": 5.24,
        "text": "where uh where we were able to get this"
      },
      {
        "start": 1247.24,
        "duration": 5.0,
        "text": "one but like uh here the interesting"
      },
      {
        "start": 1250.0,
        "duration": 5.679,
        "text": "thing is uh when I say like 100 plus"
      },
      {
        "start": 1252.24,
        "duration": 6.36,
        "text": "clust 100 plus clusters are not of the"
      },
      {
        "start": 1255.679,
        "duration": 4.601,
        "text": "same kind of a configuration so if you"
      },
      {
        "start": 1258.6,
        "duration": 5.36,
        "text": "you are like three node clusters few are"
      },
      {
        "start": 1260.28,
        "duration": 5.519,
        "text": "10 uh um like 30 node few are like 100"
      },
      {
        "start": 1263.96,
        "duration": 3.88,
        "text": "if few are like 200 few are 350 or"
      },
      {
        "start": 1265.799,
        "duration": 5.041,
        "text": "anything like that but when we talk"
      },
      {
        "start": 1267.84,
        "duration": 4.88,
        "text": "about like uh we have SSL we have nons"
      },
      {
        "start": 1270.84,
        "duration": 5.0,
        "text": "socks and non socks as well and it's"
      },
      {
        "start": 1272.72,
        "duration": 6.12,
        "text": "spread across multiple domains we use"
      },
      {
        "start": 1275.84,
        "duration": 5.12,
        "text": "only a single Docker image we thought of"
      },
      {
        "start": 1278.84,
        "duration": 3.839,
        "text": "using the community one but we had a lot"
      },
      {
        "start": 1280.96,
        "duration": 3.76,
        "text": "of customization so we started from"
      },
      {
        "start": 1282.679,
        "duration": 6.0,
        "text": "scratch so we use a kind of like a"
      },
      {
        "start": 1284.72,
        "duration": 5.52,
        "text": "Alpine based uh Docker image with which"
      },
      {
        "start": 1288.679,
        "duration": 4.801,
        "text": "it would have so you can see that c is"
      },
      {
        "start": 1290.24,
        "duration": 6.76,
        "text": "also backup So within 3 to 5 minutes all"
      },
      {
        "start": 1293.48,
        "duration": 6.079,
        "text": "the three nodes have been um migrated to"
      },
      {
        "start": 1297.0,
        "duration": 5.88,
        "text": "new set of noes with which has the kind"
      },
      {
        "start": 1299.559,
        "duration": 5.36,
        "text": "of new either new jbm version or even"
      },
      {
        "start": 1302.88,
        "duration": 4.36,
        "text": "the security vulnerabilities in this"
      },
      {
        "start": 1304.919,
        "duration": 5.081,
        "text": "approach it was very easy for us to do"
      },
      {
        "start": 1307.24,
        "duration": 5.36,
        "text": "the redeploys and we trying to compete"
      },
      {
        "start": 1310.0,
        "duration": 6.4,
        "text": "with application teams for even kind of"
      },
      {
        "start": 1312.6,
        "duration": 3.8,
        "text": "C Level DLo how it can be done"
      },
      {
        "start": 1317.279,
        "duration": 8.241,
        "text": "easily so so yeah and the uh one thing"
      },
      {
        "start": 1322.36,
        "duration": 5.76,
        "text": "when we come down to Docker is um look"
      },
      {
        "start": 1325.52,
        "duration": 4.72,
        "text": "uh let's say we try to deploy kasan to"
      },
      {
        "start": 1328.12,
        "duration": 4.2,
        "text": "kubernetes through Helm or anything and"
      },
      {
        "start": 1330.24,
        "duration": 4.0,
        "text": "then on the back end uh you have a"
      },
      {
        "start": 1332.32,
        "duration": 5.16,
        "text": "storage component which goes back and"
      },
      {
        "start": 1334.24,
        "duration": 5.48,
        "text": "forth and all so over here you would not"
      },
      {
        "start": 1337.48,
        "duration": 5.92,
        "text": "be able to log into the P or log into"
      },
      {
        "start": 1339.72,
        "duration": 5.72,
        "text": "the Container how do you actually get"
      },
      {
        "start": 1343.4,
        "duration": 3.84,
        "text": "some information or kind of like Ops"
      },
      {
        "start": 1345.44,
        "duration": 4.4,
        "text": "activities or run some certain set of"
      },
      {
        "start": 1347.24,
        "duration": 5.84,
        "text": "commands on on a particular note to get"
      },
      {
        "start": 1349.84,
        "duration": 5.76,
        "text": "some information so uh again this is a"
      },
      {
        "start": 1353.08,
        "duration": 5.16,
        "text": "kind of like a custom buildt for our"
      },
      {
        "start": 1355.6,
        "duration": 6.28,
        "text": "team which is written again in go uh"
      },
      {
        "start": 1358.24,
        "duration": 5.84,
        "text": "zanu so uh one of our TLP and the"
      },
      {
        "start": 1361.88,
        "duration": 4.039,
        "text": "chickie was written by Rebecca sin and"
      },
      {
        "start": 1364.08,
        "duration": 5.16,
        "text": "she was a pretty good engineer and"
      },
      {
        "start": 1365.919,
        "duration": 5.24,
        "text": "zanadu was written by two of our T so it"
      },
      {
        "start": 1369.24,
        "duration": 4.16,
        "text": "was like uh everyone was picking up the"
      },
      {
        "start": 1371.159,
        "duration": 4.52,
        "text": "kind of new uh innovative ideas and all"
      },
      {
        "start": 1373.4,
        "duration": 5.48,
        "text": "that stuff so so this one has the"
      },
      {
        "start": 1375.679,
        "duration": 5.12,
        "text": "capability of getting the commands or"
      },
      {
        "start": 1378.88,
        "duration": 4.48,
        "text": "executing some certain set of commands"
      },
      {
        "start": 1380.799,
        "duration": 5.0,
        "text": "or even just reading out some like for"
      },
      {
        "start": 1383.36,
        "duration": 5.16,
        "text": "example kandl and"
      },
      {
        "start": 1385.799,
        "duration": 5.641,
        "text": "kasanda. so these two are the pretty"
      },
      {
        "start": 1388.52,
        "duration": 5.56,
        "text": "much main important configuration files"
      },
      {
        "start": 1391.44,
        "duration": 5.359,
        "text": "we use it on kasanda uh for altering"
      },
      {
        "start": 1394.08,
        "duration": 4.64,
        "text": "anything because our log logb back. XML"
      },
      {
        "start": 1396.799,
        "duration": 4.521,
        "text": "is almost pretty much same across all"
      },
      {
        "start": 1398.72,
        "duration": 4.24,
        "text": "the Clusters so that uh it would be"
      },
      {
        "start": 1401.32,
        "duration": 3.64,
        "text": "integrated part of the container so it"
      },
      {
        "start": 1402.96,
        "duration": 5.04,
        "text": "shouldn't have any issues so these three"
      },
      {
        "start": 1404.96,
        "duration": 4.199,
        "text": "ones if we have you update any of the uh"
      },
      {
        "start": 1408.0,
        "duration": 3.48,
        "text": "hip"
      },
      {
        "start": 1409.159,
        "duration": 5.321,
        "text": "then uh the uh these locations we"
      },
      {
        "start": 1411.48,
        "duration": 6.0,
        "text": "generally use it on and we use even"
      },
      {
        "start": 1414.48,
        "duration": 4.72,
        "text": "zanor even for healing system of theas"
      },
      {
        "start": 1417.48,
        "duration": 5.12,
        "text": "sometimes let's say there were Network"
      },
      {
        "start": 1419.2,
        "duration": 5.88,
        "text": "lws for the uh and your network package"
      },
      {
        "start": 1422.6,
        "duration": 4.64,
        "text": "are lost and when that's the time when"
      },
      {
        "start": 1425.08,
        "duration": 4.92,
        "text": "you are updating a password or anything"
      },
      {
        "start": 1427.24,
        "duration": 4.319,
        "text": "then automatically uh the actual nodes"
      },
      {
        "start": 1430.0,
        "duration": 4.64,
        "text": "which are having it there might be the"
      },
      {
        "start": 1431.559,
        "duration": 5.24,
        "text": "system authoriz so uh it does Auto heal"
      },
      {
        "start": 1434.64,
        "duration": 4.639,
        "text": "it as well as when it tries to check it"
      },
      {
        "start": 1436.799,
        "duration": 5.12,
        "text": "and then automatically clear it off"
      },
      {
        "start": 1439.279,
        "duration": 4.801,
        "text": "and it uses a kind of a uh regular"
      },
      {
        "start": 1441.919,
        "duration": 3.88,
        "text": "mechanism of client token and password"
      },
      {
        "start": 1444.08,
        "duration": 4.68,
        "text": "or yeah it can be integrated to even"
      },
      {
        "start": 1445.799,
        "duration": 5.441,
        "text": "ldap so there bunch of options which you"
      },
      {
        "start": 1448.76,
        "duration": 5.96,
        "text": "can use authentication"
      },
      {
        "start": 1451.24,
        "duration": 5.48,
        "text": "mechanism so just to give a overall"
      },
      {
        "start": 1454.72,
        "duration": 4.52,
        "text": "performances what we have done it with"
      },
      {
        "start": 1456.72,
        "duration": 5.36,
        "text": "Docker is actually like it was amazing"
      },
      {
        "start": 1459.24,
        "duration": 5.24,
        "text": "actually so we have seen uh small better"
      },
      {
        "start": 1462.08,
        "duration": 4.4,
        "text": "performance on Docker that we can't"
      },
      {
        "start": 1464.48,
        "duration": 4.28,
        "text": "explain because the underlying compute"
      },
      {
        "start": 1466.48,
        "duration": 4.6,
        "text": "and the back end was completely"
      },
      {
        "start": 1468.76,
        "duration": 4.84,
        "text": "constantly changing evolving uh because"
      },
      {
        "start": 1471.08,
        "duration": 4.959,
        "text": "those teams were also evolving at all so"
      },
      {
        "start": 1473.6,
        "duration": 5.88,
        "text": "we had like a 300K reads per second on"
      },
      {
        "start": 1476.039,
        "duration": 5.921,
        "text": "just the 30 node or 39 node plus uh 39"
      },
      {
        "start": 1479.48,
        "duration": 5.0,
        "text": "node data center it was able to handle"
      },
      {
        "start": 1481.96,
        "duration": 5.079,
        "text": "uh 300K reads per second and the same"
      },
      {
        "start": 1484.48,
        "duration": 5.76,
        "text": "thing was able to handle 150 K rights"
      },
      {
        "start": 1487.039,
        "duration": 6.321,
        "text": "per second so and we generally all our"
      },
      {
        "start": 1490.24,
        "duration": 5.72,
        "text": "clusters are actually multi-data Center"
      },
      {
        "start": 1493.36,
        "duration": 5.919,
        "text": "so even though one data center is off"
      },
      {
        "start": 1495.96,
        "duration": 4.48,
        "text": "due to uh infra failure or something"
      },
      {
        "start": 1499.279,
        "duration": 2.4,
        "text": "then automatically they can switch it"
      },
      {
        "start": 1500.44,
        "duration": 3.64,
        "text": "over and they have that capability"
      },
      {
        "start": 1501.679,
        "duration": 4.36,
        "text": "already and the latencies for the"
      },
      {
        "start": 1504.08,
        "duration": 3.719,
        "text": "similar for the same cluster which had"
      },
      {
        "start": 1506.039,
        "duration": 3.841,
        "text": "this this amount of reads and rights"
      },
      {
        "start": 1507.799,
        "duration": 4.201,
        "text": "were just like 1 millisecond to 300"
      },
      {
        "start": 1509.88,
        "duration": 4.0,
        "text": "milliseconds on an average basing on"
      },
      {
        "start": 1512.0,
        "duration": 4.48,
        "text": "like what's the load would change and"
      },
      {
        "start": 1513.88,
        "duration": 5.2,
        "text": "the right latencies were similar to 1 to"
      },
      {
        "start": 1516.48,
        "duration": 7.04,
        "text": "100 milliseconds we know that kasanda is"
      },
      {
        "start": 1519.08,
        "duration": 6.92,
        "text": "actually a uh uh easy right kind of a"
      },
      {
        "start": 1523.52,
        "duration": 5.2,
        "text": "database and reads a little costly and"
      },
      {
        "start": 1526.0,
        "duration": 5.08,
        "text": "all uh it was almost uh for as it was"
      },
      {
        "start": 1528.72,
        "duration": 4.72,
        "text": "competing and uh we try to make sure we"
      },
      {
        "start": 1531.08,
        "duration": 5.079,
        "text": "try to properly scale it and not"
      },
      {
        "start": 1533.44,
        "duration": 7.04,
        "text": "overscale it for getting it"
      },
      {
        "start": 1536.159,
        "duration": 6.961,
        "text": "done so last like um the use cases what"
      },
      {
        "start": 1540.48,
        "duration": 6.28,
        "text": "we use for kasanda tablet are basically"
      },
      {
        "start": 1543.12,
        "duration": 5.559,
        "text": "we use it for pricing like uh uh for"
      },
      {
        "start": 1546.76,
        "duration": 3.6,
        "text": "digital as well as like for the stores"
      },
      {
        "start": 1548.679,
        "duration": 4.561,
        "text": "wherever it the pricing information"
      },
      {
        "start": 1550.36,
        "duration": 5.48,
        "text": "comes from Kasandra and the cart where"
      },
      {
        "start": 1553.24,
        "duration": 5.88,
        "text": "like uh either through uh the stores the"
      },
      {
        "start": 1555.84,
        "duration": 6.199,
        "text": "sell checkout or even uh online digital"
      },
      {
        "start": 1559.12,
        "duration": 6.559,
        "text": "ones those ones also use S"
      },
      {
        "start": 1562.039,
        "duration": 6.24,
        "text": "recommendations so um any of the uh for"
      },
      {
        "start": 1565.679,
        "duration": 6.561,
        "text": "you offers uh like whatever you see it"
      },
      {
        "start": 1568.279,
        "duration": 5.321,
        "text": "on uh previously it was uh uh we used to"
      },
      {
        "start": 1572.24,
        "duration": 3.319,
        "text": "find the cartwheel and then the red"
      },
      {
        "start": 1573.6,
        "duration": 4.04,
        "text": "perks and then finally loyalty program"
      },
      {
        "start": 1575.559,
        "duration": 4.641,
        "text": "there was related information all those"
      },
      {
        "start": 1577.64,
        "duration": 4.159,
        "text": "ones are also part of uhand which comes"
      },
      {
        "start": 1580.2,
        "duration": 4.64,
        "text": "in the inventory management which is"
      },
      {
        "start": 1581.799,
        "duration": 6.081,
        "text": "spread across uh distribution centers as"
      },
      {
        "start": 1584.84,
        "duration": 3.959,
        "text": "stores so it's basically kasanda has got"
      },
      {
        "start": 1587.88,
        "duration": 3.76,
        "text": "into"
      },
      {
        "start": 1588.799,
        "duration": 5.48,
        "text": "the the applications have wide we which"
      },
      {
        "start": 1591.64,
        "duration": 5.399,
        "text": "is digital as well as uh distribution"
      },
      {
        "start": 1594.279,
        "duration": 5.321,
        "text": "centers and stores uh in fact the recent"
      },
      {
        "start": 1597.039,
        "duration": 5.76,
        "text": "one which is like a driver uh 3 years"
      },
      {
        "start": 1599.6,
        "duration": 5.679,
        "text": "back we started this and uh uh it's been"
      },
      {
        "start": 1602.799,
        "duration": 5.321,
        "text": "pretty good like uh we have seen how it"
      },
      {
        "start": 1605.279,
        "duration": 4.961,
        "text": "has grown uh literally like threefold"
      },
      {
        "start": 1608.12,
        "duration": 4.799,
        "text": "over the last like once the co started"
      },
      {
        "start": 1610.24,
        "duration": 5.28,
        "text": "and all this so coupons related"
      },
      {
        "start": 1612.919,
        "duration": 5.0,
        "text": "promotions like during uh it's a"
      },
      {
        "start": 1615.52,
        "duration": 4.279,
        "text": "Thanksgiving time and so we see it's a"
      },
      {
        "start": 1617.919,
        "duration": 3.76,
        "text": "kind of PE and we see that promotions"
      },
      {
        "start": 1619.799,
        "duration": 3.561,
        "text": "and all that stuff and the profile"
      },
      {
        "start": 1621.679,
        "duration": 5.761,
        "text": "information that's the way like way we"
      },
      {
        "start": 1623.36,
        "duration": 6.199,
        "text": "use uh uh uh SSL part mainly and even"
      },
      {
        "start": 1627.44,
        "duration": 4.92,
        "text": "the pricing so majority of the Clusters"
      },
      {
        "start": 1629.559,
        "duration": 4.84,
        "text": "are actually SSL part and uh which are"
      },
      {
        "start": 1632.36,
        "duration": 5.6,
        "text": "the internal back office generally those"
      },
      {
        "start": 1634.399,
        "duration": 7.0,
        "text": "are kind of normal so uh other ones are"
      },
      {
        "start": 1637.96,
        "duration": 6.68,
        "text": "the TV media where you see the target"
      },
      {
        "start": 1641.399,
        "duration": 8.441,
        "text": "marketing related uh media so those ones"
      },
      {
        "start": 1644.64,
        "duration": 9.32,
        "text": "are also part of uh Kanda itself so"
      },
      {
        "start": 1649.84,
        "duration": 7.52,
        "text": "so over this uh Journey like uh a target"
      },
      {
        "start": 1653.96,
        "duration": 4.8,
        "text": "for on kasanda it was like uh we learned"
      },
      {
        "start": 1657.36,
        "duration": 3.12,
        "text": "a lot actually because when you're"
      },
      {
        "start": 1658.76,
        "duration": 3.56,
        "text": "trying to come from a relational"
      },
      {
        "start": 1660.48,
        "duration": 4.48,
        "text": "background and non- relational"
      },
      {
        "start": 1662.32,
        "duration": 4.68,
        "text": "background both are actually completely"
      },
      {
        "start": 1664.96,
        "duration": 5.0,
        "text": "different and they were pretty much good"
      },
      {
        "start": 1667.0,
        "duration": 4.96,
        "text": "at their own features actually for"
      },
      {
        "start": 1669.96,
        "duration": 4.559,
        "text": "example the multi clusters initially we"
      },
      {
        "start": 1671.96,
        "duration": 4.68,
        "text": "tried to use it like how Oracle hasard"
      },
      {
        "start": 1674.519,
        "duration": 5.121,
        "text": "like where we try to limit resources for"
      },
      {
        "start": 1676.64,
        "duration": 5.32,
        "text": "a particular schema but on kand we can't"
      },
      {
        "start": 1679.64,
        "duration": 3.56,
        "text": "so if you have multiple schemas or used"
      },
      {
        "start": 1681.96,
        "duration": 4.12,
        "text": "by multiple"
      },
      {
        "start": 1683.2,
        "duration": 7.28,
        "text": "applications uh if an application tries"
      },
      {
        "start": 1686.08,
        "duration": 6.56,
        "text": "to uh pull out like tries to run a anti"
      },
      {
        "start": 1690.48,
        "duration": 4.439,
        "text": "pattern query which uses lot amount of"
      },
      {
        "start": 1692.64,
        "duration": 4.0,
        "text": "resources then you see other schema"
      },
      {
        "start": 1694.919,
        "duration": 4.041,
        "text": "other applications also seeing the"
      },
      {
        "start": 1696.64,
        "duration": 3.72,
        "text": "degraded performance the reason is"
      },
      {
        "start": 1698.96,
        "duration": 3.4,
        "text": "because it's a kind of shade and we"
      },
      {
        "start": 1700.36,
        "duration": 5.64,
        "text": "can't put a cap for any kind of schema"
      },
      {
        "start": 1702.36,
        "duration": 5.439,
        "text": "related on uh for socks approaches uh we"
      },
      {
        "start": 1706.0,
        "duration": 4.44,
        "text": "have seen that uh the service account"
      },
      {
        "start": 1707.799,
        "duration": 5.321,
        "text": "which you users uh for authentication on"
      },
      {
        "start": 1710.44,
        "duration": 4.92,
        "text": "the application side uh like it gets"
      },
      {
        "start": 1713.12,
        "duration": 4.24,
        "text": "locked out sometimes the reason is"
      },
      {
        "start": 1715.36,
        "duration": 3.24,
        "text": "because incorrect password or bad cache"
      },
      {
        "start": 1717.36,
        "duration": 2.88,
        "text": "for the password which is trying to"
      },
      {
        "start": 1718.6,
        "duration": 4.0,
        "text": "authenticate and if the password has"
      },
      {
        "start": 1720.24,
        "duration": 4.6,
        "text": "been changed recently then all those"
      },
      {
        "start": 1722.6,
        "duration": 5.12,
        "text": "ones are those kind of Integrations"
      },
      {
        "start": 1724.84,
        "duration": 4.6,
        "text": "where a database team as well as uh"
      },
      {
        "start": 1727.72,
        "duration": 3.6,
        "text": "application teams have to collaborate"
      },
      {
        "start": 1729.44,
        "duration": 4.4,
        "text": "and do a few of the things for sure like"
      },
      {
        "start": 1731.32,
        "duration": 5.68,
        "text": "those ones we learn that we have to be"
      },
      {
        "start": 1733.84,
        "duration": 6.12,
        "text": "more collaborative and also and uh you"
      },
      {
        "start": 1737.0,
        "duration": 7.399,
        "text": "know that uh uh GMX provides a very good"
      },
      {
        "start": 1739.96,
        "duration": 6.319,
        "text": "way of metrics so uh for the larger"
      },
      {
        "start": 1744.399,
        "duration": 4.12,
        "text": "clusters not even for larger clusters so"
      },
      {
        "start": 1746.279,
        "duration": 5.24,
        "text": "even for three nodes when one particular"
      },
      {
        "start": 1748.519,
        "duration": 4.88,
        "text": "node goes down all the three nodes would"
      },
      {
        "start": 1751.519,
        "duration": 3.481,
        "text": "eject the message that okay one node is"
      },
      {
        "start": 1753.399,
        "duration": 3.561,
        "text": "down one node is down so when you try to"
      },
      {
        "start": 1755.0,
        "duration": 4.2,
        "text": "set up an alert it's very complicated"
      },
      {
        "start": 1756.96,
        "duration": 3.92,
        "text": "because you get three kind of alerts and"
      },
      {
        "start": 1759.2,
        "duration": 4.44,
        "text": "you have to group by on the particular"
      },
      {
        "start": 1760.88,
        "duration": 4.399,
        "text": "cluster so it was it was becom more"
      },
      {
        "start": 1763.64,
        "duration": 4.96,
        "text": "complicated so we found effective ways"
      },
      {
        "start": 1765.279,
        "duration": 6.64,
        "text": "for utilizing them so and then fin the"
      },
      {
        "start": 1768.6,
        "duration": 5.799,
        "text": "kind of uh uh any VI performance or the"
      },
      {
        "start": 1771.919,
        "duration": 6.201,
        "text": "like when we have seen it would be"
      },
      {
        "start": 1774.399,
        "duration": 5.841,
        "text": "either antia query or secondary indexes"
      },
      {
        "start": 1778.12,
        "duration": 4.799,
        "text": "or materialized use or the kind of"
      },
      {
        "start": 1780.24,
        "duration": 4.52,
        "text": "overloading or even like udfs or"
      },
      {
        "start": 1782.919,
        "duration": 3.36,
        "text": "collection data types or anything so"
      },
      {
        "start": 1784.76,
        "duration": 4.32,
        "text": "it's basically like you have to use"
      },
      {
        "start": 1786.279,
        "duration": 4.921,
        "text": "cassand as a kind of vanilla one not any"
      },
      {
        "start": 1789.08,
        "duration": 5.64,
        "text": "kind of additional one then you wouldn't"
      },
      {
        "start": 1791.2,
        "duration": 6.88,
        "text": "have any kind of uh uh barriers for like"
      },
      {
        "start": 1794.72,
        "duration": 4.48,
        "text": "for the same amount of 2 so for example"
      },
      {
        "start": 1798.08,
        "duration": 3.439,
        "text": "like uh the"
      },
      {
        "start": 1799.2,
        "duration": 4.8,
        "text": "150k uh application which I was talking"
      },
      {
        "start": 1801.519,
        "duration": 4.721,
        "text": "about the rights per second so they"
      },
      {
        "start": 1804.0,
        "duration": 4.08,
        "text": "didn't have any of these ones and with"
      },
      {
        "start": 1806.24,
        "duration": 5.159,
        "text": "only 30 nodes they were able to survive"
      },
      {
        "start": 1808.08,
        "duration": 7.24,
        "text": "with the kind of load so a similar one"
      },
      {
        "start": 1811.399,
        "duration": 6.0,
        "text": "not even 150k it was like only 30 to 50K"
      },
      {
        "start": 1815.32,
        "duration": 5.0,
        "text": "reads per second or wrs per"
      },
      {
        "start": 1817.399,
        "duration": 4.961,
        "text": "second they were requiring close to 99"
      },
      {
        "start": 1820.32,
        "duration": 3.52,
        "text": "noes which is almost like three times"
      },
      {
        "start": 1822.36,
        "duration": 3.0,
        "text": "the ones because they were using"
      },
      {
        "start": 1823.84,
        "duration": 4.16,
        "text": "secondary indexes and they were using"
      },
      {
        "start": 1825.36,
        "duration": 4.52,
        "text": "collection data types so that is"
      },
      {
        "start": 1828.0,
        "duration": 4.44,
        "text": "integrated uh to the architecture of"
      },
      {
        "start": 1829.88,
        "duration": 4.24,
        "text": "kasanda how it is built so for sure like"
      },
      {
        "start": 1832.44,
        "duration": 4.4,
        "text": "uh you can play it around but if you if"
      },
      {
        "start": 1834.12,
        "duration": 5.32,
        "text": "it's going to be more and more uh right"
      },
      {
        "start": 1836.84,
        "duration": 4.559,
        "text": "or through heavy then those ones it's"
      },
      {
        "start": 1839.44,
        "duration": 4.2,
        "text": "better to avoid it on all the stuff and"
      },
      {
        "start": 1841.399,
        "duration": 4.201,
        "text": "second indexes we know that it's pretty"
      },
      {
        "start": 1843.64,
        "duration": 3.919,
        "text": "easy for anyone to just uh create a"
      },
      {
        "start": 1845.6,
        "duration": 3.959,
        "text": "secondary index but you have to be very"
      },
      {
        "start": 1847.559,
        "duration": 4.801,
        "text": "cautious on which set of columns you"
      },
      {
        "start": 1849.559,
        "duration": 4.48,
        "text": "have to do the secondary indexes and all"
      },
      {
        "start": 1852.36,
        "duration": 7.039,
        "text": "so"
      },
      {
        "start": 1854.039,
        "duration": 7.52,
        "text": "yeah so finally yeah questions"
      },
      {
        "start": 1859.399,
        "duration": 3.801,
        "text": "let's see here um no one in chat has"
      },
      {
        "start": 1861.559,
        "duration": 3.561,
        "text": "asked many we got a couple nice"
      },
      {
        "start": 1863.2,
        "duration": 5.4,
        "text": "presentations and a couple of"
      },
      {
        "start": 1865.12,
        "duration": 5.08,
        "text": "congratulations so that's always good um"
      },
      {
        "start": 1868.6,
        "duration": 3.0,
        "text": "we'll give you a guys a minute here uh"
      },
      {
        "start": 1870.2,
        "duration": 4.04,
        "text": "feel free to put stuff in YouTube chat"
      },
      {
        "start": 1871.6,
        "duration": 6.039,
        "text": "we are watching it so um you know any"
      },
      {
        "start": 1874.24,
        "duration": 4.559,
        "text": "questions you've got uh just feel free"
      },
      {
        "start": 1877.639,
        "duration": 2.681,
        "text": "yeah it was interesting that that you"
      },
      {
        "start": 1878.799,
        "duration": 3.041,
        "text": "kind of had to almost go back to the"
      },
      {
        "start": 1880.32,
        "duration": 3.68,
        "text": "pure"
      },
      {
        "start": 1881.84,
        "duration": 4.439,
        "text": "Cassandra workload like you couldn't do"
      },
      {
        "start": 1884.0,
        "duration": 6.039,
        "text": "any of the the extra Frills but the the"
      },
      {
        "start": 1886.279,
        "duration": 4.921,
        "text": "Cassandra still worked as Sandra works"
      },
      {
        "start": 1890.039,
        "duration": 3.921,
        "text": "that's"
      },
      {
        "start": 1891.2,
        "duration": 4.319,
        "text": "yes that's true tic so if we tried to"
      },
      {
        "start": 1893.96,
        "duration": 5.079,
        "text": "explore like bunch of the other options"
      },
      {
        "start": 1895.519,
        "duration": 6.201,
        "text": "also uh like it does work perform well"
      },
      {
        "start": 1899.039,
        "duration": 4.921,
        "text": "but like after a certain limit it breaks"
      },
      {
        "start": 1901.72,
        "duration": 4.799,
        "text": "yeah for example like uh recently one of"
      },
      {
        "start": 1903.96,
        "duration": 5.48,
        "text": "our cluster uh entirely broke down"
      },
      {
        "start": 1906.519,
        "duration": 4.841,
        "text": "because of materialized use so they"
      },
      {
        "start": 1909.44,
        "duration": 4.64,
        "text": "created and then they dropped it but it"
      },
      {
        "start": 1911.36,
        "duration": 5.199,
        "text": "was a pretty big one so it jammed up all"
      },
      {
        "start": 1914.08,
        "duration": 5.24,
        "text": "the resources on all the notes it was it"
      },
      {
        "start": 1916.559,
        "duration": 4.24,
        "text": "was a big cluster was build of the index"
      },
      {
        "start": 1919.32,
        "duration": 5.8,
        "text": "or the"
      },
      {
        "start": 1920.799,
        "duration": 6.961,
        "text": "the yeah it was a kind of like re in the"
      },
      {
        "start": 1925.12,
        "duration": 3.88,
        "text": "so all that stuff was going down going"
      },
      {
        "start": 1927.76,
        "duration": 3.399,
        "text": "even though we tried to restart it we"
      },
      {
        "start": 1929.0,
        "duration": 3.88,
        "text": "were not able to figure so finally we"
      },
      {
        "start": 1931.159,
        "duration": 3.52,
        "text": "tried to stop everything from the"
      },
      {
        "start": 1932.88,
        "duration": 4.0,
        "text": "application side but it was a nonpr"
      },
      {
        "start": 1934.679,
        "duration": 3.24,
        "text": "environment so it was fine with all so"
      },
      {
        "start": 1936.88,
        "duration": 3.519,
        "text": "yeah yeah that's the place where it's"
      },
      {
        "start": 1937.919,
        "duration": 5.0,
        "text": "supposed to happen"
      },
      {
        "start": 1940.399,
        "duration": 5.721,
        "text": "right so the thoughts related ones when"
      },
      {
        "start": 1942.919,
        "duration": 5.12,
        "text": "we started this year it was uh every"
      },
      {
        "start": 1946.12,
        "duration": 3.96,
        "text": "time it was a little weird because when"
      },
      {
        "start": 1948.039,
        "duration": 4.6,
        "text": "we try to provision the new cluster from"
      },
      {
        "start": 1950.08,
        "duration": 4.319,
        "text": "our end they give us a service account"
      },
      {
        "start": 1952.639,
        "duration": 3.361,
        "text": "we give it but it doesn't work the"
      },
      {
        "start": 1954.399,
        "duration": 5.081,
        "text": "reason is because the the case is"
      },
      {
        "start": 1956.0,
        "duration": 5.6,
        "text": "different on the uh elap server so oh"
      },
      {
        "start": 1959.48,
        "duration": 3.679,
        "text": "wow oh that that's one of those little"
      },
      {
        "start": 1961.6,
        "duration": 4.679,
        "text": "pernicious things that you just would"
      },
      {
        "start": 1963.159,
        "duration": 5.24,
        "text": "never oh man that was"
      },
      {
        "start": 1966.279,
        "duration": 6.24,
        "text": "probably"
      },
      {
        "start": 1968.399,
        "duration": 6.88,
        "text": "yeah awesome well um I'm not seeing any"
      },
      {
        "start": 1972.519,
        "duration": 5.76,
        "text": "uh any questions here we're getting more"
      },
      {
        "start": 1975.279,
        "duration": 4.64,
        "text": "uh you are great you rock superb so so I"
      },
      {
        "start": 1978.279,
        "duration": 3.801,
        "text": "think it was a good"
      },
      {
        "start": 1979.919,
        "duration": 4.841,
        "text": "presentation this is really awesome"
      },
      {
        "start": 1982.08,
        "duration": 5.439,
        "text": "stuff um really cool that you guys are"
      },
      {
        "start": 1984.76,
        "duration": 5.12,
        "text": "using open source in such a a business"
      },
      {
        "start": 1987.519,
        "duration": 3.801,
        "text": "critical way too I'm a huge fan of Open"
      },
      {
        "start": 1989.88,
        "duration": 3.519,
        "text": "Source I don't know if you know this but"
      },
      {
        "start": 1991.32,
        "duration": 5.68,
        "text": "I actually uh I worked for a Target"
      },
      {
        "start": 1993.399,
        "duration": 6.721,
        "text": "store through college so oh"
      },
      {
        "start": 1997.0,
        "duration": 5.96,
        "text": "nice yeah long before my data stack"
      },
      {
        "start": 2000.12,
        "duration": 4.679,
        "text": "stays oh nice yeah I don't know telling"
      },
      {
        "start": 2002.96,
        "duration": 3.88,
        "text": "about you actually so like initially"
      },
      {
        "start": 2004.799,
        "duration": 4.521,
        "text": "when he came down for the data talks or"
      },
      {
        "start": 2006.84,
        "duration": 5.76,
        "text": "anything at Austin Tech and data Stacks"
      },
      {
        "start": 2009.32,
        "duration": 6.68,
        "text": "or hosting and all that stuff"
      },
      {
        "start": 2012.6,
        "duration": 5.919,
        "text": "yeah all right well if we don't have any"
      },
      {
        "start": 2016.0,
        "duration": 5.88,
        "text": "other questions here um I think we can"
      },
      {
        "start": 2018.519,
        "duration": 6.28,
        "text": "give it a a wrap um thank you so much"
      },
      {
        "start": 2021.88,
        "duration": 4.48,
        "text": "for presenting uh next time we have a"
      },
      {
        "start": 2024.799,
        "duration": 3.441,
        "text": "couple interesting things in the work"
      },
      {
        "start": 2026.36,
        "duration": 2.919,
        "text": "stay tuned there will be an announcement"
      },
      {
        "start": 2028.24,
        "duration": 3.84,
        "text": "going out on"
      },
      {
        "start": 2029.279,
        "duration": 5.681,
        "text": "Eventbrite um I know some some people at"
      },
      {
        "start": 2032.08,
        "duration": 5.199,
        "text": "some fairly uh other big Cassandra users"
      },
      {
        "start": 2034.96,
        "duration": 3.8,
        "text": "out there are are itching to get in here"
      },
      {
        "start": 2037.279,
        "duration": 3.721,
        "text": "and kind of share what they're doing"
      },
      {
        "start": 2038.76,
        "duration": 5.6,
        "text": "that's really really cool uh with"
      },
      {
        "start": 2041.0,
        "duration": 6.44,
        "text": "Cassandra so um with that I will bid you"
      },
      {
        "start": 2044.36,
        "duration": 5.44,
        "text": "all a uh a good night or good day I"
      },
      {
        "start": 2047.44,
        "duration": 7.8,
        "text": "suppose we're Global now aren't"
      },
      {
        "start": 2049.8,
        "duration": 5.44,
        "text": "we y see you thanks guys y bye bye"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T21:32:15.866374+00:00"
}