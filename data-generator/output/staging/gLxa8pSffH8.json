{
  "video_id": "gLxa8pSffH8",
  "title": "Using Unstructured for an End-to-End RAG Data Setup | Unstructured",
  "description": "In this session from RAG++ London, Ahmet Melek from Unstructured walks developers through the process of making data \"RAG ready\" (Retrieval-Augmented Generation). Learn how to efficiently extract data from multiple sources such as Slack, Notion, PDFs, and PowerPoints, and prepare it for use in large language models (LLMs).\n\nAhmet discusses the challenges of parsing unstructured data locked behind different formats and the innovative strategies Unstructured offers, such as their high-res and fast parsing options. He also demonstrates how to use Unstructured’s tools for data chunking, embedding, and uploading to various destinations like Astra DB.\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastaxdevs... \nTwitter:   / datastaxdevs  \nTwitch:   / datastaxdevs  \n\nABOUT DATASTAX\n➡️DataStax is the company that helps Developers and Companies successfully create a bold new world through GenAI. We offer a One-stop Generative AI Stack with everything needed for a faster, easier, path to production for relevant and responsive GenAI apps. \n➡️DataStax delivers a RAG-first developer experience, with first-class integrations into leading AI ecosystem partners, so we work out with developers’ existing stacks of choice. \n➡️With DataStax, anyone can quickly build smart, high-growth AI applications at unlimited scale, on any cloud. Hundreds of the world’s leading enterprises, including Audi, Bud Financial, Capital One, SkyPoint Cloud, and many more rely on DataStax.\n\n✅ Sign up to try DataStax Astra DB https://dtsx.io/3W4My1H\n\nAbout DataStax Developer:\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache Cassandra©.  Visit https://datastax.com/dev for more free learning resources.",
  "published_at": "2024-09-26T19:14:32Z",
  "thumbnail": "https://i.ytimg.com/vi/gLxa8pSffH8/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "tutorial",
    "astra",
    "demo",
    "datastax",
    "apache_cassandra",
    "cassandra",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=gLxa8pSffH8",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "um I'd like to invite Amit from unstructured to the stage let's give him a warm welcome some little testing on the sound all good all right thanks Carter all right um developers and software enthusiasts I'm Ahmed um I work as a software engineer in unstructured and today I'll tell you about how you can get your data rag from start to end so what's the problem here that we're trying to solve uh can I see if anyone is using or trying to use rag in their daily business cases today and can I see the ones who would want to try well a good majority um what we see is that llms are rising in popul in terms of a technological capability and to be able to use them everybody thinks how how it can answer my questions right uh chat GPT you can ask something on the internet but I I have some documents at home in my company how I can feed those to the llm and the answer is that data is mostly locked behind what it's locked between different sources so it can be in slack you might want to retrieve your slack chats or it can be in notion you want you might be wanting to retrieve documentation how are you going to get that text out it's it's behind different file formats it's in PDFs it's in document X Files it's it's in PowerPoints how are we going to get all this stuff out because as big as the company gets there are lots of this and even in file there are lots of formats right you can structure a PDF in many different ways you can put the table here you can put the table there and when you feed this to a machine learning model it needs to recognize it imagine that you write this software to parse all this data now you need to maintain it you need to scale it and you need to standardize it so that everyone can use it and let's take a break does anyone do this well we do we can help you what we do is we provide all these sources on the left column um and more 25 sources from S3 to notion Google Drive slack um we provide parsing support for many different file types and we provide you options to par them with I'll talk about those later we standardize the outputs in Json we chunk them to correct size and semantic coherence we embed them so you got your vectors to query and we upload them to destinations uh which are a wide variety again about the options to parse so there are two different cases one of them is you have the text in the file format let's say PowerPoints your text is already in the file format right it's beneath somewhere in that file but there are also images where that text is represented by pixels so these are different cases and if you actually have that text data in the format you can our you can use our fast strategy which is super computer intensive um efficient and it is fast as the name but if you have hard to understand uh need to make guesses um like in images or tables then you can our use our highest strategy and if you don't even want to decide you can just use Auto well this is the type of the output that we give it's a Json it gives a type of the element there's an element ID there's the content and some metadata we'll all talk about these but here are the element types that that we support well I have formulas in my documents can I just retrieve them yes you can you can filter based on the element type can I just get the addresses yes can I remove the Footers I don't like them not applicable for my use case yes you can remove them because we give that info well document hierarchy can I make drop downs in my application so that each title shows up and then when I click on it there's some nice little drop down that shows the content yes you can because we keep the parent ID it refers to the title of any paragraph so you can use this information well yeah summarize atomize the elements and preserve document structure and hierarchy and provide metadata I want to see the page five page five is a special page in my document set where there is always this table in it yeah we provide page number as metadata so you can filter that in your rag use case here's what the tables looks like after they get pared um we actually provide them as HTML so you can actually render them in your application after extracting them from the r text or the row file 20 plus sources 25 plus file types um chunking chunking is important because you want to retrieve something coherent um when you first atomize the file it might the info might uh dilute into different atoms but then you can get them together or if it's too big you can separate it so this is a paper that uh a few of our colleagues authored um if anyone wants to check it out here's the QR and basically what we do is you can chunk by title you can chunk by Page or you can actually embed the elements and compare the semantic similarity to see H should I chunk this are they relevant or not we provide embed uh Integrations and so this is the three ways that you can use unstructured first we're going to do this here as well uh partly open source you can get this code you can modify it you can um do whatever you want with it is open but then you can also use a serverless API that's more appropriate for production use cases we provide active support and then then to be released there's the platform which will if you go back here what serverless API gives you is the partitioning and chunking capabilities but platform is the entirety of it and scheduling and other features this is it and now we can going to run a code example here with Astro DB and if you want to get that code it's in a notebook you can scan this QR I made it a script so I'll switch to there let me quickly summarize what's in it so I got some Secrets say for using unru API or for using open AI embeddings I'll load them I'll Define what embedding model I want to use and the dimensionality of it let's say I got some variables here that I configure I tell unst structure where my f files are they don't have to be local it's just for the use case here and I tell it where I want it to work in so I give it a folder that it writes caches into so I can check those caches as well we provide some partition settings some chunking settings you can see by Title Here maximum number of characters chunking overlap and embedding config I want to use this model I want to use this provider and then the destination which is astb in our case I want to upload with uh this this size of batches and then some other configs we form a pipeline basically I want to show what I have in Astra I have a backup collection already let me make that bigger so this is there already I just backed it up in case but let's try to run it and this will be my new collection and let's get this big to see what's happening as we said it gets the files in local it's here um there are some PDF files some images some Exiles there it partitions them chunks them embeds them and uploads them to the destination and you can see all those logs here now let's see our new colle collection oop here we go let's retrieve right let's make some queries and what do I have there I'll just ask it from an article what are the practical difficulties for using deep learning models that's my query I'll just embed it and I'll try to retrieve similar vectors from my destination Boop here's my question here are my results however there are several practical difficulties for taking advantages of recent ADV advances in deep learning based models blah blah blah so this is how you can make your data ra rag ready and if you want to keep in touch this is me you can find it on social media media but even more importantly I want to yep so if you want this in a whole UI no code all clicks just subscribe for platform that's it from me thank you",
    "segments": [
      {
        "start": 0.12,
        "duration": 3.72,
        "text": "um I'd like to invite Amit from"
      },
      {
        "start": 2.159,
        "duration": 4.281,
        "text": "unstructured to the stage let's give him"
      },
      {
        "start": 3.84,
        "duration": 2.6,
        "text": "a warm"
      },
      {
        "start": 9.559,
        "duration": 5.601,
        "text": "welcome some little testing on the sound"
      },
      {
        "start": 12.719,
        "duration": 6.241,
        "text": "all good all right thanks"
      },
      {
        "start": 15.16,
        "duration": 7.16,
        "text": "Carter all right um developers and"
      },
      {
        "start": 18.96,
        "duration": 6.159,
        "text": "software enthusiasts I'm Ahmed um I work"
      },
      {
        "start": 22.32,
        "duration": 5.4,
        "text": "as a software engineer in unstructured"
      },
      {
        "start": 25.119,
        "duration": 5.601,
        "text": "and today I'll tell you about how you"
      },
      {
        "start": 27.72,
        "duration": 5.56,
        "text": "can get your data rag"
      },
      {
        "start": 30.72,
        "duration": 5.28,
        "text": "from start to"
      },
      {
        "start": 33.28,
        "duration": 6.0,
        "text": "end so what's the problem here that"
      },
      {
        "start": 36.0,
        "duration": 7.28,
        "text": "we're trying to solve uh can I see if"
      },
      {
        "start": 39.28,
        "duration": 7.759,
        "text": "anyone is using or trying to use rag in"
      },
      {
        "start": 43.28,
        "duration": 7.08,
        "text": "their daily business cases"
      },
      {
        "start": 47.039,
        "duration": 5.441,
        "text": "today and can I see the ones who would"
      },
      {
        "start": 50.36,
        "duration": 4.92,
        "text": "want to"
      },
      {
        "start": 52.48,
        "duration": 6.44,
        "text": "try well a good"
      },
      {
        "start": 55.28,
        "duration": 5.2,
        "text": "majority um what we see is that llms are"
      },
      {
        "start": 58.92,
        "duration": 4.76,
        "text": "rising in popul"
      },
      {
        "start": 60.48,
        "duration": 6.24,
        "text": "in terms of a technological capability"
      },
      {
        "start": 63.68,
        "duration": 5.799,
        "text": "and to be able to use them everybody"
      },
      {
        "start": 66.72,
        "duration": 5.92,
        "text": "thinks how how it can answer my"
      },
      {
        "start": 69.479,
        "duration": 5.561,
        "text": "questions right uh chat GPT you can ask"
      },
      {
        "start": 72.64,
        "duration": 5.72,
        "text": "something on the internet but I I have"
      },
      {
        "start": 75.04,
        "duration": 6.64,
        "text": "some documents at home in my company how"
      },
      {
        "start": 78.36,
        "duration": 6.119,
        "text": "I can feed those to the llm and the"
      },
      {
        "start": 81.68,
        "duration": 6.52,
        "text": "answer is that data is mostly"
      },
      {
        "start": 84.479,
        "duration": 6.481,
        "text": "locked behind what it's locked between"
      },
      {
        "start": 88.2,
        "duration": 4.48,
        "text": "different sources so it can be in slack"
      },
      {
        "start": 90.96,
        "duration": 4.96,
        "text": "you might want to retrieve your slack"
      },
      {
        "start": 92.68,
        "duration": 5.399,
        "text": "chats or it can be in notion you want"
      },
      {
        "start": 95.92,
        "duration": 4.239,
        "text": "you might be wanting to retrieve"
      },
      {
        "start": 98.079,
        "duration": 5.961,
        "text": "documentation how are you going to get"
      },
      {
        "start": 100.159,
        "duration": 6.6,
        "text": "that text out it's it's behind different"
      },
      {
        "start": 104.04,
        "duration": 5.88,
        "text": "file formats it's in PDFs it's in"
      },
      {
        "start": 106.759,
        "duration": 5.441,
        "text": "document X Files it's it's in"
      },
      {
        "start": 109.92,
        "duration": 5.36,
        "text": "PowerPoints how are we going to get all"
      },
      {
        "start": 112.2,
        "duration": 7.12,
        "text": "this stuff out because as big as the"
      },
      {
        "start": 115.28,
        "duration": 6.479,
        "text": "company gets there are lots of this and"
      },
      {
        "start": 119.32,
        "duration": 5.119,
        "text": "even in file there are lots of formats"
      },
      {
        "start": 121.759,
        "duration": 4.96,
        "text": "right you can structure a PDF in many"
      },
      {
        "start": 124.439,
        "duration": 5.04,
        "text": "different ways you can put the table"
      },
      {
        "start": 126.719,
        "duration": 4.361,
        "text": "here you can put the table there and"
      },
      {
        "start": 129.479,
        "duration": 5.76,
        "text": "when you feed this to a machine learning"
      },
      {
        "start": 131.08,
        "duration": 6.92,
        "text": "model it needs to recognize"
      },
      {
        "start": 135.239,
        "duration": 5.401,
        "text": "it imagine that you write this software"
      },
      {
        "start": 138.0,
        "duration": 5.319,
        "text": "to parse all this data now you need to"
      },
      {
        "start": 140.64,
        "duration": 4.959,
        "text": "maintain it you need to scale it and you"
      },
      {
        "start": 143.319,
        "duration": 3.56,
        "text": "need to standardize it so that everyone"
      },
      {
        "start": 145.599,
        "duration": 5.28,
        "text": "can use"
      },
      {
        "start": 146.879,
        "duration": 8.161,
        "text": "it and let's take a break does anyone do"
      },
      {
        "start": 150.879,
        "duration": 6.201,
        "text": "this well we do we can help you what we"
      },
      {
        "start": 155.04,
        "duration": 5.199,
        "text": "do"
      },
      {
        "start": 157.08,
        "duration": 7.2,
        "text": "is we provide all these sources on the"
      },
      {
        "start": 160.239,
        "duration": 6.681,
        "text": "left column um and more 25 sources from"
      },
      {
        "start": 164.28,
        "duration": 5.92,
        "text": "S3 to notion Google Drive"
      },
      {
        "start": 166.92,
        "duration": 5.44,
        "text": "slack um we provide parsing support for"
      },
      {
        "start": 170.2,
        "duration": 5.92,
        "text": "many different file"
      },
      {
        "start": 172.36,
        "duration": 6.68,
        "text": "types and we provide you options to par"
      },
      {
        "start": 176.12,
        "duration": 5.119,
        "text": "them with I'll talk about those later we"
      },
      {
        "start": 179.04,
        "duration": 5.8,
        "text": "standardize the outputs in"
      },
      {
        "start": 181.239,
        "duration": 6.761,
        "text": "Json we chunk them to correct size and"
      },
      {
        "start": 184.84,
        "duration": 6.28,
        "text": "semantic coherence we embed them so you"
      },
      {
        "start": 188.0,
        "duration": 4.239,
        "text": "got your vectors to query and we upload"
      },
      {
        "start": 191.12,
        "duration": 5.96,
        "text": "them to"
      },
      {
        "start": 192.239,
        "duration": 4.841,
        "text": "destinations uh which are a wide variety"
      },
      {
        "start": 198.319,
        "duration": 6.121,
        "text": "again about the options to parse so"
      },
      {
        "start": 201.76,
        "duration": 5.24,
        "text": "there are two different cases one of"
      },
      {
        "start": 204.44,
        "duration": 6.079,
        "text": "them is you have the text in the file"
      },
      {
        "start": 207.0,
        "duration": 6.4,
        "text": "format let's say PowerPoints your text"
      },
      {
        "start": 210.519,
        "duration": 5.64,
        "text": "is already in the file format right it's"
      },
      {
        "start": 213.4,
        "duration": 5.16,
        "text": "beneath somewhere in that file but there"
      },
      {
        "start": 216.159,
        "duration": 5.121,
        "text": "are also images where that text is"
      },
      {
        "start": 218.56,
        "duration": 5.239,
        "text": "represented by pixels so these are"
      },
      {
        "start": 221.28,
        "duration": 5.319,
        "text": "different cases and if you actually have"
      },
      {
        "start": 223.799,
        "duration": 4.921,
        "text": "that text data in the format you can our"
      },
      {
        "start": 226.599,
        "duration": 5.761,
        "text": "you can use our fast strategy which is"
      },
      {
        "start": 228.72,
        "duration": 8.079,
        "text": "super computer intensive um"
      },
      {
        "start": 232.36,
        "duration": 7.92,
        "text": "efficient and it is fast as the name but"
      },
      {
        "start": 236.799,
        "duration": 6.961,
        "text": "if you have hard to understand uh need"
      },
      {
        "start": 240.28,
        "duration": 6.599,
        "text": "to make guesses um like in images or"
      },
      {
        "start": 243.76,
        "duration": 5.6,
        "text": "tables then you can our use our highest"
      },
      {
        "start": 246.879,
        "duration": 5.761,
        "text": "strategy and if you don't even want to"
      },
      {
        "start": 249.36,
        "duration": 3.28,
        "text": "decide you can just use"
      },
      {
        "start": 253.239,
        "duration": 6.601,
        "text": "Auto well this is the type of the output"
      },
      {
        "start": 256.639,
        "duration": 6.041,
        "text": "that we give it's a Json it gives a type"
      },
      {
        "start": 259.84,
        "duration": 5.56,
        "text": "of the element there's an element ID"
      },
      {
        "start": 262.68,
        "duration": 6.12,
        "text": "there's the content and some metadata"
      },
      {
        "start": 265.4,
        "duration": 5.76,
        "text": "we'll all talk about these but here are"
      },
      {
        "start": 268.8,
        "duration": 5.679,
        "text": "the element types that that we support"
      },
      {
        "start": 271.16,
        "duration": 5.8,
        "text": "well I have formulas in my documents can"
      },
      {
        "start": 274.479,
        "duration": 4.961,
        "text": "I just retrieve them yes you can you can"
      },
      {
        "start": 276.96,
        "duration": 5.44,
        "text": "filter based on the element"
      },
      {
        "start": 279.44,
        "duration": 5.0,
        "text": "type can I just get the addresses yes"
      },
      {
        "start": 282.4,
        "duration": 4.799,
        "text": "can I remove the Footers I don't like"
      },
      {
        "start": 284.44,
        "duration": 6.0,
        "text": "them not applicable for my use case yes"
      },
      {
        "start": 287.199,
        "duration": 6.961,
        "text": "you can remove them because we give that"
      },
      {
        "start": 290.44,
        "duration": 7.12,
        "text": "info well document hierarchy can I make"
      },
      {
        "start": 294.16,
        "duration": 5.72,
        "text": "drop downs in my application so that"
      },
      {
        "start": 297.56,
        "duration": 4.4,
        "text": "each title shows up and then when I"
      },
      {
        "start": 299.88,
        "duration": 4.28,
        "text": "click on it there's some nice little"
      },
      {
        "start": 301.96,
        "duration": 5.72,
        "text": "drop down that shows the content yes you"
      },
      {
        "start": 304.16,
        "duration": 6.56,
        "text": "can because we keep the parent ID it"
      },
      {
        "start": 307.68,
        "duration": 5.76,
        "text": "refers to the title of any paragraph so"
      },
      {
        "start": 310.72,
        "duration": 2.72,
        "text": "you can use this"
      },
      {
        "start": 315.8,
        "duration": 6.64,
        "text": "information well yeah summarize atomize"
      },
      {
        "start": 319.36,
        "duration": 5.08,
        "text": "the elements and preserve document"
      },
      {
        "start": 322.44,
        "duration": 6.44,
        "text": "structure and"
      },
      {
        "start": 324.44,
        "duration": 5.84,
        "text": "hierarchy and provide metadata I want to"
      },
      {
        "start": 328.88,
        "duration": 4.4,
        "text": "see"
      },
      {
        "start": 330.28,
        "duration": 6.44,
        "text": "the page five page five is a special"
      },
      {
        "start": 333.28,
        "duration": 6.4,
        "text": "page in my document set where there is"
      },
      {
        "start": 336.72,
        "duration": 5.08,
        "text": "always this table in it yeah we provide"
      },
      {
        "start": 339.68,
        "duration": 5.56,
        "text": "page number as metadata so you can"
      },
      {
        "start": 341.8,
        "duration": 6.64,
        "text": "filter that in your rag use"
      },
      {
        "start": 345.24,
        "duration": 5.88,
        "text": "case here's what the tables looks like"
      },
      {
        "start": 348.44,
        "duration": 6.4,
        "text": "after they get pared um we actually"
      },
      {
        "start": 351.12,
        "duration": 5.72,
        "text": "provide them as HTML so you can actually"
      },
      {
        "start": 354.84,
        "duration": 4.4,
        "text": "render them in your"
      },
      {
        "start": 356.84,
        "duration": 6.639,
        "text": "application after extracting them from"
      },
      {
        "start": 359.24,
        "duration": 4.239,
        "text": "the r text or the row"
      },
      {
        "start": 363.56,
        "duration": 5.479,
        "text": "file 20 plus sources 25 plus file"
      },
      {
        "start": 369.52,
        "duration": 7.6,
        "text": "types um chunking chunking is important"
      },
      {
        "start": 373.88,
        "duration": 6.96,
        "text": "because you want to retrieve something"
      },
      {
        "start": 377.12,
        "duration": 6.639,
        "text": "coherent um when you first atomize the"
      },
      {
        "start": 380.84,
        "duration": 5.6,
        "text": "file it might the info might uh dilute"
      },
      {
        "start": 383.759,
        "duration": 4.681,
        "text": "into different atoms but then you can"
      },
      {
        "start": 386.44,
        "duration": 3.92,
        "text": "get them together or if it's too big you"
      },
      {
        "start": 388.44,
        "duration": 4.479,
        "text": "can separate it"
      },
      {
        "start": 390.36,
        "duration": 5.279,
        "text": "so this is a paper that uh a few of our"
      },
      {
        "start": 392.919,
        "duration": 6.241,
        "text": "colleagues authored um if anyone wants"
      },
      {
        "start": 395.639,
        "duration": 3.521,
        "text": "to check it out here's the"
      },
      {
        "start": 401.8,
        "duration": 6.799,
        "text": "QR and basically what we do is you can"
      },
      {
        "start": 404.919,
        "duration": 7.241,
        "text": "chunk by title you can chunk by Page or"
      },
      {
        "start": 408.599,
        "duration": 7.081,
        "text": "you can actually embed the elements and"
      },
      {
        "start": 412.16,
        "duration": 6.2,
        "text": "compare the semantic similarity to see H"
      },
      {
        "start": 415.68,
        "duration": 4.239,
        "text": "should I chunk this are they relevant or"
      },
      {
        "start": 418.36,
        "duration": 5.279,
        "text": "not"
      },
      {
        "start": 419.919,
        "duration": 3.72,
        "text": "we provide embed uh"
      },
      {
        "start": 423.84,
        "duration": 6.039,
        "text": "Integrations and so this is the three"
      },
      {
        "start": 427.039,
        "duration": 5.401,
        "text": "ways that you can use unstructured first"
      },
      {
        "start": 429.879,
        "duration": 5.681,
        "text": "we're going to do this here as well uh"
      },
      {
        "start": 432.44,
        "duration": 6.72,
        "text": "partly open source you can get this code"
      },
      {
        "start": 435.56,
        "duration": 6.24,
        "text": "you can modify it you can um do whatever"
      },
      {
        "start": 439.16,
        "duration": 5.24,
        "text": "you want with it is open but then you"
      },
      {
        "start": 441.8,
        "duration": 4.679,
        "text": "can also use a serverless API that's"
      },
      {
        "start": 444.4,
        "duration": 5.16,
        "text": "more appropriate for production use"
      },
      {
        "start": 446.479,
        "duration": 6.321,
        "text": "cases we provide active support and then"
      },
      {
        "start": 449.56,
        "duration": 5.44,
        "text": "then to be released there's the platform"
      },
      {
        "start": 452.8,
        "duration": 2.2,
        "text": "which"
      },
      {
        "start": 456.28,
        "duration": 6.52,
        "text": "will if you go back here what serverless"
      },
      {
        "start": 459.879,
        "duration": 5.961,
        "text": "API gives you is the partitioning and"
      },
      {
        "start": 462.8,
        "duration": 4.76,
        "text": "chunking capabilities but platform is"
      },
      {
        "start": 465.84,
        "duration": 6.359,
        "text": "the entirety of"
      },
      {
        "start": 467.56,
        "duration": 4.639,
        "text": "it and scheduling and other"
      },
      {
        "start": 473.52,
        "duration": 7.56,
        "text": "features this is it"
      },
      {
        "start": 477.0,
        "duration": 7.44,
        "text": "and now we can going to run a code"
      },
      {
        "start": 481.08,
        "duration": 6.76,
        "text": "example here with Astro"
      },
      {
        "start": 484.44,
        "duration": 7.36,
        "text": "DB and if you want to get that code it's"
      },
      {
        "start": 487.84,
        "duration": 3.96,
        "text": "in a notebook you can scan this"
      },
      {
        "start": 494.56,
        "duration": 4.96,
        "text": "QR I made it a script so I'll switch to"
      },
      {
        "start": 502.56,
        "duration": 6.479,
        "text": "there let me quickly summarize what's in"
      },
      {
        "start": 505.159,
        "duration": 7.961,
        "text": "it so I got some Secrets say for using"
      },
      {
        "start": 509.039,
        "duration": 6.641,
        "text": "unru API or for using open AI embeddings"
      },
      {
        "start": 513.12,
        "duration": 4.64,
        "text": "I'll load them I'll Define what"
      },
      {
        "start": 515.68,
        "duration": 4.839,
        "text": "embedding model I want to"
      },
      {
        "start": 517.76,
        "duration": 5.44,
        "text": "use and the dimensionality of it let's"
      },
      {
        "start": 520.519,
        "duration": 5.281,
        "text": "say I got some variables here that I"
      },
      {
        "start": 523.2,
        "duration": 5.36,
        "text": "configure I tell unst structure where my"
      },
      {
        "start": 525.8,
        "duration": 6.12,
        "text": "f files are they don't have to be local"
      },
      {
        "start": 528.56,
        "duration": 6.68,
        "text": "it's just for the use case here and I"
      },
      {
        "start": 531.92,
        "duration": 6.52,
        "text": "tell it where I want it to work in so I"
      },
      {
        "start": 535.24,
        "duration": 8.0,
        "text": "give it a folder that it writes caches"
      },
      {
        "start": 538.44,
        "duration": 4.8,
        "text": "into so I can check those caches as"
      },
      {
        "start": 543.8,
        "duration": 6.36,
        "text": "well we provide some partition"
      },
      {
        "start": 547.44,
        "duration": 5.68,
        "text": "settings some chunking settings you can"
      },
      {
        "start": 550.16,
        "duration": 5.48,
        "text": "see by Title Here maximum number of"
      },
      {
        "start": 553.12,
        "duration": 5.159,
        "text": "characters chunking"
      },
      {
        "start": 555.64,
        "duration": 3.8,
        "text": "overlap and embedding config I want to"
      },
      {
        "start": 558.279,
        "duration": 4.841,
        "text": "use this"
      },
      {
        "start": 559.44,
        "duration": 5.92,
        "text": "model I want to use this"
      },
      {
        "start": 563.12,
        "duration": 6.48,
        "text": "provider and then the destination which"
      },
      {
        "start": 565.36,
        "duration": 4.24,
        "text": "is astb in our case"
      },
      {
        "start": 569.68,
        "duration": 4.279,
        "text": "I want to upload with uh this this size"
      },
      {
        "start": 572.519,
        "duration": 4.361,
        "text": "of"
      },
      {
        "start": 573.959,
        "duration": 7.081,
        "text": "batches and then some other"
      },
      {
        "start": 576.88,
        "duration": 4.16,
        "text": "configs we form a pipeline"
      },
      {
        "start": 581.8,
        "duration": 5.44,
        "text": "basically I want to show what I have in"
      },
      {
        "start": 584.519,
        "duration": 5.44,
        "text": "Astra I have a backup collection already"
      },
      {
        "start": 587.24,
        "duration": 2.719,
        "text": "let me make that"
      },
      {
        "start": 593.6,
        "duration": 6.32,
        "text": "bigger so this is there already I just"
      },
      {
        "start": 596.76,
        "duration": 6.4,
        "text": "backed it up in case but let's try to"
      },
      {
        "start": 599.92,
        "duration": 3.24,
        "text": "run it"
      },
      {
        "start": 610.44,
        "duration": 4.28,
        "text": "and this will be my new"
      },
      {
        "start": 614.92,
        "duration": 6.0,
        "text": "collection and let's get this big to see"
      },
      {
        "start": 617.64,
        "duration": 3.28,
        "text": "what's happening"
      },
      {
        "start": 629.92,
        "duration": 5.08,
        "text": "as we"
      },
      {
        "start": 631.64,
        "duration": 6.24,
        "text": "said it gets the files in local it's"
      },
      {
        "start": 635.0,
        "duration": 6.24,
        "text": "here um there are some PDF files some"
      },
      {
        "start": 637.88,
        "duration": 5.72,
        "text": "images some Exiles there it partitions"
      },
      {
        "start": 641.24,
        "duration": 3.76,
        "text": "them chunks them embeds them and uploads"
      },
      {
        "start": 643.6,
        "duration": 3.44,
        "text": "them to the"
      },
      {
        "start": 645.0,
        "duration": 7.839,
        "text": "destination and you can see all those"
      },
      {
        "start": 647.04,
        "duration": 5.799,
        "text": "logs here now let's see our new colle"
      },
      {
        "start": 655.56,
        "duration": 7.88,
        "text": "collection oop here we go let's retrieve"
      },
      {
        "start": 660.48,
        "duration": 2.96,
        "text": "right let's make some"
      },
      {
        "start": 666.76,
        "duration": 6.96,
        "text": "queries and what do I have"
      },
      {
        "start": 670.16,
        "duration": 5.799,
        "text": "there I'll just ask it from an"
      },
      {
        "start": 673.72,
        "duration": 4.88,
        "text": "article what are the practical"
      },
      {
        "start": 675.959,
        "duration": 5.56,
        "text": "difficulties for using deep learning"
      },
      {
        "start": 678.6,
        "duration": 6.32,
        "text": "models that's my query I'll just embed"
      },
      {
        "start": 681.519,
        "duration": 7.401,
        "text": "it and I'll try to retrieve similar"
      },
      {
        "start": 684.92,
        "duration": 4.0,
        "text": "vectors from my destination"
      },
      {
        "start": 692.44,
        "duration": 5.12,
        "text": "Boop here's my question here are my"
      },
      {
        "start": 695.36,
        "duration": 4.4,
        "text": "results however there are several"
      },
      {
        "start": 697.56,
        "duration": 4.92,
        "text": "practical difficulties for taking"
      },
      {
        "start": 699.76,
        "duration": 4.8,
        "text": "advantages of recent ADV advances in"
      },
      {
        "start": 702.48,
        "duration": 4.599,
        "text": "deep learning based models blah blah"
      },
      {
        "start": 704.56,
        "duration": 6.399,
        "text": "blah so this is how you can make your"
      },
      {
        "start": 707.079,
        "duration": 8.081,
        "text": "data ra rag ready"
      },
      {
        "start": 710.959,
        "duration": 4.201,
        "text": "and if you want to keep in"
      },
      {
        "start": 715.2,
        "duration": 6.439,
        "text": "touch this is me you can find it on"
      },
      {
        "start": 718.76,
        "duration": 6.199,
        "text": "social media media but even more"
      },
      {
        "start": 721.639,
        "duration": 3.32,
        "text": "importantly I want"
      },
      {
        "start": 731.36,
        "duration": 8.52,
        "text": "to yep so if you want this in a whole UI"
      },
      {
        "start": 737.04,
        "duration": 7.599,
        "text": "no code all clicks just subscribe for"
      },
      {
        "start": 739.88,
        "duration": 4.759,
        "text": "platform that's it from me thank you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-11T21:38:07.743433+00:00"
}