{
  "video_id": "BW_A-qu0dWk",
  "title": "DS332.02 Gremlin OLAP Traversals | DataStax Enterprise 6 Graph Analytics",
  "description": "#DataStaxAcademy #DS332\nDS332.02 Gremlin OLAP Traversals\nA Gremlin traversal can be executed using either the real-time (OLTP) engine or analytic (OLAP) engine. The latter execution results in a Gremlin OLAP traversal. In this unit, you will learn about Gremlin OLAP traversals.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:38:23Z",
  "thumbnail": "https://i.ytimg.com/vi/BW_A-qu0dWk/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "datastax",
    "tutorial",
    "apache_cassandra"
  ],
  "url": "https://www.youtube.com/watch?v=BW_A-qu0dWk",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] a grammar traversal can be executed using either yield time otp engine or analytic lab engine the latter execution results in a gremlin a lab traversal gremlin elaborate results are still written using the same standard gremlin graph traversal language but they do start within a lab traversal source and execute as spark jobs on dsc analytics nodes currently their lab engine may not support all gremlin traversal steps but additional steps are constantly being added please refer to the documentation for the most current list of supported traversal steps here are some of the characteristics that may help you to recognize and a lab traversal and choose an appropriate execution engine any traversal that takes longer to execute involves broader scope data analysis requires expensive graph scans deals with large sap graph traverses longer paths with many branches should be strongly considered for execution by the olap engine instead of the oltp engine it is very straightforward to switch between oltp and olap engines by simply switching between oltp and allow traversal sources to start your traversal in data stack studio you can use the execution drop down menu available for each gremlin cell you have two options execute using real-time engine and execute using analytic engine in gremlin console you can use the remote command to configure an lsg that refers to either killervideo.g or killervideo.a which are oltp and a lab traversal sources respectively for the killer videograph the gremlin traversal itself does not have to change which is the great news gremlin alab traversals are translated into spark jobs that are executed on dsc analytics nodes therefore you can monitor their execution using dsc analytics tools such as spark web ui you simply need to point your web browser to a node in your cluster by supplying its public ip address and port 7080. among running application you should be able to see apache's tinker pops spark gremlin application that executes gremlin alarm traversals you can always click on the application id to drill down to specific spark jobs and execution stages to see more details let's look at a couple of examples since you already know gremlin they should look familiar the only difference could be that we use a lab traversal source to execute them here we find vertex distribution by label and we get 920 movies 8 700 persons 18 genres 1100 users in our killer video graph the screenshot of spark application web ui shows the completed spark job that corresponds to this gremlin traversal it may not be obvious to you but this particular traversal was routed to the dsc graph frames engine i can tell based on the dec graph traversal.scala file in the description indeed this traversal is a simple scan query and meets all the requirements for the olap query routing optimization we discussed earlier this next traversal starts with a random user traverses those edges in any direction six times and counts users that we were able to reach we get 1077 users out of 1100 total users in the graph which suggests that almost any two users are connected by a pass with at most six edges therefore our user social network has six degrees of separation the screenshot which is part of spark application web ui shows some of the completed jobs that corresponds to this single kremlin traversal this time the spark graph computer engine was used for execution besides the intelligent analytics query routing optimization that is applied automatically snapshots is another optimization that you have to introduce yourself snapshots can be used to optimize performance if you need to execute many alarm traversals over the same sub graph you can create a snapshot which is essentially a copy of that subgraph in spark to query it faster a snapshot is a persisted data set in spark then you can create an alarm traversal source to create that snapshot and execute multiple lab traversals using the same traversal source the snapshot api includes the snapshot method to start a snapshot definition the vertices and edges methods to specify which vertices and edges to add to the snapshot based on their labels the conf method to specify configuration properties for the snapshot the configuration properties are defined in the apache tinker pop documentation for spar graph computer and the create method to create the snapshot and return its corresponding a lab traversal source it is actually quite straightforward let's look at an example we extract a social subgraph from the killer video graph as a snapshot containing all user vertices and all nose edges and cache this snapshot in memory of spark executors we then execute four traversals over this snapshot using the respective olap traversal source called social in this example all these allowed traversals are analyzing the same subgraph counting the number of vertices and their distribution by age gender and degree the first reversal will take longer to execute because it is when the snapshot is actually materialized in memory and all subsequent traversals will be very fast it is that simple now that you know what grammy lab traversals are and how to execute them with spark let's talk a bit more about spark running environment itself since the lab traversals create many intermediate objects during execution and those objects are garbage collected by the jvm it is better to have a large pool of executors each with smaller memory and cpu resources note that this is quite different from non-graph spark jobs which typically perform better with fewer executors with higher memory and cpu resources also to reduce garbage collection pauses and improve a lab traversal performance we recommend allocating executors with no more than eight cores in most cases just one core per executor is a good choice the memory available to spark should be equally spread among the course there is a convenient gremlin spark configuration api to control spark settings for an olap traversal source g.graph.configuration.set property allows you to change both spark and spark cassandra connector properties for your particular use case there are many dozens of those configuration properties we will see a few in our examples but please refer to the docs to learn about many more one thing to keep in mind is once you change the configuration property you may need to kill and restart the apache tinker pops spark gremlin application via the spark web ui for the change to take an effect here is an example of how to change three spark properties before executing your statistical gremlin a lab traversal the spark.course.max property sets the maximum number of cores used by the apache tinker prop spark rambling application setting this property lower than the total number of course in your cluster limits the number of nodes on which the traversal can be executed the spark.executor.course property sets the number of course used for each executor again if the apache thinker pops spark kremlin application was running before you change these settings you should restart it and this is an example of changing the spark cassandra connector setting called spark dot cassandra dot input dot split dot size in mb this property sets the approximate size of data the spark cassandra connector will request with each individual cql key in this example we change this setting because when deleting many edges and vertices from a graph we may end up with many tombstones as a result we may get errors in subsequent quiz due to the large number of tombstones left in the database to avoid these errors we reduce the number of tombstones per quest by setting the spark.cassandra.input.split.size in megabytesproperty to a smaller size and the default of 64 megabytes in particular we set the property to one megabyte before dropping all our users of course there are other important applications of how data is read from cassandra and into how many spark partitions when these properties changed finally it is time for you to work on an exercise",
    "segments": [
      {
        "start": 1.42,
        "duration": 3.2,
        "text": "[Music]"
      },
      {
        "start": 5.44,
        "duration": 3.52,
        "text": "a grammar traversal can be executed"
      },
      {
        "start": 7.52,
        "duration": 4.64,
        "text": "using either yield time"
      },
      {
        "start": 8.96,
        "duration": 4.88,
        "text": "otp engine or analytic lab engine"
      },
      {
        "start": 12.16,
        "duration": 3.359,
        "text": "the latter execution results in a"
      },
      {
        "start": 13.84,
        "duration": 3.199,
        "text": "gremlin a lab traversal gremlin"
      },
      {
        "start": 15.519,
        "duration": 2.881,
        "text": "elaborate results are still written"
      },
      {
        "start": 17.039,
        "duration": 3.121,
        "text": "using the same"
      },
      {
        "start": 18.4,
        "duration": 3.52,
        "text": "standard gremlin graph traversal"
      },
      {
        "start": 20.16,
        "duration": 4.4,
        "text": "language but they do"
      },
      {
        "start": 21.92,
        "duration": 5.04,
        "text": "start within a lab traversal source and"
      },
      {
        "start": 24.56,
        "duration": 4.959,
        "text": "execute as spark jobs on dsc"
      },
      {
        "start": 26.96,
        "duration": 3.12,
        "text": "analytics nodes currently their lab"
      },
      {
        "start": 29.519,
        "duration": 3.441,
        "text": "engine may"
      },
      {
        "start": 30.08,
        "duration": 4.96,
        "text": "not support all gremlin traversal steps"
      },
      {
        "start": 32.96,
        "duration": 3.119,
        "text": "but additional steps are constantly"
      },
      {
        "start": 35.04,
        "duration": 3.359,
        "text": "being added"
      },
      {
        "start": 36.079,
        "duration": 4.0,
        "text": "please refer to the documentation for"
      },
      {
        "start": 38.399,
        "duration": 3.041,
        "text": "the most current list of supported"
      },
      {
        "start": 40.079,
        "duration": 2.961,
        "text": "traversal steps"
      },
      {
        "start": 41.44,
        "duration": 3.92,
        "text": "here are some of the characteristics"
      },
      {
        "start": 43.04,
        "duration": 3.039,
        "text": "that may help you to recognize and a lab"
      },
      {
        "start": 45.36,
        "duration": 2.8,
        "text": "traversal"
      },
      {
        "start": 46.079,
        "duration": 4.8,
        "text": "and choose an appropriate execution"
      },
      {
        "start": 48.16,
        "duration": 4.16,
        "text": "engine any traversal that takes longer"
      },
      {
        "start": 50.879,
        "duration": 4.32,
        "text": "to execute"
      },
      {
        "start": 52.32,
        "duration": 3.6,
        "text": "involves broader scope data analysis"
      },
      {
        "start": 55.199,
        "duration": 3.84,
        "text": "requires"
      },
      {
        "start": 55.92,
        "duration": 4.159,
        "text": "expensive graph scans deals with large"
      },
      {
        "start": 59.039,
        "duration": 3.121,
        "text": "sap graph"
      },
      {
        "start": 60.079,
        "duration": 3.201,
        "text": "traverses longer paths with many"
      },
      {
        "start": 62.16,
        "duration": 2.8,
        "text": "branches"
      },
      {
        "start": 63.28,
        "duration": 3.44,
        "text": "should be strongly considered for"
      },
      {
        "start": 64.96,
        "duration": 5.04,
        "text": "execution by the olap"
      },
      {
        "start": 66.72,
        "duration": 5.28,
        "text": "engine instead of the oltp engine it is"
      },
      {
        "start": 70.0,
        "duration": 4.32,
        "text": "very straightforward to switch between"
      },
      {
        "start": 72.0,
        "duration": 4.56,
        "text": "oltp and olap engines by simply"
      },
      {
        "start": 74.32,
        "duration": 3.6,
        "text": "switching between oltp and allow"
      },
      {
        "start": 76.56,
        "duration": 3.84,
        "text": "traversal sources"
      },
      {
        "start": 77.92,
        "duration": 3.199,
        "text": "to start your traversal in data stack"
      },
      {
        "start": 80.4,
        "duration": 3.52,
        "text": "studio"
      },
      {
        "start": 81.119,
        "duration": 4.961,
        "text": "you can use the execution drop down menu"
      },
      {
        "start": 83.92,
        "duration": 4.559,
        "text": "available for each gremlin cell"
      },
      {
        "start": 86.08,
        "duration": 3.92,
        "text": "you have two options execute using"
      },
      {
        "start": 88.479,
        "duration": 4.481,
        "text": "real-time engine and"
      },
      {
        "start": 90.0,
        "duration": 3.84,
        "text": "execute using analytic engine in gremlin"
      },
      {
        "start": 92.96,
        "duration": 3.28,
        "text": "console"
      },
      {
        "start": 93.84,
        "duration": 3.279,
        "text": "you can use the remote command to"
      },
      {
        "start": 96.24,
        "duration": 4.879,
        "text": "configure an"
      },
      {
        "start": 97.119,
        "duration": 7.28,
        "text": "lsg that refers to either killervideo.g"
      },
      {
        "start": 101.119,
        "duration": 5.921,
        "text": "or killervideo.a which are oltp"
      },
      {
        "start": 104.399,
        "duration": 4.321,
        "text": "and a lab traversal sources respectively"
      },
      {
        "start": 107.04,
        "duration": 4.16,
        "text": "for the killer videograph"
      },
      {
        "start": 108.72,
        "duration": 3.759,
        "text": "the gremlin traversal itself does not"
      },
      {
        "start": 111.2,
        "duration": 3.52,
        "text": "have to change"
      },
      {
        "start": 112.479,
        "duration": 4.481,
        "text": "which is the great news gremlin alab"
      },
      {
        "start": 114.72,
        "duration": 2.88,
        "text": "traversals are translated into spark"
      },
      {
        "start": 116.96,
        "duration": 3.68,
        "text": "jobs"
      },
      {
        "start": 117.6,
        "duration": 4.879,
        "text": "that are executed on dsc analytics nodes"
      },
      {
        "start": 120.64,
        "duration": 4.799,
        "text": "therefore you can monitor their"
      },
      {
        "start": 122.479,
        "duration": 4.561,
        "text": "execution using dsc analytics tools such"
      },
      {
        "start": 125.439,
        "duration": 3.44,
        "text": "as spark web ui"
      },
      {
        "start": 127.04,
        "duration": 3.999,
        "text": "you simply need to point your web"
      },
      {
        "start": 128.879,
        "duration": 5.201,
        "text": "browser to a node in your cluster"
      },
      {
        "start": 131.039,
        "duration": 6.56,
        "text": "by supplying its public ip address"
      },
      {
        "start": 134.08,
        "duration": 5.28,
        "text": "and port 7080. among running application"
      },
      {
        "start": 137.599,
        "duration": 4.081,
        "text": "you should be able to see apache's"
      },
      {
        "start": 139.36,
        "duration": 4.8,
        "text": "tinker pops spark gremlin application"
      },
      {
        "start": 141.68,
        "duration": 4.32,
        "text": "that executes gremlin alarm traversals"
      },
      {
        "start": 144.16,
        "duration": 4.64,
        "text": "you can always click on the application"
      },
      {
        "start": 146.0,
        "duration": 6.16,
        "text": "id to drill down to specific spark jobs"
      },
      {
        "start": 148.8,
        "duration": 5.76,
        "text": "and execution stages to see more details"
      },
      {
        "start": 152.16,
        "duration": 4.159,
        "text": "let's look at a couple of examples since"
      },
      {
        "start": 154.56,
        "duration": 3.679,
        "text": "you already know gremlin"
      },
      {
        "start": 156.319,
        "duration": 3.2,
        "text": "they should look familiar the only"
      },
      {
        "start": 158.239,
        "duration": 4.161,
        "text": "difference could be that"
      },
      {
        "start": 159.519,
        "duration": 3.44,
        "text": "we use a lab traversal source to execute"
      },
      {
        "start": 162.4,
        "duration": 2.559,
        "text": "them"
      },
      {
        "start": 162.959,
        "duration": 5.041,
        "text": "here we find vertex distribution by"
      },
      {
        "start": 164.959,
        "duration": 6.401,
        "text": "label and we get 920 movies"
      },
      {
        "start": 168.0,
        "duration": 6.64,
        "text": "8 700 persons"
      },
      {
        "start": 171.36,
        "duration": 5.92,
        "text": "18 genres 1100 users in our"
      },
      {
        "start": 174.64,
        "duration": 4.56,
        "text": "killer video graph the screenshot of"
      },
      {
        "start": 177.28,
        "duration": 4.16,
        "text": "spark application web ui"
      },
      {
        "start": 179.2,
        "duration": 3.759,
        "text": "shows the completed spark job that"
      },
      {
        "start": 181.44,
        "duration": 4.159,
        "text": "corresponds to this"
      },
      {
        "start": 182.959,
        "duration": 3.441,
        "text": "gremlin traversal it may not be obvious"
      },
      {
        "start": 185.599,
        "duration": 3.601,
        "text": "to you"
      },
      {
        "start": 186.4,
        "duration": 4.88,
        "text": "but this particular traversal was routed"
      },
      {
        "start": 189.2,
        "duration": 5.16,
        "text": "to the dsc graph frames"
      },
      {
        "start": 191.28,
        "duration": 5.599,
        "text": "engine i can tell based on the dec graph"
      },
      {
        "start": 194.36,
        "duration": 5.48,
        "text": "traversal.scala file in the description"
      },
      {
        "start": 196.879,
        "duration": 3.841,
        "text": "indeed this traversal is a simple scan"
      },
      {
        "start": 199.84,
        "duration": 2.72,
        "text": "query"
      },
      {
        "start": 200.72,
        "duration": 4.239,
        "text": "and meets all the requirements for the"
      },
      {
        "start": 202.56,
        "duration": 3.599,
        "text": "olap query routing optimization we"
      },
      {
        "start": 204.959,
        "duration": 3.28,
        "text": "discussed earlier"
      },
      {
        "start": 206.159,
        "duration": 4.72,
        "text": "this next traversal starts with a random"
      },
      {
        "start": 208.239,
        "duration": 6.08,
        "text": "user traverses those edges"
      },
      {
        "start": 210.879,
        "duration": 4.401,
        "text": "in any direction six times and counts"
      },
      {
        "start": 214.319,
        "duration": 5.84,
        "text": "users that we"
      },
      {
        "start": 215.28,
        "duration": 7.92,
        "text": "were able to reach we get 1077 users"
      },
      {
        "start": 220.159,
        "duration": 6.8,
        "text": "out of 1100 total users in the graph"
      },
      {
        "start": 223.2,
        "duration": 6.959,
        "text": "which suggests that almost any two users"
      },
      {
        "start": 226.959,
        "duration": 6.241,
        "text": "are connected by a pass with at most six"
      },
      {
        "start": 230.159,
        "duration": 5.041,
        "text": "edges therefore our user social network"
      },
      {
        "start": 233.2,
        "duration": 4.48,
        "text": "has six degrees of separation"
      },
      {
        "start": 235.2,
        "duration": 4.399,
        "text": "the screenshot which is part of spark"
      },
      {
        "start": 237.68,
        "duration": 4.96,
        "text": "application web ui"
      },
      {
        "start": 239.599,
        "duration": 5.36,
        "text": "shows some of the completed jobs"
      },
      {
        "start": 242.64,
        "duration": 3.12,
        "text": "that corresponds to this single kremlin"
      },
      {
        "start": 244.959,
        "duration": 2.801,
        "text": "traversal"
      },
      {
        "start": 245.76,
        "duration": 4.32,
        "text": "this time the spark graph computer"
      },
      {
        "start": 247.76,
        "duration": 5.039,
        "text": "engine was used for execution"
      },
      {
        "start": 250.08,
        "duration": 4.48,
        "text": "besides the intelligent analytics query"
      },
      {
        "start": 252.799,
        "duration": 4.96,
        "text": "routing optimization"
      },
      {
        "start": 254.56,
        "duration": 5.44,
        "text": "that is applied automatically snapshots"
      },
      {
        "start": 257.759,
        "duration": 3.6,
        "text": "is another optimization that you have to"
      },
      {
        "start": 260.0,
        "duration": 3.28,
        "text": "introduce yourself"
      },
      {
        "start": 261.359,
        "duration": 3.28,
        "text": "snapshots can be used to optimize"
      },
      {
        "start": 263.28,
        "duration": 3.52,
        "text": "performance"
      },
      {
        "start": 264.639,
        "duration": 3.921,
        "text": "if you need to execute many alarm"
      },
      {
        "start": 266.8,
        "duration": 4.399,
        "text": "traversals over the same sub"
      },
      {
        "start": 268.56,
        "duration": 4.8,
        "text": "graph you can create a snapshot which is"
      },
      {
        "start": 271.199,
        "duration": 6.161,
        "text": "essentially a copy of that subgraph"
      },
      {
        "start": 273.36,
        "duration": 7.119,
        "text": "in spark to query it faster a snapshot"
      },
      {
        "start": 277.36,
        "duration": 5.68,
        "text": "is a persisted data set in spark then"
      },
      {
        "start": 280.479,
        "duration": 5.28,
        "text": "you can create an alarm traversal source"
      },
      {
        "start": 283.04,
        "duration": 3.36,
        "text": "to create that snapshot and execute"
      },
      {
        "start": 285.759,
        "duration": 3.201,
        "text": "multiple"
      },
      {
        "start": 286.4,
        "duration": 3.12,
        "text": "lab traversals using the same traversal"
      },
      {
        "start": 288.96,
        "duration": 3.76,
        "text": "source"
      },
      {
        "start": 289.52,
        "duration": 4.08,
        "text": "the snapshot api includes the snapshot"
      },
      {
        "start": 292.72,
        "duration": 3.68,
        "text": "method"
      },
      {
        "start": 293.6,
        "duration": 4.879,
        "text": "to start a snapshot definition the"
      },
      {
        "start": 296.4,
        "duration": 4.799,
        "text": "vertices and edges methods"
      },
      {
        "start": 298.479,
        "duration": 3.921,
        "text": "to specify which vertices and edges to"
      },
      {
        "start": 301.199,
        "duration": 4.241,
        "text": "add to the snapshot"
      },
      {
        "start": 302.4,
        "duration": 3.76,
        "text": "based on their labels the conf method to"
      },
      {
        "start": 305.44,
        "duration": 2.24,
        "text": "specify"
      },
      {
        "start": 306.16,
        "duration": 3.92,
        "text": "configuration properties for the"
      },
      {
        "start": 307.68,
        "duration": 3.2,
        "text": "snapshot the configuration properties"
      },
      {
        "start": 310.08,
        "duration": 3.2,
        "text": "are defined"
      },
      {
        "start": 310.88,
        "duration": 4.08,
        "text": "in the apache tinker pop documentation"
      },
      {
        "start": 313.28,
        "duration": 3.919,
        "text": "for spar graph computer"
      },
      {
        "start": 314.96,
        "duration": 3.519,
        "text": "and the create method to create the"
      },
      {
        "start": 317.199,
        "duration": 4.0,
        "text": "snapshot and return"
      },
      {
        "start": 318.479,
        "duration": 4.481,
        "text": "its corresponding a lab traversal source"
      },
      {
        "start": 321.199,
        "duration": 3.28,
        "text": "it is actually quite straightforward"
      },
      {
        "start": 322.96,
        "duration": 4.16,
        "text": "let's look at an example"
      },
      {
        "start": 324.479,
        "duration": 3.841,
        "text": "we extract a social subgraph from the"
      },
      {
        "start": 327.12,
        "duration": 3.44,
        "text": "killer video graph"
      },
      {
        "start": 328.32,
        "duration": 3.28,
        "text": "as a snapshot containing all user"
      },
      {
        "start": 330.56,
        "duration": 4.8,
        "text": "vertices and"
      },
      {
        "start": 331.6,
        "duration": 6.4,
        "text": "all nose edges and cache this snapshot"
      },
      {
        "start": 335.36,
        "duration": 5.04,
        "text": "in memory of spark executors we"
      },
      {
        "start": 338.0,
        "duration": 3.28,
        "text": "then execute four traversals over this"
      },
      {
        "start": 340.4,
        "duration": 3.04,
        "text": "snapshot"
      },
      {
        "start": 341.28,
        "duration": 3.68,
        "text": "using the respective olap traversal"
      },
      {
        "start": 343.44,
        "duration": 4.0,
        "text": "source called social"
      },
      {
        "start": 344.96,
        "duration": 3.44,
        "text": "in this example all these allowed"
      },
      {
        "start": 347.44,
        "duration": 3.68,
        "text": "traversals are"
      },
      {
        "start": 348.4,
        "duration": 4.239,
        "text": "analyzing the same subgraph counting the"
      },
      {
        "start": 351.12,
        "duration": 3.2,
        "text": "number of vertices and their"
      },
      {
        "start": 352.639,
        "duration": 4.081,
        "text": "distribution by age"
      },
      {
        "start": 354.32,
        "duration": 3.36,
        "text": "gender and degree the first reversal"
      },
      {
        "start": 356.72,
        "duration": 3.52,
        "text": "will take"
      },
      {
        "start": 357.68,
        "duration": 4.639,
        "text": "longer to execute because it is when the"
      },
      {
        "start": 360.24,
        "duration": 2.799,
        "text": "snapshot is actually materialized in"
      },
      {
        "start": 362.319,
        "duration": 3.121,
        "text": "memory"
      },
      {
        "start": 363.039,
        "duration": 3.44,
        "text": "and all subsequent traversals will be"
      },
      {
        "start": 365.44,
        "duration": 3.28,
        "text": "very fast"
      },
      {
        "start": 366.479,
        "duration": 3.761,
        "text": "it is that simple now that you know what"
      },
      {
        "start": 368.72,
        "duration": 4.72,
        "text": "grammy lab traversals"
      },
      {
        "start": 370.24,
        "duration": 5.28,
        "text": "are and how to execute them with spark"
      },
      {
        "start": 373.44,
        "duration": 4.0,
        "text": "let's talk a bit more about spark"
      },
      {
        "start": 375.52,
        "duration": 3.76,
        "text": "running environment itself"
      },
      {
        "start": 377.44,
        "duration": 3.44,
        "text": "since the lab traversals create many"
      },
      {
        "start": 379.28,
        "duration": 4.32,
        "text": "intermediate objects during"
      },
      {
        "start": 380.88,
        "duration": 5.12,
        "text": "execution and those objects are garbage"
      },
      {
        "start": 383.6,
        "duration": 4.879,
        "text": "collected by the jvm"
      },
      {
        "start": 386.0,
        "duration": 3.84,
        "text": "it is better to have a large pool of"
      },
      {
        "start": 388.479,
        "duration": 3.44,
        "text": "executors"
      },
      {
        "start": 389.84,
        "duration": 4.16,
        "text": "each with smaller memory and cpu"
      },
      {
        "start": 391.919,
        "duration": 4.72,
        "text": "resources note that this is quite"
      },
      {
        "start": 394.0,
        "duration": 3.759,
        "text": "different from non-graph spark jobs"
      },
      {
        "start": 396.639,
        "duration": 3.68,
        "text": "which typically"
      },
      {
        "start": 397.759,
        "duration": 3.601,
        "text": "perform better with fewer executors with"
      },
      {
        "start": 400.319,
        "duration": 3.921,
        "text": "higher memory"
      },
      {
        "start": 401.36,
        "duration": 4.16,
        "text": "and cpu resources also to reduce garbage"
      },
      {
        "start": 404.24,
        "duration": 3.76,
        "text": "collection pauses"
      },
      {
        "start": 405.52,
        "duration": 4.88,
        "text": "and improve a lab traversal performance"
      },
      {
        "start": 408.0,
        "duration": 3.12,
        "text": "we recommend allocating executors with"
      },
      {
        "start": 410.4,
        "duration": 4.16,
        "text": "no more than"
      },
      {
        "start": 411.12,
        "duration": 5.44,
        "text": "eight cores in most cases just one core"
      },
      {
        "start": 414.56,
        "duration": 3.84,
        "text": "per executor is a good choice"
      },
      {
        "start": 416.56,
        "duration": 4.32,
        "text": "the memory available to spark should be"
      },
      {
        "start": 418.4,
        "duration": 4.16,
        "text": "equally spread among the course"
      },
      {
        "start": 420.88,
        "duration": 3.2,
        "text": "there is a convenient gremlin spark"
      },
      {
        "start": 422.56,
        "duration": 4.079,
        "text": "configuration api"
      },
      {
        "start": 424.08,
        "duration": 5.519,
        "text": "to control spark settings for an olap"
      },
      {
        "start": 426.639,
        "duration": 2.96,
        "text": "traversal source"
      },
      {
        "start": 429.96,
        "duration": 4.44,
        "text": "g.graph.configuration.set"
      },
      {
        "start": 431.12,
        "duration": 4.88,
        "text": "property allows you to change both spark"
      },
      {
        "start": 434.4,
        "duration": 4.32,
        "text": "and spark cassandra connector"
      },
      {
        "start": 436.0,
        "duration": 4.16,
        "text": "properties for your particular use case"
      },
      {
        "start": 438.72,
        "duration": 3.039,
        "text": "there are many dozens of those"
      },
      {
        "start": 440.16,
        "duration": 3.92,
        "text": "configuration properties"
      },
      {
        "start": 441.759,
        "duration": 4.0,
        "text": "we will see a few in our examples but"
      },
      {
        "start": 444.08,
        "duration": 4.239,
        "text": "please refer to the docs"
      },
      {
        "start": 445.759,
        "duration": 4.241,
        "text": "to learn about many more one thing to"
      },
      {
        "start": 448.319,
        "duration": 3.841,
        "text": "keep in mind is once you"
      },
      {
        "start": 450.0,
        "duration": 4.24,
        "text": "change the configuration property you"
      },
      {
        "start": 452.16,
        "duration": 3.12,
        "text": "may need to kill and restart the apache"
      },
      {
        "start": 454.24,
        "duration": 3.519,
        "text": "tinker pops"
      },
      {
        "start": 455.28,
        "duration": 3.84,
        "text": "spark gremlin application via the spark"
      },
      {
        "start": 457.759,
        "duration": 3.761,
        "text": "web ui"
      },
      {
        "start": 459.12,
        "duration": 4.4,
        "text": "for the change to take an effect here is"
      },
      {
        "start": 461.52,
        "duration": 2.56,
        "text": "an example of how to change three spark"
      },
      {
        "start": 463.52,
        "duration": 2.64,
        "text": "properties"
      },
      {
        "start": 464.08,
        "duration": 3.519,
        "text": "before executing your statistical"
      },
      {
        "start": 466.16,
        "duration": 4.879,
        "text": "gremlin a lab traversal"
      },
      {
        "start": 467.599,
        "duration": 5.44,
        "text": "the spark.course.max property sets the"
      },
      {
        "start": 471.039,
        "duration": 2.641,
        "text": "maximum number of cores used by the"
      },
      {
        "start": 473.039,
        "duration": 3.361,
        "text": "apache"
      },
      {
        "start": 473.68,
        "duration": 4.56,
        "text": "tinker prop spark rambling application"
      },
      {
        "start": 476.4,
        "duration": 4.32,
        "text": "setting this property lower than the"
      },
      {
        "start": 478.24,
        "duration": 4.72,
        "text": "total number of course in your cluster"
      },
      {
        "start": 480.72,
        "duration": 3.44,
        "text": "limits the number of nodes on which the"
      },
      {
        "start": 482.96,
        "duration": 4.4,
        "text": "traversal can be"
      },
      {
        "start": 484.16,
        "duration": 4.08,
        "text": "executed the spark.executor.course"
      },
      {
        "start": 487.36,
        "duration": 3.44,
        "text": "property"
      },
      {
        "start": 488.24,
        "duration": 3.359,
        "text": "sets the number of course used for each"
      },
      {
        "start": 490.8,
        "duration": 3.6,
        "text": "executor"
      },
      {
        "start": 491.599,
        "duration": 5.04,
        "text": "again if the apache thinker pops spark"
      },
      {
        "start": 494.4,
        "duration": 3.519,
        "text": "kremlin application was running before"
      },
      {
        "start": 496.639,
        "duration": 3.441,
        "text": "you change these settings"
      },
      {
        "start": 497.919,
        "duration": 4.241,
        "text": "you should restart it and this is an"
      },
      {
        "start": 500.08,
        "duration": 4.16,
        "text": "example of changing the spark cassandra"
      },
      {
        "start": 502.16,
        "duration": 5.2,
        "text": "connector setting called spark"
      },
      {
        "start": 504.24,
        "duration": 6.56,
        "text": "dot cassandra dot input dot split"
      },
      {
        "start": 507.36,
        "duration": 5.2,
        "text": "dot size in mb this property sets the"
      },
      {
        "start": 510.8,
        "duration": 3.52,
        "text": "approximate size of data"
      },
      {
        "start": 512.56,
        "duration": 5.039,
        "text": "the spark cassandra connector will"
      },
      {
        "start": 514.32,
        "duration": 5.519,
        "text": "request with each individual cql key"
      },
      {
        "start": 517.599,
        "duration": 4.56,
        "text": "in this example we change this setting"
      },
      {
        "start": 519.839,
        "duration": 3.921,
        "text": "because when deleting many edges and"
      },
      {
        "start": 522.159,
        "duration": 4.561,
        "text": "vertices from a graph"
      },
      {
        "start": 523.76,
        "duration": 5.44,
        "text": "we may end up with many tombstones as a"
      },
      {
        "start": 526.72,
        "duration": 3.36,
        "text": "result we may get errors in subsequent"
      },
      {
        "start": 529.2,
        "duration": 2.8,
        "text": "quiz"
      },
      {
        "start": 530.08,
        "duration": 3.52,
        "text": "due to the large number of tombstones"
      },
      {
        "start": 532.0,
        "duration": 4.0,
        "text": "left in the database"
      },
      {
        "start": 533.6,
        "duration": 4.32,
        "text": "to avoid these errors we reduce the"
      },
      {
        "start": 536.0,
        "duration": 4.8,
        "text": "number of tombstones per quest"
      },
      {
        "start": 537.92,
        "duration": 2.88,
        "text": "by setting the"
      },
      {
        "start": 541.24,
        "duration": 2.599,
        "text": "spark.cassandra.input.split.size in"
      },
      {
        "start": 542.6,
        "duration": 4.44,
        "text": "megabytesproperty"
      },
      {
        "start": 543.839,
        "duration": 4.161,
        "text": "to a smaller size and the default of 64"
      },
      {
        "start": 547.04,
        "duration": 3.2,
        "text": "megabytes"
      },
      {
        "start": 548.0,
        "duration": 3.68,
        "text": "in particular we set the property to one"
      },
      {
        "start": 550.24,
        "duration": 3.92,
        "text": "megabyte before dropping"
      },
      {
        "start": 551.68,
        "duration": 4.48,
        "text": "all our users of course there are other"
      },
      {
        "start": 554.16,
        "duration": 3.2,
        "text": "important applications of how data is"
      },
      {
        "start": 556.16,
        "duration": 3.76,
        "text": "read from cassandra"
      },
      {
        "start": 557.36,
        "duration": 4.32,
        "text": "and into how many spark partitions when"
      },
      {
        "start": 559.92,
        "duration": 5.919,
        "text": "these properties changed"
      },
      {
        "start": 561.68,
        "duration": 4.159,
        "text": "finally it is time for you to work on an"
      },
      {
        "start": 566.839,
        "duration": 3.0,
        "text": "exercise"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:05:17.174337+00:00"
}