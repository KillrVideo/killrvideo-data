{
  "video_id": "g9ZNyfq6HZw",
  "title": "Unleash Your App’s Full Potential with GenAI powered by Langflow",
  "description": "Ready to take your AI from basic to brilliant? Combine Langflow with a RAG stack architecture  and watch magic happen.\n\nLangflow  enables you to implement retrieval-augmented generation (RAG) fast, with complete control over every step in the AI data pipeline for more relevant context and answers.t.\n\nImagine creating AI that doesn’t just answer questions but truly understands the context. It’s like giving your AI a turbo boost, making it more reliable and insightful without drowning in code and complex data setups. Perfect for any developer looking to upgrade their AI game.\n\nIn this 45-minute session we will demonstrate how you can:\nImprove AI relevancy with RAG\nAccelerate your experiments\nBuild GenAI apps fast\n\r\n\r\nAbout DataStax Developer:\r\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache Cassandra©.  Visit https://datastax.com/dev for more free learning resources.",
  "published_at": "2024-07-03T07:02:20Z",
  "thumbnail": "https://i.ytimg.com/vi/g9ZNyfq6HZw/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "demo",
    "apache_cassandra",
    "workshop",
    "cassandra",
    "architecture",
    "tutorial",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=g9ZNyfq6HZw",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "and I'm joined today by Chara Parky hello Chara hello hello how are you am well how are you you know somebody mentioned earlier that she could tell I had my third cup of coffee and she was right this is literally my third cup of coffee I just finished it so uh ready to go even though I don't really need it but I I will probably have 10 cups of coffee today um not that I should but I always make the full pot and then I just keep because it gets cold and so you like top it off and top it off and top it and then at some point you're like how did I drink 10 cups of coffee it's going to be a fun live stream yeah I agree I agree so so with that hello everybody and you know just to just to mention real quick as we get started um you know separate from here on growcast we're also simoc casting on uh Twitter live or I should say x live LinkedIn live and YouTube live so everybody we're kind of across the streams right now and I see a bunch of folks coming in I saw Canada and Phoenix where's everybody from um we'd love to know kind of what part of the world you're from and Hugh Brown I see scorcher in Toronto at 75 degrees Fahrenheit I will take that any day of the week here in Florida uh where it is regularly in the 90s uh without the humidity oh we have Poland Morocco Florida how far north did you go oh I'm up in Seattle now um my uh when I left you know 2015 my mom was like could you have moved moved farther away and I said Mom Alaska and she was like okay I will visit you in Seattle point taken point taken oh I see Ido and Houston and Germany couple Germans hello hello Philadelphia awesome all right just so you know everybody you know as we we're just taking just a couple minutes for folks to get into the live stream don't worry we will start at 5 sharp um in the meantime what we can do let's go ahead and start with a poll right so I'm going to I'm going to start a poll and at the same time do a little housekeeping with everybody um so hopefully you see that poll show up but if not take a look to the right side of your screen you should see like a little menu bar here in crowdcast um and you'll see a little toolbar one of those will be polls um you'll also see a Q&A section so if at any point you want to ask us a question or something about the material about rag whatever put it in the Q&A because it allows the other attendees to vote them up we'll kind of you know it's easier for us to kind of manage them that way um so you may notice that if you do ask a question in the chat I'll probably ask you to put it in the Q&A um but yeah try to keep them over there if you can um feel free to chat with everybody else and let's see I already see some people answering on the poll so the question we just asked was what gen Frameworks are you using to build and I'm no surprise I see a bunch of people coming with Lane chain some with Lang flow that's super cool I'm glad to hear that know something else as yet though yeah I'm surprised I've heard at least a lot of uh JavaScript developers tend to roll their own or are trying out Lang chain ja I you know I met someone yesterday that didn't know that Lang chain JS existed and I was like oh yeah I I actually just learned that myself recently and I do code in JavaScript along with python others but I I'm not focused honestly I'm not focused right now on JavaScript I'm more focused on python but I was just at the AI World's Fair uh and I did learn about that I I actually learned that from um uh we had Harrison chase out for an event and from linkchain and so just through conversation I was like oh that's a thing that's like super cool I was like very excited about that all right let's see how that poll's going oh something else you know for the person who U mentioned something else I'd love to know what that something else is right why don't you pop it in the chat very curious to know um what that is and enough we have our first uh QA question but we'll we'll we'll take those as we as we go all righty well cool well that you know keep answering the poll as we go for those who come on in um because it helps give us context while we're here a chatting and by the way cha you know I did mention I'm a I'm a developer relations engineer here at data sax you know um so I've been a coder uh and and a DBA for almost 30 years I've coded a bunch of different languages obviously these days I'm super focused on gen helping people learn how to build gen apps and that kind of deal like what do you do because you have you have a very special set of knowledge right yeah uh so I am the VP of product uh strategy and operations of rag stack uh at data stacks and that's our curated tested open source uh stack for J however my background is in Ai and machine learning I've been in this space for 20 years my first job while I was still in college was using AI uh my PhD is in digital signal processing so I think I've been working with a Transformer models um since it was just an academic exercise and I was you know trying are these the ones that are more than meets the eye or is that something else right as I just I just made a Transformers joke if it just totally fell flat like I grew up in the 80s come on I can't help it all right so what you and everyone in the audience should know about me is like sarcasm is hard for me to detect we used to at work have to have people like do this so that I could tell that there was a joke in there somewhere I will do my best but yes Transformers uh no no no no not that form oh man all righty so with that we're at I think we're at a good point to go ahead and get started we have a good amount of folks in and everything like that so let's do it so here's what we're going to do today all right I am going to just start off with a very quick demo just like five minutes or less to kind of go through a very basic rag pipeline to kind of illustrate why what it does and why it works right and then what we're going to do is we're GNA move over to Chara and Chara is going to bring down the learning hammer and she is going to impart all sorts of really great knowledge and such and then we're going to go into a little bit of deeper dive and what we're doing from the rag Pipeline and such like that um so I'm going to go ahead and share my screen and once I do this if I could get some thumbs up that you all are actually seeing my screen that would be great make sure that it looks good and if anybody has oh wonderful thank you Peter thank you for thank you Jules okay great great I always like to make sure that like what I'm talking about actually shows up that's always useful okay so you know since this is a live stream where we're looking at both rag stack and Lang flow um what I'm going to do here is I'm actually GNA say if I go to um rag stack itself right uh this is just the main doc P page for rag stack now rag stack is a collection of uh various libraries that are curated and tested and such they're really especially you know for for those Enterprise devs out there I don't think it's limited to Enterprise devs um but for those folks that really want to ensure that the libraries that they're working with are you know are going to work right that's kind of a key thing there's a lot moving right now in gen there's a lot of projects out there open source projects um lots of code being slung lots of things moving very fast so rag stack kind of gives you a set of libraries that are again curated and stable right um so Turner we'll talk more about that a little bit later but what I want to point out here is that in these python packages I'll just zoom in a little bit more right you can see there is the base rag stack D aai this would be like I do a pip install rag stack d a um but there are variations on this and one of those is this rag stack AI langlow right so I'm going to go ahead and go down to the Lang flow section just open this up I have a terminal right here and um now for full disclosure um because I've set a couple things up for you all today I have pre-installed everything um but I'm still going to run through it as if I was doing it new um I did create a virtual environment here in Python I do really recommend that um we can argue the finer points of that later um but I I like having kind of a compartmentalize section um where I have my deploy um and you can see I'm just doing my pip install rag stack D A-L flow I'll actually blow this up a little bit now I for what I have set up today I am sticking my version at 106 which is one of the actually one of the very latest ones um if you were installing this freshh no need to worry about that just pip install ragsac d aai Das langlow okay now you hit it you can see again I pre-installed uh the libraries if you were to do this yourself um it takes a couple minutes there's a lot of libraries especially to support all the various models and everything but that's it you need this one library and it has been sheer rated to give you the full rag stack AI complement plus langlow and then once I'm there I can just say langlow run and that should pop up in my browser here in a second you know Jason for the graininess that that you know if anyone else can confirm sometimes that does happen on the crowdcast you may have to give it a moment um we are uh streaming at HD so okay all right okay great so L flow is now up and running so here's what I'm going to do I'm going to gloss over all the details here and I'm just going to dig into um the the rag pipeline kind of demo portion so you get an idea of what's going on now there are really two pieces to this um so if I were to go into Lang flow and start here and say new project and then use this Vector store rag template that's what you're seeing between these two these are just slight variations on that template and in this template it actually contains two flows one flow is the query flow that we're going to use to ask questions of our llm once we have data in our rag store the other one is going to be the general flow where we're going to generate the data we're going to store in our rag pipeline okay now like I said I'm purposely glossing over a bunch of details so don't worry about the individual components if you're not familiar with rag or whatever we will absolutely dig into all of that here's what I want you to point out or I want you to focus on let's pretend for a second um that I'm in chat GPT and I ask a question when did I start coding now what do you all think is going to happen here when I ask this question right there's a very good chance that chat GPT isn't going to an idea what I'm talking about right and as you can see in its response I'll I'll zoom in a little bit um okay um you'll see that the response is you know I don't have the information of when you start a coding can you provide more details I mean how would it like the the large language models are like chain you know trained on all this knowledge from the internet but they don't have my domain knowledge right they don't they don't know much about me unless it was already out there um in the internet so right now with its current context it really just doesn't know anything about me now I'm going to pull this open here now in this particular rag pipeline what I do want to point out and I'll blow this one up a little bit too um hopefully you can see my database on the right hand side this is using astb uh this is part of the data saacks stack so this is a fully-fledged vector store and I've created a collection here called rag stack demo but it's empty I haven't put anything in it right and I've done that on purpose so what I'm going to do is just to kind of demonstrate here that these are the same this top this top workflow you see here is actually the query workflow right and you can say I have the same question so I have no data in here I'm just going to ask it the same question when did I start coding and again it should pretty much say something like hey I don't know what you're talking about right and it says I'm sorry there's not enough information provided because again the llm doesn't have any context so here's what I want to do let's go ahead and this is the gener of part so I'm going to load up some file in this case it's just a PDF uh it happens to be a PDF that I built a while ago it's on my career progression funny enough but here's why this is my domain knowledge I did actually look like that I'm I was actually really amazed at how well chat GPT did in generating that image based off thep yeah that was scarily close um but I started around eight years old on a Commodore 64 right and so this is information that in my domain that I have in this in this data that the llms don't have so what I'm going to do here is I am going to I'm going to feed that in we're going to create embeddings off of that and then we're going to store it in this rag stack demo collection right so it's going to go pop through that again I'm glossing over all sorts of details right now um we will get into them a little later but what I really want to point out is now when I go to this rag stack demo collection you can see that I have some data so it's taken the data from this PDF it has put it into embedding through an embedding model and I have Vector embeddings now represent that data so now if I pull back and if I ask that same question again we should see a different answer this is always fun doing this stuff live right ah check this out so the answer being you started coding when you were around eight years old learning how to code games on your commod 64 which is pretty much exactly what I talked about right here right so the the whole idea here is that with this rag pipeline I able to augment the llm in real time right I didn't have to build a new model I don't have to fine-tune or anything I can literally pull in relevant data real time and then apply it to the llm right so anyway I just wanted to start with that kind of set the stage a little bit I'm not going to hand it over to Chara to dig into to some of the fun uh with rag itself and then we will come back and we'll do a deep dive into how this is actually working all right let me switch to some slides now as I mentioned before David feel free to interrupt and and ask questions if I'm glossing over stuff that you think will be useful for this audience sure um uh I want to make a claim that um you need rag in your your application now is this true right most of the times I would argue yes if you actually want to achieve something that's uh domain specific um personal um customized llms are amazing but they are expensive and they learn patterns not exact precise pieces of information they're great at predicting statistically the next word that's going to come but they're even if your data was indexed on the internet that's one page throughout everything that it's ingested and so it's not enough representation to actually get that data to come back so the the way I want to justify this is by starting with um some apps and and talk to you about a little bit why they had to use rag so this first example is uh an app called uh physics Walla their their learning Guru they are um an app a personalized AI tutor application in India to assist students with their academic and support queries like these are very very very technical questions that they're asking and they're asking it in many different languages they may be doing text to speech they may be typing it in but there there are multiple different languages being supported this application when it started was trying to just essentially chat with a PDF you know not building up a vector store not doing any kind of retrieval there but just like all right the students learning from this textbook can I just use a massive context window is that form of rag enough turns out it's not um so one of the things that they did was they uh collected feedback for every answer that was given you see those thumbs up and thumbs down they were able to measure essentially customer satisfaction and until they were actually able to chunk the data in the right way to retrieve the right information and to summarize that in a way that the user could actually make useful their cat was very low right they started with you know basic Rag and then they had to move to more and more Advanced Techniques and I want to bring this one up because physics is a very specific domain but it's also out in the open right so if if chat GPT has indexed all of the text for physics why on Earth can this not just answer the questions right it's not what Transformer models are built for Transformer models are built for learning patterns and Concepts and not necessarily uh giving you exact matches to what you're asking so I want to move to the next example so skypoint skypoint is a gen app that gives healthare providers instant access to public and private data and they offer this chat interface they have strict slas latency throughput cost Etc but in particular I want to tap in onto this Healthcare data right if I go to chat gbt and I ask it to interpret something certainly I have done this for healthcare before like my partner is having shoulder surgery soon and we got this massive document that talks about his different options I absolutely use chat GPT for that however if you want to have very specific patient record recommendations where only this patient's health history and only the medications that they're using and all of these types of things are used for these types of context you absolutely need to be using rag because these are not General things it's not okay to substitute one medication with another because it's close like you absolutely need to be doing something with rag on this private data and not just you know putting it out on the internet and the last example I'm going to give is this app called ovam Health again it's a it has something to do with Healthcare but it's doing rag in a different way um this application before they had gen was an application for uh women who were trying to get pregnant or were struggling with fertility to get answers from doctors there was this network of doctors that you could submit your question and it would go out to their entire network and provide those answers back but often times it would take a week to get response right these are not doctors that are getting paid for every single response they're it's sort of a volunteer network and they're trying to help people with fertility well what if we could retrieve that medical advice for this my specific case for my age for my demographic group for my particular challenge but we cannot do this simply with llms we have to do this with rag because the attribution of this medical advice to the doctor is critically important right I can't change what this advice is to just hallucinate something from the internet so my argument here is that if you're doing any sort of vertical specific transformative application rag is key fine-tuning is expensive and it's not going to learn your specific data it's going to learn a pattern F shot learning is great when you're trying to teach it something structured like here's exactly how to extract or here's how to format your question and answer but it's not going to get you what you need for something transformative in your domain so everyone is sort of settling on the same pattern rag retrieval augmented generation and what we've noticed is it's the key piece it's the piece that isn't really replaceable you can replace an llm if you need a multimodal llm or you want a smaller llm you can pop those in and out and doesn't sort of it's a trade-off you can make for cost or it's a trade-off you can make for um relevancy but without the data you don't really have that application and chat with PDFs are not really going to change the world so this seems simple right go give me my data put it into the llm but there's actually different kinds of data and we need to consider all kinds of data in order to get the right context at the right time to answer questions so sure we've got the user input I can just ask the user well when did you start coding and you would give it that information right but yeah what do we want to do with scale what are we going to do when we want to deploy this to all kinds of users to many different students to many different patients right we need more than just that user input we need to somehow back that with unstructured data with semi-structured data uh and then with fully structured data and these different kinds of data is going to bring different kinds of context into the prompt so when we're thinking about this rag problem it can get relatively complex and it can actually get kind of confusing and so part of why we want to make this really easy is because we can right we know what patterns are working we know what you're going to want to experiment with and so this is why Lang flow exists right how are we going to build these applications well we're not all experts in coding we're we're not all experts in these rag patterns but what we do know is that we need for our domain to get the right answers right if I'm an agronomist and I want to build a tool that tells Farmers what to do with their crops I don't need to know how to code but anyone trying to build an application can do it quickly with a drag and drop interface and we want to make this available with those templates that you talked about uh to make it quite easy so start with simple rack and then when you're not quite getting the results that you want move to something like graph rag or start to implement something like uh keyword searching or um hybrid searching to get you know better even better results so you'll David will go into this later but I wanted to kind of say like this is why Lang flow now there were some questions in the chat about rag stack what is rag stack we refer to these different abstractions and what do we mean by that right so rag stack as we've mentioned is a curated open-source stack um we take different abstractions from different places so uh the Llama index framework has a different abstraction than the Lang chain framework has a different abstraction than the Lang flow framework so the abstractions that you saw in Lang flow are going to help you build rapidly but maybe you want to have more control so you want to drop down into code well behind each of those little boxes on the screen you can drop down into L chain yeah but maybe you really prefer llama index because I know at least for me the academics in my life love llama Index right they just the documentation is amazing the way that they're testing the rag patterns are great um and for our application developers they prefer Lang chain so we're providing this testing between all of the stock your database your embedding providers your llms your platforms like vertex Ai and Azure ml all of that is already in the stack and when we talk about these abstractions these are the different abstractions that you have access to you can use any of them in order to build these rag applications and our goal is to serve every AI app creator no matter your coding expertise because I believe that it's going to be the domain experts that are going to revolutionize the way we work the way we think about our experiences right you are the folks that are on the ground doing every single day what you do you're a professor and you know exactly how you want to change the way we learn you are a farmer and you know exactly what you need in order to get this Farm producing crops in a way that's healthy for the land well why don't why don't you go ahead and build that so that's the why um and then I wanted to talk a little bit about why we're accelerating these use cases right it's not just what David had said sure we've got Vector similarity that's one form of rag but there's also graph rag where you have non semantically similar information that you need to pull back into the context you know you may need to do full text search you may need to do these various things and there is more to rag than meets the eye I'll give you just two examples before I hand it back over to David so that he can go into a deeper demo that he's going to explain more about langlow this first example is a support bot so the article on the left is about a very specific phone and it's saying you know how do I change the volume you'll notice on step four there's this link ensure that do not disturb switches turned off now as you can imagine there are an infinite number of phones out there I mean not technically infinite but it seems that way and so what if the llm only got the text on the left and it just hallucinated for you how to turn do not disturb off it would not only lose my confidence in Verizon as a support pot but also I would be very frustrated if somebody said like here's how you turn off do not disturb and that's not where it was on my phone so graph is a way where we can relate two pieces of information we can relate them in any way that we want to we can take that link and add it as an edge to our Vector chunks and say well why don't you pull back both of these pieces of text when you're doing your retrieval so this is something that is available in rag stack it's something that we're continuing to innovate on every day and when I say graph rag I don't mean the old traditional knowledge graphs that need onology and experts to build them I mean gen optimize specific graphs that helps you do this fast and easy so so actually I wanted to get in on on that a little bit turna so you're saying like separate from like a graph database or something like that you don't need that here that it's the way that you is it in your metadata your embeddings like where's this information being encapsulated yeah so it could be that um you've got very long PDFs and you need to chunk that in order to get the right you know snippet to return but that snippet that chunk that's returned doesn't have all of the contacts so this Edge this this linkage could be it was in the same document or these chunks were on the same page you could say keywords matter right maybe you know I need to always Traverse when there's a specific amenity being mentioned in a hotel and I want to pull all of these back you can create edges in any way and you can store these in Astra alongside of your vector chunks right so it's just adding metadata of any kind links keywords notations same blood type things like this uh to allow you to pull back seemingly unrelated information okay that's awesome and then the last example I want to give you is um this one on something called Colbert um sure it's not bear no it co bear or Colbert you know I wish somebody could tell me the the answer to this one um I say Bert just because Bert is capitalized uh and then I've tried out Co beer because that's the way my brain wants to say it uh but people kind of look at me sideways like sh what are you talking about I've had this argument now so many times with folks which one is it right yeah this one of those it's just never solved really if y'all would like to argue about it in the chat please do um especially those that have a lot of importance on like the philosophical uh debates um which I I tend to love to have but not on a day-to-day basis um so this is a real world example so we indexed over a thousand Wikipedia articles with both this uh Ada DPR embedding model on the left it's a pretty standard embedding model for vector and a Colbert embedding model on the right very different it isn't the same as uh the sparse um density of information being embedded on the left uh it is very dense um and so what happens is I think I said that opposite dense passage retrieval is when you have a lot of text into a single Vector um and sparse passage retrieval is when you have uh very short amounts of text retrieved and matched multiple times the whole point though is that when you need to have a named entity recognized when you specifically need to know this medication this person's name and you need to retrieve that accurately Vector search isn't the way to do that that's semantic similarity and in fact every time you embed the question it's going to be a slightly different embedding when you ask that question so you're not going to get the level of information in that Vector encoding that you need so you'll notice on the left when you just do normal embedding you search for this person's a name William H Heen and everything that comes back is completely unrelated the name isn't there at all but when you use these other techniques when you do this search with a Colbert embedding model before retrieving the information you get the exact name that you're looking for so it's a complex space um but it's actually uh rapidly evolving and so part of what David is going to show you is how to do this with rag stack L flow it is it is simply Lang flow but it is the tested Sable version so the versioning will often be behind the open source but it will be fully tested with the stack all of the other abstractions to ensure that what you're building is going to work in production you know by the way we were having we were having this discussion about coar and goldber in the chat and uh and I you got to check it out CH because I there's some uh funny comments about the FR friends pronunciation so by the way what right before we get into the demo what we're going to do is we we are going to give you another poll so I'm going to remove the other poll we had um and by the way it is very interesting that it looks like at least here the most the majority of you are in fact using Lane chain and you know what's cool and and try to mention this right but Langan flow itself is built on Lang chain it's not limited to Lane chain but in a in a sense it's kind of like a UI abstraction on top of it so as I show you some of the underlying components I'll get into some of the code of some of them and you'll you'll see the Lang chain call outs there um you should be seeing by the way a new poll now this one's a little chunky uh but given everything that charna just talked about right um we're curious like what kind of data are you injecting into the prom context um is it general knowledge is it really detailed information is it hyperlink support articles I see somebody just uh answered a chunks from very long PDF numerical data multimodel right and what we're curious about here is like what is the Continuum that you're all on because what Char was just getting at a moment ago is that it's not a size fits all um I'm also going to mention some of this as we go through the rag pipeline uh so again so go ahead and answer that poll and um I will pop over I'm gonna share my screen here and do this and we'll get into the demo the second part of the demo I should say alrighty well go ahead CH yeah go ahead yeah while you're getting that ready I was going to answer this question so there's a question on data files for rag how many f files and up to what size files can be uploaded here to use rag um I have a client who has two dozen files with up to 150 megabytes that we need to build a rag for great question um I have uploaded more than 59 gigabytes to do a rag application before across many different document types think about indexing every hotel in the world and every restaurant in the world World in order to give travel recommendations so how many can you do H it's pretty Limitless and it it does depend on your database right like not every Vector database is created equal but Astra absolutely can scale to handle a couple dozen files and a you know a couple hundred megabytes and into the Beyond right did we index all of Wikipedia Yes Yes actually I think that's actually made available uh and there's there's also this other cool yeah it's a really good point actually and and matter of fact it even um the the app I don't remember the URL off the top of my head maybe we can find out while we're going through um but it real time listens for any changes to Wikipedia and automatically then adds them into the rag store and updates any context of a particular document right so it's pretty darn real time you can actually interact with it as things are being added right yeah yeah I forgot about that one well there's also that Swifty GPT which I really love too where you could chat with Taylor Swift um but I certainly I also have a podcast called open source data and I would love to go chat with myself sometimes I know that sounds weird uh but sometimes especially when we're doing something live like this and especially when I get into a conversation I don't actually remember everything that I've said um and so I just have to trust my producer to you know edit the right things or not that's great and I we would kind of like to know could you summarize the points I made in this episode just just for that checkpoint you know funny enough there is I'm not going to show it now but there is actually a component in lenlow that somebody built that allows you to pull the transcripts from YouTube videos yeah and for me is a content creator that's actually pretty relevant right uh to that point right and you can yank those out and then work with them from the LM so that's pretty fun yeah for checkpoint yeah that's right I'm familiar with aotter AI as well thank you Peter all right so here's what I'm gonna do again um just to make sure as we're going give me a thumbs up that you can see the screen here so where I did that kind of quick demo I glossed over a bunch of stuff before this time I'm going to go just a little bit deeper into um everything that is going on um okay so what I've done again this is just um thank you for that Peter I saw the thumbs up um this is again just a slight modification of the vector store rag template and by the way when when you're starting in L flow you can absolutely start with one of the blank flows it's it's really not that hard but I do suggest start with the template start with the basic prompting the memory chapot things like that it'll very quickly show you just how to you know build up capability um and the vector store rag honestly is just it is an extension if you will that's why I call it a rag bot it's like an extension of a chatbot with more capabilities that are than being augmented with our information in the vector store okay so let's go ahead and set this up a little bit so like before we have one part of the flow that is this generative part of the flow it's the part where I'm ingesting data and I'm converting it into vector embeddings and storing it into my uh into my rag store now one thing I want to point out here if you remember before I just had a single file um and I was ingesting just that one PDF file you are not limited I I want to be very clear about something the UI in Lang flow is this kind of quick development no code style of doing things um but at the end of the day it wouldn't be very useful if I was only limited to what's hardcoded here in the UI right um it obviously it has inputs in outputs that allows you to programmatically set these values I will show you that in here in a couple minutes right um so you're not limited to just a single PDF you're also not limited to just files right uh notice below I actually yanked in I'm scraping URLs from one of my old blogs that I have not contributed to in years um and you know it's like actually to your point chal like trying to remember some of the things you've done um I don't remember all of the material from that kind of thing right so what I did in this case was I also pulled those in um if you take here on the left hand side or take a look here on the left hand side you can see there's all sorts of different ways you can get data and tying in is something else that charna said underneath the hood even though you could build out everything from a no code standpoint right um you do not need to be a programmer to use llow but if you are and if you are versed in Python even just a little bit you can get at all the code right all of it is here matter of fact every one of these components is actually a holy contone python module or holy contained python module if you were to pull this out and put it into a module it would work just fine right um You can also modify these right so you can just come in here if I wanted to and I can say you know um make some change whatever you know blah blah blah save that and then I could just save that as a new component and now I have a new component right so it's super flexible from that standpoint you are not limited to the things that are only here that you see in the UI okay great so we have our PDF but now I want to add something else like I said I'm going to go ahead and scrape in uh one of my old blog URLs um and then I'm gonna pull it into this split text component now this is part of the template this is provided for you as part of the template right now what is this actually doing what I'm going to do real quick I'm going to run this part so I can talk through some things I do just want to point out that as I do it I have this collection name langlow this does not exist in my database right so I'm going to go ahead and run this and what this component this ASB component will do it will actually create the collection for me right so it's going to create the collection and put the data in but while that's happening I want to talk to what's going on here um and I should point out that the initial load expect on your first run especially like when I'm doing here since I don't have the collection it has to go create that that takes a moment right here it's already done um but expect on your first load it'll be little slower than subsequent loads okay so I've got data I want to now chunk the data what does this mean this means you see this chunk size here this means I'm going to iterate through whatever data was returned from my my data inputs here and I'm going to iterate through that data in Thousand character lengths and then I'm going to take those thousand character chunks and I'm going to convert those into Vector embeddings now you might ask what Vector embeddings how which ones am I use where does this come into play you may have noticed right below it you see this open AI embeddings component now in Ling flow it is model agnostic and what I mean by that is there are a ton of different providers right you are not limited to open AI or anything as a matter of fact there's even support for a llama if you're using local models right so there's a lot you can do here but for this case in our template I'm using open AI right now notice here underneath model there are set of models so we're saying that open ey has these three embedding models that we've exposed three small three large and 82 now in this case as part of the template I'm using three small now something I want to point out here and this gets back to some of what Kara was talking about earlier right this is actually a super important step and here's why whatever embedding model I choose to embed my data in in my rag store I need to also ensure that when I'm querying I'm also using the same model or at least be in the same family if I try to mix and match right if I were to say uh embed in open AI text embedding three small but then later on I'm like you know I'm going to go ahead and I'm just gonna like throw anthropic in here that that's probably not going to work very well right um now interestingly enough uh if there was something that was not based off the data in your rag store maybe you had some prompt chaining maybe you have a decision tree that um you know like makes a makes some conditional decision based off of some piece of your return data that says oh I want to go process that over here maybe then you can bring in a different model but as far as when you are querying against your rag store you need to make sure that your models match or in the same family right that's a really key part of this and then finally oh yeah go ahead yeah I would say that like if you're thinking about um any kind of search right like if you were trying to use English language to search a French dictionary you would not get back what you expect right it's it's similar to that it's essentially en coding information differently so if you're embedding a video with a video embedding model that's so different embeding language right it's just a completely different representation um and when I think about like how colors are represented for example we've got RGB we've got Cy whatever whatever whatever you cannot compare those two things you can convert between them but you can't compare them and that's that's how I think about embedding models purpose built for the thing you're trying to achieve and not compatible yes absolutely and by the way um I did see your comment Greg um about the color scheme so I did flip it um hopefully this is better um and that is across the board if anybody's having a hard time seeing the Dark theme um you know but if this is also difficult to see let me know and by the way uh Claude support yes Peter uh anthropic um am I thinking the wrong yeah yeah du that was it yeah there is support for Claude uh 35 Sonet is actually in there and and such so um hopefully it answers your question Peter okay okay great so we have our inputs we chunk through the data now what's super cool here you see this little inspection so you have this little inspection icon in all these components and since I ran through it we can actually see the results so now we can see that I have both results that came from the URL that I scraped and from the PDF now one thing that linkflow is doing you'll notice that there's this little data these outputs right you see these little colors in these nodes and everything um and there are types right uh that they conform to so when you're pulling data in it'll actually convert into these data chunks um and when I look at those you can see I have each of these now what this is actually doing it is separating the data out into useful like Fields source title things like that for our particular needs I'm interested in text mostly right and you can see each one of these is actually going to be a chunk of about a thousand characters right this each one of these this text is actually what is being converted into our vector embeddings and then St in our rag store so let's go ahead and take a look real quick now remember before I did not have the Lang flow collection right that didn't exist and now if I go to the langlow collection I should have a bunch of data and I do right so you can see now that I have um some I'm gonna open this up a little bit there we go right you can see that I've I've got the data itself the content that was stored in embeddings and then I've got these Vector embeddings that represent that data mathematically if you will semantic great so now I have a store that actually has some information in it okay and by the way just to point out um another little feature here of LF flow you may notice um that like for the database or for open AI like there's open AI key and you just see this kind of abstract open AI API key what's going on there um could I just go get an open AI key and paste it in the Raw I could but one of the neat features is globals or our globals I should say and when I create a global I can actually store secure values like an API key and then it's available to all of my components so I don't have to keep pasting it all over the place and stuff like that I just sort in a global and that's exactly what I did here for the uh database so this token and endpoint if I go over to the database itself and I go to the overview you'll see I have this endpoint and a token I just copi it over generate one copy it into my Global and now it's available to all of my components so it's a nice little feature for convenience and it is in fact quite convenient okay great so have data in my rag store let's go ahead and take a look at the query the query flow I'm going to kind of focus in here I'm going to change up the question a little bit this time is g to be when did I get married all these things you know like it is my domain knowledge that's why I pulled it in right um but again this is something that if you just ask that question to the llm it's not gonna know what you're talking about right um so we can we can add this context so what's going on in this part of the flow this really ties into something where you might be thinking well how do I bring this into my application right um because I need some types of inputs and outputs in the flow in this workflow in order for my application to be able to programmatically affect the input and then get the output right in this chat input since I'm going to have a conversation with this rag bot this chat input is how I do that and you'll see up in the top here there's a set of inputs chat is um you know pretty darn standard for this kind of uh use case um and you know I'm either going to type in something like it did here or again it has an input where I can programmatically set it and I'm going to show you that the programmatic part here in a moment again I have the same setup the database the rag store or the vector store the same exact collection the same thing with the embeddings now why are we tied why do we need this tied in if I already have my data in my rag store and we've already gener generated the embeddings because of this here so when I ask a question or I have some query I also need to convert that I need to vectorize that right there into the same Vector space as the data that's in my rag store so I can perform a vector similarity search so this whole step when I say build it's actually now going to my my Vector store it is converting this into an embedding it is doing a vector search and it's going to turn my top K results right so it's going to essentially turn the top results based off of the search so if you notice notice that when I look at this I have more data in here than what I return right so it's just returning the top results based off the vector search the question that I asked and again it's converting it into this data object that you see and the text is pretty much what I'm interested in here this is the stuff this these responses this is when I want to feed in to the context of the llm so that ties into our next step which is this pars data step notice that it is saying Hey I want to pars the data from text so again I look at my data object I have text so I can do this nice little convenience thing just wrap that in curries and then this will extract all the test now I didn't actually run that part so let's go ahead and actually I'm going to run the whole flow is what I'm going to do I'll let that finish for a second that way I can talk to it as I'm going I'm just going to run this through so now when I look at text now it's pulled out all the text from all of those results right so you can see this big chunk of text that came out and these are the most relevant results based off of my my Vector search now this next part is actually I think we're this is I think this is one of the coolest pieces this prompt this prompt component and here's what's going on this would be the same exact thing as if I went and Chap gbt and I started typing this kind of stuff in right um but notice how for these variable names I have these crer Braes so a really neat convenience function in langlow will allow you if you have any variable you can put curly braces around it and that will then expose that as an input so I could say something like this some cool input put curly braces around it say check and save boom now I have an input where I can just drag and drop and hook it up makes it super easy to wire these things up right okay so I'm gonna get rid of that one come back over here we ran that through so now if I look at my prompt I have all of this context that was pulled in from my rag store and then the prompt actually says give the contact above answer this question and you can see it piped in my my question I'm actually going to pull that out and again again from the UI standpoint the context came out of this parge data right there but that question that was the same one we started with right we started when did I get married I just pop that over and run it in now what's super cool is I just pulled that prompt out if I were to start a new chat gbt and I did this actually I clearly did not copy that properly let's try that again or maybe I oh I did and I just didn't realize it okay my bad I have it so zoomed in I just didn't see it all all right so notice how now that I've given my llm this all this context and asked the question now it actually can some answer something that has some relevance right and it's the same exact thing we should see uh here in our answer okay then finally that prompt is wired up into our llm again in this particular case um I want to be using a a model that is in the same family is you know how I how I vectorize my data so again I'm using open AI um you know but again we open AI is just part of the example right there are all sorts of different models you can choose from and then finally there's this chat output so when I come out here you can see this was me as I was hitting those builds by the way um but you could see at the end you know it was able to pull some information out that actually came from the blog um that I got some idea now I don't in my blog mention exactly when I get married but it inferred that from information it pull up right so again if I asked that without that particular context it wouldn't have known this at all now finally I've got this flow working I feel like yeah this is doing what I want it to do how do I get this into my app look down at the lower right hand corner you see this API here right so what what llow allows you to do you have a bunch of options here honestly um you could treat this as you know just like a regular API endpoint and you know curl you know hit this in your app um you've got these tweaks here um these are actually the references to the the components themselves right so let's say that I was curious and I'm like you know how do I do this how do I change how do I change this if I edit the tweak then it will automatically update the code right so now I can see oh that's how I do it um so whether this is in the python API now I like using the python API or the um the the JavaScript API if I'm in active development and what I mean by that is in some of the apps I've been building I am actively experimenting with different models and different flows and everything I may not be ready to say um version my flow so I use this as an API server and that way I can come in here and I can just be making all sorts of changes I can just change I can say you know something I actually don't want to use gp24 I want to use for Turbo preview and I can just change it and anytime I make subsequent requests from my app it will reflect that right so that's one way that you could approach it another here you know if if you are in Python you can actually export this whole flow as a Jason blob and then you can import that into your app right and then that essentially brings all of the Gen workflow logic right into your app right so you're not you don't have to use Lang flows an API server if you don't want to um so there's all sorts of different options there it really depends on your your particular need all right so um that that's it from that standpoint like how are we questions how are we doing from that standpoint I think I saw some stuff come in yeah me some of these up here so have a little time left to hit some of these yeah what do we have yeah so if I want to run llama 3 on my PC can I include inference of this model and L flow um absolutely I put in the comments a section in the llow docs that shows how to do the ol Lama local LM llms so local support for llms is supported if you don't see the component there for your specific model uh it as David had showed like it's pretty easy to just make a custom component and you set the basically the API endpoint to Local Host right and then it'll it'll call into where it needs to go awesome what else do we have in there this one for rag Lan flow currently it seems uh there's only support for astb with support being planned for DC and Cassandra any timelines actually yes uh we have I believe it was maybe 18 days ago that released the Cassandra component um so this one here yeah yeah yeah and so this is in the latest version um and in the latest version of langlow and rag stack L flow again one is always slightly behind the other um and so you can use this to connect to Cassandra or to dsse yeah and also to point out like again you know L flow is store agnostic of course we're biased you know we're data Stacks we love Cassandra and Astro DB and and all that kind of deal but you're not limited to just those right you can use other Vector stores and such like that um but you know you lot of our templates I think the rag one for sure will start you with Astro DB uh and luckily it's actually really easy to get going from that standpoint but yes the answer is yeah you have support there okay what about this last one Chara maybe a little off topic how good is llama Index right now in creating agents for processing done uh data in rag for example in comparison to L graph it's a great question um I have a couple of different answers to this I don't know if if our opinions will differ here um and and I don't know how many folks know that you can actually use llama index and Lang chain together so um if you've never used llama parse before um I've actually really like how well it does on getting data ready for Gen we also you know we llama index is one of our partners Lang chain is one of our partners unstructured is one of our partners um and all of these different tools have different sort of purposes for the kind of data and the kind of agent that you're trying to build now that said when I'm thinking about the customer base that I work with the users of rag stack the majority of our users are using Lang chain or langlow um I don't know exactly why that is um but but that's just the choice that they tend to make um and then we have some folks who are using llama index it's not zero but it is a you know a fraction of those I tend to work with Enterprises though so I can't speak for the developer community so maybe David you have a better answer of like you know does it matter if you're Enterprise does it matter if you're startup or an academic oh as far as um llama index compared to Lang chain yeah you know I so far you know I have I I actually just did a tour of a bunch of conferences there's been a lot you know and you know what's interesting is depending on the conference and the context there are some different answers you know at Pon um compared to the AI World's Fair I was having some different conversations you know I will say that a lot more I I definitely run into a lot more folks who have at least try Lan chain right um but again you know these are folks that I'm having conversations with you know kind of on the front lines uh at these developer conferences and such like that I definitely there are definitely some out there doing LL index but kind of to your point earlier um it it seems like it kind of depends on the use case um there are some that are for compared to others like you know an analogy I can make is on actual models right um I know in some of my own development I've been experiencing I've I've been actually experimenting with doing language translation and all sorts of things and and sentiment analysis and that kind of deal and there are some models that are really good at doing the translation piece um but maybe they're not as good at like sentiment analysis piece and that kind of deal and so part of it is just kind of experimenting right and that's actually one of the neat things about link flow it allowed me to go and iterate through um so I I don't have a direct answer from that standpoint um other than I do know i' I've heard Lang chain that I have her llama index but again I think it's totally use case driven yeah I I agree um well we don't have more questions but I wanted to bring us back to the poll that we had put up earlier on what kind of data are people injecting into the prompt so I'm looking at this and it looks like most people aren't doing general knowledge great you probably don't need to be injecting general knowledge into the prop so glad we kind of glossed right over that detail product information things like Medical in these cases you should be taking a look at rag techniques using something like Colbert or something like hybrid search so asra for example supports metadata filters as well as Vector so if there is a specific product that you need to return exact where it match add that as metadata you don't just store the vector you store the metadata as well you can filter on the metadata and do the vector search so when you're doing something very detailed very entity specific know that basic rag sometimes is not going to work especially as your data set gets bigger and bigger and bigger and you have more and more products to recognize um hyperlink support articles if you haven't tried the course range knowled course grained Knowledge Graph or graph rag approach take a look at it we just contribut it to Lang chain it's open source with two code changes of Co lines of code changed you can stand up a a graph with your vector data linked to your other data um and there's some helper functions in there to generate those edges for you uh same with chunks from very long PDFs like if the problem you're running into is that you're getting the right chunk back but you're missing the surrounding context graph can be very useful you could say oh this chunk was on the same page and it was in this document right so sometimes depending on the complexity of the question you're answering you may need to pull back the entire document or you may just need to bring back that page right think about the different kinds of ways that you can relate this information together when you can get semantic similarity back but you can't get some of the related information that you need numerical data this is I am so excited that you guys are doing numerical data uh we were talking about this the other day David where you know uh language embedding model does not embed numerical dat data the same way thats language data the way that you store and retrieve this information is going to be incredibly important in fact you may not vectorize this data at all you may instead want to do something like text to cql or text to SQL in order to actually get that language query to generate a structured query to pull the exact data back right this this type of numerical data isn't always going to I don't know have you ever now have you ever tried to have chat GPT count the words for a little snippet oh my God it yeah it has challenges it can't do it I I try to like shorten the description of my podcast all the time cannot be done do it in less than 200 words it won't do it right it's just not the same information so consider things like text to cql text to SQL uh in order to retrieve that information in a way that's going to reserve the actual data that you need to inject into the prompt yeah and by the way so I Do by the way I know everybody we're we're essentially at time however there is a question here in QA that I can answer real fast there was another one there in the in the comment so I'm going to go ahead and just knock one of these out real fast um so we tried using one of the other Vector store blocks I'm gonna start answering this um we8 on rag stack Lane flow but it did not work it did not it did work on open source L flow the response received on stack Overflow was that rag stack L flow will only work for asro DSC Cassandra as Vector store can you please confirm whether there's this true no it it's not limited to only data Stacks products now one thing to point out is that as charna mentioned earlier because of the nature of rag stack and the fact that all the libraries and things like that they go through all sorts of extensive testing and and they're curated and stuff it the version May lag slightly behind open source but here's a fun trick for you so by the way it should work so I will make sure to get that information back to the team if they don't already if they're not already aware um but here's a neat trick for you remember all of this stuff is just code right underneath the hood so let's say you're in the open source version and you're like hey the we v81 works just copy the code you can go then into um you can do this a couple different ways you can actually go for a custom component if you want pop the code in oh wait I didn't actually copy it from here so let me just do that to illustrate this copy it and I can go into a custom component paste it do this rename it and say we8 or whatever it is right um and now when I here we go now when I save this I now have a component right that is actually I could one I can share this across any of my flows I can also upload it one thing I didn't mention is there is in fact a LF flow store is community store where folks all sorts of folks can actually store their own flow you can upload your flows and your components and such um so it could just be that you know like this would be a way um that you can get that but it is if you have a if you see something like that again you know if it's in if it's an open source and it's working the way that the process goes rag stack isn't very far behind um so it will probably resolve itself but yes if you do see something like that you should be able to pull it from one and go right to the other um one last thing I'm going to say about this here um the store is a really neat way like when I have a flow that is in a good place and I'm like you know something I want to like hold on to this for posterity posterity or maybe I want to version it if I go to share I can say share flow once you know once I've hooked up my store with my API key um it will automatically upload that and so if I come over here to those that are created by me I need to log in apparently been a minute probably uh timed out there we go do this again okay if I go back to um yeah I want the store I want the store there we go if I those they're created by me you can see these are flows that I've been uploading so this is a really neat way um I can kind of again I can like version things put them up there these are not public right when you when you say share you can it's step by default you can uncheck that and keep them for yourself um so that's a nice way you can kind of share components so if you did have a case where you did see something like that or maybe you made a custom component in one but you wanted it in the other one share it up to the store just use the store and pull it down right it it's super easy from that standpoint um let's see and then the last one I think I saw gmax C actually put it into the QA which was you used a web scraper in your demo for ingestion recently I saw you all have integrated with fir crawl can fir crawl be used in place of web scraper um I don't see why not again these components are super flexible um I don't know I don't know turn if you know I don't personally know about the fir crawl component or an integration there but again this is one of those cases where um as that kind of capability is ADD into L flow if it is a component may show up it may be in the store but you can modify these to do whatever and then I actually did one with with 11 Labs I was doing voice transcription and so I just made a very quick component with 11 Labs right and and so you can just upload that stuff up um so anyway yeah it's flexible um I I think there are definitely some some options for you there all right and I do know that we are at time I know we've gone a little over I want to be respectful of that um I think at this point Char unless you want to follow up with anything um if if there are any questions or whatever we didn't get to um we can just follow up an email and get those anbody yeah all righty well with that thank you everybody for your attention today we appreciate you coming on thank you Chara for bringing the information I think uh given some conversations we we may we may do some live streams on some graph rag right and yeah I'd love to show some examples like it's sort of once you see it for the first time you're like hold on I I can do so much with this it's it's a pretty great uh method awesome well thank you everybody that's it for the live stream today and we'll see you in the future take care bye",
    "segments": [
      {
        "start": 0.199,
        "duration": 5.961,
        "text": "and I'm joined today by Chara Parky"
      },
      {
        "start": 2.44,
        "duration": 5.319,
        "text": "hello Chara hello hello how are you am"
      },
      {
        "start": 6.16,
        "duration": 3.519,
        "text": "well how are you you know somebody"
      },
      {
        "start": 7.759,
        "duration": 5.321,
        "text": "mentioned earlier that she could tell I"
      },
      {
        "start": 9.679,
        "duration": 4.92,
        "text": "had my third cup of coffee and she was"
      },
      {
        "start": 13.08,
        "duration": 3.8,
        "text": "right this is literally my third cup of"
      },
      {
        "start": 14.599,
        "duration": 3.801,
        "text": "coffee I just finished it so uh ready to"
      },
      {
        "start": 16.88,
        "duration": 4.479,
        "text": "go even though I don't really need it"
      },
      {
        "start": 18.4,
        "duration": 6.959,
        "text": "but I I will probably have 10 cups of"
      },
      {
        "start": 21.359,
        "duration": 5.92,
        "text": "coffee today um not that I should but I"
      },
      {
        "start": 25.359,
        "duration": 3.92,
        "text": "always make the full pot and then I just"
      },
      {
        "start": 27.279,
        "duration": 3.641,
        "text": "keep because it gets cold and so you"
      },
      {
        "start": 29.279,
        "duration": 2.681,
        "text": "like top it off and top it off and top"
      },
      {
        "start": 30.92,
        "duration": 4.28,
        "text": "it and then at some point you're like"
      },
      {
        "start": 31.96,
        "duration": 3.24,
        "text": "how did I drink 10 cups of"
      },
      {
        "start": 35.52,
        "duration": 4.519,
        "text": "coffee it's going to be a fun live"
      },
      {
        "start": 37.52,
        "duration": 4.12,
        "text": "stream yeah I agree I agree so so with"
      },
      {
        "start": 40.039,
        "duration": 3.121,
        "text": "that hello everybody and you know just"
      },
      {
        "start": 41.64,
        "duration": 3.439,
        "text": "to just to mention real quick as we get"
      },
      {
        "start": 43.16,
        "duration": 4.919,
        "text": "started um you know separate from here"
      },
      {
        "start": 45.079,
        "duration": 4.681,
        "text": "on growcast we're also simoc casting on"
      },
      {
        "start": 48.079,
        "duration": 4.16,
        "text": "uh Twitter live or I should say x live"
      },
      {
        "start": 49.76,
        "duration": 4.319,
        "text": "LinkedIn live and YouTube live so"
      },
      {
        "start": 52.239,
        "duration": 4.081,
        "text": "everybody we're kind of across the"
      },
      {
        "start": 54.079,
        "duration": 4.761,
        "text": "streams right now and I see a bunch of"
      },
      {
        "start": 56.32,
        "duration": 4.12,
        "text": "folks coming in I saw Canada and Phoenix"
      },
      {
        "start": 58.84,
        "duration": 3.32,
        "text": "where's everybody from"
      },
      {
        "start": 60.44,
        "duration": 4.32,
        "text": "um we'd love to know kind of what part"
      },
      {
        "start": 62.16,
        "duration": 2.6,
        "text": "of the world you're"
      },
      {
        "start": 64.92,
        "duration": 4.72,
        "text": "from and Hugh Brown I see scorcher in"
      },
      {
        "start": 67.36,
        "duration": 5.0,
        "text": "Toronto at 75 degrees Fahrenheit I will"
      },
      {
        "start": 69.64,
        "duration": 5.0,
        "text": "take that any day of the week here in"
      },
      {
        "start": 72.36,
        "duration": 5.119,
        "text": "Florida uh where it is regularly in the"
      },
      {
        "start": 74.64,
        "duration": 5.76,
        "text": "90s uh without the humidity oh we have"
      },
      {
        "start": 77.479,
        "duration": 6.28,
        "text": "Poland Morocco"
      },
      {
        "start": 80.4,
        "duration": 6.88,
        "text": "Florida how far north did you go oh I'm"
      },
      {
        "start": 83.759,
        "duration": 5.641,
        "text": "up in Seattle now um my uh when I left"
      },
      {
        "start": 87.28,
        "duration": 4.32,
        "text": "you know 2015 my mom was like could you"
      },
      {
        "start": 89.4,
        "duration": 5.6,
        "text": "have moved moved farther away and I said"
      },
      {
        "start": 91.6,
        "duration": 6.559,
        "text": "Mom Alaska and she was like"
      },
      {
        "start": 95.0,
        "duration": 5.479,
        "text": "okay I will visit you in Seattle point"
      },
      {
        "start": 98.159,
        "duration": 4.841,
        "text": "taken point taken oh I see Ido and"
      },
      {
        "start": 100.479,
        "duration": 3.721,
        "text": "Houston and Germany couple Germans hello"
      },
      {
        "start": 103.0,
        "duration": 4.159,
        "text": "hello"
      },
      {
        "start": 104.2,
        "duration": 4.64,
        "text": "Philadelphia"
      },
      {
        "start": 107.159,
        "duration": 3.681,
        "text": "awesome all right just so you know"
      },
      {
        "start": 108.84,
        "duration": 4.08,
        "text": "everybody you know as we we're just"
      },
      {
        "start": 110.84,
        "duration": 3.959,
        "text": "taking just a couple minutes for folks"
      },
      {
        "start": 112.92,
        "duration": 5.32,
        "text": "to get into the live stream don't worry"
      },
      {
        "start": 114.799,
        "duration": 5.841,
        "text": "we will start at 5 sharp um in the"
      },
      {
        "start": 118.24,
        "duration": 4.879,
        "text": "meantime what we can do let's go ahead"
      },
      {
        "start": 120.64,
        "duration": 4.28,
        "text": "and start with a poll right so I'm going"
      },
      {
        "start": 123.119,
        "duration": 3.36,
        "text": "to I'm going to start a poll and at the"
      },
      {
        "start": 124.92,
        "duration": 4.08,
        "text": "same time do a little housekeeping with"
      },
      {
        "start": 126.479,
        "duration": 4.921,
        "text": "everybody um so hopefully you see that"
      },
      {
        "start": 129.0,
        "duration": 4.16,
        "text": "poll show up but if not take a look to"
      },
      {
        "start": 131.4,
        "duration": 3.32,
        "text": "the right side of your screen you should"
      },
      {
        "start": 133.16,
        "duration": 3.88,
        "text": "see like a little menu bar here in"
      },
      {
        "start": 134.72,
        "duration": 4.68,
        "text": "crowdcast um and you'll see a little"
      },
      {
        "start": 137.04,
        "duration": 4.96,
        "text": "toolbar one of those will be polls um"
      },
      {
        "start": 139.4,
        "duration": 4.24,
        "text": "you'll also see a Q&A section so if at"
      },
      {
        "start": 142.0,
        "duration": 3.2,
        "text": "any point you want to ask us a question"
      },
      {
        "start": 143.64,
        "duration": 4.36,
        "text": "or something about the material about"
      },
      {
        "start": 145.2,
        "duration": 4.759,
        "text": "rag whatever put it in the Q&A because"
      },
      {
        "start": 148.0,
        "duration": 3.36,
        "text": "it allows the other attendees to vote"
      },
      {
        "start": 149.959,
        "duration": 2.681,
        "text": "them up we'll kind of you know it's"
      },
      {
        "start": 151.36,
        "duration": 3.48,
        "text": "easier for us to kind of manage them"
      },
      {
        "start": 152.64,
        "duration": 3.92,
        "text": "that way um so you may notice that if"
      },
      {
        "start": 154.84,
        "duration": 3.6,
        "text": "you do ask a question in the chat I'll"
      },
      {
        "start": 156.56,
        "duration": 3.16,
        "text": "probably ask you to put it in the Q&A um"
      },
      {
        "start": 158.44,
        "duration": 3.12,
        "text": "but yeah try to keep them over there if"
      },
      {
        "start": 159.72,
        "duration": 3.879,
        "text": "you can um feel free to chat with"
      },
      {
        "start": 161.56,
        "duration": 4.2,
        "text": "everybody else and let's see I already"
      },
      {
        "start": 163.599,
        "duration": 4.841,
        "text": "see some people answering on the poll so"
      },
      {
        "start": 165.76,
        "duration": 5.64,
        "text": "the question we just asked was what gen"
      },
      {
        "start": 168.44,
        "duration": 4.64,
        "text": "Frameworks are you using to build and"
      },
      {
        "start": 171.4,
        "duration": 3.88,
        "text": "I'm no surprise I see a bunch of people"
      },
      {
        "start": 173.08,
        "duration": 4.04,
        "text": "coming with Lane chain some with Lang"
      },
      {
        "start": 175.28,
        "duration": 4.84,
        "text": "flow that's super cool I'm glad to hear"
      },
      {
        "start": 177.12,
        "duration": 3.0,
        "text": "that"
      },
      {
        "start": 180.879,
        "duration": 5.121,
        "text": "know something else as yet though yeah"
      },
      {
        "start": 183.2,
        "duration": 5.28,
        "text": "I'm surprised I've heard at least a lot"
      },
      {
        "start": 186.0,
        "duration": 6.48,
        "text": "of uh JavaScript developers tend to roll"
      },
      {
        "start": 188.48,
        "duration": 6.399,
        "text": "their own or are trying out Lang chain"
      },
      {
        "start": 192.48,
        "duration": 3.96,
        "text": "ja I you know I met someone yesterday"
      },
      {
        "start": 194.879,
        "duration": 4.401,
        "text": "that didn't know that Lang chain JS"
      },
      {
        "start": 196.44,
        "duration": 3.879,
        "text": "existed and I was like oh yeah I I"
      },
      {
        "start": 199.28,
        "duration": 3.2,
        "text": "actually just learned that myself"
      },
      {
        "start": 200.319,
        "duration": 4.081,
        "text": "recently and I do code in JavaScript"
      },
      {
        "start": 202.48,
        "duration": 3.319,
        "text": "along with python others but I I'm not"
      },
      {
        "start": 204.4,
        "duration": 2.8,
        "text": "focused honestly I'm not focused right"
      },
      {
        "start": 205.799,
        "duration": 3.36,
        "text": "now on JavaScript I'm more focused on"
      },
      {
        "start": 207.2,
        "duration": 6.44,
        "text": "python but I was just at the AI World's"
      },
      {
        "start": 209.159,
        "duration": 6.961,
        "text": "Fair uh and I did learn about that I I"
      },
      {
        "start": 213.64,
        "duration": 4.44,
        "text": "actually learned that from um uh we had"
      },
      {
        "start": 216.12,
        "duration": 3.759,
        "text": "Harrison chase out for an event and from"
      },
      {
        "start": 218.08,
        "duration": 3.32,
        "text": "linkchain and so just through"
      },
      {
        "start": 219.879,
        "duration": 3.121,
        "text": "conversation I was like oh that's a"
      },
      {
        "start": 221.4,
        "duration": 4.28,
        "text": "thing that's like super cool I was like"
      },
      {
        "start": 223.0,
        "duration": 4.959,
        "text": "very excited about"
      },
      {
        "start": 225.68,
        "duration": 4.72,
        "text": "that all right let's see how that poll's"
      },
      {
        "start": 227.959,
        "duration": 5.161,
        "text": "going oh something else you know for the"
      },
      {
        "start": 230.4,
        "duration": 4.039,
        "text": "person who U mentioned something else"
      },
      {
        "start": 233.12,
        "duration": 2.92,
        "text": "I'd love to know what that something"
      },
      {
        "start": 234.439,
        "duration": 4.321,
        "text": "else is right why don't you pop it in"
      },
      {
        "start": 236.04,
        "duration": 5.6,
        "text": "the chat very curious to know um what"
      },
      {
        "start": 238.76,
        "duration": 5.36,
        "text": "that is and enough we have our first uh"
      },
      {
        "start": 241.64,
        "duration": 6.879,
        "text": "QA question but we'll we'll we'll take"
      },
      {
        "start": 244.12,
        "duration": 4.399,
        "text": "those as we as we"
      },
      {
        "start": 253.76,
        "duration": 4.36,
        "text": "go all righty well cool well that you"
      },
      {
        "start": 255.879,
        "duration": 4.76,
        "text": "know keep answering the poll as we go"
      },
      {
        "start": 258.12,
        "duration": 5.639,
        "text": "for those who come on in um because it"
      },
      {
        "start": 260.639,
        "duration": 5.241,
        "text": "helps give us context while we're here a"
      },
      {
        "start": 263.759,
        "duration": 4.0,
        "text": "chatting and by the way cha you know I"
      },
      {
        "start": 265.88,
        "duration": 3.68,
        "text": "did mention I'm a I'm a developer"
      },
      {
        "start": 267.759,
        "duration": 2.521,
        "text": "relations engineer here at data sax you"
      },
      {
        "start": 269.56,
        "duration": 3.56,
        "text": "know"
      },
      {
        "start": 270.28,
        "duration": 4.96,
        "text": "um so I've been a coder uh and and a DBA"
      },
      {
        "start": 273.12,
        "duration": 3.72,
        "text": "for almost 30 years I've coded a bunch"
      },
      {
        "start": 275.24,
        "duration": 4.32,
        "text": "of different languages obviously these"
      },
      {
        "start": 276.84,
        "duration": 5.04,
        "text": "days I'm super focused on gen helping"
      },
      {
        "start": 279.56,
        "duration": 4.479,
        "text": "people learn how to build gen apps and"
      },
      {
        "start": 281.88,
        "duration": 4.08,
        "text": "that kind of deal like what do you do"
      },
      {
        "start": 284.039,
        "duration": 5.401,
        "text": "because you have you have a very special"
      },
      {
        "start": 285.96,
        "duration": 6.679,
        "text": "set of knowledge right yeah uh so I am"
      },
      {
        "start": 289.44,
        "duration": 5.96,
        "text": "the VP of product uh strategy and"
      },
      {
        "start": 292.639,
        "duration": 5.4,
        "text": "operations of rag stack uh at data"
      },
      {
        "start": 295.4,
        "duration": 6.48,
        "text": "stacks and that's our curated tested"
      },
      {
        "start": 298.039,
        "duration": 6.201,
        "text": "open source uh stack for J however my"
      },
      {
        "start": 301.88,
        "duration": 4.28,
        "text": "background is in Ai and machine learning"
      },
      {
        "start": 304.24,
        "duration": 3.92,
        "text": "I've been in this space for 20 years my"
      },
      {
        "start": 306.16,
        "duration": 4.96,
        "text": "first job while I was still in college"
      },
      {
        "start": 308.16,
        "duration": 5.28,
        "text": "was using AI uh my PhD is in digital"
      },
      {
        "start": 311.12,
        "duration": 5.4,
        "text": "signal processing so I think I've been"
      },
      {
        "start": 313.44,
        "duration": 5.039,
        "text": "working with a Transformer models um"
      },
      {
        "start": 316.52,
        "duration": 4.239,
        "text": "since it was just an academic exercise"
      },
      {
        "start": 318.479,
        "duration": 4.28,
        "text": "and I was you know trying are these the"
      },
      {
        "start": 320.759,
        "duration": 4.921,
        "text": "ones that are more than meets the eye or"
      },
      {
        "start": 322.759,
        "duration": 4.401,
        "text": "is that something else right as I just I"
      },
      {
        "start": 325.68,
        "duration": 3.4,
        "text": "just made a Transformers joke if it just"
      },
      {
        "start": 327.16,
        "duration": 4.24,
        "text": "totally fell flat like I grew up in the"
      },
      {
        "start": 329.08,
        "duration": 5.0,
        "text": "80s come on I can't help"
      },
      {
        "start": 331.4,
        "duration": 3.799,
        "text": "it all right so what you and everyone in"
      },
      {
        "start": 334.08,
        "duration": 3.28,
        "text": "the audience should know about me is"
      },
      {
        "start": 335.199,
        "duration": 4.161,
        "text": "like sarcasm is hard for me to detect we"
      },
      {
        "start": 337.36,
        "duration": 4.0,
        "text": "used to at work have to have people like"
      },
      {
        "start": 339.36,
        "duration": 4.399,
        "text": "do this so that I could tell that there"
      },
      {
        "start": 341.36,
        "duration": 5.679,
        "text": "was a joke in there somewhere I will do"
      },
      {
        "start": 343.759,
        "duration": 6.121,
        "text": "my best but yes Transformers uh no no no"
      },
      {
        "start": 347.039,
        "duration": 2.841,
        "text": "no not that"
      },
      {
        "start": 350.759,
        "duration": 5.16,
        "text": "form oh"
      },
      {
        "start": 353.08,
        "duration": 4.239,
        "text": "man all righty so with that we're at I"
      },
      {
        "start": 355.919,
        "duration": 3.28,
        "text": "think we're at a good point to go ahead"
      },
      {
        "start": 357.319,
        "duration": 3.72,
        "text": "and get started we have a good amount of"
      },
      {
        "start": 359.199,
        "duration": 3.12,
        "text": "folks in and everything like that so"
      },
      {
        "start": 361.039,
        "duration": 3.72,
        "text": "let's do it so here's what we're going"
      },
      {
        "start": 362.319,
        "duration": 5.681,
        "text": "to do today all right I am going to just"
      },
      {
        "start": 364.759,
        "duration": 5.641,
        "text": "start off with a very quick demo just"
      },
      {
        "start": 368.0,
        "duration": 4.16,
        "text": "like five minutes or less to kind of go"
      },
      {
        "start": 370.4,
        "duration": 4.76,
        "text": "through a very basic rag pipeline to"
      },
      {
        "start": 372.16,
        "duration": 4.84,
        "text": "kind of illustrate why what it does and"
      },
      {
        "start": 375.16,
        "duration": 2.8,
        "text": "why it works right and then what we're"
      },
      {
        "start": 377.0,
        "duration": 2.44,
        "text": "going to do is we're GNA move over to"
      },
      {
        "start": 377.96,
        "duration": 3.639,
        "text": "Chara and Chara is going to bring down"
      },
      {
        "start": 379.44,
        "duration": 3.479,
        "text": "the learning hammer and she is going to"
      },
      {
        "start": 381.599,
        "duration": 2.401,
        "text": "impart all sorts of really great"
      },
      {
        "start": 382.919,
        "duration": 3.481,
        "text": "knowledge and such and then we're going"
      },
      {
        "start": 384.0,
        "duration": 4.56,
        "text": "to go into a little bit of deeper dive"
      },
      {
        "start": 386.4,
        "duration": 4.28,
        "text": "and what we're doing from the rag"
      },
      {
        "start": 388.56,
        "duration": 6.52,
        "text": "Pipeline and such like that um so I'm"
      },
      {
        "start": 390.68,
        "duration": 4.4,
        "text": "going to go ahead and share my"
      },
      {
        "start": 395.28,
        "duration": 6.319,
        "text": "screen and once I do this if I could get"
      },
      {
        "start": 399.28,
        "duration": 5.4,
        "text": "some thumbs up that you all are actually"
      },
      {
        "start": 401.599,
        "duration": 7.04,
        "text": "seeing my screen that would be"
      },
      {
        "start": 404.68,
        "duration": 3.959,
        "text": "great make sure that it looks"
      },
      {
        "start": 412.599,
        "duration": 4.0,
        "text": "good and if anybody has oh wonderful"
      },
      {
        "start": 415.319,
        "duration": 3.121,
        "text": "thank you Peter thank you for thank you"
      },
      {
        "start": 416.599,
        "duration": 3.241,
        "text": "Jules okay great great I always like to"
      },
      {
        "start": 418.44,
        "duration": 2.84,
        "text": "make sure that like what I'm talking"
      },
      {
        "start": 419.84,
        "duration": 4.68,
        "text": "about actually shows up that's always"
      },
      {
        "start": 421.28,
        "duration": 5.4,
        "text": "useful okay so you know since this is a"
      },
      {
        "start": 424.52,
        "duration": 4.76,
        "text": "live stream where we're looking at both"
      },
      {
        "start": 426.68,
        "duration": 4.6,
        "text": "rag stack and Lang flow um what I'm"
      },
      {
        "start": 429.28,
        "duration": 5.72,
        "text": "going to do here is I'm actually GNA say"
      },
      {
        "start": 431.28,
        "duration": 5.84,
        "text": "if I go to um rag stack itself right uh"
      },
      {
        "start": 435.0,
        "duration": 6.319,
        "text": "this is just the main doc P page for rag"
      },
      {
        "start": 437.12,
        "duration": 6.04,
        "text": "stack now rag stack is a collection of"
      },
      {
        "start": 441.319,
        "duration": 3.481,
        "text": "uh various libraries that are curated"
      },
      {
        "start": 443.16,
        "duration": 3.24,
        "text": "and tested and such they're really"
      },
      {
        "start": 444.8,
        "duration": 2.88,
        "text": "especially you know for for those"
      },
      {
        "start": 446.4,
        "duration": 3.479,
        "text": "Enterprise devs out there I don't think"
      },
      {
        "start": 447.68,
        "duration": 3.48,
        "text": "it's limited to Enterprise devs um but"
      },
      {
        "start": 449.879,
        "duration": 3.241,
        "text": "for those folks that really want to"
      },
      {
        "start": 451.16,
        "duration": 4.439,
        "text": "ensure that the libraries that they're"
      },
      {
        "start": 453.12,
        "duration": 4.199,
        "text": "working with are you know are going to"
      },
      {
        "start": 455.599,
        "duration": 3.72,
        "text": "work right that's kind of a key thing"
      },
      {
        "start": 457.319,
        "duration": 4.041,
        "text": "there's a lot moving right now in gen"
      },
      {
        "start": 459.319,
        "duration": 4.16,
        "text": "there's a lot of projects out there open"
      },
      {
        "start": 461.36,
        "duration": 4.399,
        "text": "source projects um lots of code being"
      },
      {
        "start": 463.479,
        "duration": 3.961,
        "text": "slung lots of things moving very fast so"
      },
      {
        "start": 465.759,
        "duration": 3.321,
        "text": "rag stack kind of gives you a set of"
      },
      {
        "start": 467.44,
        "duration": 3.719,
        "text": "libraries that are again curated and"
      },
      {
        "start": 469.08,
        "duration": 4.079,
        "text": "stable right um so Turner we'll talk"
      },
      {
        "start": 471.159,
        "duration": 4.0,
        "text": "more about that a little bit later but"
      },
      {
        "start": 473.159,
        "duration": 3.641,
        "text": "what I want to point out here is that in"
      },
      {
        "start": 475.159,
        "duration": 3.76,
        "text": "these python packages I'll just zoom in"
      },
      {
        "start": 476.8,
        "duration": 4.519,
        "text": "a little bit more right you can see"
      },
      {
        "start": 478.919,
        "duration": 4.12,
        "text": "there is the base rag stack D aai this"
      },
      {
        "start": 481.319,
        "duration": 4.681,
        "text": "would be like I do a pip install rag"
      },
      {
        "start": 483.039,
        "duration": 5.481,
        "text": "stack d a um but there are variations on"
      },
      {
        "start": 486.0,
        "duration": 4.96,
        "text": "this and one of those is this rag stack"
      },
      {
        "start": 488.52,
        "duration": 3.6,
        "text": "AI langlow right so I'm going to go"
      },
      {
        "start": 490.96,
        "duration": 3.519,
        "text": "ahead and go down to the Lang flow"
      },
      {
        "start": 492.12,
        "duration": 6.68,
        "text": "section just open this up I have a"
      },
      {
        "start": 494.479,
        "duration": 6.4,
        "text": "terminal right here and um now for full"
      },
      {
        "start": 498.8,
        "duration": 4.119,
        "text": "disclosure um because I've set a couple"
      },
      {
        "start": 500.879,
        "duration": 4.681,
        "text": "things up for you all today I have"
      },
      {
        "start": 502.919,
        "duration": 4.041,
        "text": "pre-installed everything um but I'm"
      },
      {
        "start": 505.56,
        "duration": 3.44,
        "text": "still going to run through it as if I"
      },
      {
        "start": 506.96,
        "duration": 4.199,
        "text": "was doing it new um I did create a"
      },
      {
        "start": 509.0,
        "duration": 4.76,
        "text": "virtual environment here in Python I do"
      },
      {
        "start": 511.159,
        "duration": 4.8,
        "text": "really recommend that um we can argue"
      },
      {
        "start": 513.76,
        "duration": 4.12,
        "text": "the finer points of that later um but I"
      },
      {
        "start": 515.959,
        "duration": 5.041,
        "text": "I like having kind of a compartmentalize"
      },
      {
        "start": 517.88,
        "duration": 4.279,
        "text": "section um where I have my deploy um and"
      },
      {
        "start": 521.0,
        "duration": 3.88,
        "text": "you can see I'm just doing my pip"
      },
      {
        "start": 522.159,
        "duration": 5.001,
        "text": "install rag stack D A-L flow I'll"
      },
      {
        "start": 524.88,
        "duration": 4.639,
        "text": "actually blow this up a little bit now I"
      },
      {
        "start": 527.16,
        "duration": 4.6,
        "text": "for what I have set up today I am"
      },
      {
        "start": 529.519,
        "duration": 3.961,
        "text": "sticking my version at 106 which is one"
      },
      {
        "start": 531.76,
        "duration": 3.44,
        "text": "of the actually one of the very latest"
      },
      {
        "start": 533.48,
        "duration": 3.56,
        "text": "ones um if you were installing this"
      },
      {
        "start": 535.2,
        "duration": 5.879,
        "text": "freshh no need to worry about that just"
      },
      {
        "start": 537.04,
        "duration": 6.0,
        "text": "pip install ragsac d aai Das langlow"
      },
      {
        "start": 541.079,
        "duration": 4.2,
        "text": "okay now you hit it you can see again I"
      },
      {
        "start": 543.04,
        "duration": 4.039,
        "text": "pre-installed uh the libraries if you"
      },
      {
        "start": 545.279,
        "duration": 2.961,
        "text": "were to do this yourself um it takes a"
      },
      {
        "start": 547.079,
        "duration": 2.76,
        "text": "couple minutes there's a lot of"
      },
      {
        "start": 548.24,
        "duration": 3.8,
        "text": "libraries especially to support all the"
      },
      {
        "start": 549.839,
        "duration": 4.481,
        "text": "various models and everything but that's"
      },
      {
        "start": 552.04,
        "duration": 4.52,
        "text": "it you need this one library and it has"
      },
      {
        "start": 554.32,
        "duration": 5.759,
        "text": "been sheer rated to give you the full"
      },
      {
        "start": 556.56,
        "duration": 7.12,
        "text": "rag stack AI complement plus langlow and"
      },
      {
        "start": 560.079,
        "duration": 6.2,
        "text": "then once I'm there I can just say"
      },
      {
        "start": 563.68,
        "duration": 6.44,
        "text": "langlow run and that should pop up in my"
      },
      {
        "start": 566.279,
        "duration": 3.841,
        "text": "browser here in a second"
      },
      {
        "start": 572.16,
        "duration": 4.04,
        "text": "you know Jason for the graininess that"
      },
      {
        "start": 574.2,
        "duration": 3.36,
        "text": "that you know if anyone else can confirm"
      },
      {
        "start": 576.2,
        "duration": 2.68,
        "text": "sometimes that does happen on the"
      },
      {
        "start": 577.56,
        "duration": 6.04,
        "text": "crowdcast you may have to give it a"
      },
      {
        "start": 578.88,
        "duration": 4.72,
        "text": "moment um we are uh streaming at HD"
      },
      {
        "start": 583.88,
        "duration": 6.36,
        "text": "so okay all right okay great so L flow"
      },
      {
        "start": 588.56,
        "duration": 4.68,
        "text": "is now up and running so here's what I'm"
      },
      {
        "start": 590.24,
        "duration": 4.64,
        "text": "going to do I'm going to gloss over all"
      },
      {
        "start": 593.24,
        "duration": 4.96,
        "text": "the details here and I'm just going to"
      },
      {
        "start": 594.88,
        "duration": 5.519,
        "text": "dig into um the the rag pipeline kind of"
      },
      {
        "start": 598.2,
        "duration": 3.24,
        "text": "demo portion so you get an idea of"
      },
      {
        "start": 600.399,
        "duration": 4.161,
        "text": "what's going"
      },
      {
        "start": 601.44,
        "duration": 5.04,
        "text": "on now there are really two pieces to"
      },
      {
        "start": 604.56,
        "duration": 4.92,
        "text": "this um so if I were to go into Lang"
      },
      {
        "start": 606.48,
        "duration": 5.12,
        "text": "flow and start here and say new project"
      },
      {
        "start": 609.48,
        "duration": 3.68,
        "text": "and then use this Vector store rag"
      },
      {
        "start": 611.6,
        "duration": 3.2,
        "text": "template that's what you're seeing"
      },
      {
        "start": 613.16,
        "duration": 4.2,
        "text": "between these two these are just slight"
      },
      {
        "start": 614.8,
        "duration": 6.0,
        "text": "variations on that template and in this"
      },
      {
        "start": 617.36,
        "duration": 5.12,
        "text": "template it actually contains two flows"
      },
      {
        "start": 620.8,
        "duration": 4.68,
        "text": "one flow is the query flow that we're"
      },
      {
        "start": 622.48,
        "duration": 5.24,
        "text": "going to use to ask questions of our llm"
      },
      {
        "start": 625.48,
        "duration": 3.96,
        "text": "once we have data in our rag store the"
      },
      {
        "start": 627.72,
        "duration": 3.28,
        "text": "other one is going to be the general"
      },
      {
        "start": 629.44,
        "duration": 3.48,
        "text": "flow where we're going to generate the"
      },
      {
        "start": 631.0,
        "duration": 4.12,
        "text": "data we're going to store in our rag"
      },
      {
        "start": 632.92,
        "duration": 3.52,
        "text": "pipeline okay now like I said I'm"
      },
      {
        "start": 635.12,
        "duration": 2.64,
        "text": "purposely glossing over a bunch of"
      },
      {
        "start": 636.44,
        "duration": 2.68,
        "text": "details so don't worry about the"
      },
      {
        "start": 637.76,
        "duration": 3.4,
        "text": "individual components if you're not"
      },
      {
        "start": 639.12,
        "duration": 3.839,
        "text": "familiar with rag or whatever we will"
      },
      {
        "start": 641.16,
        "duration": 3.0,
        "text": "absolutely dig into all of that here's"
      },
      {
        "start": 642.959,
        "duration": 3.521,
        "text": "what I want you to point out or I want"
      },
      {
        "start": 644.16,
        "duration": 4.679,
        "text": "you to focus on let's pretend for a"
      },
      {
        "start": 646.48,
        "duration": 6.56,
        "text": "second um that I'm in chat GPT and I ask"
      },
      {
        "start": 648.839,
        "duration": 5.56,
        "text": "a question when did I start coding now"
      },
      {
        "start": 653.04,
        "duration": 3.88,
        "text": "what do you all think is going to happen"
      },
      {
        "start": 654.399,
        "duration": 4.361,
        "text": "here when I ask this question right"
      },
      {
        "start": 656.92,
        "duration": 3.52,
        "text": "there's a very good chance that chat GPT"
      },
      {
        "start": 658.76,
        "duration": 3.48,
        "text": "isn't going to an idea what I'm talking"
      },
      {
        "start": 660.44,
        "duration": 5.0,
        "text": "about right and as you can see in its"
      },
      {
        "start": 662.24,
        "duration": 6.64,
        "text": "response I'll I'll zoom in a little bit"
      },
      {
        "start": 665.44,
        "duration": 5.24,
        "text": "um okay um you'll see that the response"
      },
      {
        "start": 668.88,
        "duration": 3.199,
        "text": "is you know I don't have the information"
      },
      {
        "start": 670.68,
        "duration": 3.2,
        "text": "of when you start a coding can you"
      },
      {
        "start": 672.079,
        "duration": 3.32,
        "text": "provide more details I mean how would it"
      },
      {
        "start": 673.88,
        "duration": 3.399,
        "text": "like the the large language models are"
      },
      {
        "start": 675.399,
        "duration": 2.961,
        "text": "like chain you know trained on all this"
      },
      {
        "start": 677.279,
        "duration": 3.12,
        "text": "knowledge from the internet but they"
      },
      {
        "start": 678.36,
        "duration": 3.8,
        "text": "don't have my domain knowledge right"
      },
      {
        "start": 680.399,
        "duration": 3.721,
        "text": "they don't they don't know much about me"
      },
      {
        "start": 682.16,
        "duration": 3.84,
        "text": "unless it was already out there um in"
      },
      {
        "start": 684.12,
        "duration": 4.36,
        "text": "the internet so right now with its"
      },
      {
        "start": 686.0,
        "duration": 5.279,
        "text": "current context it really just doesn't"
      },
      {
        "start": 688.48,
        "duration": 5.2,
        "text": "know anything about me"
      },
      {
        "start": 691.279,
        "duration": 5.281,
        "text": "now I'm going to pull this open here now"
      },
      {
        "start": 693.68,
        "duration": 4.48,
        "text": "in this particular rag pipeline what I"
      },
      {
        "start": 696.56,
        "duration": 4.279,
        "text": "do want to point out and I'll blow this"
      },
      {
        "start": 698.16,
        "duration": 4.919,
        "text": "one up a little bit too um hopefully you"
      },
      {
        "start": 700.839,
        "duration": 4.68,
        "text": "can see my database on the right hand"
      },
      {
        "start": 703.079,
        "duration": 4.44,
        "text": "side this is using astb uh this is part"
      },
      {
        "start": 705.519,
        "duration": 4.041,
        "text": "of the data saacks stack so this is a"
      },
      {
        "start": 707.519,
        "duration": 3.601,
        "text": "fully-fledged vector store and I've"
      },
      {
        "start": 709.56,
        "duration": 4.0,
        "text": "created a collection here called rag"
      },
      {
        "start": 711.12,
        "duration": 4.04,
        "text": "stack demo but it's empty I haven't put"
      },
      {
        "start": 713.56,
        "duration": 4.519,
        "text": "anything in it right and I've done that"
      },
      {
        "start": 715.16,
        "duration": 4.88,
        "text": "on purpose so what I'm going to do is"
      },
      {
        "start": 718.079,
        "duration": 4.041,
        "text": "just to kind of"
      },
      {
        "start": 720.04,
        "duration": 5.799,
        "text": "demonstrate here that these are the same"
      },
      {
        "start": 722.12,
        "duration": 6.159,
        "text": "this top this top workflow you see here"
      },
      {
        "start": 725.839,
        "duration": 4.481,
        "text": "is actually the query workflow right and"
      },
      {
        "start": 728.279,
        "duration": 3.921,
        "text": "you can say I have the same question so"
      },
      {
        "start": 730.32,
        "duration": 3.92,
        "text": "I have no data in here I'm just going to"
      },
      {
        "start": 732.2,
        "duration": 3.199,
        "text": "ask it the same question when did I"
      },
      {
        "start": 734.24,
        "duration": 4.399,
        "text": "start"
      },
      {
        "start": 735.399,
        "duration": 5.281,
        "text": "coding and again it should pretty much"
      },
      {
        "start": 738.639,
        "duration": 3.601,
        "text": "say something like hey I don't know what"
      },
      {
        "start": 740.68,
        "duration": 3.279,
        "text": "you're talking about right and it says"
      },
      {
        "start": 742.24,
        "duration": 3.92,
        "text": "I'm sorry there's not enough information"
      },
      {
        "start": 743.959,
        "duration": 3.12,
        "text": "provided because again the llm doesn't"
      },
      {
        "start": 746.16,
        "duration": 3.919,
        "text": "have any"
      },
      {
        "start": 747.079,
        "duration": 5.56,
        "text": "context so here's what I want to do"
      },
      {
        "start": 750.079,
        "duration": 4.841,
        "text": "let's go ahead and this is the gener of"
      },
      {
        "start": 752.639,
        "duration": 4.64,
        "text": "part so I'm going to load up some file"
      },
      {
        "start": 754.92,
        "duration": 4.359,
        "text": "in this case it's just a PDF uh it"
      },
      {
        "start": 757.279,
        "duration": 4.68,
        "text": "happens to be a PDF that I built a while"
      },
      {
        "start": 759.279,
        "duration": 4.961,
        "text": "ago it's on my career progression funny"
      },
      {
        "start": 761.959,
        "duration": 3.88,
        "text": "enough but here's why this is my domain"
      },
      {
        "start": 764.24,
        "duration": 3.36,
        "text": "knowledge I did actually look like that"
      },
      {
        "start": 765.839,
        "duration": 4.0,
        "text": "I'm I was actually really amazed at how"
      },
      {
        "start": 767.6,
        "duration": 4.599,
        "text": "well chat GPT did in generating that"
      },
      {
        "start": 769.839,
        "duration": 4.161,
        "text": "image based off thep yeah that was"
      },
      {
        "start": 772.199,
        "duration": 4.32,
        "text": "scarily close"
      },
      {
        "start": 774.0,
        "duration": 5.199,
        "text": "um but I started around eight years old"
      },
      {
        "start": 776.519,
        "duration": 4.801,
        "text": "on a Commodore 64 right and so this is"
      },
      {
        "start": 779.199,
        "duration": 5.2,
        "text": "information that in my domain that I"
      },
      {
        "start": 781.32,
        "duration": 5.519,
        "text": "have in this in this data that the llms"
      },
      {
        "start": 784.399,
        "duration": 5.961,
        "text": "don't have so what I'm going to do here"
      },
      {
        "start": 786.839,
        "duration": 5.481,
        "text": "is I am going to I'm going to feed that"
      },
      {
        "start": 790.36,
        "duration": 3.52,
        "text": "in we're going to create embeddings off"
      },
      {
        "start": 792.32,
        "duration": 4.759,
        "text": "of that and then we're going to store it"
      },
      {
        "start": 793.88,
        "duration": 4.639,
        "text": "in this rag stack demo collection right"
      },
      {
        "start": 797.079,
        "duration": 2.88,
        "text": "so it's going to go pop through that"
      },
      {
        "start": 798.519,
        "duration": 3.401,
        "text": "again I'm glossing over all sorts of"
      },
      {
        "start": 799.959,
        "duration": 4.44,
        "text": "details right now um we will get into"
      },
      {
        "start": 801.92,
        "duration": 5.08,
        "text": "them a little later but what I really"
      },
      {
        "start": 804.399,
        "duration": 4.8,
        "text": "want to point out is now when I go to"
      },
      {
        "start": 807.0,
        "duration": 4.16,
        "text": "this rag stack demo collection you can"
      },
      {
        "start": 809.199,
        "duration": 4.76,
        "text": "see that I have some data so it's taken"
      },
      {
        "start": 811.16,
        "duration": 4.96,
        "text": "the data from this PDF it has put it"
      },
      {
        "start": 813.959,
        "duration": 4.041,
        "text": "into embedding through an embedding"
      },
      {
        "start": 816.12,
        "duration": 5.279,
        "text": "model and I have Vector embeddings now"
      },
      {
        "start": 818.0,
        "duration": 7.8,
        "text": "represent that data so now if I pull"
      },
      {
        "start": 821.399,
        "duration": 4.401,
        "text": "back and if I ask that same question"
      },
      {
        "start": 826.68,
        "duration": 3.959,
        "text": "again we should see a different answer"
      },
      {
        "start": 829.24,
        "duration": 4.0,
        "text": "this is always fun doing this stuff live"
      },
      {
        "start": 830.639,
        "duration": 3.801,
        "text": "right ah check this out so the answer"
      },
      {
        "start": 833.24,
        "duration": 2.8,
        "text": "being you started coding when you were"
      },
      {
        "start": 834.44,
        "duration": 5.6,
        "text": "around eight years old learning how to"
      },
      {
        "start": 836.04,
        "duration": 5.4,
        "text": "code games on your commod 64 which is"
      },
      {
        "start": 840.04,
        "duration": 4.479,
        "text": "pretty much exactly what I talked about"
      },
      {
        "start": 841.44,
        "duration": 7.16,
        "text": "right here right so the the whole idea"
      },
      {
        "start": 844.519,
        "duration": 7.76,
        "text": "here is that with this rag pipeline I"
      },
      {
        "start": 848.6,
        "duration": 5.359,
        "text": "able to augment the llm in real time"
      },
      {
        "start": 852.279,
        "duration": 3.601,
        "text": "right I didn't have to build a new model"
      },
      {
        "start": 853.959,
        "duration": 5.081,
        "text": "I don't have to fine-tune or anything I"
      },
      {
        "start": 855.88,
        "duration": 6.16,
        "text": "can literally pull in relevant data real"
      },
      {
        "start": 859.04,
        "duration": 4.4,
        "text": "time and then apply it to the llm right"
      },
      {
        "start": 862.04,
        "duration": 3.2,
        "text": "so anyway I just wanted to start with"
      },
      {
        "start": 863.44,
        "duration": 4.639,
        "text": "that kind of set the stage a little bit"
      },
      {
        "start": 865.24,
        "duration": 5.519,
        "text": "I'm not going to hand it over to Chara"
      },
      {
        "start": 868.079,
        "duration": 4.521,
        "text": "to dig into to some of the fun uh with"
      },
      {
        "start": 870.759,
        "duration": 3.44,
        "text": "rag itself and then we will come back"
      },
      {
        "start": 872.6,
        "duration": 3.96,
        "text": "and we'll do a deep dive into how this"
      },
      {
        "start": 874.199,
        "duration": 2.361,
        "text": "is actually"
      },
      {
        "start": 876.6,
        "duration": 6.679,
        "text": "working all right let me switch to some"
      },
      {
        "start": 880.079,
        "duration": 5.721,
        "text": "slides now as I mentioned before David"
      },
      {
        "start": 883.279,
        "duration": 4.441,
        "text": "feel free to interrupt and and ask"
      },
      {
        "start": 885.8,
        "duration": 3.32,
        "text": "questions if I'm glossing over stuff"
      },
      {
        "start": 887.72,
        "duration": 4.679,
        "text": "that you think will be useful for this"
      },
      {
        "start": 889.12,
        "duration": 6.719,
        "text": "audience sure um uh I want to make a"
      },
      {
        "start": 892.399,
        "duration": 7.721,
        "text": "claim that"
      },
      {
        "start": 895.839,
        "duration": 8.24,
        "text": "um you need rag in your your application"
      },
      {
        "start": 900.12,
        "duration": 6.2,
        "text": "now is this true right most of the times"
      },
      {
        "start": 904.079,
        "duration": 6.081,
        "text": "I would argue yes if you actually want"
      },
      {
        "start": 906.32,
        "duration": 8.84,
        "text": "to achieve something that's uh domain"
      },
      {
        "start": 910.16,
        "duration": 8.72,
        "text": "specific um personal um customized llms"
      },
      {
        "start": 915.16,
        "duration": 7.44,
        "text": "are amazing but they are expensive and"
      },
      {
        "start": 918.88,
        "duration": 6.519,
        "text": "they learn patterns not exact precise"
      },
      {
        "start": 922.6,
        "duration": 4.88,
        "text": "pieces of information they're great at"
      },
      {
        "start": 925.399,
        "duration": 4.321,
        "text": "predicting statistically the next word"
      },
      {
        "start": 927.48,
        "duration": 4.719,
        "text": "that's going to come but they're even if"
      },
      {
        "start": 929.72,
        "duration": 5.559,
        "text": "your data was indexed on the internet"
      },
      {
        "start": 932.199,
        "duration": 5.161,
        "text": "that's one page throughout everything"
      },
      {
        "start": 935.279,
        "duration": 4.48,
        "text": "that it's ingested and so it's not"
      },
      {
        "start": 937.36,
        "duration": 5.44,
        "text": "enough representation to actually get"
      },
      {
        "start": 939.759,
        "duration": 6.08,
        "text": "that data to come back so the the way I"
      },
      {
        "start": 942.8,
        "duration": 6.88,
        "text": "want to justify this is by starting with"
      },
      {
        "start": 945.839,
        "duration": 7.161,
        "text": "um some apps and and talk to you about a"
      },
      {
        "start": 949.68,
        "duration": 6.0,
        "text": "little bit why they had to use rag so"
      },
      {
        "start": 953.0,
        "duration": 4.72,
        "text": "this first example is uh an app called"
      },
      {
        "start": 955.68,
        "duration": 3.8,
        "text": "uh physics Walla their their learning"
      },
      {
        "start": 957.72,
        "duration": 5.64,
        "text": "Guru"
      },
      {
        "start": 959.48,
        "duration": 6.88,
        "text": "they are um an app a personalized AI"
      },
      {
        "start": 963.36,
        "duration": 5.479,
        "text": "tutor application in India to assist"
      },
      {
        "start": 966.36,
        "duration": 5.599,
        "text": "students with their academic and support"
      },
      {
        "start": 968.839,
        "duration": 5.68,
        "text": "queries like these are very very very"
      },
      {
        "start": 971.959,
        "duration": 4.601,
        "text": "technical questions that they're asking"
      },
      {
        "start": 974.519,
        "duration": 4.24,
        "text": "and they're asking it in many different"
      },
      {
        "start": 976.56,
        "duration": 4.199,
        "text": "languages they may be doing text to"
      },
      {
        "start": 978.759,
        "duration": 3.041,
        "text": "speech they may be typing it in but"
      },
      {
        "start": 980.759,
        "duration": 3.481,
        "text": "there there are multiple different"
      },
      {
        "start": 981.8,
        "duration": 5.32,
        "text": "languages being supported this"
      },
      {
        "start": 984.24,
        "duration": 5.959,
        "text": "application when it started was trying"
      },
      {
        "start": 987.12,
        "duration": 6.36,
        "text": "to just essentially chat with a PDF you"
      },
      {
        "start": 990.199,
        "duration": 4.841,
        "text": "know not building up a vector store not"
      },
      {
        "start": 993.48,
        "duration": 3.039,
        "text": "doing any kind of retrieval there but"
      },
      {
        "start": 995.04,
        "duration": 3.32,
        "text": "just like all right the students"
      },
      {
        "start": 996.519,
        "duration": 3.8,
        "text": "learning from this textbook can I just"
      },
      {
        "start": 998.36,
        "duration": 5.399,
        "text": "use a massive context window is that"
      },
      {
        "start": 1000.319,
        "duration": 5.2,
        "text": "form of rag enough turns out it's not um"
      },
      {
        "start": 1003.759,
        "duration": 4.481,
        "text": "so one of the things that they did was"
      },
      {
        "start": 1005.519,
        "duration": 4.081,
        "text": "they uh collected feedback for every"
      },
      {
        "start": 1008.24,
        "duration": 3.399,
        "text": "answer that was given you see those"
      },
      {
        "start": 1009.6,
        "duration": 4.719,
        "text": "thumbs up and thumbs down they were able"
      },
      {
        "start": 1011.639,
        "duration": 5.041,
        "text": "to measure essentially customer"
      },
      {
        "start": 1014.319,
        "duration": 5.0,
        "text": "satisfaction and until they were"
      },
      {
        "start": 1016.68,
        "duration": 4.68,
        "text": "actually able to chunk the data in the"
      },
      {
        "start": 1019.319,
        "duration": 4.44,
        "text": "right way to retrieve the right"
      },
      {
        "start": 1021.36,
        "duration": 4.719,
        "text": "information and to summarize that in a"
      },
      {
        "start": 1023.759,
        "duration": 5.68,
        "text": "way that the user could actually make"
      },
      {
        "start": 1026.079,
        "duration": 6.201,
        "text": "useful their cat was very low right they"
      },
      {
        "start": 1029.439,
        "duration": 4.161,
        "text": "started with you know basic Rag and then"
      },
      {
        "start": 1032.28,
        "duration": 4.44,
        "text": "they had to move to more and more"
      },
      {
        "start": 1033.6,
        "duration": 7.16,
        "text": "Advanced Techniques and I want to bring"
      },
      {
        "start": 1036.72,
        "duration": 6.839,
        "text": "this one up because physics is a very"
      },
      {
        "start": 1040.76,
        "duration": 6.799,
        "text": "specific domain but it's also out in the"
      },
      {
        "start": 1043.559,
        "duration": 7.521,
        "text": "open right so if if chat GPT has indexed"
      },
      {
        "start": 1047.559,
        "duration": 5.161,
        "text": "all of the text for physics why on Earth"
      },
      {
        "start": 1051.08,
        "duration": 3.76,
        "text": "can this not just answer the questions"
      },
      {
        "start": 1052.72,
        "duration": 4.92,
        "text": "right it's not what Transformer models"
      },
      {
        "start": 1054.84,
        "duration": 5.719,
        "text": "are built for Transformer models are"
      },
      {
        "start": 1057.64,
        "duration": 5.919,
        "text": "built for learning patterns and Concepts"
      },
      {
        "start": 1060.559,
        "duration": 5.841,
        "text": "and not necessarily uh giving you exact"
      },
      {
        "start": 1063.559,
        "duration": 4.281,
        "text": "matches to what you're asking so I want"
      },
      {
        "start": 1066.4,
        "duration": 5.48,
        "text": "to move to the next"
      },
      {
        "start": 1067.84,
        "duration": 6.16,
        "text": "example so skypoint skypoint is a gen"
      },
      {
        "start": 1071.88,
        "duration": 4.32,
        "text": "app that gives healthare providers"
      },
      {
        "start": 1074.0,
        "duration": 4.84,
        "text": "instant access to public and private"
      },
      {
        "start": 1076.2,
        "duration": 5.4,
        "text": "data and they offer this chat interface"
      },
      {
        "start": 1078.84,
        "duration": 5.8,
        "text": "they have strict slas latency throughput"
      },
      {
        "start": 1081.6,
        "duration": 7.199,
        "text": "cost Etc but in particular I want to tap"
      },
      {
        "start": 1084.64,
        "duration": 6.76,
        "text": "in onto this Healthcare data right if I"
      },
      {
        "start": 1088.799,
        "duration": 4.921,
        "text": "go to chat gbt and I ask it to interpret"
      },
      {
        "start": 1091.4,
        "duration": 4.2,
        "text": "something certainly I have done this for"
      },
      {
        "start": 1093.72,
        "duration": 4.48,
        "text": "healthcare before like my partner is"
      },
      {
        "start": 1095.6,
        "duration": 4.24,
        "text": "having shoulder surgery soon and we got"
      },
      {
        "start": 1098.2,
        "duration": 3.959,
        "text": "this massive document that talks about"
      },
      {
        "start": 1099.84,
        "duration": 6.319,
        "text": "his different options I absolutely use"
      },
      {
        "start": 1102.159,
        "duration": 6.921,
        "text": "chat GPT for that however if you want to"
      },
      {
        "start": 1106.159,
        "duration": 4.961,
        "text": "have very specific patient record"
      },
      {
        "start": 1109.08,
        "duration": 4.64,
        "text": "recommendations where only this"
      },
      {
        "start": 1111.12,
        "duration": 4.52,
        "text": "patient's health history and only the"
      },
      {
        "start": 1113.72,
        "duration": 4.36,
        "text": "medications that they're using and all"
      },
      {
        "start": 1115.64,
        "duration": 5.2,
        "text": "of these types of things are used for"
      },
      {
        "start": 1118.08,
        "duration": 4.52,
        "text": "these types of context you absolutely"
      },
      {
        "start": 1120.84,
        "duration": 4.44,
        "text": "need to be using rag because these are"
      },
      {
        "start": 1122.6,
        "duration": 4.48,
        "text": "not General things it's not okay to"
      },
      {
        "start": 1125.28,
        "duration": 4.2,
        "text": "substitute one medication with another"
      },
      {
        "start": 1127.08,
        "duration": 5.12,
        "text": "because it's close like you absolutely"
      },
      {
        "start": 1129.48,
        "duration": 4.6,
        "text": "need to be doing something with rag on"
      },
      {
        "start": 1132.2,
        "duration": 3.64,
        "text": "this private data and not just you know"
      },
      {
        "start": 1134.08,
        "duration": 3.68,
        "text": "putting it out on the"
      },
      {
        "start": 1135.84,
        "duration": 4.36,
        "text": "internet and the last example I'm going"
      },
      {
        "start": 1137.76,
        "duration": 4.68,
        "text": "to give is this app called ovam Health"
      },
      {
        "start": 1140.2,
        "duration": 5.24,
        "text": "again it's a it has something to do with"
      },
      {
        "start": 1142.44,
        "duration": 6.599,
        "text": "Healthcare but it's doing rag in a"
      },
      {
        "start": 1145.44,
        "duration": 7.68,
        "text": "different way um this application before"
      },
      {
        "start": 1149.039,
        "duration": 6.601,
        "text": "they had gen was an application for uh"
      },
      {
        "start": 1153.12,
        "duration": 4.6,
        "text": "women who were trying to get pregnant or"
      },
      {
        "start": 1155.64,
        "duration": 3.519,
        "text": "were struggling with fertility to get"
      },
      {
        "start": 1157.72,
        "duration": 3.8,
        "text": "answers from doctors there was this"
      },
      {
        "start": 1159.159,
        "duration": 3.921,
        "text": "network of doctors that you could submit"
      },
      {
        "start": 1161.52,
        "duration": 3.279,
        "text": "your question and it would go out to"
      },
      {
        "start": 1163.08,
        "duration": 4.04,
        "text": "their entire network and provide those"
      },
      {
        "start": 1164.799,
        "duration": 4.681,
        "text": "answers back but often times it would"
      },
      {
        "start": 1167.12,
        "duration": 4.48,
        "text": "take a week to get response right these"
      },
      {
        "start": 1169.48,
        "duration": 3.84,
        "text": "are not doctors that are getting paid"
      },
      {
        "start": 1171.6,
        "duration": 3.36,
        "text": "for every single response they're it's"
      },
      {
        "start": 1173.32,
        "duration": 3.56,
        "text": "sort of a volunteer network and they're"
      },
      {
        "start": 1174.96,
        "duration": 4.16,
        "text": "trying to help people with fertility"
      },
      {
        "start": 1176.88,
        "duration": 5.96,
        "text": "well what if we could retrieve that"
      },
      {
        "start": 1179.12,
        "duration": 6.0,
        "text": "medical advice for this my specific case"
      },
      {
        "start": 1182.84,
        "duration": 4.28,
        "text": "for my age for my demographic group for"
      },
      {
        "start": 1185.12,
        "duration": 4.36,
        "text": "my particular"
      },
      {
        "start": 1187.12,
        "duration": 4.4,
        "text": "challenge but we cannot do this simply"
      },
      {
        "start": 1189.48,
        "duration": 4.96,
        "text": "with llms we have to do this with rag"
      },
      {
        "start": 1191.52,
        "duration": 5.36,
        "text": "because the attribution of this medical"
      },
      {
        "start": 1194.44,
        "duration": 5.16,
        "text": "advice to the doctor is critically"
      },
      {
        "start": 1196.88,
        "duration": 4.72,
        "text": "important right I can't change what this"
      },
      {
        "start": 1199.6,
        "duration": 3.04,
        "text": "advice is to just hallucinate something"
      },
      {
        "start": 1201.6,
        "duration": 4.24,
        "text": "from the"
      },
      {
        "start": 1202.64,
        "duration": 6.44,
        "text": "internet so my argument here is that if"
      },
      {
        "start": 1205.84,
        "duration": 6.28,
        "text": "you're doing any sort of vertical"
      },
      {
        "start": 1209.08,
        "duration": 6.52,
        "text": "specific transformative application rag"
      },
      {
        "start": 1212.12,
        "duration": 5.439,
        "text": "is key fine-tuning is expensive and it's"
      },
      {
        "start": 1215.6,
        "duration": 3.8,
        "text": "not going to learn your specific data"
      },
      {
        "start": 1217.559,
        "duration": 3.521,
        "text": "it's going to learn a pattern F shot"
      },
      {
        "start": 1219.4,
        "duration": 4.04,
        "text": "learning is great when you're trying to"
      },
      {
        "start": 1221.08,
        "duration": 4.24,
        "text": "teach it something structured like"
      },
      {
        "start": 1223.44,
        "duration": 3.88,
        "text": "here's exactly how to extract or here's"
      },
      {
        "start": 1225.32,
        "duration": 3.44,
        "text": "how to format your question and answer"
      },
      {
        "start": 1227.32,
        "duration": 3.8,
        "text": "but it's not going to get you what you"
      },
      {
        "start": 1228.76,
        "duration": 3.08,
        "text": "need for something transformative in"
      },
      {
        "start": 1231.12,
        "duration": 3.76,
        "text": "your"
      },
      {
        "start": 1231.84,
        "duration": 4.88,
        "text": "domain so everyone is sort of settling"
      },
      {
        "start": 1234.88,
        "duration": 5.039,
        "text": "on the same pattern rag retrieval"
      },
      {
        "start": 1236.72,
        "duration": 6.079,
        "text": "augmented generation and what we've"
      },
      {
        "start": 1239.919,
        "duration": 4.841,
        "text": "noticed is it's the key piece it's the"
      },
      {
        "start": 1242.799,
        "duration": 4.36,
        "text": "piece that isn't really replaceable you"
      },
      {
        "start": 1244.76,
        "duration": 5.64,
        "text": "can replace an llm if you need a"
      },
      {
        "start": 1247.159,
        "duration": 7.121,
        "text": "multimodal llm or you want a smaller llm"
      },
      {
        "start": 1250.4,
        "duration": 5.56,
        "text": "you can pop those in and out and doesn't"
      },
      {
        "start": 1254.28,
        "duration": 3.36,
        "text": "sort of it's a trade-off you can make"
      },
      {
        "start": 1255.96,
        "duration": 4.719,
        "text": "for cost or it's a trade-off you can"
      },
      {
        "start": 1257.64,
        "duration": 5.0,
        "text": "make for um relevancy but without the"
      },
      {
        "start": 1260.679,
        "duration": 5.201,
        "text": "data you don't really have that"
      },
      {
        "start": 1262.64,
        "duration": 6.32,
        "text": "application and chat with PDFs are not"
      },
      {
        "start": 1265.88,
        "duration": 6.679,
        "text": "really going to change the world"
      },
      {
        "start": 1268.96,
        "duration": 6.16,
        "text": "so this seems simple right go give me my"
      },
      {
        "start": 1272.559,
        "duration": 4.321,
        "text": "data put it into the llm but there's"
      },
      {
        "start": 1275.12,
        "duration": 4.52,
        "text": "actually different kinds of"
      },
      {
        "start": 1276.88,
        "duration": 5.32,
        "text": "data and we need to"
      },
      {
        "start": 1279.64,
        "duration": 4.8,
        "text": "consider all kinds of data in order to"
      },
      {
        "start": 1282.2,
        "duration": 4.56,
        "text": "get the right context at the right time"
      },
      {
        "start": 1284.44,
        "duration": 5.599,
        "text": "to answer questions so sure we've got"
      },
      {
        "start": 1286.76,
        "duration": 4.84,
        "text": "the user input I can just ask the user"
      },
      {
        "start": 1290.039,
        "duration": 4.0,
        "text": "well when did you start coding and you"
      },
      {
        "start": 1291.6,
        "duration": 4.84,
        "text": "would give it that information right but"
      },
      {
        "start": 1294.039,
        "duration": 4.201,
        "text": "yeah what do we want to do with scale"
      },
      {
        "start": 1296.44,
        "duration": 4.0,
        "text": "what are we going to do when we want to"
      },
      {
        "start": 1298.24,
        "duration": 4.2,
        "text": "deploy this to all kinds of users to"
      },
      {
        "start": 1300.44,
        "duration": 3.76,
        "text": "many different students to many"
      },
      {
        "start": 1302.44,
        "duration": 3.719,
        "text": "different patients right we need more"
      },
      {
        "start": 1304.2,
        "duration": 5.0,
        "text": "than just that user input we need to"
      },
      {
        "start": 1306.159,
        "duration": 5.441,
        "text": "somehow back that with unstructured data"
      },
      {
        "start": 1309.2,
        "duration": 4.359,
        "text": "with semi-structured data uh and then"
      },
      {
        "start": 1311.6,
        "duration": 3.439,
        "text": "with fully structured data and these"
      },
      {
        "start": 1313.559,
        "duration": 3.761,
        "text": "different kinds of data is going to"
      },
      {
        "start": 1315.039,
        "duration": 6.441,
        "text": "bring different kinds of context into"
      },
      {
        "start": 1317.32,
        "duration": 7.12,
        "text": "the prompt so when we're thinking about"
      },
      {
        "start": 1321.48,
        "duration": 7.36,
        "text": "this rag problem it can get relatively"
      },
      {
        "start": 1324.44,
        "duration": 7.08,
        "text": "complex and it can actually get kind of"
      },
      {
        "start": 1328.84,
        "duration": 5.839,
        "text": "confusing and so part of why we want to"
      },
      {
        "start": 1331.52,
        "duration": 5.72,
        "text": "make this really easy is"
      },
      {
        "start": 1334.679,
        "duration": 3.921,
        "text": "because we can right we know what"
      },
      {
        "start": 1337.24,
        "duration": 2.919,
        "text": "patterns are working we know what you're"
      },
      {
        "start": 1338.6,
        "duration": 3.959,
        "text": "going to want to experiment with and so"
      },
      {
        "start": 1340.159,
        "duration": 4.601,
        "text": "this is why Lang flow exists right how"
      },
      {
        "start": 1342.559,
        "duration": 5.761,
        "text": "are we going to build these applications"
      },
      {
        "start": 1344.76,
        "duration": 5.279,
        "text": "well we're not all experts in coding"
      },
      {
        "start": 1348.32,
        "duration": 4.959,
        "text": "we're we're not all experts in these rag"
      },
      {
        "start": 1350.039,
        "duration": 6.281,
        "text": "patterns but what we do know is that we"
      },
      {
        "start": 1353.279,
        "duration": 5.361,
        "text": "need for our domain to get the right"
      },
      {
        "start": 1356.32,
        "duration": 4.599,
        "text": "answers right if I'm an agronomist and I"
      },
      {
        "start": 1358.64,
        "duration": 4.36,
        "text": "want to build a tool that tells Farmers"
      },
      {
        "start": 1360.919,
        "duration": 4.841,
        "text": "what to do with their crops I don't need"
      },
      {
        "start": 1363.0,
        "duration": 5.2,
        "text": "to know how to code but anyone trying to"
      },
      {
        "start": 1365.76,
        "duration": 4.519,
        "text": "build an application can do it quickly"
      },
      {
        "start": 1368.2,
        "duration": 3.959,
        "text": "with a drag and drop interface and we"
      },
      {
        "start": 1370.279,
        "duration": 4.241,
        "text": "want to make this available with those"
      },
      {
        "start": 1372.159,
        "duration": 4.76,
        "text": "templates that you talked about uh to"
      },
      {
        "start": 1374.52,
        "duration": 4.039,
        "text": "make it quite easy so start with simple"
      },
      {
        "start": 1376.919,
        "duration": 3.36,
        "text": "rack and then when you're not quite"
      },
      {
        "start": 1378.559,
        "duration": 4.0,
        "text": "getting the results that you want move"
      },
      {
        "start": 1380.279,
        "duration": 4.441,
        "text": "to something like graph rag or start to"
      },
      {
        "start": 1382.559,
        "duration": 5.281,
        "text": "implement something like uh keyword"
      },
      {
        "start": 1384.72,
        "duration": 6.559,
        "text": "searching or um hybrid searching to get"
      },
      {
        "start": 1387.84,
        "duration": 5.76,
        "text": "you know better even better results so"
      },
      {
        "start": 1391.279,
        "duration": 4.721,
        "text": "you'll David will go into this later but"
      },
      {
        "start": 1393.6,
        "duration": 4.88,
        "text": "I wanted to kind of say like this is why"
      },
      {
        "start": 1396.0,
        "duration": 5.6,
        "text": "Lang flow now there were some questions"
      },
      {
        "start": 1398.48,
        "duration": 5.64,
        "text": "in the chat about rag stack what is rag"
      },
      {
        "start": 1401.6,
        "duration": 5.24,
        "text": "stack we refer to these different"
      },
      {
        "start": 1404.12,
        "duration": 6.12,
        "text": "abstractions and what do we mean by that"
      },
      {
        "start": 1406.84,
        "duration": 8.28,
        "text": "right so rag stack as we've mentioned is"
      },
      {
        "start": 1410.24,
        "duration": 6.24,
        "text": "a curated open-source stack um we take"
      },
      {
        "start": 1415.12,
        "duration": 5.039,
        "text": "different abstractions from different"
      },
      {
        "start": 1416.48,
        "duration": 5.64,
        "text": "places so uh the Llama index framework"
      },
      {
        "start": 1420.159,
        "duration": 4.081,
        "text": "has a different abstraction than the"
      },
      {
        "start": 1422.12,
        "duration": 4.36,
        "text": "Lang chain framework has a different"
      },
      {
        "start": 1424.24,
        "duration": 4.76,
        "text": "abstraction than the Lang flow framework"
      },
      {
        "start": 1426.48,
        "duration": 6.28,
        "text": "so the abstractions that you saw in Lang"
      },
      {
        "start": 1429.0,
        "duration": 6.12,
        "text": "flow are going to help you build rapidly"
      },
      {
        "start": 1432.76,
        "duration": 5.0,
        "text": "but maybe you want to have more control"
      },
      {
        "start": 1435.12,
        "duration": 4.32,
        "text": "so you want to drop down into code well"
      },
      {
        "start": 1437.76,
        "duration": 4.159,
        "text": "behind each of those little boxes on the"
      },
      {
        "start": 1439.44,
        "duration": 5.68,
        "text": "screen you can drop down into L chain"
      },
      {
        "start": 1441.919,
        "duration": 5.721,
        "text": "yeah but maybe you really prefer llama"
      },
      {
        "start": 1445.12,
        "duration": 5.039,
        "text": "index because I know at least for me the"
      },
      {
        "start": 1447.64,
        "duration": 4.24,
        "text": "academics in my life love llama Index"
      },
      {
        "start": 1450.159,
        "duration": 3.321,
        "text": "right they just the documentation is"
      },
      {
        "start": 1451.88,
        "duration": 4.279,
        "text": "amazing the way that they're testing the"
      },
      {
        "start": 1453.48,
        "duration": 4.92,
        "text": "rag patterns are great um and for our"
      },
      {
        "start": 1456.159,
        "duration": 5.4,
        "text": "application developers they prefer Lang"
      },
      {
        "start": 1458.4,
        "duration": 5.759,
        "text": "chain so we're providing this testing"
      },
      {
        "start": 1461.559,
        "duration": 6.281,
        "text": "between all of the stock your database"
      },
      {
        "start": 1464.159,
        "duration": 6.601,
        "text": "your embedding providers your llms your"
      },
      {
        "start": 1467.84,
        "duration": 5.319,
        "text": "platforms like vertex Ai and Azure ml"
      },
      {
        "start": 1470.76,
        "duration": 4.6,
        "text": "all of that is already in the stack and"
      },
      {
        "start": 1473.159,
        "duration": 3.601,
        "text": "when we talk about these abstractions"
      },
      {
        "start": 1475.36,
        "duration": 3.559,
        "text": "these are the different abstractions"
      },
      {
        "start": 1476.76,
        "duration": 4.799,
        "text": "that you have access to you can use any"
      },
      {
        "start": 1478.919,
        "duration": 6.081,
        "text": "of them in order to build these rag"
      },
      {
        "start": 1481.559,
        "duration": 5.401,
        "text": "applications and our goal is to serve"
      },
      {
        "start": 1485.0,
        "duration": 4.48,
        "text": "every AI app creator no matter your"
      },
      {
        "start": 1486.96,
        "duration": 5.079,
        "text": "coding expertise because I believe that"
      },
      {
        "start": 1489.48,
        "duration": 4.559,
        "text": "it's going to be the domain experts that"
      },
      {
        "start": 1492.039,
        "duration": 4.201,
        "text": "are going to revolutionize the way we"
      },
      {
        "start": 1494.039,
        "duration": 5.52,
        "text": "work the way we think about our"
      },
      {
        "start": 1496.24,
        "duration": 7.439,
        "text": "experiences right you are the folks that"
      },
      {
        "start": 1499.559,
        "duration": 5.961,
        "text": "are on the ground doing every single day"
      },
      {
        "start": 1503.679,
        "duration": 3.921,
        "text": "what you do you're a professor and you"
      },
      {
        "start": 1505.52,
        "duration": 4.8,
        "text": "know exactly how you want to change the"
      },
      {
        "start": 1507.6,
        "duration": 4.88,
        "text": "way we learn you are a farmer and you"
      },
      {
        "start": 1510.32,
        "duration": 4.76,
        "text": "know exactly what you need in order to"
      },
      {
        "start": 1512.48,
        "duration": 4.36,
        "text": "get this Farm producing crops in a way"
      },
      {
        "start": 1515.08,
        "duration": 3.319,
        "text": "that's healthy for the land well why"
      },
      {
        "start": 1516.84,
        "duration": 5.76,
        "text": "don't why don't you go ahead and build"
      },
      {
        "start": 1518.399,
        "duration": 5.921,
        "text": "that so that's the why um and then I"
      },
      {
        "start": 1522.6,
        "duration": 4.84,
        "text": "wanted to talk a little bit"
      },
      {
        "start": 1524.32,
        "duration": 5.479,
        "text": "about why we're accelerating these use"
      },
      {
        "start": 1527.44,
        "duration": 4.2,
        "text": "cases right it's not just what David had"
      },
      {
        "start": 1529.799,
        "duration": 4.161,
        "text": "said sure we've got Vector similarity"
      },
      {
        "start": 1531.64,
        "duration": 4.8,
        "text": "that's one form of rag but there's also"
      },
      {
        "start": 1533.96,
        "duration": 4.28,
        "text": "graph rag where you have non"
      },
      {
        "start": 1536.44,
        "duration": 3.92,
        "text": "semantically similar information that"
      },
      {
        "start": 1538.24,
        "duration": 4.28,
        "text": "you need to pull back into the context"
      },
      {
        "start": 1540.36,
        "duration": 4.039,
        "text": "you know you may need to do full text"
      },
      {
        "start": 1542.52,
        "duration": 3.96,
        "text": "search you may need to do these various"
      },
      {
        "start": 1544.399,
        "duration": 4.561,
        "text": "things and there is more to rag than"
      },
      {
        "start": 1546.48,
        "duration": 4.16,
        "text": "meets the eye I'll give you just two"
      },
      {
        "start": 1548.96,
        "duration": 3.959,
        "text": "examples before I hand it back over to"
      },
      {
        "start": 1550.64,
        "duration": 3.639,
        "text": "David so that he can go into a deeper"
      },
      {
        "start": 1552.919,
        "duration": 4.24,
        "text": "demo that he's going to explain more"
      },
      {
        "start": 1554.279,
        "duration": 6.201,
        "text": "about langlow this first example is a"
      },
      {
        "start": 1557.159,
        "duration": 5.721,
        "text": "support bot so the article on the left"
      },
      {
        "start": 1560.48,
        "duration": 4.36,
        "text": "is about a very specific phone and it's"
      },
      {
        "start": 1562.88,
        "duration": 4.88,
        "text": "saying you know how do I change the"
      },
      {
        "start": 1564.84,
        "duration": 5.319,
        "text": "volume you'll notice on step four"
      },
      {
        "start": 1567.76,
        "duration": 5.68,
        "text": "there's this link ensure that do not"
      },
      {
        "start": 1570.159,
        "duration": 5.12,
        "text": "disturb switches turned off now as you"
      },
      {
        "start": 1573.44,
        "duration": 3.2,
        "text": "can imagine there are an infinite number"
      },
      {
        "start": 1575.279,
        "duration": 3.561,
        "text": "of phones out there I mean not"
      },
      {
        "start": 1576.64,
        "duration": 6.399,
        "text": "technically infinite but it seems that"
      },
      {
        "start": 1578.84,
        "duration": 5.88,
        "text": "way and so what if the llm only got the"
      },
      {
        "start": 1583.039,
        "duration": 3.961,
        "text": "text on the left and it just"
      },
      {
        "start": 1584.72,
        "duration": 6.16,
        "text": "hallucinated for you how to turn do not"
      },
      {
        "start": 1587.0,
        "duration": 7.84,
        "text": "disturb off it would not only lose my"
      },
      {
        "start": 1590.88,
        "duration": 6.56,
        "text": "confidence in Verizon as a support pot"
      },
      {
        "start": 1594.84,
        "duration": 4.16,
        "text": "but also I would be very frustrated if"
      },
      {
        "start": 1597.44,
        "duration": 3.239,
        "text": "somebody said like here's how you turn"
      },
      {
        "start": 1599.0,
        "duration": 4.159,
        "text": "off do not disturb and that's not where"
      },
      {
        "start": 1600.679,
        "duration": 3.921,
        "text": "it was on my phone so graph is a way"
      },
      {
        "start": 1603.159,
        "duration": 3.081,
        "text": "where we can relate two pieces of"
      },
      {
        "start": 1604.6,
        "duration": 4.16,
        "text": "information we can relate them in any"
      },
      {
        "start": 1606.24,
        "duration": 5.0,
        "text": "way that we want to we can take that"
      },
      {
        "start": 1608.76,
        "duration": 4.159,
        "text": "link and add it as an edge to our Vector"
      },
      {
        "start": 1611.24,
        "duration": 4.08,
        "text": "chunks and say well why don't you pull"
      },
      {
        "start": 1612.919,
        "duration": 5.441,
        "text": "back both of these pieces of text when"
      },
      {
        "start": 1615.32,
        "duration": 4.68,
        "text": "you're doing your retrieval so this is"
      },
      {
        "start": 1618.36,
        "duration": 2.96,
        "text": "something that is available in rag stack"
      },
      {
        "start": 1620.0,
        "duration": 3.12,
        "text": "it's something that we're continuing to"
      },
      {
        "start": 1621.32,
        "duration": 3.52,
        "text": "innovate on every day and when I say"
      },
      {
        "start": 1623.12,
        "duration": 3.32,
        "text": "graph rag I don't mean the old"
      },
      {
        "start": 1624.84,
        "duration": 4.16,
        "text": "traditional knowledge graphs that need"
      },
      {
        "start": 1626.44,
        "duration": 5.4,
        "text": "onology and experts to build them I mean"
      },
      {
        "start": 1629.0,
        "duration": 5.279,
        "text": "gen optimize specific graphs that helps"
      },
      {
        "start": 1631.84,
        "duration": 4.0,
        "text": "you do this fast and easy so so actually"
      },
      {
        "start": 1634.279,
        "duration": 3.561,
        "text": "I wanted to get in on on that a little"
      },
      {
        "start": 1635.84,
        "duration": 4.24,
        "text": "bit turna so you're saying like separate"
      },
      {
        "start": 1637.84,
        "duration": 4.4,
        "text": "from like a graph database or something"
      },
      {
        "start": 1640.08,
        "duration": 3.959,
        "text": "like that you don't need that here that"
      },
      {
        "start": 1642.24,
        "duration": 3.84,
        "text": "it's the way that you is it in your"
      },
      {
        "start": 1644.039,
        "duration": 4.64,
        "text": "metadata your embeddings like where's"
      },
      {
        "start": 1646.08,
        "duration": 5.52,
        "text": "this information being encapsulated"
      },
      {
        "start": 1648.679,
        "duration": 5.961,
        "text": "yeah so it could be that um you've got"
      },
      {
        "start": 1651.6,
        "duration": 4.88,
        "text": "very long PDFs and you need to chunk"
      },
      {
        "start": 1654.64,
        "duration": 4.8,
        "text": "that in order to get the right you know"
      },
      {
        "start": 1656.48,
        "duration": 5.039,
        "text": "snippet to return but that snippet that"
      },
      {
        "start": 1659.44,
        "duration": 5.0,
        "text": "chunk that's returned doesn't have all"
      },
      {
        "start": 1661.519,
        "duration": 5.04,
        "text": "of the contacts so this Edge this this"
      },
      {
        "start": 1664.44,
        "duration": 4.239,
        "text": "linkage could be it was in the same"
      },
      {
        "start": 1666.559,
        "duration": 6.201,
        "text": "document or these chunks were on the"
      },
      {
        "start": 1668.679,
        "duration": 7.12,
        "text": "same page you could say keywords matter"
      },
      {
        "start": 1672.76,
        "duration": 5.44,
        "text": "right maybe you know I need to always"
      },
      {
        "start": 1675.799,
        "duration": 4.76,
        "text": "Traverse when there's a specific amenity"
      },
      {
        "start": 1678.2,
        "duration": 4.16,
        "text": "being mentioned in a hotel and I want to"
      },
      {
        "start": 1680.559,
        "duration": 3.921,
        "text": "pull all of these back you can create"
      },
      {
        "start": 1682.36,
        "duration": 5.439,
        "text": "edges in any way and you can store these"
      },
      {
        "start": 1684.48,
        "duration": 6.24,
        "text": "in Astra alongside of your vector chunks"
      },
      {
        "start": 1687.799,
        "duration": 5.561,
        "text": "right so it's just adding metadata of"
      },
      {
        "start": 1690.72,
        "duration": 6.04,
        "text": "any kind links"
      },
      {
        "start": 1693.36,
        "duration": 5.679,
        "text": "keywords notations same blood type"
      },
      {
        "start": 1696.76,
        "duration": 4.32,
        "text": "things like this uh to allow you to pull"
      },
      {
        "start": 1699.039,
        "duration": 4.161,
        "text": "back seemingly unrelated"
      },
      {
        "start": 1701.08,
        "duration": 4.199,
        "text": "information okay that's"
      },
      {
        "start": 1703.2,
        "duration": 4.44,
        "text": "awesome and then the last example I want"
      },
      {
        "start": 1705.279,
        "duration": 6.201,
        "text": "to give you is um this one on something"
      },
      {
        "start": 1707.64,
        "duration": 8.8,
        "text": "called Colbert um sure it's not bear"
      },
      {
        "start": 1711.48,
        "duration": 7.36,
        "text": "no it co bear or Colbert you know I wish"
      },
      {
        "start": 1716.44,
        "duration": 5.64,
        "text": "somebody could tell me the the answer to"
      },
      {
        "start": 1718.84,
        "duration": 5.64,
        "text": "this one um I say Bert just because Bert"
      },
      {
        "start": 1722.08,
        "duration": 4.439,
        "text": "is capitalized uh and then I've tried"
      },
      {
        "start": 1724.48,
        "duration": 4.52,
        "text": "out Co beer because that's the way my"
      },
      {
        "start": 1726.519,
        "duration": 4.201,
        "text": "brain wants to say it uh but people kind"
      },
      {
        "start": 1729.0,
        "duration": 3.64,
        "text": "of look at me sideways like sh what are"
      },
      {
        "start": 1730.72,
        "duration": 3.4,
        "text": "you talking about I've had this argument"
      },
      {
        "start": 1732.64,
        "duration": 4.279,
        "text": "now so many times with folks which one"
      },
      {
        "start": 1734.12,
        "duration": 4.919,
        "text": "is it right yeah this one of those it's"
      },
      {
        "start": 1736.919,
        "duration": 3.521,
        "text": "just never solved really if y'all would"
      },
      {
        "start": 1739.039,
        "duration": 2.721,
        "text": "like to argue about it in the chat"
      },
      {
        "start": 1740.44,
        "duration": 5.44,
        "text": "please"
      },
      {
        "start": 1741.76,
        "duration": 6.44,
        "text": "do um especially those that have a lot"
      },
      {
        "start": 1745.88,
        "duration": 6.0,
        "text": "of importance on like the philosophical"
      },
      {
        "start": 1748.2,
        "duration": 6.4,
        "text": "uh debates um which I I tend to love to"
      },
      {
        "start": 1751.88,
        "duration": 5.12,
        "text": "have but not on a day-to-day basis um so"
      },
      {
        "start": 1754.6,
        "duration": 4.319,
        "text": "this is a real world example so we"
      },
      {
        "start": 1757.0,
        "duration": 4.919,
        "text": "indexed over a thousand Wikipedia"
      },
      {
        "start": 1758.919,
        "duration": 4.24,
        "text": "articles with both this uh Ada DPR"
      },
      {
        "start": 1761.919,
        "duration": 3.441,
        "text": "embedding model on the left it's a"
      },
      {
        "start": 1763.159,
        "duration": 4.481,
        "text": "pretty standard embedding model for"
      },
      {
        "start": 1765.36,
        "duration": 5.0,
        "text": "vector and a Colbert embedding model on"
      },
      {
        "start": 1767.64,
        "duration": 6.519,
        "text": "the right very different it isn't the"
      },
      {
        "start": 1770.36,
        "duration": 6.08,
        "text": "same as uh the sparse um density of"
      },
      {
        "start": 1774.159,
        "duration": 3.961,
        "text": "information being embedded on the left"
      },
      {
        "start": 1776.44,
        "duration": 4.719,
        "text": "uh it is very"
      },
      {
        "start": 1778.12,
        "duration": 4.36,
        "text": "dense um and so what happens is I think"
      },
      {
        "start": 1781.159,
        "duration": 3.721,
        "text": "I said that opposite dense passage"
      },
      {
        "start": 1782.48,
        "duration": 4.919,
        "text": "retrieval is when you have a lot of text"
      },
      {
        "start": 1784.88,
        "duration": 4.44,
        "text": "into a single Vector um and sparse"
      },
      {
        "start": 1787.399,
        "duration": 4.081,
        "text": "passage retrieval is when you have uh"
      },
      {
        "start": 1789.32,
        "duration": 4.68,
        "text": "very short amounts of text retrieved and"
      },
      {
        "start": 1791.48,
        "duration": 5.319,
        "text": "matched multiple times the whole point"
      },
      {
        "start": 1794.0,
        "duration": 5.44,
        "text": "though is that when you need to have a"
      },
      {
        "start": 1796.799,
        "duration": 4.681,
        "text": "named entity recognized when you"
      },
      {
        "start": 1799.44,
        "duration": 4.52,
        "text": "specifically need to know this"
      },
      {
        "start": 1801.48,
        "duration": 4.6,
        "text": "medication this person's name and you"
      },
      {
        "start": 1803.96,
        "duration": 4.319,
        "text": "need to retrieve that accurately Vector"
      },
      {
        "start": 1806.08,
        "duration": 4.719,
        "text": "search isn't the way to do that that's"
      },
      {
        "start": 1808.279,
        "duration": 5.201,
        "text": "semantic similarity and in fact every"
      },
      {
        "start": 1810.799,
        "duration": 4.441,
        "text": "time you embed the question it's going"
      },
      {
        "start": 1813.48,
        "duration": 3.28,
        "text": "to be a slightly different embedding"
      },
      {
        "start": 1815.24,
        "duration": 5.12,
        "text": "when you ask that question so you're not"
      },
      {
        "start": 1816.76,
        "duration": 6.44,
        "text": "going to get the level of information in"
      },
      {
        "start": 1820.36,
        "duration": 5.039,
        "text": "that Vector encoding that you need so"
      },
      {
        "start": 1823.2,
        "duration": 4.28,
        "text": "you'll notice on the left when you just"
      },
      {
        "start": 1825.399,
        "duration": 4.801,
        "text": "do normal embedding you search for this"
      },
      {
        "start": 1827.48,
        "duration": 4.199,
        "text": "person's a name William H Heen and"
      },
      {
        "start": 1830.2,
        "duration": 4.16,
        "text": "everything that comes back is completely"
      },
      {
        "start": 1831.679,
        "duration": 4.441,
        "text": "unrelated the name isn't there at all"
      },
      {
        "start": 1834.36,
        "duration": 3.6,
        "text": "but when you use these other techniques"
      },
      {
        "start": 1836.12,
        "duration": 3.52,
        "text": "when you do this search with a Colbert"
      },
      {
        "start": 1837.96,
        "duration": 4.199,
        "text": "embedding model before retrieving the"
      },
      {
        "start": 1839.64,
        "duration": 5.639,
        "text": "information you get the exact name that"
      },
      {
        "start": 1842.159,
        "duration": 6.481,
        "text": "you're looking for so it's a complex"
      },
      {
        "start": 1845.279,
        "duration": 6.0,
        "text": "space um but it's actually uh rapidly"
      },
      {
        "start": 1848.64,
        "duration": 4.639,
        "text": "evolving and so part of what David is"
      },
      {
        "start": 1851.279,
        "duration": 4.88,
        "text": "going to show you is how to do this with"
      },
      {
        "start": 1853.279,
        "duration": 5.201,
        "text": "rag stack L flow it is it is simply Lang"
      },
      {
        "start": 1856.159,
        "duration": 5.4,
        "text": "flow but it is the tested Sable version"
      },
      {
        "start": 1858.48,
        "duration": 5.36,
        "text": "so the versioning will often be behind"
      },
      {
        "start": 1861.559,
        "duration": 4.441,
        "text": "the open source but it will be fully"
      },
      {
        "start": 1863.84,
        "duration": 4.12,
        "text": "tested with the stack all of the other"
      },
      {
        "start": 1866.0,
        "duration": 5.0,
        "text": "abstractions to ensure that what you're"
      },
      {
        "start": 1867.96,
        "duration": 4.52,
        "text": "building is going to work in production"
      },
      {
        "start": 1871.0,
        "duration": 3.6,
        "text": "you know by the way we were having we"
      },
      {
        "start": 1872.48,
        "duration": 5.319,
        "text": "were having this discussion about coar"
      },
      {
        "start": 1874.6,
        "duration": 4.799,
        "text": "and goldber in the chat and uh and I you"
      },
      {
        "start": 1877.799,
        "duration": 4.201,
        "text": "got to check it out CH because I there's"
      },
      {
        "start": 1879.399,
        "duration": 4.24,
        "text": "some uh funny comments about the FR"
      },
      {
        "start": 1882.0,
        "duration": 3.799,
        "text": "friends"
      },
      {
        "start": 1883.639,
        "duration": 3.201,
        "text": "pronunciation so by the way what right"
      },
      {
        "start": 1885.799,
        "duration": 3.081,
        "text": "before we get into the demo what we're"
      },
      {
        "start": 1886.84,
        "duration": 3.719,
        "text": "going to do is we we are going to give"
      },
      {
        "start": 1888.88,
        "duration": 4.12,
        "text": "you another poll so I'm going to remove"
      },
      {
        "start": 1890.559,
        "duration": 4.201,
        "text": "the other poll we had um and by the way"
      },
      {
        "start": 1893.0,
        "duration": 3.2,
        "text": "it is very interesting that it looks"
      },
      {
        "start": 1894.76,
        "duration": 3.48,
        "text": "like at least here the most the majority"
      },
      {
        "start": 1896.2,
        "duration": 4.079,
        "text": "of you are in fact using Lane chain and"
      },
      {
        "start": 1898.24,
        "duration": 3.399,
        "text": "you know what's cool and and try to"
      },
      {
        "start": 1900.279,
        "duration": 3.601,
        "text": "mention this right but Langan flow"
      },
      {
        "start": 1901.639,
        "duration": 4.92,
        "text": "itself is built on Lang chain it's not"
      },
      {
        "start": 1903.88,
        "duration": 4.2,
        "text": "limited to Lane chain but in a in a"
      },
      {
        "start": 1906.559,
        "duration": 3.281,
        "text": "sense it's kind of like a UI abstraction"
      },
      {
        "start": 1908.08,
        "duration": 3.559,
        "text": "on top of it so as I show you some of"
      },
      {
        "start": 1909.84,
        "duration": 3.199,
        "text": "the underlying components I'll get into"
      },
      {
        "start": 1911.639,
        "duration": 2.681,
        "text": "some of the code of some of them and"
      },
      {
        "start": 1913.039,
        "duration": 3.76,
        "text": "you'll you'll see the Lang chain call"
      },
      {
        "start": 1914.32,
        "duration": 4.04,
        "text": "outs there um you should be seeing by"
      },
      {
        "start": 1916.799,
        "duration": 4.48,
        "text": "the way a new poll now this one's a"
      },
      {
        "start": 1918.36,
        "duration": 5.319,
        "text": "little chunky uh but given everything"
      },
      {
        "start": 1921.279,
        "duration": 4.4,
        "text": "that charna just talked about right um"
      },
      {
        "start": 1923.679,
        "duration": 4.48,
        "text": "we're curious like what kind of data are"
      },
      {
        "start": 1925.679,
        "duration": 4.081,
        "text": "you injecting into the prom context um"
      },
      {
        "start": 1928.159,
        "duration": 3.48,
        "text": "is it general knowledge is it really"
      },
      {
        "start": 1929.76,
        "duration": 4.36,
        "text": "detailed information is it hyperlink"
      },
      {
        "start": 1931.639,
        "duration": 4.441,
        "text": "support articles I see somebody just uh"
      },
      {
        "start": 1934.12,
        "duration": 4.919,
        "text": "answered a chunks from very long PDF"
      },
      {
        "start": 1936.08,
        "duration": 6.24,
        "text": "numerical data multimodel right and what"
      },
      {
        "start": 1939.039,
        "duration": 5.52,
        "text": "we're curious about here is like what is"
      },
      {
        "start": 1942.32,
        "duration": 3.959,
        "text": "the Continuum that you're all on because"
      },
      {
        "start": 1944.559,
        "duration": 4.921,
        "text": "what Char was just getting at a moment"
      },
      {
        "start": 1946.279,
        "duration": 4.36,
        "text": "ago is that it's not a size fits all um"
      },
      {
        "start": 1949.48,
        "duration": 3.159,
        "text": "I'm also going to mention some of this"
      },
      {
        "start": 1950.639,
        "duration": 4.04,
        "text": "as we go through the rag pipeline uh so"
      },
      {
        "start": 1952.639,
        "duration": 4.92,
        "text": "again so go ahead and answer that poll"
      },
      {
        "start": 1954.679,
        "duration": 4.081,
        "text": "and um I will pop over I'm gonna share"
      },
      {
        "start": 1957.559,
        "duration": 5.24,
        "text": "my screen"
      },
      {
        "start": 1958.76,
        "duration": 7.72,
        "text": "here and do"
      },
      {
        "start": 1962.799,
        "duration": 6.72,
        "text": "this and we'll get into the demo the"
      },
      {
        "start": 1966.48,
        "duration": 5.319,
        "text": "second part of the demo I should say"
      },
      {
        "start": 1969.519,
        "duration": 3.76,
        "text": "alrighty well go ahead CH yeah go ahead"
      },
      {
        "start": 1971.799,
        "duration": 3.0,
        "text": "yeah while you're getting that ready I"
      },
      {
        "start": 1973.279,
        "duration": 3.801,
        "text": "was going to answer this question so"
      },
      {
        "start": 1974.799,
        "duration": 4.921,
        "text": "there's a question on data files for rag"
      },
      {
        "start": 1977.08,
        "duration": 6.839,
        "text": "how many f files and up to what size"
      },
      {
        "start": 1979.72,
        "duration": 6.36,
        "text": "files can be uploaded here to use rag um"
      },
      {
        "start": 1983.919,
        "duration": 4.041,
        "text": "I have a client who has two dozen files"
      },
      {
        "start": 1986.08,
        "duration": 6.719,
        "text": "with up to 150 megabytes that we need to"
      },
      {
        "start": 1987.96,
        "duration": 10.04,
        "text": "build a rag for great question um I have"
      },
      {
        "start": 1992.799,
        "duration": 7.961,
        "text": "uploaded more than 59 gigabytes to do a"
      },
      {
        "start": 1998.0,
        "duration": 5.279,
        "text": "rag application before across many"
      },
      {
        "start": 2000.76,
        "duration": 5.639,
        "text": "different document types think about"
      },
      {
        "start": 2003.279,
        "duration": 5.041,
        "text": "indexing every hotel in the world and"
      },
      {
        "start": 2006.399,
        "duration": 5.16,
        "text": "every restaurant in the world World in"
      },
      {
        "start": 2008.32,
        "duration": 5.199,
        "text": "order to give travel recommendations so"
      },
      {
        "start": 2011.559,
        "duration": 3.921,
        "text": "how many can you do H it's pretty"
      },
      {
        "start": 2013.519,
        "duration": 4.081,
        "text": "Limitless and it it does depend on your"
      },
      {
        "start": 2015.48,
        "duration": 4.76,
        "text": "database right like not every Vector"
      },
      {
        "start": 2017.6,
        "duration": 5.84,
        "text": "database is created equal but Astra"
      },
      {
        "start": 2020.24,
        "duration": 5.039,
        "text": "absolutely can scale to handle a couple"
      },
      {
        "start": 2023.44,
        "duration": 2.839,
        "text": "dozen files and a you know a couple"
      },
      {
        "start": 2025.279,
        "duration": 3.601,
        "text": "hundred"
      },
      {
        "start": 2026.279,
        "duration": 4.081,
        "text": "megabytes and into the Beyond right did"
      },
      {
        "start": 2028.88,
        "duration": 5.08,
        "text": "we index all of"
      },
      {
        "start": 2030.36,
        "duration": 5.6,
        "text": "Wikipedia Yes Yes actually I think"
      },
      {
        "start": 2033.96,
        "duration": 3.92,
        "text": "that's actually made available uh and"
      },
      {
        "start": 2035.96,
        "duration": 3.04,
        "text": "there's there's also this other cool"
      },
      {
        "start": 2037.88,
        "duration": 3.24,
        "text": "yeah it's a really good point actually"
      },
      {
        "start": 2039.0,
        "duration": 4.2,
        "text": "and and matter of fact it even um the"
      },
      {
        "start": 2041.12,
        "duration": 3.12,
        "text": "the app I don't remember the URL off the"
      },
      {
        "start": 2043.2,
        "duration": 3.599,
        "text": "top of my head maybe we can find out"
      },
      {
        "start": 2044.24,
        "duration": 4.599,
        "text": "while we're going through um but it real"
      },
      {
        "start": 2046.799,
        "duration": 4.04,
        "text": "time listens for any changes to"
      },
      {
        "start": 2048.839,
        "duration": 4.921,
        "text": "Wikipedia and automatically then adds"
      },
      {
        "start": 2050.839,
        "duration": 5.0,
        "text": "them into the rag store and updates any"
      },
      {
        "start": 2053.76,
        "duration": 3.839,
        "text": "context of a particular document right"
      },
      {
        "start": 2055.839,
        "duration": 3.961,
        "text": "so it's pretty darn real time you can"
      },
      {
        "start": 2057.599,
        "duration": 4.441,
        "text": "actually interact with it as things are"
      },
      {
        "start": 2059.8,
        "duration": 5.279,
        "text": "being added right"
      },
      {
        "start": 2062.04,
        "duration": 4.76,
        "text": "yeah yeah I forgot about that one well"
      },
      {
        "start": 2065.079,
        "duration": 3.641,
        "text": "there's also that Swifty GPT which I"
      },
      {
        "start": 2066.8,
        "duration": 5.039,
        "text": "really love too where you could chat"
      },
      {
        "start": 2068.72,
        "duration": 4.919,
        "text": "with Taylor Swift um but I certainly I"
      },
      {
        "start": 2071.839,
        "duration": 4.121,
        "text": "also have a podcast called open source"
      },
      {
        "start": 2073.639,
        "duration": 3.841,
        "text": "data and I would love to go chat with"
      },
      {
        "start": 2075.96,
        "duration": 4.439,
        "text": "myself"
      },
      {
        "start": 2077.48,
        "duration": 4.359,
        "text": "sometimes I know that sounds weird uh"
      },
      {
        "start": 2080.399,
        "duration": 2.921,
        "text": "but sometimes especially when we're"
      },
      {
        "start": 2081.839,
        "duration": 2.961,
        "text": "doing something live like this and"
      },
      {
        "start": 2083.32,
        "duration": 4.12,
        "text": "especially when I get into a"
      },
      {
        "start": 2084.8,
        "duration": 5.879,
        "text": "conversation I don't actually remember"
      },
      {
        "start": 2087.44,
        "duration": 5.159,
        "text": "everything that I've said um and so I"
      },
      {
        "start": 2090.679,
        "duration": 5.68,
        "text": "just have to trust my producer to you"
      },
      {
        "start": 2092.599,
        "duration": 5.801,
        "text": "know edit the right things or not that's"
      },
      {
        "start": 2096.359,
        "duration": 3.881,
        "text": "great and I we would kind of like to"
      },
      {
        "start": 2098.4,
        "duration": 5.24,
        "text": "know could you summarize the points I"
      },
      {
        "start": 2100.24,
        "duration": 5.32,
        "text": "made in this episode just just for that"
      },
      {
        "start": 2103.64,
        "duration": 3.84,
        "text": "checkpoint you know funny enough there"
      },
      {
        "start": 2105.56,
        "duration": 3.88,
        "text": "is I'm not going to show it now but"
      },
      {
        "start": 2107.48,
        "duration": 3.44,
        "text": "there is actually a component in lenlow"
      },
      {
        "start": 2109.44,
        "duration": 4.639,
        "text": "that somebody built that allows you to"
      },
      {
        "start": 2110.92,
        "duration": 4.96,
        "text": "pull the transcripts from YouTube videos"
      },
      {
        "start": 2114.079,
        "duration": 4.121,
        "text": "yeah and for me is a content creator"
      },
      {
        "start": 2115.88,
        "duration": 4.199,
        "text": "that's actually pretty relevant right uh"
      },
      {
        "start": 2118.2,
        "duration": 3.12,
        "text": "to that point right and you can yank"
      },
      {
        "start": 2120.079,
        "duration": 3.841,
        "text": "those out and then work with them from"
      },
      {
        "start": 2121.32,
        "duration": 4.72,
        "text": "the LM so that's pretty fun"
      },
      {
        "start": 2123.92,
        "duration": 3.88,
        "text": "yeah for checkpoint yeah that's right"
      },
      {
        "start": 2126.04,
        "duration": 4.2,
        "text": "I'm familiar with aotter AI as well"
      },
      {
        "start": 2127.8,
        "duration": 4.799,
        "text": "thank you Peter all right so here's what"
      },
      {
        "start": 2130.24,
        "duration": 3.839,
        "text": "I'm gonna do again um just to make sure"
      },
      {
        "start": 2132.599,
        "duration": 3.561,
        "text": "as we're going give me a thumbs up that"
      },
      {
        "start": 2134.079,
        "duration": 3.921,
        "text": "you can see the screen here so where I"
      },
      {
        "start": 2136.16,
        "duration": 3.439,
        "text": "did that kind of quick demo I glossed"
      },
      {
        "start": 2138.0,
        "duration": 3.64,
        "text": "over a bunch of stuff before this time"
      },
      {
        "start": 2139.599,
        "duration": 6.041,
        "text": "I'm going to go just a little bit deeper"
      },
      {
        "start": 2141.64,
        "duration": 7.36,
        "text": "into um everything that is going on um"
      },
      {
        "start": 2145.64,
        "duration": 5.24,
        "text": "okay so what I've done again this is"
      },
      {
        "start": 2149.0,
        "duration": 4.04,
        "text": "just um thank you for that Peter I saw"
      },
      {
        "start": 2150.88,
        "duration": 4.959,
        "text": "the thumbs up um this is again just a"
      },
      {
        "start": 2153.04,
        "duration": 4.68,
        "text": "slight modification of the vector store"
      },
      {
        "start": 2155.839,
        "duration": 3.401,
        "text": "rag template and by the way when when"
      },
      {
        "start": 2157.72,
        "duration": 2.76,
        "text": "you're starting in L flow you can"
      },
      {
        "start": 2159.24,
        "duration": 3.24,
        "text": "absolutely start with one of the blank"
      },
      {
        "start": 2160.48,
        "duration": 3.56,
        "text": "flows it's it's really not that hard but"
      },
      {
        "start": 2162.48,
        "duration": 2.72,
        "text": "I do suggest start with the template"
      },
      {
        "start": 2164.04,
        "duration": 2.88,
        "text": "start with the basic prompting the"
      },
      {
        "start": 2165.2,
        "duration": 3.56,
        "text": "memory chapot things like that it'll"
      },
      {
        "start": 2166.92,
        "duration": 4.399,
        "text": "very quickly show you just how to you"
      },
      {
        "start": 2168.76,
        "duration": 6.359,
        "text": "know build up capability um and the"
      },
      {
        "start": 2171.319,
        "duration": 5.52,
        "text": "vector store rag honestly is just it is"
      },
      {
        "start": 2175.119,
        "duration": 3.801,
        "text": "an extension if you will that's why I"
      },
      {
        "start": 2176.839,
        "duration": 4.401,
        "text": "call it a rag bot it's like an extension"
      },
      {
        "start": 2178.92,
        "duration": 4.159,
        "text": "of a chatbot with more capabilities that"
      },
      {
        "start": 2181.24,
        "duration": 4.24,
        "text": "are than being augmented with our"
      },
      {
        "start": 2183.079,
        "duration": 3.601,
        "text": "information in the vector store okay so"
      },
      {
        "start": 2185.48,
        "duration": 3.2,
        "text": "let's go ahead and set this up a little"
      },
      {
        "start": 2186.68,
        "duration": 5.32,
        "text": "bit so"
      },
      {
        "start": 2188.68,
        "duration": 5.04,
        "text": "like before we have one part of the flow"
      },
      {
        "start": 2192.0,
        "duration": 4.0,
        "text": "that is this generative part of the flow"
      },
      {
        "start": 2193.72,
        "duration": 3.639,
        "text": "it's the part where I'm ingesting data"
      },
      {
        "start": 2196.0,
        "duration": 3.68,
        "text": "and I'm converting it into vector"
      },
      {
        "start": 2197.359,
        "duration": 4.0,
        "text": "embeddings and storing it into my uh"
      },
      {
        "start": 2199.68,
        "duration": 4.08,
        "text": "into my rag store now one thing I want"
      },
      {
        "start": 2201.359,
        "duration": 5.561,
        "text": "to point out here if you remember before"
      },
      {
        "start": 2203.76,
        "duration": 6.0,
        "text": "I just had a single file um and I was"
      },
      {
        "start": 2206.92,
        "duration": 4.32,
        "text": "ingesting just that one PDF file you are"
      },
      {
        "start": 2209.76,
        "duration": 4.4,
        "text": "not limited I I want to be very clear"
      },
      {
        "start": 2211.24,
        "duration": 4.76,
        "text": "about something the UI in Lang flow is"
      },
      {
        "start": 2214.16,
        "duration": 3.76,
        "text": "this kind of quick development no code"
      },
      {
        "start": 2216.0,
        "duration": 4.119,
        "text": "style of doing things um but at the end"
      },
      {
        "start": 2217.92,
        "duration": 4.12,
        "text": "of the day it wouldn't be very useful if"
      },
      {
        "start": 2220.119,
        "duration": 4.841,
        "text": "I was only limited to what's hardcoded"
      },
      {
        "start": 2222.04,
        "duration": 5.0,
        "text": "here in the UI right um it obviously it"
      },
      {
        "start": 2224.96,
        "duration": 4.24,
        "text": "has inputs in outputs that allows you to"
      },
      {
        "start": 2227.04,
        "duration": 3.88,
        "text": "programmatically set these values I will"
      },
      {
        "start": 2229.2,
        "duration": 3.72,
        "text": "show you that in here in a couple"
      },
      {
        "start": 2230.92,
        "duration": 4.199,
        "text": "minutes right um so you're not limited"
      },
      {
        "start": 2232.92,
        "duration": 5.12,
        "text": "to just a single PDF you're also not"
      },
      {
        "start": 2235.119,
        "duration": 5.041,
        "text": "limited to just files right uh notice"
      },
      {
        "start": 2238.04,
        "duration": 4.24,
        "text": "below I actually yanked in I'm scraping"
      },
      {
        "start": 2240.16,
        "duration": 4.52,
        "text": "URLs from one of my old blogs that I"
      },
      {
        "start": 2242.28,
        "duration": 3.6,
        "text": "have not contributed to in years um and"
      },
      {
        "start": 2244.68,
        "duration": 2.6,
        "text": "you know it's like actually to your"
      },
      {
        "start": 2245.88,
        "duration": 3.68,
        "text": "point chal like trying to remember some"
      },
      {
        "start": 2247.28,
        "duration": 4.4,
        "text": "of the things you've done um I don't"
      },
      {
        "start": 2249.56,
        "duration": 3.759,
        "text": "remember all of the material from that"
      },
      {
        "start": 2251.68,
        "duration": 4.639,
        "text": "kind of thing right so what I did in"
      },
      {
        "start": 2253.319,
        "duration": 4.441,
        "text": "this case was I also pulled those in um"
      },
      {
        "start": 2256.319,
        "duration": 3.0,
        "text": "if you take here on the left hand side"
      },
      {
        "start": 2257.76,
        "duration": 3.319,
        "text": "or take a look here on the left hand"
      },
      {
        "start": 2259.319,
        "duration": 3.681,
        "text": "side you can see there's all sorts of"
      },
      {
        "start": 2261.079,
        "duration": 3.441,
        "text": "different ways you can get data and"
      },
      {
        "start": 2263.0,
        "duration": 3.68,
        "text": "tying in is something else that charna"
      },
      {
        "start": 2264.52,
        "duration": 3.92,
        "text": "said underneath the hood even though you"
      },
      {
        "start": 2266.68,
        "duration": 4.24,
        "text": "could build out everything from a no"
      },
      {
        "start": 2268.44,
        "duration": 6.24,
        "text": "code standpoint right um you do not need"
      },
      {
        "start": 2270.92,
        "duration": 6.76,
        "text": "to be a programmer to use llow but if"
      },
      {
        "start": 2274.68,
        "duration": 5.28,
        "text": "you are and if you are versed in Python"
      },
      {
        "start": 2277.68,
        "duration": 4.919,
        "text": "even just a little bit you can get at"
      },
      {
        "start": 2279.96,
        "duration": 3.879,
        "text": "all the code right all of it is here"
      },
      {
        "start": 2282.599,
        "duration": 2.881,
        "text": "matter of fact every one of these"
      },
      {
        "start": 2283.839,
        "duration": 3.841,
        "text": "components is actually a holy contone"
      },
      {
        "start": 2285.48,
        "duration": 4.04,
        "text": "python module or holy contained python"
      },
      {
        "start": 2287.68,
        "duration": 3.439,
        "text": "module if you were to pull this out and"
      },
      {
        "start": 2289.52,
        "duration": 5.16,
        "text": "put it into a module it would work just"
      },
      {
        "start": 2291.119,
        "duration": 4.881,
        "text": "fine right um You can also modify these"
      },
      {
        "start": 2294.68,
        "duration": 4.0,
        "text": "right so you can just come in here if I"
      },
      {
        "start": 2296.0,
        "duration": 4.359,
        "text": "wanted to and I can say you know um make"
      },
      {
        "start": 2298.68,
        "duration": 3.36,
        "text": "some change whatever you know blah blah"
      },
      {
        "start": 2300.359,
        "duration": 3.281,
        "text": "blah save that and then I could just"
      },
      {
        "start": 2302.04,
        "duration": 3.36,
        "text": "save that as a new component and now I"
      },
      {
        "start": 2303.64,
        "duration": 3.199,
        "text": "have a new component right so it's super"
      },
      {
        "start": 2305.4,
        "duration": 3.959,
        "text": "flexible from that standpoint you are"
      },
      {
        "start": 2306.839,
        "duration": 4.961,
        "text": "not limited to the things that are only"
      },
      {
        "start": 2309.359,
        "duration": 5.161,
        "text": "here that you see in the UI okay great"
      },
      {
        "start": 2311.8,
        "duration": 4.24,
        "text": "so we have our PDF but now I want to add"
      },
      {
        "start": 2314.52,
        "duration": 3.52,
        "text": "something else like I said I'm going to"
      },
      {
        "start": 2316.04,
        "duration": 5.2,
        "text": "go ahead and scrape in uh one of my old"
      },
      {
        "start": 2318.04,
        "duration": 5.12,
        "text": "blog URLs um and then I'm gonna pull it"
      },
      {
        "start": 2321.24,
        "duration": 3.68,
        "text": "into this split text component now this"
      },
      {
        "start": 2323.16,
        "duration": 4.04,
        "text": "is part of the template this is provided"
      },
      {
        "start": 2324.92,
        "duration": 3.679,
        "text": "for you as part of the template right"
      },
      {
        "start": 2327.2,
        "duration": 3.04,
        "text": "now what is this actually doing what I'm"
      },
      {
        "start": 2328.599,
        "duration": 3.24,
        "text": "going to do real quick I'm going to run"
      },
      {
        "start": 2330.24,
        "duration": 3.8,
        "text": "this part so I can talk through some"
      },
      {
        "start": 2331.839,
        "duration": 4.201,
        "text": "things I do just want to point out that"
      },
      {
        "start": 2334.04,
        "duration": 3.88,
        "text": "as I do it I have this collection name"
      },
      {
        "start": 2336.04,
        "duration": 3.559,
        "text": "langlow this does not exist in my"
      },
      {
        "start": 2337.92,
        "duration": 4.159,
        "text": "database right so I'm going to go ahead"
      },
      {
        "start": 2339.599,
        "duration": 4.441,
        "text": "and run this and what this component"
      },
      {
        "start": 2342.079,
        "duration": 4.561,
        "text": "this ASB component will do it will"
      },
      {
        "start": 2344.04,
        "duration": 3.4,
        "text": "actually create the collection for me"
      },
      {
        "start": 2346.64,
        "duration": 2.4,
        "text": "right so it's going to create the"
      },
      {
        "start": 2347.44,
        "duration": 3.879,
        "text": "collection and put the data in but while"
      },
      {
        "start": 2349.04,
        "duration": 4.079,
        "text": "that's happening I want to talk to"
      },
      {
        "start": 2351.319,
        "duration": 5.161,
        "text": "what's going on here um and I should"
      },
      {
        "start": 2353.119,
        "duration": 5.641,
        "text": "point out that the initial load expect"
      },
      {
        "start": 2356.48,
        "duration": 4.32,
        "text": "on your first run especially like when"
      },
      {
        "start": 2358.76,
        "duration": 3.96,
        "text": "I'm doing here since I don't have the"
      },
      {
        "start": 2360.8,
        "duration": 3.559,
        "text": "collection it has to go create that that"
      },
      {
        "start": 2362.72,
        "duration": 4.16,
        "text": "takes a moment right here it's already"
      },
      {
        "start": 2364.359,
        "duration": 5.041,
        "text": "done um but expect on your first load"
      },
      {
        "start": 2366.88,
        "duration": 5.88,
        "text": "it'll be little slower than subsequent"
      },
      {
        "start": 2369.4,
        "duration": 5.6,
        "text": "loads okay so I've got data I want to"
      },
      {
        "start": 2372.76,
        "duration": 4.4,
        "text": "now chunk the data what does this mean"
      },
      {
        "start": 2375.0,
        "duration": 4.24,
        "text": "this means you see this chunk size here"
      },
      {
        "start": 2377.16,
        "duration": 4.88,
        "text": "this means I'm going to iterate through"
      },
      {
        "start": 2379.24,
        "duration": 4.839,
        "text": "whatever data was returned from my my"
      },
      {
        "start": 2382.04,
        "duration": 4.48,
        "text": "data inputs here and I'm going to"
      },
      {
        "start": 2384.079,
        "duration": 4.881,
        "text": "iterate through that data in Thousand"
      },
      {
        "start": 2386.52,
        "duration": 4.76,
        "text": "character lengths and then I'm going to"
      },
      {
        "start": 2388.96,
        "duration": 4.28,
        "text": "take those thousand character chunks and"
      },
      {
        "start": 2391.28,
        "duration": 4.319,
        "text": "I'm going to convert those into Vector"
      },
      {
        "start": 2393.24,
        "duration": 4.4,
        "text": "embeddings now you might ask what Vector"
      },
      {
        "start": 2395.599,
        "duration": 3.601,
        "text": "embeddings how which ones am I use where"
      },
      {
        "start": 2397.64,
        "duration": 4.08,
        "text": "does this come into play you may have"
      },
      {
        "start": 2399.2,
        "duration": 5.48,
        "text": "noticed right below it you see this open"
      },
      {
        "start": 2401.72,
        "duration": 5.28,
        "text": "AI embeddings component now in Ling flow"
      },
      {
        "start": 2404.68,
        "duration": 3.72,
        "text": "it is model agnostic and what I mean by"
      },
      {
        "start": 2407.0,
        "duration": 2.96,
        "text": "that is there are a ton of different"
      },
      {
        "start": 2408.4,
        "duration": 3.56,
        "text": "providers right you are not limited to"
      },
      {
        "start": 2409.96,
        "duration": 3.52,
        "text": "open AI or anything as a matter of fact"
      },
      {
        "start": 2411.96,
        "duration": 2.84,
        "text": "there's even support for a llama if"
      },
      {
        "start": 2413.48,
        "duration": 3.359,
        "text": "you're using local models right so"
      },
      {
        "start": 2414.8,
        "duration": 4.4,
        "text": "there's a lot you can do here but for"
      },
      {
        "start": 2416.839,
        "duration": 5.601,
        "text": "this case in our template I'm using open"
      },
      {
        "start": 2419.2,
        "duration": 5.0,
        "text": "AI right now notice here underneath"
      },
      {
        "start": 2422.44,
        "duration": 3.48,
        "text": "model there are set of models so we're"
      },
      {
        "start": 2424.2,
        "duration": 3.72,
        "text": "saying that open ey has these three"
      },
      {
        "start": 2425.92,
        "duration": 4.439,
        "text": "embedding models that we've exposed"
      },
      {
        "start": 2427.92,
        "duration": 3.88,
        "text": "three small three large and 82 now in"
      },
      {
        "start": 2430.359,
        "duration": 3.521,
        "text": "this case as part of the template I'm"
      },
      {
        "start": 2431.8,
        "duration": 4.44,
        "text": "using three small now something I want"
      },
      {
        "start": 2433.88,
        "duration": 4.08,
        "text": "to point out here and this gets back to"
      },
      {
        "start": 2436.24,
        "duration": 4.4,
        "text": "some of what Kara was talking about"
      },
      {
        "start": 2437.96,
        "duration": 4.399,
        "text": "earlier right this is actually a super"
      },
      {
        "start": 2440.64,
        "duration": 5.32,
        "text": "important step and here's"
      },
      {
        "start": 2442.359,
        "duration": 6.921,
        "text": "why whatever embedding model I choose to"
      },
      {
        "start": 2445.96,
        "duration": 6.72,
        "text": "embed my data in in my rag store I need"
      },
      {
        "start": 2449.28,
        "duration": 5.72,
        "text": "to also ensure that when I'm querying"
      },
      {
        "start": 2452.68,
        "duration": 4.84,
        "text": "I'm also using the same model or at"
      },
      {
        "start": 2455.0,
        "duration": 5.2,
        "text": "least be in the same family if I try to"
      },
      {
        "start": 2457.52,
        "duration": 5.2,
        "text": "mix and match right if I were to say uh"
      },
      {
        "start": 2460.2,
        "duration": 4.44,
        "text": "embed in open AI text embedding three"
      },
      {
        "start": 2462.72,
        "duration": 3.399,
        "text": "small but then later on I'm like you"
      },
      {
        "start": 2464.64,
        "duration": 3.76,
        "text": "know I'm going to go ahead and I'm just"
      },
      {
        "start": 2466.119,
        "duration": 3.681,
        "text": "gonna like throw anthropic in here that"
      },
      {
        "start": 2468.4,
        "duration": 4.52,
        "text": "that's probably not going to work very"
      },
      {
        "start": 2469.8,
        "duration": 5.08,
        "text": "well right um now interestingly enough"
      },
      {
        "start": 2472.92,
        "duration": 4.08,
        "text": "uh if there was something that was not"
      },
      {
        "start": 2474.88,
        "duration": 3.68,
        "text": "based off the data in your rag store"
      },
      {
        "start": 2477.0,
        "duration": 3.92,
        "text": "maybe you had some prompt chaining maybe"
      },
      {
        "start": 2478.56,
        "duration": 5.12,
        "text": "you have a decision tree that um you"
      },
      {
        "start": 2480.92,
        "duration": 5.12,
        "text": "know like makes a makes some conditional"
      },
      {
        "start": 2483.68,
        "duration": 4.52,
        "text": "decision based off of some piece of your"
      },
      {
        "start": 2486.04,
        "duration": 3.96,
        "text": "return data that says oh I want to go"
      },
      {
        "start": 2488.2,
        "duration": 3.52,
        "text": "process that over here maybe then you"
      },
      {
        "start": 2490.0,
        "duration": 4.599,
        "text": "can bring in a different model but as"
      },
      {
        "start": 2491.72,
        "duration": 4.599,
        "text": "far as when you are querying against"
      },
      {
        "start": 2494.599,
        "duration": 3.321,
        "text": "your rag store you need to make sure"
      },
      {
        "start": 2496.319,
        "duration": 3.76,
        "text": "that your models match or in the same"
      },
      {
        "start": 2497.92,
        "duration": 5.56,
        "text": "family right that's a really key part of"
      },
      {
        "start": 2500.079,
        "duration": 4.801,
        "text": "this and then finally oh yeah go ahead"
      },
      {
        "start": 2503.48,
        "duration": 4.68,
        "text": "yeah I would say that like if you're"
      },
      {
        "start": 2504.88,
        "duration": 4.84,
        "text": "thinking about um any kind of search"
      },
      {
        "start": 2508.16,
        "duration": 3.84,
        "text": "right like if you were trying to use"
      },
      {
        "start": 2509.72,
        "duration": 3.879,
        "text": "English language to search a French"
      },
      {
        "start": 2512.0,
        "duration": 4.079,
        "text": "dictionary you would not get back what"
      },
      {
        "start": 2513.599,
        "duration": 4.121,
        "text": "you expect right it's it's similar to"
      },
      {
        "start": 2516.079,
        "duration": 4.0,
        "text": "that it's essentially en coding"
      },
      {
        "start": 2517.72,
        "duration": 4.8,
        "text": "information differently so if you're"
      },
      {
        "start": 2520.079,
        "duration": 5.401,
        "text": "embedding a video with a video embedding"
      },
      {
        "start": 2522.52,
        "duration": 4.88,
        "text": "model that's so different embeding"
      },
      {
        "start": 2525.48,
        "duration": 4.56,
        "text": "language right it's just a completely"
      },
      {
        "start": 2527.4,
        "duration": 3.919,
        "text": "different representation um and when I"
      },
      {
        "start": 2530.04,
        "duration": 4.079,
        "text": "think about like how colors are"
      },
      {
        "start": 2531.319,
        "duration": 5.76,
        "text": "represented for example we've got RGB"
      },
      {
        "start": 2534.119,
        "duration": 4.72,
        "text": "we've got Cy whatever whatever whatever"
      },
      {
        "start": 2537.079,
        "duration": 3.681,
        "text": "you cannot compare those two things you"
      },
      {
        "start": 2538.839,
        "duration": 3.28,
        "text": "can convert between them but you can't"
      },
      {
        "start": 2540.76,
        "duration": 3.0,
        "text": "compare them and that's that's how I"
      },
      {
        "start": 2542.119,
        "duration": 3.0,
        "text": "think about embedding models purpose"
      },
      {
        "start": 2543.76,
        "duration": 4.44,
        "text": "built for the thing you're trying to"
      },
      {
        "start": 2545.119,
        "duration": 4.96,
        "text": "achieve and not compatible"
      },
      {
        "start": 2548.2,
        "duration": 3.879,
        "text": "yes absolutely and by the way um I did"
      },
      {
        "start": 2550.079,
        "duration": 4.441,
        "text": "see your comment Greg um about the color"
      },
      {
        "start": 2552.079,
        "duration": 4.601,
        "text": "scheme so I did flip it um hopefully"
      },
      {
        "start": 2554.52,
        "duration": 3.52,
        "text": "this is better um and that is across the"
      },
      {
        "start": 2556.68,
        "duration": 4.04,
        "text": "board if anybody's having a hard time"
      },
      {
        "start": 2558.04,
        "duration": 4.079,
        "text": "seeing the Dark theme um you know but if"
      },
      {
        "start": 2560.72,
        "duration": 3.359,
        "text": "this is also difficult to see let me"
      },
      {
        "start": 2562.119,
        "duration": 5.0,
        "text": "know and by the way uh Claude support"
      },
      {
        "start": 2564.079,
        "duration": 5.361,
        "text": "yes Peter uh anthropic um am I thinking"
      },
      {
        "start": 2567.119,
        "duration": 5.121,
        "text": "the wrong yeah yeah du that was it yeah"
      },
      {
        "start": 2569.44,
        "duration": 5.8,
        "text": "there is support for Claude uh 35 Sonet"
      },
      {
        "start": 2572.24,
        "duration": 4.72,
        "text": "is actually in there and and such so um"
      },
      {
        "start": 2575.24,
        "duration": 4.44,
        "text": "hopefully it answers your question Peter"
      },
      {
        "start": 2576.96,
        "duration": 4.159,
        "text": "okay okay great so we have our inputs we"
      },
      {
        "start": 2579.68,
        "duration": 3.48,
        "text": "chunk through the data now what's super"
      },
      {
        "start": 2581.119,
        "duration": 3.601,
        "text": "cool here you see this little inspection"
      },
      {
        "start": 2583.16,
        "duration": 3.76,
        "text": "so you have this little inspection icon"
      },
      {
        "start": 2584.72,
        "duration": 3.639,
        "text": "in all these components and since I ran"
      },
      {
        "start": 2586.92,
        "duration": 4.679,
        "text": "through it we can actually see the"
      },
      {
        "start": 2588.359,
        "duration": 5.561,
        "text": "results so now we can see that I have"
      },
      {
        "start": 2591.599,
        "duration": 5.52,
        "text": "both results that came from the URL that"
      },
      {
        "start": 2593.92,
        "duration": 4.56,
        "text": "I scraped and from the PDF now one thing"
      },
      {
        "start": 2597.119,
        "duration": 2.521,
        "text": "that linkflow is doing you'll notice"
      },
      {
        "start": 2598.48,
        "duration": 2.24,
        "text": "that there's this little data these"
      },
      {
        "start": 2599.64,
        "duration": 3.0,
        "text": "outputs right you see these little"
      },
      {
        "start": 2600.72,
        "duration": 4.28,
        "text": "colors in these nodes and everything um"
      },
      {
        "start": 2602.64,
        "duration": 4.24,
        "text": "and there are types right uh that they"
      },
      {
        "start": 2605.0,
        "duration": 4.319,
        "text": "conform to so when you're pulling data"
      },
      {
        "start": 2606.88,
        "duration": 5.28,
        "text": "in it'll actually convert into these"
      },
      {
        "start": 2609.319,
        "duration": 5.361,
        "text": "data chunks um and when I look at those"
      },
      {
        "start": 2612.16,
        "duration": 4.12,
        "text": "you can see I have each of these now"
      },
      {
        "start": 2614.68,
        "duration": 3.56,
        "text": "what this is actually doing it is"
      },
      {
        "start": 2616.28,
        "duration": 4.12,
        "text": "separating the data out into useful like"
      },
      {
        "start": 2618.24,
        "duration": 4.4,
        "text": "Fields source title things like that for"
      },
      {
        "start": 2620.4,
        "duration": 5.439,
        "text": "our particular needs I'm interested in"
      },
      {
        "start": 2622.64,
        "duration": 5.919,
        "text": "text mostly right and you can see each"
      },
      {
        "start": 2625.839,
        "duration": 5.0,
        "text": "one of these is actually going to be a"
      },
      {
        "start": 2628.559,
        "duration": 4.8,
        "text": "chunk of about a thousand characters"
      },
      {
        "start": 2630.839,
        "duration": 4.401,
        "text": "right this each one of these this text"
      },
      {
        "start": 2633.359,
        "duration": 4.48,
        "text": "is actually what is being converted into"
      },
      {
        "start": 2635.24,
        "duration": 4.879,
        "text": "our vector embeddings and then St"
      },
      {
        "start": 2637.839,
        "duration": 3.72,
        "text": "in our rag store so let's go ahead and"
      },
      {
        "start": 2640.119,
        "duration": 3.641,
        "text": "take a look real quick now remember"
      },
      {
        "start": 2641.559,
        "duration": 5.0,
        "text": "before I did not have the Lang flow"
      },
      {
        "start": 2643.76,
        "duration": 5.079,
        "text": "collection right that didn't exist and"
      },
      {
        "start": 2646.559,
        "duration": 4.0,
        "text": "now if I go to the langlow collection I"
      },
      {
        "start": 2648.839,
        "duration": 4.401,
        "text": "should have a bunch of data and I do"
      },
      {
        "start": 2650.559,
        "duration": 5.401,
        "text": "right so you can see now that I have um"
      },
      {
        "start": 2653.24,
        "duration": 5.28,
        "text": "some I'm gonna open this up a little"
      },
      {
        "start": 2655.96,
        "duration": 4.8,
        "text": "bit there we go right you can see that"
      },
      {
        "start": 2658.52,
        "duration": 4.319,
        "text": "I've I've got the data itself the"
      },
      {
        "start": 2660.76,
        "duration": 3.12,
        "text": "content that was stored in embeddings"
      },
      {
        "start": 2662.839,
        "duration": 3.041,
        "text": "and then I've got these Vector"
      },
      {
        "start": 2663.88,
        "duration": 4.6,
        "text": "embeddings that represent that data"
      },
      {
        "start": 2665.88,
        "duration": 3.92,
        "text": "mathematically if you will semantic"
      },
      {
        "start": 2668.48,
        "duration": 3.359,
        "text": "great so now I have a store that"
      },
      {
        "start": 2669.8,
        "duration": 4.6,
        "text": "actually has some information in"
      },
      {
        "start": 2671.839,
        "duration": 4.52,
        "text": "it okay and by the way just to point out"
      },
      {
        "start": 2674.4,
        "duration": 4.76,
        "text": "um another little feature here of LF"
      },
      {
        "start": 2676.359,
        "duration": 5.041,
        "text": "flow you may notice um that like for the"
      },
      {
        "start": 2679.16,
        "duration": 4.159,
        "text": "database or for open AI like there's"
      },
      {
        "start": 2681.4,
        "duration": 4.64,
        "text": "open AI key and you just see this kind"
      },
      {
        "start": 2683.319,
        "duration": 5.24,
        "text": "of abstract open AI API key what's going"
      },
      {
        "start": 2686.04,
        "duration": 4.76,
        "text": "on there um could I just go get an open"
      },
      {
        "start": 2688.559,
        "duration": 4.321,
        "text": "AI key and paste it in the Raw I could"
      },
      {
        "start": 2690.8,
        "duration": 4.039,
        "text": "but one of the neat features is globals"
      },
      {
        "start": 2692.88,
        "duration": 4.36,
        "text": "or our globals I should say and when I"
      },
      {
        "start": 2694.839,
        "duration": 5.321,
        "text": "create a global I can actually store"
      },
      {
        "start": 2697.24,
        "duration": 4.64,
        "text": "secure values like an API key and then"
      },
      {
        "start": 2700.16,
        "duration": 3.04,
        "text": "it's available to all of my components"
      },
      {
        "start": 2701.88,
        "duration": 2.56,
        "text": "so I don't have to keep pasting it all"
      },
      {
        "start": 2703.2,
        "duration": 3.04,
        "text": "over the place and stuff like that I"
      },
      {
        "start": 2704.44,
        "duration": 4.04,
        "text": "just sort in a global and that's exactly"
      },
      {
        "start": 2706.24,
        "duration": 4.44,
        "text": "what I did here for the uh database so"
      },
      {
        "start": 2708.48,
        "duration": 4.24,
        "text": "this token and endpoint if I go over to"
      },
      {
        "start": 2710.68,
        "duration": 4.04,
        "text": "the database itself and I go to the"
      },
      {
        "start": 2712.72,
        "duration": 4.76,
        "text": "overview you'll see I have this endpoint"
      },
      {
        "start": 2714.72,
        "duration": 4.92,
        "text": "and a token I just copi it over generate"
      },
      {
        "start": 2717.48,
        "duration": 4.079,
        "text": "one copy it into my Global and now it's"
      },
      {
        "start": 2719.64,
        "duration": 3.56,
        "text": "available to all of my components so"
      },
      {
        "start": 2721.559,
        "duration": 3.56,
        "text": "it's a nice little feature for"
      },
      {
        "start": 2723.2,
        "duration": 4.48,
        "text": "convenience and it is in fact quite"
      },
      {
        "start": 2725.119,
        "duration": 4.161,
        "text": "convenient okay great so have data in my"
      },
      {
        "start": 2727.68,
        "duration": 3.96,
        "text": "rag store let's go ahead and take a look"
      },
      {
        "start": 2729.28,
        "duration": 4.24,
        "text": "at the query the query flow I'm going to"
      },
      {
        "start": 2731.64,
        "duration": 3.08,
        "text": "kind of focus in here I'm going to"
      },
      {
        "start": 2733.52,
        "duration": 2.48,
        "text": "change up the question a little bit this"
      },
      {
        "start": 2734.72,
        "duration": 3.839,
        "text": "time is g to be when did I get married"
      },
      {
        "start": 2736.0,
        "duration": 3.8,
        "text": "all these things you know like it is my"
      },
      {
        "start": 2738.559,
        "duration": 3.881,
        "text": "domain knowledge that's why I pulled it"
      },
      {
        "start": 2739.8,
        "duration": 3.92,
        "text": "in right um but again this is something"
      },
      {
        "start": 2742.44,
        "duration": 2.6,
        "text": "that if you just ask that question to"
      },
      {
        "start": 2743.72,
        "duration": 3.56,
        "text": "the llm it's not gonna know what you're"
      },
      {
        "start": 2745.04,
        "duration": 4.559,
        "text": "talking about right um so we can we can"
      },
      {
        "start": 2747.28,
        "duration": 4.44,
        "text": "add this context so what's going on in"
      },
      {
        "start": 2749.599,
        "duration": 3.76,
        "text": "this part of the flow this really ties"
      },
      {
        "start": 2751.72,
        "duration": 3.04,
        "text": "into something where you might be"
      },
      {
        "start": 2753.359,
        "duration": 4.081,
        "text": "thinking well how do I bring this into"
      },
      {
        "start": 2754.76,
        "duration": 4.96,
        "text": "my application right um because I need"
      },
      {
        "start": 2757.44,
        "duration": 4.56,
        "text": "some types of inputs and outputs in the"
      },
      {
        "start": 2759.72,
        "duration": 3.599,
        "text": "flow in this workflow in order for my"
      },
      {
        "start": 2762.0,
        "duration": 3.119,
        "text": "application to be able to"
      },
      {
        "start": 2763.319,
        "duration": 4.081,
        "text": "programmatically affect the input and"
      },
      {
        "start": 2765.119,
        "duration": 4.041,
        "text": "then get the output right in this chat"
      },
      {
        "start": 2767.4,
        "duration": 4.64,
        "text": "input since I'm going to have a"
      },
      {
        "start": 2769.16,
        "duration": 5.0,
        "text": "conversation with this rag bot this chat"
      },
      {
        "start": 2772.04,
        "duration": 3.799,
        "text": "input is how I do that and you'll see up"
      },
      {
        "start": 2774.16,
        "duration": 4.32,
        "text": "in the top here there's a set of inputs"
      },
      {
        "start": 2775.839,
        "duration": 5.72,
        "text": "chat is um you know pretty darn standard"
      },
      {
        "start": 2778.48,
        "duration": 4.56,
        "text": "for this kind of uh use case um and you"
      },
      {
        "start": 2781.559,
        "duration": 3.081,
        "text": "know I'm either going to type in"
      },
      {
        "start": 2783.04,
        "duration": 2.72,
        "text": "something like it did here or again it"
      },
      {
        "start": 2784.64,
        "duration": 2.679,
        "text": "has an input where I can"
      },
      {
        "start": 2785.76,
        "duration": 2.88,
        "text": "programmatically set it and I'm going to"
      },
      {
        "start": 2787.319,
        "duration": 3.841,
        "text": "show you that the programmatic part here"
      },
      {
        "start": 2788.64,
        "duration": 4.479,
        "text": "in a moment again I have the same setup"
      },
      {
        "start": 2791.16,
        "duration": 4.36,
        "text": "the database the rag store or the vector"
      },
      {
        "start": 2793.119,
        "duration": 4.761,
        "text": "store the same exact collection the same"
      },
      {
        "start": 2795.52,
        "duration": 5.48,
        "text": "thing with the embeddings now why are we"
      },
      {
        "start": 2797.88,
        "duration": 5.8,
        "text": "tied why do we need this tied in if I"
      },
      {
        "start": 2801.0,
        "duration": 4.2,
        "text": "already have my data in my rag store and"
      },
      {
        "start": 2803.68,
        "duration": 4.639,
        "text": "we've already gener generated the"
      },
      {
        "start": 2805.2,
        "duration": 5.68,
        "text": "embeddings because of this here so when"
      },
      {
        "start": 2808.319,
        "duration": 4.24,
        "text": "I ask a question or I have some query I"
      },
      {
        "start": 2810.88,
        "duration": 4.439,
        "text": "also need to convert that I need to"
      },
      {
        "start": 2812.559,
        "duration": 4.361,
        "text": "vectorize that right there into the same"
      },
      {
        "start": 2815.319,
        "duration": 3.201,
        "text": "Vector space as the data that's in my"
      },
      {
        "start": 2816.92,
        "duration": 4.6,
        "text": "rag store so I can perform a vector"
      },
      {
        "start": 2818.52,
        "duration": 6.12,
        "text": "similarity search so this whole step"
      },
      {
        "start": 2821.52,
        "duration": 6.68,
        "text": "when I say build it's actually now going"
      },
      {
        "start": 2824.64,
        "duration": 6.4,
        "text": "to my my Vector store it is converting"
      },
      {
        "start": 2828.2,
        "duration": 5.04,
        "text": "this into an embedding it is doing a"
      },
      {
        "start": 2831.04,
        "duration": 4.48,
        "text": "vector search and it's going to turn my"
      },
      {
        "start": 2833.24,
        "duration": 5.24,
        "text": "top K results right so it's going to"
      },
      {
        "start": 2835.52,
        "duration": 5.92,
        "text": "essentially turn the top results based"
      },
      {
        "start": 2838.48,
        "duration": 5.76,
        "text": "off of the search so if you notice"
      },
      {
        "start": 2841.44,
        "duration": 6.08,
        "text": "notice that when I look at this I have"
      },
      {
        "start": 2844.24,
        "duration": 4.879,
        "text": "more data in here than what I return"
      },
      {
        "start": 2847.52,
        "duration": 3.36,
        "text": "right so it's just returning the top"
      },
      {
        "start": 2849.119,
        "duration": 3.841,
        "text": "results based off the vector search the"
      },
      {
        "start": 2850.88,
        "duration": 3.6,
        "text": "question that I asked and again it's"
      },
      {
        "start": 2852.96,
        "duration": 4.159,
        "text": "converting it into this data object that"
      },
      {
        "start": 2854.48,
        "duration": 5.32,
        "text": "you see and the text is pretty much what"
      },
      {
        "start": 2857.119,
        "duration": 5.281,
        "text": "I'm interested in here this is the stuff"
      },
      {
        "start": 2859.8,
        "duration": 6.279,
        "text": "this these responses this is when I want"
      },
      {
        "start": 2862.4,
        "duration": 5.4,
        "text": "to feed in to the context of the llm so"
      },
      {
        "start": 2866.079,
        "duration": 4.201,
        "text": "that ties into our next step which is"
      },
      {
        "start": 2867.8,
        "duration": 5.24,
        "text": "this pars data step notice that it is"
      },
      {
        "start": 2870.28,
        "duration": 5.52,
        "text": "saying Hey I want to pars the data from"
      },
      {
        "start": 2873.04,
        "duration": 4.799,
        "text": "text so again I look at my data object I"
      },
      {
        "start": 2875.8,
        "duration": 3.279,
        "text": "have text so I can do this nice little"
      },
      {
        "start": 2877.839,
        "duration": 4.201,
        "text": "convenience thing just wrap that in"
      },
      {
        "start": 2879.079,
        "duration": 4.201,
        "text": "curries and then this will extract all"
      },
      {
        "start": 2882.04,
        "duration": 2.68,
        "text": "the test now I didn't actually run that"
      },
      {
        "start": 2883.28,
        "duration": 2.6,
        "text": "part so let's go ahead and actually I'm"
      },
      {
        "start": 2884.72,
        "duration": 2.24,
        "text": "going to run the whole flow is what I'm"
      },
      {
        "start": 2885.88,
        "duration": 2.6,
        "text": "going to do I'll let that finish for a"
      },
      {
        "start": 2886.96,
        "duration": 3.76,
        "text": "second that way I can talk to it as I'm"
      },
      {
        "start": 2888.48,
        "duration": 5.2,
        "text": "going I'm just going to run this"
      },
      {
        "start": 2890.72,
        "duration": 5.92,
        "text": "through so now when I look at text now"
      },
      {
        "start": 2893.68,
        "duration": 5.2,
        "text": "it's pulled out all the text from all of"
      },
      {
        "start": 2896.64,
        "duration": 3.959,
        "text": "those results right so you can see this"
      },
      {
        "start": 2898.88,
        "duration": 3.439,
        "text": "big chunk of text that came out and"
      },
      {
        "start": 2900.599,
        "duration": 4.401,
        "text": "these are the most relevant results"
      },
      {
        "start": 2902.319,
        "duration": 5.161,
        "text": "based off of my my Vector search now"
      },
      {
        "start": 2905.0,
        "duration": 3.52,
        "text": "this next part is actually I think we're"
      },
      {
        "start": 2907.48,
        "duration": 2.56,
        "text": "this is I think this is one of the"
      },
      {
        "start": 2908.52,
        "duration": 3.88,
        "text": "coolest pieces this prompt this prompt"
      },
      {
        "start": 2910.04,
        "duration": 4.16,
        "text": "component and here's what's going on"
      },
      {
        "start": 2912.4,
        "duration": 4.56,
        "text": "this would be the same exact thing as if"
      },
      {
        "start": 2914.2,
        "duration": 5.879,
        "text": "I went and Chap gbt and I started typing"
      },
      {
        "start": 2916.96,
        "duration": 4.84,
        "text": "this kind of stuff in right um but"
      },
      {
        "start": 2920.079,
        "duration": 3.721,
        "text": "notice how for these variable names I"
      },
      {
        "start": 2921.8,
        "duration": 4.0,
        "text": "have these crer Braes so a really neat"
      },
      {
        "start": 2923.8,
        "duration": 3.6,
        "text": "convenience function in langlow will"
      },
      {
        "start": 2925.8,
        "duration": 4.319,
        "text": "allow you if you have any variable you"
      },
      {
        "start": 2927.4,
        "duration": 5.28,
        "text": "can put curly braces around it and that"
      },
      {
        "start": 2930.119,
        "duration": 6.161,
        "text": "will then expose that as an input so I"
      },
      {
        "start": 2932.68,
        "duration": 4.72,
        "text": "could say something like this some cool"
      },
      {
        "start": 2936.28,
        "duration": 2.88,
        "text": "input"
      },
      {
        "start": 2937.4,
        "duration": 4.6,
        "text": "put curly braces around it say check and"
      },
      {
        "start": 2939.16,
        "duration": 4.48,
        "text": "save boom now I have an input where I"
      },
      {
        "start": 2942.0,
        "duration": 3.48,
        "text": "can just drag and drop and hook it up"
      },
      {
        "start": 2943.64,
        "duration": 3.919,
        "text": "makes it super easy to wire these things"
      },
      {
        "start": 2945.48,
        "duration": 5.68,
        "text": "up right okay so I'm gonna get rid of"
      },
      {
        "start": 2947.559,
        "duration": 6.24,
        "text": "that one come back over here we ran that"
      },
      {
        "start": 2951.16,
        "duration": 4.52,
        "text": "through so now if I look at my prompt I"
      },
      {
        "start": 2953.799,
        "duration": 4.441,
        "text": "have all of this context that was pulled"
      },
      {
        "start": 2955.68,
        "duration": 4.36,
        "text": "in from my rag store and then the prompt"
      },
      {
        "start": 2958.24,
        "duration": 3.28,
        "text": "actually says give the contact above"
      },
      {
        "start": 2960.04,
        "duration": 4.039,
        "text": "answer this question and you can see it"
      },
      {
        "start": 2961.52,
        "duration": 5.44,
        "text": "piped in my my question I'm actually"
      },
      {
        "start": 2964.079,
        "duration": 5.201,
        "text": "going to pull that out and again again"
      },
      {
        "start": 2966.96,
        "duration": 4.04,
        "text": "from the UI standpoint the context came"
      },
      {
        "start": 2969.28,
        "duration": 3.64,
        "text": "out of this parge data right there but"
      },
      {
        "start": 2971.0,
        "duration": 3.4,
        "text": "that question that was the same one we"
      },
      {
        "start": 2972.92,
        "duration": 4.28,
        "text": "started with right we started when did I"
      },
      {
        "start": 2974.4,
        "duration": 4.8,
        "text": "get married I just pop that over and run"
      },
      {
        "start": 2977.2,
        "duration": 4.119,
        "text": "it in now what's super cool is I just"
      },
      {
        "start": 2979.2,
        "duration": 5.2,
        "text": "pulled that prompt out if I were to"
      },
      {
        "start": 2981.319,
        "duration": 5.04,
        "text": "start a new chat gbt and I did this"
      },
      {
        "start": 2984.4,
        "duration": 4.84,
        "text": "actually I clearly did not copy that"
      },
      {
        "start": 2986.359,
        "duration": 2.881,
        "text": "properly let's try that"
      },
      {
        "start": 2991.16,
        "duration": 4.36,
        "text": "again or maybe I oh I did and I just"
      },
      {
        "start": 2993.48,
        "duration": 4.72,
        "text": "didn't realize it okay my bad I have it"
      },
      {
        "start": 2995.52,
        "duration": 4.88,
        "text": "so zoomed in I just didn't see it all"
      },
      {
        "start": 2998.2,
        "duration": 5.68,
        "text": "all right so notice how now that I've"
      },
      {
        "start": 3000.4,
        "duration": 5.76,
        "text": "given my llm this all this context and"
      },
      {
        "start": 3003.88,
        "duration": 3.479,
        "text": "asked the question now it actually can"
      },
      {
        "start": 3006.16,
        "duration": 3.28,
        "text": "some answer something that has some"
      },
      {
        "start": 3007.359,
        "duration": 4.121,
        "text": "relevance right and it's the same exact"
      },
      {
        "start": 3009.44,
        "duration": 5.32,
        "text": "thing we should see uh here in our"
      },
      {
        "start": 3011.48,
        "duration": 6.44,
        "text": "answer okay then finally that prompt is"
      },
      {
        "start": 3014.76,
        "duration": 6.559,
        "text": "wired up into our llm again in this"
      },
      {
        "start": 3017.92,
        "duration": 5.679,
        "text": "particular case um I want to be using a"
      },
      {
        "start": 3021.319,
        "duration": 4.52,
        "text": "a model that is in the same family is"
      },
      {
        "start": 3023.599,
        "duration": 5.121,
        "text": "you know how I how I vectorize my data"
      },
      {
        "start": 3025.839,
        "duration": 4.76,
        "text": "so again I'm using open AI um you know"
      },
      {
        "start": 3028.72,
        "duration": 3.48,
        "text": "but again we open AI is just part of the"
      },
      {
        "start": 3030.599,
        "duration": 3.041,
        "text": "example right there are all sorts of"
      },
      {
        "start": 3032.2,
        "duration": 3.8,
        "text": "different models you can choose from and"
      },
      {
        "start": 3033.64,
        "duration": 3.6,
        "text": "then finally there's this chat output so"
      },
      {
        "start": 3036.0,
        "duration": 2.72,
        "text": "when I come out here you can see this"
      },
      {
        "start": 3037.24,
        "duration": 3.879,
        "text": "was me as I was hitting those builds by"
      },
      {
        "start": 3038.72,
        "duration": 3.599,
        "text": "the way um but you could see at the end"
      },
      {
        "start": 3041.119,
        "duration": 2.401,
        "text": "you know it was able to pull some"
      },
      {
        "start": 3042.319,
        "duration": 3.52,
        "text": "information out that actually came from"
      },
      {
        "start": 3043.52,
        "duration": 4.0,
        "text": "the blog um that I got some idea now I"
      },
      {
        "start": 3045.839,
        "duration": 4.081,
        "text": "don't in my blog mention exactly when I"
      },
      {
        "start": 3047.52,
        "duration": 4.64,
        "text": "get married but it inferred that from"
      },
      {
        "start": 3049.92,
        "duration": 3.96,
        "text": "information it pull up right so again if"
      },
      {
        "start": 3052.16,
        "duration": 3.48,
        "text": "I asked that without that particular"
      },
      {
        "start": 3053.88,
        "duration": 5.239,
        "text": "context it wouldn't have known this at"
      },
      {
        "start": 3055.64,
        "duration": 5.36,
        "text": "all now finally I've got this flow"
      },
      {
        "start": 3059.119,
        "duration": 3.68,
        "text": "working I feel like yeah this is doing"
      },
      {
        "start": 3061.0,
        "duration": 4.28,
        "text": "what I want it to do how do I get this"
      },
      {
        "start": 3062.799,
        "duration": 5.121,
        "text": "into my app look down at the lower right"
      },
      {
        "start": 3065.28,
        "duration": 5.0,
        "text": "hand corner you see this API here right"
      },
      {
        "start": 3067.92,
        "duration": 4.36,
        "text": "so what what llow allows you to do you"
      },
      {
        "start": 3070.28,
        "duration": 4.0,
        "text": "have a bunch of options here honestly um"
      },
      {
        "start": 3072.28,
        "duration": 4.24,
        "text": "you could treat this as you know just"
      },
      {
        "start": 3074.28,
        "duration": 4.6,
        "text": "like a regular API endpoint and you know"
      },
      {
        "start": 3076.52,
        "duration": 4.559,
        "text": "curl you know hit this in your app um"
      },
      {
        "start": 3078.88,
        "duration": 4.84,
        "text": "you've got these tweaks here um these"
      },
      {
        "start": 3081.079,
        "duration": 4.881,
        "text": "are actually the references to the the"
      },
      {
        "start": 3083.72,
        "duration": 4.04,
        "text": "components themselves right so let's say"
      },
      {
        "start": 3085.96,
        "duration": 4.48,
        "text": "that I was curious and I'm like you know"
      },
      {
        "start": 3087.76,
        "duration": 5.4,
        "text": "how do I do this how do I change how do"
      },
      {
        "start": 3090.44,
        "duration": 5.96,
        "text": "I change"
      },
      {
        "start": 3093.16,
        "duration": 5.12,
        "text": "this if I edit the tweak then it will"
      },
      {
        "start": 3096.4,
        "duration": 4.24,
        "text": "automatically update the code right so"
      },
      {
        "start": 3098.28,
        "duration": 5.559,
        "text": "now I can see oh that's how I do it um"
      },
      {
        "start": 3100.64,
        "duration": 6.24,
        "text": "so whether this is in the python API now"
      },
      {
        "start": 3103.839,
        "duration": 5.681,
        "text": "I like using the python API or the um"
      },
      {
        "start": 3106.88,
        "duration": 4.919,
        "text": "the the JavaScript API if I'm in active"
      },
      {
        "start": 3109.52,
        "duration": 4.039,
        "text": "development and what I mean by that is"
      },
      {
        "start": 3111.799,
        "duration": 3.881,
        "text": "in some of the apps I've been building I"
      },
      {
        "start": 3113.559,
        "duration": 3.401,
        "text": "am actively experimenting with different"
      },
      {
        "start": 3115.68,
        "duration": 4.439,
        "text": "models and different flows and"
      },
      {
        "start": 3116.96,
        "duration": 6.879,
        "text": "everything I may not be ready to say um"
      },
      {
        "start": 3120.119,
        "duration": 6.44,
        "text": "version my flow so I use this as an API"
      },
      {
        "start": 3123.839,
        "duration": 4.201,
        "text": "server and that way I can come in here"
      },
      {
        "start": 3126.559,
        "duration": 3.161,
        "text": "and I can just be making all sorts of"
      },
      {
        "start": 3128.04,
        "duration": 3.079,
        "text": "changes I can just change I can say you"
      },
      {
        "start": 3129.72,
        "duration": 3.76,
        "text": "know something I actually don't want to"
      },
      {
        "start": 3131.119,
        "duration": 4.641,
        "text": "use gp24 I want to use for Turbo preview"
      },
      {
        "start": 3133.48,
        "duration": 4.119,
        "text": "and I can just change it and anytime I"
      },
      {
        "start": 3135.76,
        "duration": 4.52,
        "text": "make subsequent requests from my app it"
      },
      {
        "start": 3137.599,
        "duration": 4.561,
        "text": "will reflect that right so that's one"
      },
      {
        "start": 3140.28,
        "duration": 4.12,
        "text": "way that you could approach it another"
      },
      {
        "start": 3142.16,
        "duration": 4.88,
        "text": "here you know if if you are in Python"
      },
      {
        "start": 3144.4,
        "duration": 4.08,
        "text": "you can actually export this whole flow"
      },
      {
        "start": 3147.04,
        "duration": 3.84,
        "text": "as a Jason"
      },
      {
        "start": 3148.48,
        "duration": 4.879,
        "text": "blob and then you can import that into"
      },
      {
        "start": 3150.88,
        "duration": 5.0,
        "text": "your app right and then that essentially"
      },
      {
        "start": 3153.359,
        "duration": 4.121,
        "text": "brings all of the Gen workflow logic"
      },
      {
        "start": 3155.88,
        "duration": 4.0,
        "text": "right into your app right so you're not"
      },
      {
        "start": 3157.48,
        "duration": 4.28,
        "text": "you don't have to use Lang flows an API"
      },
      {
        "start": 3159.88,
        "duration": 3.36,
        "text": "server if you don't want to um so"
      },
      {
        "start": 3161.76,
        "duration": 3.68,
        "text": "there's all sorts of different options"
      },
      {
        "start": 3163.24,
        "duration": 3.2,
        "text": "there it really depends on your your"
      },
      {
        "start": 3165.44,
        "duration": 3.84,
        "text": "particular"
      },
      {
        "start": 3166.44,
        "duration": 4.48,
        "text": "need all right so um that that's it from"
      },
      {
        "start": 3169.28,
        "duration": 3.4,
        "text": "that standpoint like how are we"
      },
      {
        "start": 3170.92,
        "duration": 3.12,
        "text": "questions how are we doing from that"
      },
      {
        "start": 3172.68,
        "duration": 4.8,
        "text": "standpoint I think I saw some stuff come"
      },
      {
        "start": 3174.04,
        "duration": 4.88,
        "text": "in yeah me some of these up here so have"
      },
      {
        "start": 3177.48,
        "duration": 3.04,
        "text": "a little time left to hit some of these"
      },
      {
        "start": 3178.92,
        "duration": 3.84,
        "text": "yeah what do we have yeah so if I want"
      },
      {
        "start": 3180.52,
        "duration": 4.279,
        "text": "to run llama 3 on my PC can I include"
      },
      {
        "start": 3182.76,
        "duration": 4.48,
        "text": "inference of this model and L flow um"
      },
      {
        "start": 3184.799,
        "duration": 4.56,
        "text": "absolutely I put in the comments a"
      },
      {
        "start": 3187.24,
        "duration": 5.68,
        "text": "section in the llow docs that shows how"
      },
      {
        "start": 3189.359,
        "duration": 5.521,
        "text": "to do the ol Lama local LM llms so local"
      },
      {
        "start": 3192.92,
        "duration": 3.8,
        "text": "support for llms is supported if you"
      },
      {
        "start": 3194.88,
        "duration": 5.239,
        "text": "don't see the component there for your"
      },
      {
        "start": 3196.72,
        "duration": 5.119,
        "text": "specific model uh it as David had showed"
      },
      {
        "start": 3200.119,
        "duration": 3.921,
        "text": "like it's pretty easy to just make a"
      },
      {
        "start": 3201.839,
        "duration": 4.201,
        "text": "custom component and you set the"
      },
      {
        "start": 3204.04,
        "duration": 4.039,
        "text": "basically the API endpoint to Local Host"
      },
      {
        "start": 3206.04,
        "duration": 4.2,
        "text": "right and then it'll it'll call into"
      },
      {
        "start": 3208.079,
        "duration": 6.28,
        "text": "where it needs to"
      },
      {
        "start": 3210.24,
        "duration": 7.879,
        "text": "go awesome what else do we have in"
      },
      {
        "start": 3214.359,
        "duration": 5.561,
        "text": "there this one for rag Lan flow"
      },
      {
        "start": 3218.119,
        "duration": 3.96,
        "text": "currently it seems uh there's only"
      },
      {
        "start": 3219.92,
        "duration": 4.439,
        "text": "support for astb with support being"
      },
      {
        "start": 3222.079,
        "duration": 5.28,
        "text": "planned for DC and Cassandra any"
      },
      {
        "start": 3224.359,
        "duration": 6.121,
        "text": "timelines actually yes uh we"
      },
      {
        "start": 3227.359,
        "duration": 6.881,
        "text": "have I believe it was maybe 18 days ago"
      },
      {
        "start": 3230.48,
        "duration": 6.52,
        "text": "that released the Cassandra component um"
      },
      {
        "start": 3234.24,
        "duration": 5.28,
        "text": "so this one here yeah yeah yeah and so"
      },
      {
        "start": 3237.0,
        "duration": 5.0,
        "text": "this is in the latest version um and in"
      },
      {
        "start": 3239.52,
        "duration": 4.36,
        "text": "the latest version of langlow and rag"
      },
      {
        "start": 3242.0,
        "duration": 3.96,
        "text": "stack L flow again one is always"
      },
      {
        "start": 3243.88,
        "duration": 4.239,
        "text": "slightly behind the other um and so you"
      },
      {
        "start": 3245.96,
        "duration": 2.92,
        "text": "can use this to connect to Cassandra or"
      },
      {
        "start": 3248.119,
        "duration": 3.121,
        "text": "to"
      },
      {
        "start": 3248.88,
        "duration": 5.32,
        "text": "dsse yeah and also to point out like"
      },
      {
        "start": 3251.24,
        "duration": 5.68,
        "text": "again you know L flow is store agnostic"
      },
      {
        "start": 3254.2,
        "duration": 4.32,
        "text": "of course we're biased you know we're"
      },
      {
        "start": 3256.92,
        "duration": 3.24,
        "text": "data Stacks we love Cassandra and Astro"
      },
      {
        "start": 3258.52,
        "duration": 3.319,
        "text": "DB and and all that kind of deal but"
      },
      {
        "start": 3260.16,
        "duration": 3.28,
        "text": "you're not limited to just those right"
      },
      {
        "start": 3261.839,
        "duration": 4.841,
        "text": "you can use other Vector stores and such"
      },
      {
        "start": 3263.44,
        "duration": 4.84,
        "text": "like that um but you know you lot of our"
      },
      {
        "start": 3266.68,
        "duration": 3.56,
        "text": "templates I think the rag one for sure"
      },
      {
        "start": 3268.28,
        "duration": 3.2,
        "text": "will start you with Astro DB uh and"
      },
      {
        "start": 3270.24,
        "duration": 2.72,
        "text": "luckily it's actually really easy to get"
      },
      {
        "start": 3271.48,
        "duration": 4.359,
        "text": "going from that standpoint but yes the"
      },
      {
        "start": 3272.96,
        "duration": 5.32,
        "text": "answer is yeah you have support"
      },
      {
        "start": 3275.839,
        "duration": 4.52,
        "text": "there okay what about this last one"
      },
      {
        "start": 3278.28,
        "duration": 3.64,
        "text": "Chara maybe a little off topic how good"
      },
      {
        "start": 3280.359,
        "duration": 5.321,
        "text": "is llama Index right now in creating"
      },
      {
        "start": 3281.92,
        "duration": 6.36,
        "text": "agents for processing done uh data in"
      },
      {
        "start": 3285.68,
        "duration": 6.0,
        "text": "rag for example in comparison to L graph"
      },
      {
        "start": 3288.28,
        "duration": 4.799,
        "text": "it's a great question um I have a couple"
      },
      {
        "start": 3291.68,
        "duration": 4.24,
        "text": "of different answers to this I don't"
      },
      {
        "start": 3293.079,
        "duration": 4.121,
        "text": "know if if our opinions will differ here"
      },
      {
        "start": 3295.92,
        "duration": 2.84,
        "text": "um"
      },
      {
        "start": 3297.2,
        "duration": 3.08,
        "text": "and and I don't know how many folks know"
      },
      {
        "start": 3298.76,
        "duration": 5.359,
        "text": "that you can actually use llama index"
      },
      {
        "start": 3300.28,
        "duration": 7.079,
        "text": "and Lang chain together so um if you've"
      },
      {
        "start": 3304.119,
        "duration": 7.24,
        "text": "never used llama parse before um I've"
      },
      {
        "start": 3307.359,
        "duration": 6.921,
        "text": "actually really like how well it does on"
      },
      {
        "start": 3311.359,
        "duration": 4.44,
        "text": "getting data ready for Gen we also you"
      },
      {
        "start": 3314.28,
        "duration": 2.68,
        "text": "know we llama index is one of our"
      },
      {
        "start": 3315.799,
        "duration": 2.8,
        "text": "partners Lang chain is one of our"
      },
      {
        "start": 3316.96,
        "duration": 3.76,
        "text": "partners unstructured is one of our"
      },
      {
        "start": 3318.599,
        "duration": 4.401,
        "text": "partners um and all of these different"
      },
      {
        "start": 3320.72,
        "duration": 4.079,
        "text": "tools have different sort of purposes"
      },
      {
        "start": 3323.0,
        "duration": 3.72,
        "text": "for the kind of data and the kind of"
      },
      {
        "start": 3324.799,
        "duration": 4.0,
        "text": "agent that you're trying to build now"
      },
      {
        "start": 3326.72,
        "duration": 4.319,
        "text": "that said when I'm thinking about the"
      },
      {
        "start": 3328.799,
        "duration": 4.28,
        "text": "customer base that I work with the users"
      },
      {
        "start": 3331.039,
        "duration": 4.56,
        "text": "of rag stack the majority of our users"
      },
      {
        "start": 3333.079,
        "duration": 6.561,
        "text": "are using Lang chain or langlow um I"
      },
      {
        "start": 3335.599,
        "duration": 5.281,
        "text": "don't know exactly why that is um but"
      },
      {
        "start": 3339.64,
        "duration": 3.88,
        "text": "but that's just the choice that they"
      },
      {
        "start": 3340.88,
        "duration": 5.239,
        "text": "tend to make um and then we have some"
      },
      {
        "start": 3343.52,
        "duration": 4.88,
        "text": "folks who are using llama index it's not"
      },
      {
        "start": 3346.119,
        "duration": 4.521,
        "text": "zero but it is a you know a fraction of"
      },
      {
        "start": 3348.4,
        "duration": 3.959,
        "text": "those I tend to work with Enterprises"
      },
      {
        "start": 3350.64,
        "duration": 3.64,
        "text": "though so I can't speak for the"
      },
      {
        "start": 3352.359,
        "duration": 4.281,
        "text": "developer community so maybe David you"
      },
      {
        "start": 3354.28,
        "duration": 5.079,
        "text": "have a better answer of like you know"
      },
      {
        "start": 3356.64,
        "duration": 5.36,
        "text": "does it matter if you're Enterprise does"
      },
      {
        "start": 3359.359,
        "duration": 6.121,
        "text": "it matter if you're startup or an"
      },
      {
        "start": 3362.0,
        "duration": 6.119,
        "text": "academic oh as far as um llama index"
      },
      {
        "start": 3365.48,
        "duration": 6.119,
        "text": "compared to Lang chain yeah you know I"
      },
      {
        "start": 3368.119,
        "duration": 6.041,
        "text": "so far you know I have I I actually just"
      },
      {
        "start": 3371.599,
        "duration": 4.281,
        "text": "did a tour of a bunch of conferences"
      },
      {
        "start": 3374.16,
        "duration": 2.84,
        "text": "there's been a lot you know and you know"
      },
      {
        "start": 3375.88,
        "duration": 3.4,
        "text": "what's interesting is depending on the"
      },
      {
        "start": 3377.0,
        "duration": 4.599,
        "text": "conference and the context there are"
      },
      {
        "start": 3379.28,
        "duration": 4.519,
        "text": "some different answers you know at Pon"
      },
      {
        "start": 3381.599,
        "duration": 3.72,
        "text": "um compared to the AI World's Fair I was"
      },
      {
        "start": 3383.799,
        "duration": 3.721,
        "text": "having some different conversations you"
      },
      {
        "start": 3385.319,
        "duration": 3.961,
        "text": "know I will say that a lot more I I"
      },
      {
        "start": 3387.52,
        "duration": 6.279,
        "text": "definitely run into a lot more folks who"
      },
      {
        "start": 3389.28,
        "duration": 5.839,
        "text": "have at least try Lan chain right um but"
      },
      {
        "start": 3393.799,
        "duration": 2.961,
        "text": "again you know these are folks that I'm"
      },
      {
        "start": 3395.119,
        "duration": 3.601,
        "text": "having conversations with you know kind"
      },
      {
        "start": 3396.76,
        "duration": 3.799,
        "text": "of on the front lines uh at these"
      },
      {
        "start": 3398.72,
        "duration": 3.0,
        "text": "developer conferences and such like that"
      },
      {
        "start": 3400.559,
        "duration": 2.56,
        "text": "I definitely there are definitely some"
      },
      {
        "start": 3401.72,
        "duration": 3.72,
        "text": "out there doing LL index but kind of to"
      },
      {
        "start": 3403.119,
        "duration": 4.321,
        "text": "your point earlier um it it seems like"
      },
      {
        "start": 3405.44,
        "duration": 3.639,
        "text": "it kind of depends on the use case um"
      },
      {
        "start": 3407.44,
        "duration": 4.72,
        "text": "there are some that are for compared to"
      },
      {
        "start": 3409.079,
        "duration": 5.76,
        "text": "others like you know an analogy I can"
      },
      {
        "start": 3412.16,
        "duration": 4.679,
        "text": "make is on actual models right um I know"
      },
      {
        "start": 3414.839,
        "duration": 3.76,
        "text": "in some of my own development I've been"
      },
      {
        "start": 3416.839,
        "duration": 3.841,
        "text": "experiencing I've I've been actually"
      },
      {
        "start": 3418.599,
        "duration": 3.76,
        "text": "experimenting with doing language"
      },
      {
        "start": 3420.68,
        "duration": 2.96,
        "text": "translation and all sorts of things and"
      },
      {
        "start": 3422.359,
        "duration": 4.041,
        "text": "and sentiment analysis and that kind of"
      },
      {
        "start": 3423.64,
        "duration": 4.959,
        "text": "deal and there are some models that are"
      },
      {
        "start": 3426.4,
        "duration": 5.24,
        "text": "really good at doing the translation"
      },
      {
        "start": 3428.599,
        "duration": 4.681,
        "text": "piece um but maybe they're not as good"
      },
      {
        "start": 3431.64,
        "duration": 3.24,
        "text": "at like sentiment analysis piece and"
      },
      {
        "start": 3433.28,
        "duration": 3.4,
        "text": "that kind of deal and so part of it is"
      },
      {
        "start": 3434.88,
        "duration": 2.84,
        "text": "just kind of experimenting right and"
      },
      {
        "start": 3436.68,
        "duration": 2.679,
        "text": "that's actually one of the neat things"
      },
      {
        "start": 3437.72,
        "duration": 4.48,
        "text": "about link flow it allowed me to go and"
      },
      {
        "start": 3439.359,
        "duration": 5.2,
        "text": "iterate through um so I I don't have a"
      },
      {
        "start": 3442.2,
        "duration": 4.28,
        "text": "direct answer from that standpoint um"
      },
      {
        "start": 3444.559,
        "duration": 3.881,
        "text": "other than I do know i' I've heard Lang"
      },
      {
        "start": 3446.48,
        "duration": 3.52,
        "text": "chain that I have her llama index but"
      },
      {
        "start": 3448.44,
        "duration": 5.8,
        "text": "again I think it's totally use case"
      },
      {
        "start": 3450.0,
        "duration": 5.92,
        "text": "driven yeah I I agree um well we don't"
      },
      {
        "start": 3454.24,
        "duration": 3.48,
        "text": "have more questions but I wanted to"
      },
      {
        "start": 3455.92,
        "duration": 4.199,
        "text": "bring us back to the poll that we had"
      },
      {
        "start": 3457.72,
        "duration": 4.839,
        "text": "put up earlier on what kind of data are"
      },
      {
        "start": 3460.119,
        "duration": 4.641,
        "text": "people injecting into the prompt so I'm"
      },
      {
        "start": 3462.559,
        "duration": 3.961,
        "text": "looking at this and it looks like most"
      },
      {
        "start": 3464.76,
        "duration": 3.839,
        "text": "people aren't doing general knowledge"
      },
      {
        "start": 3466.52,
        "duration": 3.599,
        "text": "great you probably don't need to be"
      },
      {
        "start": 3468.599,
        "duration": 4.2,
        "text": "injecting general knowledge into the"
      },
      {
        "start": 3470.119,
        "duration": 4.601,
        "text": "prop so glad we kind of glossed right"
      },
      {
        "start": 3472.799,
        "duration": 4.52,
        "text": "over"
      },
      {
        "start": 3474.72,
        "duration": 5.72,
        "text": "that detail product information things"
      },
      {
        "start": 3477.319,
        "duration": 5.8,
        "text": "like Medical in these cases you should"
      },
      {
        "start": 3480.44,
        "duration": 4.72,
        "text": "be taking a look at rag techniques using"
      },
      {
        "start": 3483.119,
        "duration": 4.96,
        "text": "something like Colbert or something like"
      },
      {
        "start": 3485.16,
        "duration": 6.159,
        "text": "hybrid search so asra for example"
      },
      {
        "start": 3488.079,
        "duration": 6.441,
        "text": "supports metadata filters as well as"
      },
      {
        "start": 3491.319,
        "duration": 5.361,
        "text": "Vector so if there is a specific product"
      },
      {
        "start": 3494.52,
        "duration": 4.599,
        "text": "that you need to return exact where it"
      },
      {
        "start": 3496.68,
        "duration": 3.679,
        "text": "match add that as metadata you don't"
      },
      {
        "start": 3499.119,
        "duration": 2.881,
        "text": "just store the vector you store the"
      },
      {
        "start": 3500.359,
        "duration": 3.321,
        "text": "metadata as well you can filter on the"
      },
      {
        "start": 3502.0,
        "duration": 3.119,
        "text": "metadata and do the vector search so"
      },
      {
        "start": 3503.68,
        "duration": 5.0,
        "text": "when you're doing something very"
      },
      {
        "start": 3505.119,
        "duration": 5.601,
        "text": "detailed very entity specific know that"
      },
      {
        "start": 3508.68,
        "duration": 4.0,
        "text": "basic rag sometimes is not going to work"
      },
      {
        "start": 3510.72,
        "duration": 3.879,
        "text": "especially as your data set gets bigger"
      },
      {
        "start": 3512.68,
        "duration": 4.359,
        "text": "and bigger and bigger and you have more"
      },
      {
        "start": 3514.599,
        "duration": 4.96,
        "text": "and more products to"
      },
      {
        "start": 3517.039,
        "duration": 4.721,
        "text": "recognize um hyperlink support articles"
      },
      {
        "start": 3519.559,
        "duration": 4.28,
        "text": "if you haven't tried the course range"
      },
      {
        "start": 3521.76,
        "duration": 4.279,
        "text": "knowled course grained Knowledge Graph"
      },
      {
        "start": 3523.839,
        "duration": 4.401,
        "text": "or graph rag approach take a look at it"
      },
      {
        "start": 3526.039,
        "duration": 5.401,
        "text": "we just contribut it to Lang chain it's"
      },
      {
        "start": 3528.24,
        "duration": 6.64,
        "text": "open source with two code changes of Co"
      },
      {
        "start": 3531.44,
        "duration": 7.08,
        "text": "lines of code changed you can stand up a"
      },
      {
        "start": 3534.88,
        "duration": 5.8,
        "text": "a graph with your vector data linked to"
      },
      {
        "start": 3538.52,
        "duration": 3.68,
        "text": "your other data um and there's some"
      },
      {
        "start": 3540.68,
        "duration": 4.08,
        "text": "helper functions in there to generate"
      },
      {
        "start": 3542.2,
        "duration": 6.32,
        "text": "those edges for you uh same with chunks"
      },
      {
        "start": 3544.76,
        "duration": 5.76,
        "text": "from very long PDFs like if the problem"
      },
      {
        "start": 3548.52,
        "duration": 3.88,
        "text": "you're running into is that you're"
      },
      {
        "start": 3550.52,
        "duration": 4.36,
        "text": "getting the right chunk back but you're"
      },
      {
        "start": 3552.4,
        "duration": 5.919,
        "text": "missing the surrounding context graph"
      },
      {
        "start": 3554.88,
        "duration": 6.32,
        "text": "can be very useful you could say oh this"
      },
      {
        "start": 3558.319,
        "duration": 4.8,
        "text": "chunk was on the same page and it was in"
      },
      {
        "start": 3561.2,
        "duration": 3.639,
        "text": "this document right so sometimes"
      },
      {
        "start": 3563.119,
        "duration": 3.48,
        "text": "depending on the complexity of the"
      },
      {
        "start": 3564.839,
        "duration": 4.161,
        "text": "question you're answering you may need"
      },
      {
        "start": 3566.599,
        "duration": 3.96,
        "text": "to pull back the entire document or you"
      },
      {
        "start": 3569.0,
        "duration": 3.52,
        "text": "may just need to bring back that page"
      },
      {
        "start": 3570.559,
        "duration": 3.321,
        "text": "right think about the different kinds of"
      },
      {
        "start": 3572.52,
        "duration": 3.36,
        "text": "ways that you can relate this"
      },
      {
        "start": 3573.88,
        "duration": 4.12,
        "text": "information together when you can get"
      },
      {
        "start": 3575.88,
        "duration": 4.04,
        "text": "semantic similarity back but you can't"
      },
      {
        "start": 3578.0,
        "duration": 2.799,
        "text": "get some of the related information that"
      },
      {
        "start": 3579.92,
        "duration": 4.32,
        "text": "you"
      },
      {
        "start": 3580.799,
        "duration": 4.641,
        "text": "need numerical data this is I am so"
      },
      {
        "start": 3584.24,
        "duration": 3.879,
        "text": "excited that you guys are doing"
      },
      {
        "start": 3585.44,
        "duration": 5.32,
        "text": "numerical data uh we were talking about"
      },
      {
        "start": 3588.119,
        "duration": 6.521,
        "text": "this the other day David where you know"
      },
      {
        "start": 3590.76,
        "duration": 6.599,
        "text": "uh language embedding model does not"
      },
      {
        "start": 3594.64,
        "duration": 5.84,
        "text": "embed numerical dat data the same way"
      },
      {
        "start": 3597.359,
        "duration": 5.601,
        "text": "thats language"
      },
      {
        "start": 3600.48,
        "duration": 3.359,
        "text": "data the way that you store and retrieve"
      },
      {
        "start": 3602.96,
        "duration": 3.879,
        "text": "this information is going to be"
      },
      {
        "start": 3603.839,
        "duration": 5.52,
        "text": "incredibly important in fact you may not"
      },
      {
        "start": 3606.839,
        "duration": 4.52,
        "text": "vectorize this data at all you may"
      },
      {
        "start": 3609.359,
        "duration": 4.881,
        "text": "instead want to do something like text"
      },
      {
        "start": 3611.359,
        "duration": 5.361,
        "text": "to cql or text to SQL in order to"
      },
      {
        "start": 3614.24,
        "duration": 5.559,
        "text": "actually get that language query to"
      },
      {
        "start": 3616.72,
        "duration": 6.76,
        "text": "generate a structured query to pull the"
      },
      {
        "start": 3619.799,
        "duration": 5.841,
        "text": "exact data back right this this type of"
      },
      {
        "start": 3623.48,
        "duration": 4.559,
        "text": "numerical data isn't always going to I"
      },
      {
        "start": 3625.64,
        "duration": 5.24,
        "text": "don't know have you ever now have you"
      },
      {
        "start": 3628.039,
        "duration": 4.601,
        "text": "ever tried to have chat GPT count the"
      },
      {
        "start": 3630.88,
        "duration": 5.12,
        "text": "words for a little"
      },
      {
        "start": 3632.64,
        "duration": 7.08,
        "text": "snippet oh my God it yeah it has"
      },
      {
        "start": 3636.0,
        "duration": 5.039,
        "text": "challenges it can't do it I I try to"
      },
      {
        "start": 3639.72,
        "duration": 3.52,
        "text": "like shorten the description of my"
      },
      {
        "start": 3641.039,
        "duration": 5.441,
        "text": "podcast all the time cannot be done do"
      },
      {
        "start": 3643.24,
        "duration": 5.599,
        "text": "it in less than 200 words it won't do it"
      },
      {
        "start": 3646.48,
        "duration": 4.92,
        "text": "right it's just not the same information"
      },
      {
        "start": 3648.839,
        "duration": 5.041,
        "text": "so consider things like text to cql text"
      },
      {
        "start": 3651.4,
        "duration": 4.919,
        "text": "to SQL uh in order to retrieve that"
      },
      {
        "start": 3653.88,
        "duration": 5.919,
        "text": "information in a way that's going to"
      },
      {
        "start": 3656.319,
        "duration": 5.48,
        "text": "reserve the actual data that you need to"
      },
      {
        "start": 3659.799,
        "duration": 3.601,
        "text": "inject into the prompt yeah and by the"
      },
      {
        "start": 3661.799,
        "duration": 3.721,
        "text": "way so I Do by the way I know everybody"
      },
      {
        "start": 3663.4,
        "duration": 3.6,
        "text": "we're we're essentially at time however"
      },
      {
        "start": 3665.52,
        "duration": 2.64,
        "text": "there is a question here in QA that I"
      },
      {
        "start": 3667.0,
        "duration": 2.88,
        "text": "can answer real fast there was another"
      },
      {
        "start": 3668.16,
        "duration": 3.399,
        "text": "one there in the in the comment so I'm"
      },
      {
        "start": 3669.88,
        "duration": 4.6,
        "text": "going to go ahead and just knock one of"
      },
      {
        "start": 3671.559,
        "duration": 4.56,
        "text": "these out real fast um so we tried using"
      },
      {
        "start": 3674.48,
        "duration": 4.4,
        "text": "one of the other Vector store blocks I'm"
      },
      {
        "start": 3676.119,
        "duration": 4.161,
        "text": "gonna start answering this um we8 on rag"
      },
      {
        "start": 3678.88,
        "duration": 3.4,
        "text": "stack Lane flow but it did not work it"
      },
      {
        "start": 3680.28,
        "duration": 3.64,
        "text": "did not it did work on open source L"
      },
      {
        "start": 3682.28,
        "duration": 3.24,
        "text": "flow the response received on stack"
      },
      {
        "start": 3683.92,
        "duration": 3.28,
        "text": "Overflow was that rag stack L flow will"
      },
      {
        "start": 3685.52,
        "duration": 3.319,
        "text": "only work for asro DSC Cassandra as"
      },
      {
        "start": 3687.2,
        "duration": 3.8,
        "text": "Vector store can you please confirm"
      },
      {
        "start": 3688.839,
        "duration": 5.52,
        "text": "whether there's this true no it it's not"
      },
      {
        "start": 3691.0,
        "duration": 5.4,
        "text": "limited to only data Stacks products now"
      },
      {
        "start": 3694.359,
        "duration": 4.2,
        "text": "one thing to point out is that as charna"
      },
      {
        "start": 3696.4,
        "duration": 4.679,
        "text": "mentioned earlier because of the nature"
      },
      {
        "start": 3698.559,
        "duration": 3.961,
        "text": "of rag stack and the fact that all the"
      },
      {
        "start": 3701.079,
        "duration": 2.841,
        "text": "libraries and things like that they go"
      },
      {
        "start": 3702.52,
        "duration": 3.24,
        "text": "through all sorts of extensive testing"
      },
      {
        "start": 3703.92,
        "duration": 4.04,
        "text": "and and they're curated and stuff it the"
      },
      {
        "start": 3705.76,
        "duration": 5.2,
        "text": "version May lag slightly behind open"
      },
      {
        "start": 3707.96,
        "duration": 5.599,
        "text": "source but here's a fun trick for you so"
      },
      {
        "start": 3710.96,
        "duration": 4.48,
        "text": "by the way it should work so I will make"
      },
      {
        "start": 3713.559,
        "duration": 3.321,
        "text": "sure to get that information back to the"
      },
      {
        "start": 3715.44,
        "duration": 3.879,
        "text": "team if they don't already if they're"
      },
      {
        "start": 3716.88,
        "duration": 5.159,
        "text": "not already aware um but here's a neat"
      },
      {
        "start": 3719.319,
        "duration": 5.441,
        "text": "trick for you remember all of this stuff"
      },
      {
        "start": 3722.039,
        "duration": 4.121,
        "text": "is just code right underneath the hood"
      },
      {
        "start": 3724.76,
        "duration": 3.519,
        "text": "so let's say you're in the open source"
      },
      {
        "start": 3726.16,
        "duration": 5.04,
        "text": "version and you're like hey the we v81"
      },
      {
        "start": 3728.279,
        "duration": 4.641,
        "text": "works just copy the code you can go then"
      },
      {
        "start": 3731.2,
        "duration": 3.32,
        "text": "into um you can do this a couple"
      },
      {
        "start": 3732.92,
        "duration": 4.199,
        "text": "different ways you can actually go for a"
      },
      {
        "start": 3734.52,
        "duration": 5.039,
        "text": "custom component if you"
      },
      {
        "start": 3737.119,
        "duration": 3.68,
        "text": "want pop the code in oh wait I didn't"
      },
      {
        "start": 3739.559,
        "duration": 4.24,
        "text": "actually copy it from here so let me"
      },
      {
        "start": 3740.799,
        "duration": 5.401,
        "text": "just do that to illustrate this copy it"
      },
      {
        "start": 3743.799,
        "duration": 6.121,
        "text": "and I can go into a custom component"
      },
      {
        "start": 3746.2,
        "duration": 7.399,
        "text": "paste it do this rename it and say we8"
      },
      {
        "start": 3749.92,
        "duration": 8.96,
        "text": "or whatever it is right um and now when"
      },
      {
        "start": 3753.599,
        "duration": 8.44,
        "text": "I here we go now when I save this I now"
      },
      {
        "start": 3758.88,
        "duration": 5.32,
        "text": "have a component right that is actually"
      },
      {
        "start": 3762.039,
        "duration": 4.721,
        "text": "I could one I can share this across any"
      },
      {
        "start": 3764.2,
        "duration": 3.839,
        "text": "of my flows I can also upload it one"
      },
      {
        "start": 3766.76,
        "duration": 3.76,
        "text": "thing I didn't mention is there is in"
      },
      {
        "start": 3768.039,
        "duration": 4.841,
        "text": "fact a LF flow store is community store"
      },
      {
        "start": 3770.52,
        "duration": 3.68,
        "text": "where folks all sorts of folks can"
      },
      {
        "start": 3772.88,
        "duration": 2.64,
        "text": "actually store their own flow you can"
      },
      {
        "start": 3774.2,
        "duration": 2.32,
        "text": "upload your flows and your components"
      },
      {
        "start": 3775.52,
        "duration": 3.64,
        "text": "and such"
      },
      {
        "start": 3776.52,
        "duration": 4.839,
        "text": "um so it could just be that you know"
      },
      {
        "start": 3779.16,
        "duration": 3.6,
        "text": "like this would be a way um that you can"
      },
      {
        "start": 3781.359,
        "duration": 3.361,
        "text": "get that but it is if you have a if you"
      },
      {
        "start": 3782.76,
        "duration": 3.559,
        "text": "see something like that again you know"
      },
      {
        "start": 3784.72,
        "duration": 3.879,
        "text": "if it's in if it's an open source and"
      },
      {
        "start": 3786.319,
        "duration": 4.841,
        "text": "it's working the way that the process"
      },
      {
        "start": 3788.599,
        "duration": 4.601,
        "text": "goes rag stack isn't very far behind um"
      },
      {
        "start": 3791.16,
        "duration": 3.36,
        "text": "so it will probably resolve itself but"
      },
      {
        "start": 3793.2,
        "duration": 2.399,
        "text": "yes if you do see something like that"
      },
      {
        "start": 3794.52,
        "duration": 3.039,
        "text": "you should be able to pull it from one"
      },
      {
        "start": 3795.599,
        "duration": 3.52,
        "text": "and go right to the other um one last"
      },
      {
        "start": 3797.559,
        "duration": 3.841,
        "text": "thing I'm going to say about this here"
      },
      {
        "start": 3799.119,
        "duration": 4.521,
        "text": "um the store is a really neat way like"
      },
      {
        "start": 3801.4,
        "duration": 3.679,
        "text": "when I have a flow that is in a good"
      },
      {
        "start": 3803.64,
        "duration": 2.919,
        "text": "place and I'm like you know something I"
      },
      {
        "start": 3805.079,
        "duration": 3.121,
        "text": "want to like hold on to this for"
      },
      {
        "start": 3806.559,
        "duration": 4.641,
        "text": "posterity posterity or maybe I want to"
      },
      {
        "start": 3808.2,
        "duration": 5.24,
        "text": "version it if I go to share I can say"
      },
      {
        "start": 3811.2,
        "duration": 4.52,
        "text": "share flow once you know once I've"
      },
      {
        "start": 3813.44,
        "duration": 4.2,
        "text": "hooked up my store with my API key um it"
      },
      {
        "start": 3815.72,
        "duration": 3.079,
        "text": "will automatically upload that and so if"
      },
      {
        "start": 3817.64,
        "duration": 3.12,
        "text": "I come over here to those that are"
      },
      {
        "start": 3818.799,
        "duration": 5.121,
        "text": "created by me I need to log in"
      },
      {
        "start": 3820.76,
        "duration": 6.76,
        "text": "apparently been a minute probably uh"
      },
      {
        "start": 3823.92,
        "duration": 8.119,
        "text": "timed out there we go do this"
      },
      {
        "start": 3827.52,
        "duration": 7.039,
        "text": "again okay if I go back to"
      },
      {
        "start": 3832.039,
        "duration": 4.401,
        "text": "um yeah I want the store I want the"
      },
      {
        "start": 3834.559,
        "duration": 3.201,
        "text": "store there we go if I those they're"
      },
      {
        "start": 3836.44,
        "duration": 2.96,
        "text": "created by me you can see these are"
      },
      {
        "start": 3837.76,
        "duration": 4.2,
        "text": "flows that I've been uploading so this"
      },
      {
        "start": 3839.4,
        "duration": 4.28,
        "text": "is a really neat way um I can kind of"
      },
      {
        "start": 3841.96,
        "duration": 3.879,
        "text": "again I can like version things put them"
      },
      {
        "start": 3843.68,
        "duration": 4.639,
        "text": "up there these are not public right when"
      },
      {
        "start": 3845.839,
        "duration": 3.96,
        "text": "you when you say share you can it's step"
      },
      {
        "start": 3848.319,
        "duration": 3.161,
        "text": "by default you can uncheck that and keep"
      },
      {
        "start": 3849.799,
        "duration": 3.641,
        "text": "them for yourself um so that's a nice"
      },
      {
        "start": 3851.48,
        "duration": 4.24,
        "text": "way you can kind of share components so"
      },
      {
        "start": 3853.44,
        "duration": 3.639,
        "text": "if you did have a case where you did see"
      },
      {
        "start": 3855.72,
        "duration": 3.24,
        "text": "something like that or maybe you made a"
      },
      {
        "start": 3857.079,
        "duration": 3.681,
        "text": "custom component in one but you wanted"
      },
      {
        "start": 3858.96,
        "duration": 3.44,
        "text": "it in the other one share it up to the"
      },
      {
        "start": 3860.76,
        "duration": 3.599,
        "text": "store just use the store and pull it"
      },
      {
        "start": 3862.4,
        "duration": 3.879,
        "text": "down right it it's super easy from that"
      },
      {
        "start": 3864.359,
        "duration": 3.76,
        "text": "standpoint um"
      },
      {
        "start": 3866.279,
        "duration": 5.04,
        "text": "let's see and then the last one I think"
      },
      {
        "start": 3868.119,
        "duration": 4.92,
        "text": "I saw gmax C actually put it into the QA"
      },
      {
        "start": 3871.319,
        "duration": 3.561,
        "text": "which was you used a web scraper in your"
      },
      {
        "start": 3873.039,
        "duration": 3.481,
        "text": "demo for ingestion recently I saw you"
      },
      {
        "start": 3874.88,
        "duration": 3.12,
        "text": "all have integrated with fir crawl can"
      },
      {
        "start": 3876.52,
        "duration": 4.279,
        "text": "fir crawl be used in place of web"
      },
      {
        "start": 3878.0,
        "duration": 5.16,
        "text": "scraper um I don't see why not again"
      },
      {
        "start": 3880.799,
        "duration": 4.161,
        "text": "these components are super flexible um I"
      },
      {
        "start": 3883.16,
        "duration": 4.28,
        "text": "don't know I don't know turn if you know"
      },
      {
        "start": 3884.96,
        "duration": 5.119,
        "text": "I don't personally know about the fir"
      },
      {
        "start": 3887.44,
        "duration": 5.28,
        "text": "crawl component or an integration there"
      },
      {
        "start": 3890.079,
        "duration": 5.321,
        "text": "but again this is one of those cases"
      },
      {
        "start": 3892.72,
        "duration": 5.28,
        "text": "where um as that kind of capability is"
      },
      {
        "start": 3895.4,
        "duration": 4.719,
        "text": "ADD into L flow if it is a component may"
      },
      {
        "start": 3898.0,
        "duration": 5.559,
        "text": "show up it may be in the store but you"
      },
      {
        "start": 3900.119,
        "duration": 5.401,
        "text": "can modify these to do whatever and then"
      },
      {
        "start": 3903.559,
        "duration": 3.72,
        "text": "I actually did one with with 11 Labs I"
      },
      {
        "start": 3905.52,
        "duration": 3.44,
        "text": "was doing voice transcription and so I"
      },
      {
        "start": 3907.279,
        "duration": 3.601,
        "text": "just made a very quick component with 11"
      },
      {
        "start": 3908.96,
        "duration": 4.24,
        "text": "Labs right and and so you can just"
      },
      {
        "start": 3910.88,
        "duration": 4.88,
        "text": "upload that stuff up um so anyway yeah"
      },
      {
        "start": 3913.2,
        "duration": 5.8,
        "text": "it's flexible um I I think there are"
      },
      {
        "start": 3915.76,
        "duration": 5.4,
        "text": "definitely some some options for you"
      },
      {
        "start": 3919.0,
        "duration": 3.64,
        "text": "there all right and I do know that we"
      },
      {
        "start": 3921.16,
        "duration": 4.56,
        "text": "are at time I know we've gone a little"
      },
      {
        "start": 3922.64,
        "duration": 4.28,
        "text": "over I want to be respectful of that um"
      },
      {
        "start": 3925.72,
        "duration": 3.52,
        "text": "I think at this point Char unless you"
      },
      {
        "start": 3926.92,
        "duration": 3.6,
        "text": "want to follow up with anything um if if"
      },
      {
        "start": 3929.24,
        "duration": 3.599,
        "text": "there are any questions or whatever we"
      },
      {
        "start": 3930.52,
        "duration": 4.279,
        "text": "didn't get to um we can just follow up"
      },
      {
        "start": 3932.839,
        "duration": 5.841,
        "text": "an email and get those"
      },
      {
        "start": 3934.799,
        "duration": 5.201,
        "text": "anbody yeah all righty well with that"
      },
      {
        "start": 3938.68,
        "duration": 3.48,
        "text": "thank you everybody for your attention"
      },
      {
        "start": 3940.0,
        "duration": 3.88,
        "text": "today we appreciate you coming on thank"
      },
      {
        "start": 3942.16,
        "duration": 3.84,
        "text": "you Chara for bringing the information I"
      },
      {
        "start": 3943.88,
        "duration": 3.679,
        "text": "think uh given some conversations we we"
      },
      {
        "start": 3946.0,
        "duration": 4.64,
        "text": "may we may do some live streams on some"
      },
      {
        "start": 3947.559,
        "duration": 5.601,
        "text": "graph rag right and yeah I'd love to"
      },
      {
        "start": 3950.64,
        "duration": 4.6,
        "text": "show some examples like it's sort of"
      },
      {
        "start": 3953.16,
        "duration": 3.199,
        "text": "once you see it for the first time"
      },
      {
        "start": 3955.24,
        "duration": 2.359,
        "text": "you're like"
      },
      {
        "start": 3956.359,
        "duration": 4.121,
        "text": "hold"
      },
      {
        "start": 3957.599,
        "duration": 4.561,
        "text": "on I I can do so much with this it's"
      },
      {
        "start": 3960.48,
        "duration": 4.24,
        "text": "it's a pretty great uh"
      },
      {
        "start": 3962.16,
        "duration": 4.52,
        "text": "method awesome well thank you everybody"
      },
      {
        "start": 3964.72,
        "duration": 4.24,
        "text": "that's it for the live stream today and"
      },
      {
        "start": 3966.68,
        "duration": 5.28,
        "text": "we'll see you in the future take care"
      },
      {
        "start": 3968.96,
        "duration": 3.0,
        "text": "bye"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-12T17:38:50.627803+00:00"
}