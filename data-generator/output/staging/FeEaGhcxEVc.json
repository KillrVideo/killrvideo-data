{
  "video_id": "FeEaGhcxEVc",
  "title": "Classification and Clustering Algorithms with Wine and Chocolate by Amanda Moran | DataStax Presents",
  "description": "Hearts, wine, and chocolate .. it must be February! Can machine learning determine a wineâ€™s region and quality? Can machine learning determine what makes chocolate delicious? Short answer, yes it can! This talk will focus on using classification and clustering algorithms to do analytics at scale using Apache Cassandra and Apache Spark. Publicly available datasets, Jupyter notebooks, Pyspark, and DataStax Analytics will power this talk and live demo.\n\nAmanda Moran AKA @AmandaDataStax\nTwitter: https://twitter.com/AmandaDataStax\nLinkedIn: https://www.linkedin.com/in/amanda-kay-moran/",
  "published_at": "2019-04-17T13:00:07Z",
  "thumbnail": "https://i.ytimg.com/vi/FeEaGhcxEVc/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "apache_cassandra",
    "talk",
    "demo",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=FeEaGhcxEVc",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "so this is my talk classification and clustering with wine and chocolate and sorry there is no wine or chocolate but nevertheless you're gonna learn something about them today so but first let's talk a little bit about me so I actually I lived in Seattle for ten years I did my undergrad here at the University of Washington you can see this here why you Doug there I actually have a BS in biology from u-dub we don't need to know the year that I graduated um but after that I wouldn't I got a actually I was gonna be a middle school teacher and I decided that that was kind of terrible and so then I went and I got a I said what am I gonna do with my life I guess I'll go be a computer scientist so I wouldn't got my masters in computer science from Santa Clara and the bay area and that's where I live now I got that in 2012 so I've worked as a software engineer for about the last six years since I graduated and I've always worked on distributed databases somehow that became my passion I think I've worked on four different distributed databases I worked at Lockheed Martin HP Teradata a startup called s Jin I'm also and now I'm a developer advocate for data stacks so I'm gonna patch a committer and pmc member on a project called Truffaut daeun it is a sequel on HBase solution and I did all the initial installation and deployment work for that project and then just to my fun things about me is that I love Disney I love the cloud I love dogs Linux and obviously distributed databases so what are we gonna talk about today so first what problem are we really trying to solve here right these are the important questions can I use a clustering machine learning algorithm to find which wine came from which vineyard okay and then can I use a classification machine learning algorithm to find which country a candy bar comes from right think about like what is it a Mars bar those come from Europe and Eminem's come from America like that so but first before we get into that we're gonna talk we're gonna do an introduction to Apache Cassandra I know this is Seattle scalability this is probably something most of you are very familiar with so we're just gonna do a brief introduction to these things a very brief introduction to Apache spark again probably this crowd this like the back of their hand but we're gonna do it very quickly just to make sure we're all on the same page then we're going to talk about why are we talking about Cassandra and SPARC together with these machine learning algorithms and we're gonna bring that all together then we're gonna do what is k-means because that's going to be the machine learning algorithm we're gonna use for our clustering and then we're gonna talk about naive Bayes which is what we're gonna use for classification we're gonna use that on our chocolate we're gonna have two demos with that as well so again the problem we're really trying to solve here can I use machine learning with Apache spark with wine and chocolate and yes you can but the main goal around this for me is what I'm trying to convey to all of you is that data analytics doesn't have to be complicated a lot of times you're seeing these types of things and it's always on very complicated use cases but if you just scale it down to something simple you can learn how to use it and then go off and do your complicated use cases from there so we're gonna use it'll eyes the power of big data using Apache Cassandra Apache spark spark machine learning libraries Jupiter notebooks and Python that's what we're I'm going to show you in the demo so let's talk a little bit about Apache Cassandra so it was developed by Facebook and and donated into the open-source community community around 2008 and then he graduated to a top level project in 2010 now I was actually with like I said an Apache project that went from incubation all the way to graduation and I can tell you what a strenuous process this actually is to actually get your project committed and graduated to top level so because of that you know that this is a product that the community has really gathered around and are supporting very strongly the Apache foundation is very strong on that um so what it is is a distributed decentralized database it's elastically scalable you can add and remove nodes with no downtime and that's high performance it's very fast has high availability and low fault tolerance there's no single point of failure I'm just gonna let that sink in that's one of my favorite things uh and one of the reasons I actually came to data stacks because once I learned a little bit more about it and why it truly does have no single point of failure I was on board because all the other products I'd ever work done had the opposite of that like a million points of failure so this is very exciting for me and it solves many of the problems faced with traditional databases for certain workloads right when we're talking about no sequel it has its place relational database it has its path the police right so what does this all mean so let's talk about four big topics in the no sequel or Apache Cassandra space distributed replication elastically scalable and high availability so distributed every node in the cluster has the exact same role so I put here really that's actually true cassandra does not have a master worker architecture so any client can connect to any node and all nodes are read and write ready but this is not to say that all nodes contain all the data right that just doesn't make sense so that's we're talking about replication here so to be able to survive a node going down the date the data obviously needs to be copied because we're running a distributed system right needs to be copied to the other nodes so the replication factor how many times your data is going to be copied it's actually set by the user right so maybe if you're working with dated that you don't really mind if it were to be lost right maybe some IOT sensor data something like that that if I lose one node of you know a couple hours worth of data I'm okay with just losing that that's fine I can just have my replication factor set to one if I have incredibly important data that I can't I can't stand to lose then maybe I would set it all the way to the number of nodes that I actually have in my cluster we don't really recommend that it's kind of overkill probably just three copies of the data is probably plenty so the data is async asynchronously replicated across the nodes so that's automatic and it's peer-to-peer and it's really within milliseconds so elastically scalable so as you add more nodes the performance actually will increase linearly you can scale up and scale down with no downtime you don't even need a restart which is actually really surprising to me and pretty cool reads and write both scale linearly so this is just actually just a little graphic from Netflix showing that as they added more nodes they will have more rights so let's talk about high availability so again this lack of single point of failure this lack of a master node allows for high availability because there is and that means there's no single point of failure so replication Aloud's notes to fail and data to still be available right because now my data is distributed across notes if I lose one I'm still good to go so cassandra expects notes to fail and it doesn't panic so multiple data center support is also right out of the box even multi cloud support so I just I would like to bring this up because I'm an engineer and talking to other engineers so we've talked about a lot of magic right Cassandra Apache Cassandra seems like it's kind of magical but there's got to be some kind of trade-off right and so that's what I like to bring up because I don't want to sound like I'm trying to sell something that maybe isn't true so you should definitely check out this a cap theorem it basically says that at any one point in time if you have a network failure you can only have so many of these things at one particular time you can either have you can have availability consistency and partitioning those are the three things but you can only have two when you have a network failure so because of this it's impossible to have all three during a network failure Cassandra chooses to be eventually consistent so eventually consistent means it does not have acid transactions and that's for Apache Cassandra and really any no sequel database so you can prioritize consistency over availability these are things that are actually tunable by you so this is just kind of why you might need Cassandra if you have big data you have you need high throughput high availability so we're gonna do a really brief introduction to Apache spark and then here very soon we're gonna be wrapping this all together of why we're talking about these things before we start talking about machine learning so Apache spark is a unified analytics engine for large-scale data processing okay so right so we have our data stored in Cassandra and then we can do analytics on top of it using Apache spark so it's a hundred times faster than Hadoop for analytics and it utilizes in-memory processing to get that speed so we're gonna be using some of the spark ml lib libraries and so those are located there there's also spark sequel spark streaming graphics and spark with R so then what is data sex analytics this is where we're wrapping this all together so just a quick word about this because I just want to mention it because this is actually what I'm gonna be doing the demo with it's actually installed here just on my laptop as a single node so with data sex analytics you have spark and Cassandra combined together so you're not having to move the data that you have stored in Cassandra off into another spark cluster they're co-located on the same node so you have spark executors and Cassandra all locate on the same node they're just not in the same JVM so on each node you'll have your Cassandra and the connector and then into SPARC and that's where you can do use your spark ml lib all right so now we kind of have an understanding of where our data it's gonna live and how we're gonna actually be able to run these so now let's talk about what we're actually gonna run so what is clustering so clustering algorithms are it's pretty much just as simple as what you would imagine right it's basically trying to group certain things into clusters or groups right so it's the task of actually grouping these objects into the site into different groups so and you want the things to be in that group as similar as possible it's pretty straightforward but so then what is k-means came into the implementation of these clustering algorithms and it's basically it's very simple unsupervised learning algorithm that's used to solve clustering problems so it follows a simple procedure of classifying a given data set into a number of clusters defined by the letter K so you actually define how many clusters you want before you start running the algorithm so it's fixed beforehand so with k-means it's not it's not going to determine the number of clusters that you have you're telling it I want these number of clusters and then classify my data as such and hopefully this will become a little bit more clear as we to the demo so the clusters are positioned as points and all observation or data points are associated with the nearest cluster right they pick a point and then they try to assign each one to fit the centroid of those clusters as closely as possible so and then just iterates over time so when would you actually use k-means like why would this be something that you want to do it's pretty simple right do you want to find it's an unsupervised learning so imagine this is not data that you already have labeled with a group you're trying to label it with a group so this can have good business assumptions around like say if you're trying to figure out a buyer behavior by segmenting them into a particular group or finding anomalies all right so let's get to the demo so can I use clustering machine learning algorithms to find which wine comes from which vineyard right so of course up here at the bottom let me make this a little bit bigger and this is all available on github so if you can't see it so well now you can definitely just go it's it's all here on my it's all here in my github all right so we're gonna use this this is just a jupiter notebook we're gonna be using Apache Cassandra Apache spark Python the stupider notebook and k-means and this is a real data set it's available here you can just go and pick it up and use it for this so what are we trying to learn right using the qualities of the wine can its vineyard be determined can we find its cluster so these are just some Python packages that I need to import so I'm gonna do that and this here is something just a little funky it's essentially when we're gonna be using some scatter plots to look at the various clusters and so what you can do is you can you can create a scatter plot but you have to actually tell it to display and that's what that's doing there so let's run these and this is just a nice function to show just like our data frame in a nice pretty format all right so first what we're gonna do is we're gonna take that data set we're gonna load it into a table but before we do that we need to connect to our cluster and we're gonna create a key space own Apache Cassandra or data stacks instead of like a schema or database it has a key space and so I'm just gonna create it here if it doesn't already exist and it's called wine and chocolate so in this case don't really worry too much about this but this is just our replication strategy so we're just gonna do a simple strategy and with a replication factor of 1 because I just have a single node here on my laptop right it's not a cluster but this is what I was talking about before this is if I had a proper cluster I'd set it to 3 so I'm just gonna set the key space so I don't have to worry about that anymore and then we're gonna create a table called wine so I don't want to get too much into Cassandra data modeling but we do need a primary key so this primary key is how our data is actually going to be distributed across the cluster and so what I'm gonna do it needs to be unique so I'm just gonna generate an ID and I'm gonna use that to partition my data so I'm just gonna create an ID called wine ID so this will result in an even distribution of the data but you'll have to utilize the primary key when you're using doing your where Clause but again we don't want to talk too much about data modeling so I'll just show you that below but so I'm basically I'm just creating the table off of all these columns here I'll run that so what you think so there's 15 columns here of the different attributes and each one of these wines so I don't want to go over each one of them because it's a lot of things it's to talk about but essentially we have our wine ID which we just created randomly it's just a random number then we have our cluster this actually remember we've been talking about with K means that it's an unsupervised learning well this actually we actually do have the labels it is it is a supervised learning so because of that we can actually do some nice comparisons to see if we're actually getting kind of the results we were expecting so we're we're not cheating because you may this very well may happen to you when you have this type of data set but it's just interesting to see so here we have basically three vineyard we have vineyard one two and three and it's like I said it's already been later both in the data set then we have like alcohol content malic acid things like that things that are making up the properties of the wine of the plant that grows in that vineyard all right so then we're gonna actually load that from the CSV which was so nice in the CSV file I didn't require any pre-processing I was just able to load it just straight away I'm just iterating through and doing an insert I could have also like used like a bulk loader or something like that but just for the sake of this because there's a small data set I just loop through and loaded it alright so then we're gonna do a select star just to ensure that we actually loaded the data this is what I was talking about before we're waiting to utilize that where clause so I just picked a random ID that I know I had generated to make sure that it was there and it is alright so now we're actually time for a patchy spark so what we need to do again because I'm using data stacks analytics with a two are co-located I can just build this I can get a spark session here and then from there just using this one line of code I can load all that data that's in my table and Cassandra and actually load it into a spark data frame and so that's all this line here is doing so once I do that I can just do a count on the table to see that all of my rows for my table went into my data frame and they do I have a hundred and seventy eight rows and so I can just do a show on that wine table and again we're just kind of saying again that wine ID on all these properties and then I just want to highlight this is the cluster here by the vineyard so let's visualize let's visualize this data with a scatter plot so in this case what we're gonna do our x-axis because we're trying to find a unique data point for each wine right so in this case we're gonna use on the x-axis the alcohol content and the protein on the y-axis you say why are we doing that we're just doing that to make sure that each point in each wine is unique so these are just two easy ones to make sure it's unique but we probably could have chosen other properties as well and the color of the dot will be assigned based on if it's a klutz on the cluster that it belongs to basically the vineyard so and we're just trying to make sure that don't overlap so let's create this scatterplot here okay cool so straight away we're seeing three clusters just like we imagined right so here in blue is vineyard one here in green and I apologize to anybody if we have any issues with seeing the difference between a red and green because this is kind of difficult to see but cluster two is green here and then vineyard three is in red here here at the bottom sure seeing those three clusters all right so let's actually see if k-means will give us the same output that's what we're looking for right so again it's an unsupervised learning algorithm and it's used to solve clustering problems so what we need to do with k-means it needs to have your your columns the elements in the row you have when you're using it with apache spark you have to make it into a vector so we're gonna assemble our vector here and in this case i'm just gonna add all of the columns that were available to me except i'm gonna remove the cluster the cluster column right we don't want to it doesn't mean a whole lot and we don't want it swaying anything in the data or in the result of the data and then I removed the wine ID because that's just something I generated it doesn't actually have any properties with the wine so then we'll just create a new data frame based on it now being a vector so then here this is not actually where we're gonna start doing the k-means and building our model so we're gonna set the k here remember we talked about the number of clusters that we wanted to create 2/3 and then we're gonna fit the data and generate our model so one of the downsides of k-means even though i mean look how easy this is it's just two lines right at building this model one of the downsides is when you you have to set that clustering in advance and because you do that k-means is happy to just distribute your data just like he told it so if i said separate this data into six clusters it'll do that even though i know because right this is just a pretend dataset that there's really only three but it'll happily do that so you just again data science is an iterative process right so you just keep going back through until your results are making sense so then we're just gonna transform our training data set with our model and get our predictions so here I can just show some of the predictions let me run this again okay so here we see our original cluster and over here we see our prediction so like in this case in this first row we see cluster 3 but then prediction 0 now don't be alarmed this may this may be wrong and and wrongly classified or like it doesn't the prediction has no idea what the labels are actually called so 3 could be 0 3 could be 2 it's a little bit confusing but nevertheless so what we can do here is just try to get like an eyeball on it there's a couple different ways that you can try to verify this when you do have a supervised learning situation like here you can do a confusion matrix or are matching matrix in this case I'm just gonna to do different things I'm just gonna do a count on each one of the predictions versus our original cluster see if they kind of line up just by just quick eyeball right it's just a demo right so let's just quickly eyeball it see if it looks the same then maybe we got good results and then I'm gonna do another scatter plot so then we can just quickly see if it looks like our original scatter plot so if we do a group of we do a count here actually the numbers are not looking not looking too bad the playing it's lining up pretty nicely but let's look at our scatter plot and see if it's the same ok so in this case we're seeing here in the green which it labeled 1 which in our case was vineyard 1 so they happen to have the same label it seems to classify that pretty well seems like it's able to determine clusters from vineyard 1 the wines there pretty easily but here on this vineyard 2 & 3 it's kind of struggling I don't know if you can see that so well in the back but it's kind of struggling there so again one of the downsides of k-means is that the more variables that you add the more difficulty it has in actually determining the clusters so let's see how we are doing on time I've run through this a couple of times but basically essentially what I'm doing is I'm starting to strip away things that I don't think are necessarily going to add to the clustering right so maybe the alcohol content maybe that's not going to affect the wine in which plant it comes from or maybe the ash or the color so I'm going to remove those and then run it again and see if that effects it so just because I want to make sure that we can get to our next demo I'm gonna skip I did two of these here but the results are didn't work so great so let's move on to the last one here because that was kind of the winner so [Music] okay so the very last one here I decided to do and actually it's kind of funny because I got that first set of results I mean this one I was really doing this I got that first set a result they didn't look so great so that's that oh you know what I'm gonna remove it all the way down to just two variables I'm gonna remove it to the sub variables that we're actually using in the scatter plot let's see what that will do well that was actually our winner so if I just use the alcohol content in the protein in the wine and I put that into my object I do a vector of that then I train my model and do my predictions okay and then I do my counts okay my counts are lining up pretty well you just kind of eyeball it and then I do my scatter plot and boom straight away you can see it's clustering it perfectly just like how we originally had in our original clusters so that was it the wine the alcohol content and the protein content are really the elements that make it you can shoot know which wine belongs to which vineyard so again just kind of a reminder that data science again it's an its analytics it's a science it's an iterative process it's all about hypothesis testing and analysis and so let's talk about classification so classification again just like clustering it's pretty straightforward of what we're trying to do here we're trying to identify a set of categories so if we train our dataset on some known categories and how they're labeled that when we have new ones we can also where we gonna be able to predict those based on the training that we've done so a good example of like classification would be like in our spam in our in our email inboxes right spam versus not spam so it's basically a pattern recognition right so sorry there's a lot of text on this but nave Bayes or naive Bayes I always say wrong is a simple technique for constructing classifiers so you're gonna you're gonna generate a model that assigns class labels to various problem instances that's kind of some fancy language for in our case the rows of data right so there's actually many different algorithms that have been implemented for naive Bayes but they all assume kind of the same set of things which is the value of each particular instance feature right so in our case like the columns in our row of our of our item that we're trying to classify are all independent okay so now we're gonna go to our second demo which is naive Bayes and chocolate so can I use a classification machine learning algorithms to find which country a candy bar comes from that's the end ok let's switch to that now ok so we're gonna do the same thing same kind of thing so I'm gonna go walk through this one a little bit more quickly and so again if I have some information about a chocolate bar can I predict which country the data that the chocolate came from so again this is a real data set I found it on Kegel but also it's it again it's not only it's a real data set that's out there on on the internet but it's actually real data it's the Manhattan chocolate Society they actually maintain this data set yeah right who knew so I just have a caveat here because this is an actual real data set this is not a demo data set the results from this data set are not crystal clear right normally when I do demos like this I generate my own data so that way I can get these nice you know 90% accuracy right it looks real slick but this is actually real so we're gonna see that when you're working with real data sometimes you get those crystal clear answers that you're looking for okay so we're just gonna do our imports here and again we're using data sax analytics underneath under the hood as our back-end we're gonna connect to our database create our key space and again we're gonna create a table of our chocolate table same same way we're gonna partition the data and Cassandra by our primary key again I'm just the same kind of thing I'm just making a chocolate I call it chocolate ID okay and so in this case we have way less columns that we had on the wine we have our chocolate ID we have our company so that's like the company the name that actually produces this chocolate bar we have the bar location like we're the specific bean that created this chocolate where originated we have this re F ID it's not really important but essentially what the higher number it was the more recently the chocolate bar had been rated then we have the review date of Wynn had been rated the coke cocoa percentage so the higher that value right 70% cacao 60% right then we have our company location that's gonna be the key that's what we're actually looking for as the manufacturers based company then we have a rating on that chocolate bar anywhere from one to five five being you know it's an awesome chocolate bar we have the bean type like the actually variety of the breed of the plant and the bean origin so again like where it's on the geolocation of the world right okay so now we're gonna load this CSV file so in this case this was not like the wine data set this data set was actually very ugly and to do quite a bit of cleaning removing a bunch of things it was not pretty ly comma separate eliminated right so if anyone's interested in this data set I actually do have it all cleaned up and able to use and I have it on my github so I call it chocolate final because it's the final one that's finally clean and you can even see here I'm still doing a little bit of cleaning it's still not perfect but right there's only so many hours in the day so when I'm gonna just loop through each row and then insert it into the database then I'm gonna do a select star just to validate that it actually there which it is okay so same here what we're doing is we're gonna create that sparks session and then we're gonna connect to our table and then create a data frame from that table so we've moved our data that was living in Cassandra into spark into a data frame so that we can use machine learning on it so in this case we have 1700 rows and I'm just gonna do a show here again we can see the chocolate ID the company location like USA New Zealand also because this is a real data set you're gonna see a lot of misspellings you actually can't see it here but the whole data set instead of France it says face just go with it so because we're using again this is different than k-means we're using naive bayes and so we need to be able to split our data set into a training and to a testing data set so what we're gonna do is we're gonna split it 8020 so we're gonna put 80% of our data into our training set and 20% into our test set so we do that okay so that that seems like 80 80/20 split so naive Bayes again is a class of our algorithm and we need to predict a label we need to tell it which label that we're trying to predict on so that way it knows right so a couple of things each of these to be vectorized needs to be a float it can't if we're with a spark some actually naive Bayes that I've used on other products actually could take in text input but this cannot so because of that I'm gonna need to take because the beam origin like the where it was actually grown that's a like text so we actually need to create a float based on that so what I'm gonna do is I'm going to use this string indexer and it's gonna be each time it sees a new instance it's gonna give it an ID so it's basically taking our text and making it into a number write a number identifier and in this case every time I see a new one so after a certain point it'll say if you keep seeing new instances maybe I'm just saying okay that's enough I don't need I don't need a thousand different indexes for this but I'm saying yeah go ahead do that I'll take a thousand that's fine so again I'm just transforming this into my data frame so I'm able also to do for my company location that's the ultimate label that I want it to label it with and again same thing with the k-means it actually needs to be vectorized okay so here we can see let's see our company location oh yeah now you can see it face so face has been labeled as one so every instance of face is now one and we need to do this for our test set as well and we vectorized that alright so now it's actually time so now we've just been doing all this prep to get into the data frame now we're done now we're actually able to use nave bayes so i'm gonna set this up i'm gonna fit the model with my training data fit the model with my training data and get a model then I'm gonna use that model and I'm gonna put my testing data and score it off of that model to get my predictions so let's run this and then I'm gonna show that so here we have our so in this case face so so that's one so if we go over here to our prediction okay straight away we're seeing it was not able to predict that it's giving it a prediction of 12 but so let's take another look here so we can see in this it actually wasn't able to predict it at all anywhere where it was France not the same Oh actually USA it was able to predict that correctly okay great so now because of this we can actually use a multi classifier evaluator to give us our percentage of how well we were actually able to score this model because we do have a training set right so let's run that alright so we got our test accuracy was about 20 percent so 20 percent of the time if you know that the cacao percentage where the beam was grown and the rating we can figure out which country of origin that the candy bar was actually produced so it's not a great value right you'd like to see 90 percent right but this is a real data set and so I wasn't able to do that actually originally when I had started working with this data set what to show you cause I wanted to show you if I knew all these things about the bar the chocolate bar where it was produced the rating how much chocolate you know cocoa percentage that I could determine the rating I thought oh that would be really interesting that's how we'd know if it was a good chocolate bar or not but actually the that test set accuracy was just it was really bad it was horrible so I said okay I'm gonna go with this because at least I'm getting 20% here all right okay all righty so wrapping it up that was awesome but now what do I do so again like I said this is on github if you want to take a look at it if you want to learn more about Cassandra or spark so we have data stacks Academy it's free online that you can utilize to learn more about Cassandra and data sex and you can follow me on Twitter I post a lot of just the stuff that I'm doing and sometimes I try to post interesting things and thank you [Applause]",
    "segments": [
      {
        "start": 0.45,
        "duration": 5.849,
        "text": "so this is my talk classification and"
      },
      {
        "start": 3.39,
        "duration": 5.37,
        "text": "clustering with wine and chocolate and"
      },
      {
        "start": 6.299,
        "duration": 3.51,
        "text": "sorry there is no wine or chocolate but"
      },
      {
        "start": 8.76,
        "duration": 3.9,
        "text": "nevertheless you're gonna learn"
      },
      {
        "start": 9.809,
        "duration": 5.221,
        "text": "something about them today so but first"
      },
      {
        "start": 12.66,
        "duration": 3.869,
        "text": "let's talk a little bit about me so I"
      },
      {
        "start": 15.03,
        "duration": 4.74,
        "text": "actually I lived in Seattle for ten"
      },
      {
        "start": 16.529,
        "duration": 4.561,
        "text": "years I did my undergrad here at the"
      },
      {
        "start": 19.77,
        "duration": 3.96,
        "text": "University of Washington you can see"
      },
      {
        "start": 21.09,
        "duration": 5.82,
        "text": "this here why you Doug there I actually"
      },
      {
        "start": 23.73,
        "duration": 4.94,
        "text": "have a BS in biology from u-dub we don't"
      },
      {
        "start": 26.91,
        "duration": 5.789,
        "text": "need to know the year that I graduated"
      },
      {
        "start": 28.67,
        "duration": 5.409,
        "text": "um but after that I wouldn't I got a"
      },
      {
        "start": 32.699,
        "duration": 3.901,
        "text": "actually I was gonna be a middle school"
      },
      {
        "start": 34.079,
        "duration": 5.4,
        "text": "teacher and I decided that that was kind"
      },
      {
        "start": 36.6,
        "duration": 4.619,
        "text": "of terrible and so then I went and I got"
      },
      {
        "start": 39.479,
        "duration": 3.781,
        "text": "a I said what am I gonna do with my life"
      },
      {
        "start": 41.219,
        "duration": 3.75,
        "text": "I guess I'll go be a computer scientist"
      },
      {
        "start": 43.26,
        "duration": 3.539,
        "text": "so I wouldn't got my masters in computer"
      },
      {
        "start": 44.969,
        "duration": 3.48,
        "text": "science from Santa Clara and the bay"
      },
      {
        "start": 46.799,
        "duration": 4.08,
        "text": "area and that's where I live now I got"
      },
      {
        "start": 48.449,
        "duration": 4.14,
        "text": "that in 2012 so I've worked as a"
      },
      {
        "start": 50.879,
        "duration": 3.69,
        "text": "software engineer for about the last six"
      },
      {
        "start": 52.589,
        "duration": 4.351,
        "text": "years since I graduated and I've always"
      },
      {
        "start": 54.569,
        "duration": 3.9,
        "text": "worked on distributed databases somehow"
      },
      {
        "start": 56.94,
        "duration": 2.969,
        "text": "that became my passion I think I've"
      },
      {
        "start": 58.469,
        "duration": 4.56,
        "text": "worked on four different distributed"
      },
      {
        "start": 59.909,
        "duration": 6.541,
        "text": "databases I worked at Lockheed Martin HP"
      },
      {
        "start": 63.029,
        "duration": 5.19,
        "text": "Teradata a startup called s Jin I'm also"
      },
      {
        "start": 66.45,
        "duration": 3.989,
        "text": "and now I'm a developer advocate for"
      },
      {
        "start": 68.219,
        "duration": 4.47,
        "text": "data stacks so I'm gonna patch a"
      },
      {
        "start": 70.439,
        "duration": 4.441,
        "text": "committer and pmc member on a project"
      },
      {
        "start": 72.689,
        "duration": 6.0,
        "text": "called Truffaut daeun it is a sequel on"
      },
      {
        "start": 74.88,
        "duration": 5.129,
        "text": "HBase solution and I did all the initial"
      },
      {
        "start": 78.689,
        "duration": 3.42,
        "text": "installation and deployment work for"
      },
      {
        "start": 80.009,
        "duration": 3.72,
        "text": "that project and then just to my fun"
      },
      {
        "start": 82.109,
        "duration": 4.201,
        "text": "things about me is that I love Disney I"
      },
      {
        "start": 83.729,
        "duration": 5.46,
        "text": "love the cloud I love dogs Linux and"
      },
      {
        "start": 86.31,
        "duration": 5.549,
        "text": "obviously distributed databases so what"
      },
      {
        "start": 89.189,
        "duration": 4.35,
        "text": "are we gonna talk about today so first"
      },
      {
        "start": 91.859,
        "duration": 3.241,
        "text": "what problem are we really trying to"
      },
      {
        "start": 93.539,
        "duration": 4.531,
        "text": "solve here right these are the important"
      },
      {
        "start": 95.1,
        "duration": 5.309,
        "text": "questions can I use a clustering machine"
      },
      {
        "start": 98.07,
        "duration": 6.359,
        "text": "learning algorithm to find which wine"
      },
      {
        "start": 100.409,
        "duration": 6.06,
        "text": "came from which vineyard okay and then"
      },
      {
        "start": 104.429,
        "duration": 4.47,
        "text": "can I use a classification machine"
      },
      {
        "start": 106.469,
        "duration": 5.551,
        "text": "learning algorithm to find which country"
      },
      {
        "start": 108.899,
        "duration": 5.07,
        "text": "a candy bar comes from right think about"
      },
      {
        "start": 112.02,
        "duration": 4.169,
        "text": "like what is it a Mars bar those come"
      },
      {
        "start": 113.969,
        "duration": 5.64,
        "text": "from Europe and Eminem's come from"
      },
      {
        "start": 116.189,
        "duration": 5.22,
        "text": "America like that so but first before we"
      },
      {
        "start": 119.609,
        "duration": 3.15,
        "text": "get into that we're gonna talk we're"
      },
      {
        "start": 121.409,
        "duration": 3.09,
        "text": "gonna do an introduction to Apache"
      },
      {
        "start": 122.759,
        "duration": 3.271,
        "text": "Cassandra I know this is Seattle"
      },
      {
        "start": 124.499,
        "duration": 3.3,
        "text": "scalability this is probably something"
      },
      {
        "start": 126.03,
        "duration": 3.149,
        "text": "most of you are very familiar with so"
      },
      {
        "start": 127.799,
        "duration": 3.18,
        "text": "we're just gonna do a brief introduction"
      },
      {
        "start": 129.179,
        "duration": 3.48,
        "text": "to these things a very brief"
      },
      {
        "start": 130.979,
        "duration": 2.771,
        "text": "introduction to Apache spark again"
      },
      {
        "start": 132.659,
        "duration": 2.951,
        "text": "probably this crowd"
      },
      {
        "start": 133.75,
        "duration": 3.06,
        "text": "this like the back of their hand but"
      },
      {
        "start": 135.61,
        "duration": 2.31,
        "text": "we're gonna do it very quickly just to"
      },
      {
        "start": 136.81,
        "duration": 3.09,
        "text": "make sure we're all on the same page"
      },
      {
        "start": 137.92,
        "duration": 3.63,
        "text": "then we're going to talk about why are"
      },
      {
        "start": 139.9,
        "duration": 3.86,
        "text": "we talking about Cassandra and SPARC"
      },
      {
        "start": 141.55,
        "duration": 4.17,
        "text": "together with these machine learning"
      },
      {
        "start": 143.76,
        "duration": 4.87,
        "text": "algorithms and we're gonna bring that"
      },
      {
        "start": 145.72,
        "duration": 4.95,
        "text": "all together then we're gonna do what is"
      },
      {
        "start": 148.63,
        "duration": 3.36,
        "text": "k-means because that's going to be the"
      },
      {
        "start": 150.67,
        "duration": 4.08,
        "text": "machine learning algorithm we're gonna"
      },
      {
        "start": 151.99,
        "duration": 4.26,
        "text": "use for our clustering and then we're"
      },
      {
        "start": 154.75,
        "duration": 2.84,
        "text": "gonna talk about naive Bayes which is"
      },
      {
        "start": 156.25,
        "duration": 3.48,
        "text": "what we're gonna use for classification"
      },
      {
        "start": 157.59,
        "duration": 3.4,
        "text": "we're gonna use that on our chocolate"
      },
      {
        "start": 159.73,
        "duration": 4.35,
        "text": "we're gonna have two demos with that as"
      },
      {
        "start": 160.99,
        "duration": 5.88,
        "text": "well so again the problem we're really"
      },
      {
        "start": 164.08,
        "duration": 4.83,
        "text": "trying to solve here can I use machine"
      },
      {
        "start": 166.87,
        "duration": 4.47,
        "text": "learning with Apache spark with wine and"
      },
      {
        "start": 168.91,
        "duration": 4.41,
        "text": "chocolate and yes you can but the main"
      },
      {
        "start": 171.34,
        "duration": 3.69,
        "text": "goal around this for me is what I'm"
      },
      {
        "start": 173.32,
        "duration": 3.72,
        "text": "trying to convey to all of you is that"
      },
      {
        "start": 175.03,
        "duration": 4.05,
        "text": "data analytics doesn't have to be"
      },
      {
        "start": 177.04,
        "duration": 3.84,
        "text": "complicated a lot of times you're seeing"
      },
      {
        "start": 179.08,
        "duration": 3.87,
        "text": "these types of things and it's always on"
      },
      {
        "start": 180.88,
        "duration": 3.87,
        "text": "very complicated use cases but if you"
      },
      {
        "start": 182.95,
        "duration": 4.08,
        "text": "just scale it down to something simple"
      },
      {
        "start": 184.75,
        "duration": 3.9,
        "text": "you can learn how to use it and then go"
      },
      {
        "start": 187.03,
        "duration": 4.08,
        "text": "off and do your complicated use cases"
      },
      {
        "start": 188.65,
        "duration": 4.5,
        "text": "from there so we're gonna use it'll eyes"
      },
      {
        "start": 191.11,
        "duration": 4.62,
        "text": "the power of big data using Apache"
      },
      {
        "start": 193.15,
        "duration": 5.22,
        "text": "Cassandra Apache spark spark machine"
      },
      {
        "start": 195.73,
        "duration": 4.47,
        "text": "learning libraries Jupiter notebooks and"
      },
      {
        "start": 198.37,
        "duration": 3.869,
        "text": "Python that's what we're I'm going to"
      },
      {
        "start": 200.2,
        "duration": 5.039,
        "text": "show you in the demo so let's talk a"
      },
      {
        "start": 202.239,
        "duration": 4.891,
        "text": "little bit about Apache Cassandra so it"
      },
      {
        "start": 205.239,
        "duration": 3.591,
        "text": "was developed by Facebook and and"
      },
      {
        "start": 207.13,
        "duration": 4.65,
        "text": "donated into the open-source community"
      },
      {
        "start": 208.83,
        "duration": 5.26,
        "text": "community around 2008 and then he"
      },
      {
        "start": 211.78,
        "duration": 4.83,
        "text": "graduated to a top level project in 2010"
      },
      {
        "start": 214.09,
        "duration": 5.13,
        "text": "now I was actually with like I said an"
      },
      {
        "start": 216.61,
        "duration": 4.439,
        "text": "Apache project that went from incubation"
      },
      {
        "start": 219.22,
        "duration": 3.81,
        "text": "all the way to graduation and I can tell"
      },
      {
        "start": 221.049,
        "duration": 4.711,
        "text": "you what a strenuous process this"
      },
      {
        "start": 223.03,
        "duration": 6.48,
        "text": "actually is to actually get your project"
      },
      {
        "start": 225.76,
        "duration": 6.18,
        "text": "committed and graduated to top level so"
      },
      {
        "start": 229.51,
        "duration": 4.02,
        "text": "because of that you know that this is a"
      },
      {
        "start": 231.94,
        "duration": 3.78,
        "text": "product that the community has really"
      },
      {
        "start": 233.53,
        "duration": 4.049,
        "text": "gathered around and are supporting very"
      },
      {
        "start": 235.72,
        "duration": 4.079,
        "text": "strongly the Apache foundation is very"
      },
      {
        "start": 237.579,
        "duration": 5.25,
        "text": "strong on that um so what it is is a"
      },
      {
        "start": 239.799,
        "duration": 5.041,
        "text": "distributed decentralized database it's"
      },
      {
        "start": 242.829,
        "duration": 5.011,
        "text": "elastically scalable you can add and"
      },
      {
        "start": 244.84,
        "duration": 5.369,
        "text": "remove nodes with no downtime and that's"
      },
      {
        "start": 247.84,
        "duration": 4.38,
        "text": "high performance it's very fast has high"
      },
      {
        "start": 250.209,
        "duration": 4.801,
        "text": "availability and low fault tolerance"
      },
      {
        "start": 252.22,
        "duration": 4.38,
        "text": "there's no single point of failure I'm"
      },
      {
        "start": 255.01,
        "duration": 4.14,
        "text": "just gonna let that sink in that's one"
      },
      {
        "start": 256.6,
        "duration": 3.87,
        "text": "of my favorite things uh and one of the"
      },
      {
        "start": 259.15,
        "duration": 2.639,
        "text": "reasons I actually came to data stacks"
      },
      {
        "start": 260.47,
        "duration": 3.24,
        "text": "because once I learned a little bit more"
      },
      {
        "start": 261.789,
        "duration": 3.69,
        "text": "about it and why it truly does have no"
      },
      {
        "start": 263.71,
        "duration": 3.86,
        "text": "single point of failure I was on board"
      },
      {
        "start": 265.479,
        "duration": 4.611,
        "text": "because all the other products I'd ever"
      },
      {
        "start": 267.57,
        "duration": 4.83,
        "text": "work done had the opposite of that like"
      },
      {
        "start": 270.09,
        "duration": 4.89,
        "text": "a million points of failure so this is"
      },
      {
        "start": 272.4,
        "duration": 3.9,
        "text": "very exciting for me and it solves many"
      },
      {
        "start": 274.98,
        "duration": 3.99,
        "text": "of the problems faced with traditional"
      },
      {
        "start": 276.3,
        "duration": 4.26,
        "text": "databases for certain workloads right"
      },
      {
        "start": 278.97,
        "duration": 4.26,
        "text": "when we're talking about no sequel it"
      },
      {
        "start": 280.56,
        "duration": 5.4,
        "text": "has its place relational database it has"
      },
      {
        "start": 283.23,
        "duration": 4.38,
        "text": "its path the police right so what does"
      },
      {
        "start": 285.96,
        "duration": 3.54,
        "text": "this all mean so let's talk about four"
      },
      {
        "start": 287.61,
        "duration": 4.92,
        "text": "big topics in the no sequel or Apache"
      },
      {
        "start": 289.5,
        "duration": 4.83,
        "text": "Cassandra space distributed replication"
      },
      {
        "start": 292.53,
        "duration": 5.22,
        "text": "elastically scalable and high"
      },
      {
        "start": 294.33,
        "duration": 6.59,
        "text": "availability so distributed every node"
      },
      {
        "start": 297.75,
        "duration": 6.12,
        "text": "in the cluster has the exact same role"
      },
      {
        "start": 300.92,
        "duration": 5.53,
        "text": "so I put here really that's actually"
      },
      {
        "start": 303.87,
        "duration": 6.27,
        "text": "true cassandra does not have a master"
      },
      {
        "start": 306.45,
        "duration": 6.3,
        "text": "worker architecture so any client can"
      },
      {
        "start": 310.14,
        "duration": 5.61,
        "text": "connect to any node and all nodes are"
      },
      {
        "start": 312.75,
        "duration": 5.73,
        "text": "read and write ready but this is not to"
      },
      {
        "start": 315.75,
        "duration": 4.8,
        "text": "say that all nodes contain all the data"
      },
      {
        "start": 318.48,
        "duration": 3.39,
        "text": "right that just doesn't make sense so"
      },
      {
        "start": 320.55,
        "duration": 3.75,
        "text": "that's we're talking about replication"
      },
      {
        "start": 321.87,
        "duration": 4.89,
        "text": "here so to be able to survive a node"
      },
      {
        "start": 324.3,
        "duration": 3.84,
        "text": "going down the date the data obviously"
      },
      {
        "start": 326.76,
        "duration": 3.3,
        "text": "needs to be copied because we're running"
      },
      {
        "start": 328.14,
        "duration": 3.99,
        "text": "a distributed system right needs to be"
      },
      {
        "start": 330.06,
        "duration": 3.75,
        "text": "copied to the other nodes so the"
      },
      {
        "start": 332.13,
        "duration": 3.81,
        "text": "replication factor how many times your"
      },
      {
        "start": 333.81,
        "duration": 4.77,
        "text": "data is going to be copied it's actually"
      },
      {
        "start": 335.94,
        "duration": 5.37,
        "text": "set by the user right so maybe if you're"
      },
      {
        "start": 338.58,
        "duration": 4.83,
        "text": "working with dated that you don't really"
      },
      {
        "start": 341.31,
        "duration": 3.9,
        "text": "mind if it were to be lost right maybe"
      },
      {
        "start": 343.41,
        "duration": 3.63,
        "text": "some IOT sensor data something like that"
      },
      {
        "start": 345.21,
        "duration": 3.96,
        "text": "that if I lose one node of you know a"
      },
      {
        "start": 347.04,
        "duration": 3.81,
        "text": "couple hours worth of data I'm okay with"
      },
      {
        "start": 349.17,
        "duration": 3.57,
        "text": "just losing that that's fine I can just"
      },
      {
        "start": 350.85,
        "duration": 4.77,
        "text": "have my replication factor set to one if"
      },
      {
        "start": 352.74,
        "duration": 5.25,
        "text": "I have incredibly important data that I"
      },
      {
        "start": 355.62,
        "duration": 3.6,
        "text": "can't I can't stand to lose then maybe I"
      },
      {
        "start": 357.99,
        "duration": 2.37,
        "text": "would set it all the way to the number"
      },
      {
        "start": 359.22,
        "duration": 3.03,
        "text": "of nodes that I actually have in my"
      },
      {
        "start": 360.36,
        "duration": 3.87,
        "text": "cluster we don't really recommend that"
      },
      {
        "start": 362.25,
        "duration": 3.33,
        "text": "it's kind of overkill probably just"
      },
      {
        "start": 364.23,
        "duration": 4.14,
        "text": "three copies of the data is probably"
      },
      {
        "start": 365.58,
        "duration": 5.37,
        "text": "plenty so the data is async"
      },
      {
        "start": 368.37,
        "duration": 4.56,
        "text": "asynchronously replicated across the"
      },
      {
        "start": 370.95,
        "duration": 3.63,
        "text": "nodes so that's automatic and it's"
      },
      {
        "start": 372.93,
        "duration": 5.34,
        "text": "peer-to-peer and it's really within"
      },
      {
        "start": 374.58,
        "duration": 6.51,
        "text": "milliseconds so elastically scalable so"
      },
      {
        "start": 378.27,
        "duration": 5.22,
        "text": "as you add more nodes the performance"
      },
      {
        "start": 381.09,
        "duration": 4.32,
        "text": "actually will increase linearly you can"
      },
      {
        "start": 383.49,
        "duration": 4.47,
        "text": "scale up and scale down with no downtime"
      },
      {
        "start": 385.41,
        "duration": 4.89,
        "text": "you don't even need a restart which is"
      },
      {
        "start": 387.96,
        "duration": 3.75,
        "text": "actually really surprising to me and"
      },
      {
        "start": 390.3,
        "duration": 3.9,
        "text": "pretty cool"
      },
      {
        "start": 391.71,
        "duration": 3.39,
        "text": "reads and write both scale linearly so"
      },
      {
        "start": 394.2,
        "duration": 3.21,
        "text": "this is just actually just a little"
      },
      {
        "start": 395.1,
        "duration": 4.2,
        "text": "graphic from Netflix showing that as"
      },
      {
        "start": 397.41,
        "duration": 2.95,
        "text": "they added more nodes they will have"
      },
      {
        "start": 399.3,
        "duration": 4.119,
        "text": "more"
      },
      {
        "start": 400.36,
        "duration": 5.579,
        "text": "rights so let's talk about high"
      },
      {
        "start": 403.419,
        "duration": 4.65,
        "text": "availability so again this lack of"
      },
      {
        "start": 405.939,
        "duration": 4.171,
        "text": "single point of failure this lack of a"
      },
      {
        "start": 408.069,
        "duration": 4.081,
        "text": "master node allows for high availability"
      },
      {
        "start": 410.11,
        "duration": 3.48,
        "text": "because there is and that means there's"
      },
      {
        "start": 412.15,
        "duration": 3.78,
        "text": "no single point of failure so"
      },
      {
        "start": 413.59,
        "duration": 4.859,
        "text": "replication Aloud's notes to fail"
      },
      {
        "start": 415.93,
        "duration": 3.87,
        "text": "and data to still be available right"
      },
      {
        "start": 418.449,
        "duration": 3.391,
        "text": "because now my data is distributed"
      },
      {
        "start": 419.8,
        "duration": 2.57,
        "text": "across notes if I lose one I'm still"
      },
      {
        "start": 421.84,
        "duration": 3.299,
        "text": "good to go"
      },
      {
        "start": 422.37,
        "duration": 5.74,
        "text": "so cassandra expects notes to fail and"
      },
      {
        "start": 425.139,
        "duration": 4.53,
        "text": "it doesn't panic so multiple data center"
      },
      {
        "start": 428.11,
        "duration": 6.51,
        "text": "support is also right out of the box"
      },
      {
        "start": 429.669,
        "duration": 6.72,
        "text": "even multi cloud support so I just I"
      },
      {
        "start": 434.62,
        "duration": 3.539,
        "text": "would like to bring this up because I'm"
      },
      {
        "start": 436.389,
        "duration": 4.261,
        "text": "an engineer and talking to other"
      },
      {
        "start": 438.159,
        "duration": 4.921,
        "text": "engineers so we've talked about a lot of"
      },
      {
        "start": 440.65,
        "duration": 4.379,
        "text": "magic right Cassandra Apache Cassandra"
      },
      {
        "start": 443.08,
        "duration": 3.269,
        "text": "seems like it's kind of magical but"
      },
      {
        "start": 445.029,
        "duration": 3.57,
        "text": "there's got to be some kind of trade-off"
      },
      {
        "start": 446.349,
        "duration": 3.271,
        "text": "right and so that's what I like to bring"
      },
      {
        "start": 448.599,
        "duration": 2.611,
        "text": "up because I don't want to sound like"
      },
      {
        "start": 449.62,
        "duration": 4.65,
        "text": "I'm trying to sell something that maybe"
      },
      {
        "start": 451.21,
        "duration": 5.4,
        "text": "isn't true so you should definitely"
      },
      {
        "start": 454.27,
        "duration": 4.049,
        "text": "check out this a cap theorem it"
      },
      {
        "start": 456.61,
        "duration": 4.079,
        "text": "basically says that at any one point in"
      },
      {
        "start": 458.319,
        "duration": 4.41,
        "text": "time if you have a network failure you"
      },
      {
        "start": 460.689,
        "duration": 3.871,
        "text": "can only have so many of these things at"
      },
      {
        "start": 462.729,
        "duration": 3.78,
        "text": "one particular time you can either have"
      },
      {
        "start": 464.56,
        "duration": 3.419,
        "text": "you can have availability consistency"
      },
      {
        "start": 466.509,
        "duration": 3.63,
        "text": "and partitioning those are the three"
      },
      {
        "start": 467.979,
        "duration": 4.8,
        "text": "things but you can only have two when"
      },
      {
        "start": 470.139,
        "duration": 4.471,
        "text": "you have a network failure so because of"
      },
      {
        "start": 472.779,
        "duration": 2.73,
        "text": "this it's impossible to have all three"
      },
      {
        "start": 474.61,
        "duration": 3.179,
        "text": "during a network failure"
      },
      {
        "start": 475.509,
        "duration": 4.62,
        "text": "Cassandra chooses to be eventually"
      },
      {
        "start": 477.789,
        "duration": 4.861,
        "text": "consistent so eventually consistent"
      },
      {
        "start": 480.129,
        "duration": 4.111,
        "text": "means it does not have acid transactions"
      },
      {
        "start": 482.65,
        "duration": 4.739,
        "text": "and that's for Apache Cassandra and"
      },
      {
        "start": 484.24,
        "duration": 5.549,
        "text": "really any no sequel database so you can"
      },
      {
        "start": 487.389,
        "duration": 3.39,
        "text": "prioritize consistency over availability"
      },
      {
        "start": 489.789,
        "duration": 5.731,
        "text": "these are things that are actually"
      },
      {
        "start": 490.779,
        "duration": 6.241,
        "text": "tunable by you so this is just kind of"
      },
      {
        "start": 495.52,
        "duration": 3.209,
        "text": "why you might need Cassandra if you have"
      },
      {
        "start": 497.02,
        "duration": 4.56,
        "text": "big data you have you need high"
      },
      {
        "start": 498.729,
        "duration": 5.041,
        "text": "throughput high availability so we're"
      },
      {
        "start": 501.58,
        "duration": 4.92,
        "text": "gonna do a really brief introduction to"
      },
      {
        "start": 503.77,
        "duration": 4.56,
        "text": "Apache spark and then here very soon"
      },
      {
        "start": 506.5,
        "duration": 3.27,
        "text": "we're gonna be wrapping this all"
      },
      {
        "start": 508.33,
        "duration": 2.549,
        "text": "together of why we're talking about"
      },
      {
        "start": 509.77,
        "duration": 4.17,
        "text": "these things before we start talking"
      },
      {
        "start": 510.879,
        "duration": 5.52,
        "text": "about machine learning so Apache spark"
      },
      {
        "start": 513.94,
        "duration": 7.519,
        "text": "is a unified analytics engine for"
      },
      {
        "start": 516.399,
        "duration": 7.951,
        "text": "large-scale data processing okay"
      },
      {
        "start": 521.459,
        "duration": 5.591,
        "text": "so right so we have our data stored in"
      },
      {
        "start": 524.35,
        "duration": 5.429,
        "text": "Cassandra and then we can do analytics"
      },
      {
        "start": 527.05,
        "duration": 4.229,
        "text": "on top of it using Apache spark so it's"
      },
      {
        "start": 529.779,
        "duration": 4.081,
        "text": "a hundred times faster than Hadoop for"
      },
      {
        "start": 531.279,
        "duration": 5.131,
        "text": "analytics and it utilizes in-memory"
      },
      {
        "start": 533.86,
        "duration": 4.669,
        "text": "processing to get that speed so we're"
      },
      {
        "start": 536.41,
        "duration": 6.419,
        "text": "gonna be using some of the spark ml lib"
      },
      {
        "start": 538.529,
        "duration": 6.49,
        "text": "libraries and so those are located there"
      },
      {
        "start": 542.829,
        "duration": 6.091,
        "text": "there's also spark sequel spark"
      },
      {
        "start": 545.019,
        "duration": 6.0,
        "text": "streaming graphics and spark with R so"
      },
      {
        "start": 548.92,
        "duration": 3.15,
        "text": "then what is data sex analytics this is"
      },
      {
        "start": 551.019,
        "duration": 3.331,
        "text": "where we're wrapping this all together"
      },
      {
        "start": 552.07,
        "duration": 3.42,
        "text": "so just a quick word about this because"
      },
      {
        "start": 554.35,
        "duration": 2.25,
        "text": "I just want to mention it because this"
      },
      {
        "start": 555.49,
        "duration": 2.76,
        "text": "is actually what I'm gonna be doing the"
      },
      {
        "start": 556.6,
        "duration": 4.38,
        "text": "demo with it's actually installed here"
      },
      {
        "start": 558.25,
        "duration": 4.95,
        "text": "just on my laptop as a single node so"
      },
      {
        "start": 560.98,
        "duration": 4.71,
        "text": "with data sex analytics you have spark"
      },
      {
        "start": 563.2,
        "duration": 4.139,
        "text": "and Cassandra combined together so"
      },
      {
        "start": 565.69,
        "duration": 3.12,
        "text": "you're not having to move the data that"
      },
      {
        "start": 567.339,
        "duration": 3.541,
        "text": "you have stored in Cassandra off into"
      },
      {
        "start": 568.81,
        "duration": 4.8,
        "text": "another spark cluster they're co-located"
      },
      {
        "start": 570.88,
        "duration": 6.329,
        "text": "on the same node so you have spark"
      },
      {
        "start": 573.61,
        "duration": 5.13,
        "text": "executors and Cassandra all locate on"
      },
      {
        "start": 577.209,
        "duration": 5.13,
        "text": "the same node they're just not in the"
      },
      {
        "start": 578.74,
        "duration": 5.76,
        "text": "same JVM so on each node you'll have"
      },
      {
        "start": 582.339,
        "duration": 3.721,
        "text": "your Cassandra and the connector and"
      },
      {
        "start": 584.5,
        "duration": 5.55,
        "text": "then into SPARC and that's where you can"
      },
      {
        "start": 586.06,
        "duration": 5.519,
        "text": "do use your spark ml lib all right so"
      },
      {
        "start": 590.05,
        "duration": 2.82,
        "text": "now we kind of have an understanding of"
      },
      {
        "start": 591.579,
        "duration": 2.551,
        "text": "where our data it's gonna live and how"
      },
      {
        "start": 592.87,
        "duration": 3.149,
        "text": "we're gonna actually be able to run"
      },
      {
        "start": 594.13,
        "duration": 4.949,
        "text": "these so now let's talk about what we're"
      },
      {
        "start": 596.019,
        "duration": 7.591,
        "text": "actually gonna run so what is clustering"
      },
      {
        "start": 599.079,
        "duration": 6.57,
        "text": "so clustering algorithms are it's pretty"
      },
      {
        "start": 603.61,
        "duration": 4.77,
        "text": "much just as simple as what you would"
      },
      {
        "start": 605.649,
        "duration": 5.911,
        "text": "imagine right it's basically trying to"
      },
      {
        "start": 608.38,
        "duration": 7.5,
        "text": "group certain things into clusters or"
      },
      {
        "start": 611.56,
        "duration": 6.269,
        "text": "groups right so it's the task of"
      },
      {
        "start": 615.88,
        "duration": 4.56,
        "text": "actually grouping these objects into the"
      },
      {
        "start": 617.829,
        "duration": 4.26,
        "text": "site into different groups so and you"
      },
      {
        "start": 620.44,
        "duration": 4.139,
        "text": "want the things to be in that group as"
      },
      {
        "start": 622.089,
        "duration": 4.5,
        "text": "similar as possible it's pretty"
      },
      {
        "start": 624.579,
        "duration": 4.021,
        "text": "straightforward but so then what is"
      },
      {
        "start": 626.589,
        "duration": 4.021,
        "text": "k-means came into the implementation of"
      },
      {
        "start": 628.6,
        "duration": 4.41,
        "text": "these clustering algorithms and it's"
      },
      {
        "start": 630.61,
        "duration": 4.56,
        "text": "basically it's very simple unsupervised"
      },
      {
        "start": 633.01,
        "duration": 5.37,
        "text": "learning algorithm that's used to solve"
      },
      {
        "start": 635.17,
        "duration": 5.669,
        "text": "clustering problems so it follows a"
      },
      {
        "start": 638.38,
        "duration": 4.47,
        "text": "simple procedure of classifying a given"
      },
      {
        "start": 640.839,
        "duration": 4.831,
        "text": "data set into a number of clusters"
      },
      {
        "start": 642.85,
        "duration": 6.06,
        "text": "defined by the letter K so you actually"
      },
      {
        "start": 645.67,
        "duration": 6.57,
        "text": "define how many clusters you want before"
      },
      {
        "start": 648.91,
        "duration": 5.73,
        "text": "you start running the algorithm so it's"
      },
      {
        "start": 652.24,
        "duration": 4.349,
        "text": "fixed beforehand so with k-means it's"
      },
      {
        "start": 654.64,
        "duration": 4.319,
        "text": "not it's not going to determine the"
      },
      {
        "start": 656.589,
        "duration": 4.11,
        "text": "number of clusters that you have you're"
      },
      {
        "start": 658.959,
        "duration": 3.661,
        "text": "telling it I want these number of"
      },
      {
        "start": 660.699,
        "duration": 5.64,
        "text": "clusters and then classify my data as"
      },
      {
        "start": 662.62,
        "duration": 4.72,
        "text": "such and hopefully this will become a"
      },
      {
        "start": 666.339,
        "duration": 3.941,
        "text": "little bit more clear as we"
      },
      {
        "start": 667.34,
        "duration": 5.82,
        "text": "to the demo so the clusters are"
      },
      {
        "start": 670.28,
        "duration": 4.62,
        "text": "positioned as points and all observation"
      },
      {
        "start": 673.16,
        "duration": 3.93,
        "text": "or data points are associated with the"
      },
      {
        "start": 674.9,
        "duration": 4.53,
        "text": "nearest cluster right they pick a point"
      },
      {
        "start": 677.09,
        "duration": 4.14,
        "text": "and then they try to assign each one to"
      },
      {
        "start": 679.43,
        "duration": 4.98,
        "text": "fit the centroid of those clusters as"
      },
      {
        "start": 681.23,
        "duration": 8.94,
        "text": "closely as possible so and then just"
      },
      {
        "start": 684.41,
        "duration": 7.44,
        "text": "iterates over time so when would you"
      },
      {
        "start": 690.17,
        "duration": 3.84,
        "text": "actually use k-means like why would this"
      },
      {
        "start": 691.85,
        "duration": 3.9,
        "text": "be something that you want to do it's"
      },
      {
        "start": 694.01,
        "duration": 3.9,
        "text": "pretty simple right do you want to find"
      },
      {
        "start": 695.75,
        "duration": 4.02,
        "text": "it's an unsupervised learning so imagine"
      },
      {
        "start": 697.91,
        "duration": 4.2,
        "text": "this is not data that you already have"
      },
      {
        "start": 699.77,
        "duration": 5.88,
        "text": "labeled with a group you're trying to"
      },
      {
        "start": 702.11,
        "duration": 6.39,
        "text": "label it with a group so this can have"
      },
      {
        "start": 705.65,
        "duration": 3.96,
        "text": "good business assumptions around like"
      },
      {
        "start": 708.5,
        "duration": 4.17,
        "text": "say if you're trying to figure out a"
      },
      {
        "start": 709.61,
        "duration": 5.54,
        "text": "buyer behavior by segmenting them into a"
      },
      {
        "start": 712.67,
        "duration": 6.33,
        "text": "particular group or finding anomalies"
      },
      {
        "start": 715.15,
        "duration": 7.27,
        "text": "all right so let's get to the demo so"
      },
      {
        "start": 719.0,
        "duration": 6.09,
        "text": "can I use clustering machine learning"
      },
      {
        "start": 722.42,
        "duration": 9.45,
        "text": "algorithms to find which wine comes from"
      },
      {
        "start": 725.09,
        "duration": 8.79,
        "text": "which vineyard right so of course up"
      },
      {
        "start": 731.87,
        "duration": 7.59,
        "text": "here at the bottom let me make this a"
      },
      {
        "start": 733.88,
        "duration": 7.05,
        "text": "little bit bigger and this is all"
      },
      {
        "start": 739.46,
        "duration": 3.48,
        "text": "available on github so if you can't see"
      },
      {
        "start": 740.93,
        "duration": 4.65,
        "text": "it so well now you can definitely just"
      },
      {
        "start": 742.94,
        "duration": 7.14,
        "text": "go it's it's all here on my it's all"
      },
      {
        "start": 745.58,
        "duration": 5.91,
        "text": "here in my github all right so we're"
      },
      {
        "start": 750.08,
        "duration": 3.15,
        "text": "gonna use this this is just a jupiter"
      },
      {
        "start": 751.49,
        "duration": 3.99,
        "text": "notebook we're gonna be using Apache"
      },
      {
        "start": 753.23,
        "duration": 4.68,
        "text": "Cassandra Apache spark Python the"
      },
      {
        "start": 755.48,
        "duration": 4.35,
        "text": "stupider notebook and k-means and this"
      },
      {
        "start": 757.91,
        "duration": 5.13,
        "text": "is a real data set it's available here"
      },
      {
        "start": 759.83,
        "duration": 5.49,
        "text": "you can just go and pick it up and use"
      },
      {
        "start": 763.04,
        "duration": 4.71,
        "text": "it for this so what are we trying to"
      },
      {
        "start": 765.32,
        "duration": 5.43,
        "text": "learn right using the qualities of the"
      },
      {
        "start": 767.75,
        "duration": 6.36,
        "text": "wine can its vineyard be determined can"
      },
      {
        "start": 770.75,
        "duration": 5.01,
        "text": "we find its cluster so these are just"
      },
      {
        "start": 774.11,
        "duration": 4.14,
        "text": "some Python packages that I need to"
      },
      {
        "start": 775.76,
        "duration": 3.78,
        "text": "import so I'm gonna do that and this"
      },
      {
        "start": 778.25,
        "duration": 3.0,
        "text": "here is something just a little funky"
      },
      {
        "start": 779.54,
        "duration": 4.02,
        "text": "it's essentially when we're gonna be"
      },
      {
        "start": 781.25,
        "duration": 4.71,
        "text": "using some scatter plots to look at the"
      },
      {
        "start": 783.56,
        "duration": 3.99,
        "text": "various clusters and so what you can do"
      },
      {
        "start": 785.96,
        "duration": 2.97,
        "text": "is you can you can create a scatter plot"
      },
      {
        "start": 787.55,
        "duration": 3.0,
        "text": "but you have to actually tell it to"
      },
      {
        "start": 788.93,
        "duration": 1.95,
        "text": "display and that's what that's doing"
      },
      {
        "start": 790.55,
        "duration": 3.33,
        "text": "there"
      },
      {
        "start": 790.88,
        "duration": 6.15,
        "text": "so let's run these and this is just a"
      },
      {
        "start": 793.88,
        "duration": 5.8,
        "text": "nice function to show just like our data"
      },
      {
        "start": 797.03,
        "duration": 5.33,
        "text": "frame in a nice pretty format"
      },
      {
        "start": 799.68,
        "duration": 4.27,
        "text": "all right so first what we're gonna do"
      },
      {
        "start": 802.36,
        "duration": 3.54,
        "text": "is we're gonna take that data set we're"
      },
      {
        "start": 803.95,
        "duration": 3.12,
        "text": "gonna load it into a table but before we"
      },
      {
        "start": 805.9,
        "duration": 4.32,
        "text": "do that we need to connect to our"
      },
      {
        "start": 807.07,
        "duration": 4.68,
        "text": "cluster and we're gonna create a key"
      },
      {
        "start": 810.22,
        "duration": 3.6,
        "text": "space own Apache Cassandra or data"
      },
      {
        "start": 811.75,
        "duration": 5.55,
        "text": "stacks instead of like a schema or"
      },
      {
        "start": 813.82,
        "duration": 4.95,
        "text": "database it has a key space and so I'm"
      },
      {
        "start": 817.3,
        "duration": 3.12,
        "text": "just gonna create it here if it doesn't"
      },
      {
        "start": 818.77,
        "duration": 4.38,
        "text": "already exist and it's called wine and"
      },
      {
        "start": 820.42,
        "duration": 3.96,
        "text": "chocolate so in this case don't really"
      },
      {
        "start": 823.15,
        "duration": 3.63,
        "text": "worry too much about this but this is"
      },
      {
        "start": 824.38,
        "duration": 3.99,
        "text": "just our replication strategy so we're"
      },
      {
        "start": 826.78,
        "duration": 4.05,
        "text": "just gonna do a simple strategy and with"
      },
      {
        "start": 828.37,
        "duration": 3.75,
        "text": "a replication factor of 1 because I just"
      },
      {
        "start": 830.83,
        "duration": 4.59,
        "text": "have a single node here on my laptop"
      },
      {
        "start": 832.12,
        "duration": 4.71,
        "text": "right it's not a cluster but this is"
      },
      {
        "start": 835.42,
        "duration": 3.39,
        "text": "what I was talking about before this is"
      },
      {
        "start": 836.83,
        "duration": 5.1,
        "text": "if I had a proper cluster I'd set it to"
      },
      {
        "start": 838.81,
        "duration": 5.22,
        "text": "3 so I'm just gonna set the key space so"
      },
      {
        "start": 841.93,
        "duration": 4.14,
        "text": "I don't have to worry about that anymore"
      },
      {
        "start": 844.03,
        "duration": 4.68,
        "text": "and then we're gonna create a table"
      },
      {
        "start": 846.07,
        "duration": 5.13,
        "text": "called wine so I don't want to get too"
      },
      {
        "start": 848.71,
        "duration": 5.76,
        "text": "much into Cassandra data modeling but we"
      },
      {
        "start": 851.2,
        "duration": 4.56,
        "text": "do need a primary key so this primary"
      },
      {
        "start": 854.47,
        "duration": 4.14,
        "text": "key is how our data is actually going to"
      },
      {
        "start": 855.76,
        "duration": 4.92,
        "text": "be distributed across the cluster and so"
      },
      {
        "start": 858.61,
        "duration": 5.34,
        "text": "what I'm gonna do it needs to be unique"
      },
      {
        "start": 860.68,
        "duration": 6.0,
        "text": "so I'm just gonna generate an ID and I'm"
      },
      {
        "start": 863.95,
        "duration": 4.77,
        "text": "gonna use that to partition my data so"
      },
      {
        "start": 866.68,
        "duration": 5.16,
        "text": "I'm just gonna create an ID called wine"
      },
      {
        "start": 868.72,
        "duration": 5.1,
        "text": "ID so this will result in an even"
      },
      {
        "start": 871.84,
        "duration": 3.78,
        "text": "distribution of the data but you'll have"
      },
      {
        "start": 873.82,
        "duration": 4.23,
        "text": "to utilize the primary key when you're"
      },
      {
        "start": 875.62,
        "duration": 3.42,
        "text": "using doing your where Clause but again"
      },
      {
        "start": 878.05,
        "duration": 2.7,
        "text": "we don't want to talk too much about"
      },
      {
        "start": 879.04,
        "duration": 3.75,
        "text": "data modeling so I'll just show you that"
      },
      {
        "start": 880.75,
        "duration": 4.47,
        "text": "below but so I'm basically I'm just"
      },
      {
        "start": 882.79,
        "duration": 5.43,
        "text": "creating the table off of all these"
      },
      {
        "start": 885.22,
        "duration": 4.98,
        "text": "columns here I'll run that so what you"
      },
      {
        "start": 888.22,
        "duration": 3.63,
        "text": "think so there's 15 columns here of the"
      },
      {
        "start": 890.2,
        "duration": 3.84,
        "text": "different attributes and each one of"
      },
      {
        "start": 891.85,
        "duration": 4.02,
        "text": "these wines so I don't want to go over"
      },
      {
        "start": 894.04,
        "duration": 4.05,
        "text": "each one of them because it's a lot of"
      },
      {
        "start": 895.87,
        "duration": 4.05,
        "text": "things it's to talk about but"
      },
      {
        "start": 898.09,
        "duration": 4.11,
        "text": "essentially we have our wine ID which we"
      },
      {
        "start": 899.92,
        "duration": 4.32,
        "text": "just created randomly it's just a random"
      },
      {
        "start": 902.2,
        "duration": 3.63,
        "text": "number then we have our cluster this"
      },
      {
        "start": 904.24,
        "duration": 2.55,
        "text": "actually remember we've been talking"
      },
      {
        "start": 905.83,
        "duration": 3.53,
        "text": "about with K means that it's an"
      },
      {
        "start": 906.79,
        "duration": 6.09,
        "text": "unsupervised learning well this actually"
      },
      {
        "start": 909.36,
        "duration": 6.7,
        "text": "we actually do have the labels it is it"
      },
      {
        "start": 912.88,
        "duration": 4.17,
        "text": "is a supervised learning so because of"
      },
      {
        "start": 916.06,
        "duration": 2.4,
        "text": "that we can actually do some nice"
      },
      {
        "start": 917.05,
        "duration": 3.0,
        "text": "comparisons to see if we're actually"
      },
      {
        "start": 918.46,
        "duration": 5.19,
        "text": "getting kind of the results we were"
      },
      {
        "start": 920.05,
        "duration": 5.22,
        "text": "expecting so we're we're not cheating"
      },
      {
        "start": 923.65,
        "duration": 3.06,
        "text": "because you may this very well may"
      },
      {
        "start": 925.27,
        "duration": 3.36,
        "text": "happen to you when you have this type of"
      },
      {
        "start": 926.71,
        "duration": 4.68,
        "text": "data set but it's just interesting to"
      },
      {
        "start": 928.63,
        "duration": 3.77,
        "text": "see so here we have basically three"
      },
      {
        "start": 931.39,
        "duration": 4.01,
        "text": "vineyard"
      },
      {
        "start": 932.4,
        "duration": 4.86,
        "text": "we have vineyard one two and three and"
      },
      {
        "start": 935.4,
        "duration": 3.6,
        "text": "it's like I said it's already been later"
      },
      {
        "start": 937.26,
        "duration": 3.54,
        "text": "both in the data set then we have like"
      },
      {
        "start": 939.0,
        "duration": 4.26,
        "text": "alcohol content"
      },
      {
        "start": 940.8,
        "duration": 3.96,
        "text": "malic acid things like that things that"
      },
      {
        "start": 943.26,
        "duration": 4.4,
        "text": "are making up the properties of the wine"
      },
      {
        "start": 944.76,
        "duration": 4.35,
        "text": "of the plant that grows in that vineyard"
      },
      {
        "start": 947.66,
        "duration": 2.98,
        "text": "all right"
      },
      {
        "start": 949.11,
        "duration": 3.84,
        "text": "so then we're gonna actually load that"
      },
      {
        "start": 950.64,
        "duration": 3.66,
        "text": "from the CSV which was so nice in the"
      },
      {
        "start": 952.95,
        "duration": 3.57,
        "text": "CSV file I didn't require any"
      },
      {
        "start": 954.3,
        "duration": 3.63,
        "text": "pre-processing I was just able to load"
      },
      {
        "start": 956.52,
        "duration": 2.94,
        "text": "it just straight away"
      },
      {
        "start": 957.93,
        "duration": 3.63,
        "text": "I'm just iterating through and doing an"
      },
      {
        "start": 959.46,
        "duration": 3.78,
        "text": "insert I could have also like used like"
      },
      {
        "start": 961.56,
        "duration": 3.18,
        "text": "a bulk loader or something like that but"
      },
      {
        "start": 963.24,
        "duration": 3.45,
        "text": "just for the sake of this because"
      },
      {
        "start": 964.74,
        "duration": 5.849,
        "text": "there's a small data set I just loop"
      },
      {
        "start": 966.69,
        "duration": 5.07,
        "text": "through and loaded it alright so then"
      },
      {
        "start": 970.589,
        "duration": 2.641,
        "text": "we're gonna do a select star just to"
      },
      {
        "start": 971.76,
        "duration": 2.85,
        "text": "ensure that we actually loaded the data"
      },
      {
        "start": 973.23,
        "duration": 3.12,
        "text": "this is what I was talking about before"
      },
      {
        "start": 974.61,
        "duration": 3.87,
        "text": "we're waiting to utilize that where"
      },
      {
        "start": 976.35,
        "duration": 3.66,
        "text": "clause so I just picked a random ID that"
      },
      {
        "start": 978.48,
        "duration": 4.95,
        "text": "I know I had generated to make sure that"
      },
      {
        "start": 980.01,
        "duration": 4.86,
        "text": "it was there and it is alright so now"
      },
      {
        "start": 983.43,
        "duration": 3.81,
        "text": "we're actually time for a patchy spark"
      },
      {
        "start": 984.87,
        "duration": 4.44,
        "text": "so what we need to do again because I'm"
      },
      {
        "start": 987.24,
        "duration": 4.47,
        "text": "using data stacks analytics with a two"
      },
      {
        "start": 989.31,
        "duration": 5.19,
        "text": "are co-located I can just build this I"
      },
      {
        "start": 991.71,
        "duration": 4.29,
        "text": "can get a spark session here and then"
      },
      {
        "start": 994.5,
        "duration": 3.93,
        "text": "from there just using this one line of"
      },
      {
        "start": 996.0,
        "duration": 4.74,
        "text": "code I can load all that data that's in"
      },
      {
        "start": 998.43,
        "duration": 4.95,
        "text": "my table and Cassandra and actually load"
      },
      {
        "start": 1000.74,
        "duration": 5.73,
        "text": "it into a spark data frame and so that's"
      },
      {
        "start": 1003.38,
        "duration": 5.1,
        "text": "all this line here is doing so once I do"
      },
      {
        "start": 1006.47,
        "duration": 4.14,
        "text": "that I can just do a count on the table"
      },
      {
        "start": 1008.48,
        "duration": 5.01,
        "text": "to see that all of my rows for my table"
      },
      {
        "start": 1010.61,
        "duration": 4.169,
        "text": "went into my data frame and they do I"
      },
      {
        "start": 1013.49,
        "duration": 4.289,
        "text": "have a hundred and seventy eight rows"
      },
      {
        "start": 1014.779,
        "duration": 4.741,
        "text": "and so I can just do a show on that wine"
      },
      {
        "start": 1017.779,
        "duration": 3.721,
        "text": "table and again we're just kind of"
      },
      {
        "start": 1019.52,
        "duration": 3.21,
        "text": "saying again that wine ID on all these"
      },
      {
        "start": 1021.5,
        "duration": 4.35,
        "text": "properties and then I just want to"
      },
      {
        "start": 1022.73,
        "duration": 7.109,
        "text": "highlight this is the cluster here by"
      },
      {
        "start": 1025.85,
        "duration": 5.93,
        "text": "the vineyard so let's visualize let's"
      },
      {
        "start": 1029.839,
        "duration": 4.85,
        "text": "visualize this data with a scatter plot"
      },
      {
        "start": 1031.78,
        "duration": 5.44,
        "text": "so in this case what we're gonna do our"
      },
      {
        "start": 1034.689,
        "duration": 5.801,
        "text": "x-axis because we're trying to find a"
      },
      {
        "start": 1037.22,
        "duration": 4.56,
        "text": "unique data point for each wine right so"
      },
      {
        "start": 1040.49,
        "duration": 3.81,
        "text": "in this case we're gonna use on the"
      },
      {
        "start": 1041.78,
        "duration": 5.13,
        "text": "x-axis the alcohol content and the"
      },
      {
        "start": 1044.3,
        "duration": 4.2,
        "text": "protein on the y-axis you say why are we"
      },
      {
        "start": 1046.91,
        "duration": 3.63,
        "text": "doing that we're just doing that to make"
      },
      {
        "start": 1048.5,
        "duration": 4.35,
        "text": "sure that each point in each wine is"
      },
      {
        "start": 1050.54,
        "duration": 3.72,
        "text": "unique so these are just two easy ones"
      },
      {
        "start": 1052.85,
        "duration": 3.09,
        "text": "to make sure it's unique but we probably"
      },
      {
        "start": 1054.26,
        "duration": 4.68,
        "text": "could have chosen other properties as"
      },
      {
        "start": 1055.94,
        "duration": 5.82,
        "text": "well and the color of the dot will be"
      },
      {
        "start": 1058.94,
        "duration": 4.59,
        "text": "assigned based on if it's a klutz on the"
      },
      {
        "start": 1061.76,
        "duration": 3.029,
        "text": "cluster that it belongs to basically the"
      },
      {
        "start": 1063.53,
        "duration": 3.07,
        "text": "vineyard"
      },
      {
        "start": 1064.789,
        "duration": 3.821,
        "text": "so and we're just trying to make sure"
      },
      {
        "start": 1066.6,
        "duration": 5.55,
        "text": "that don't overlap so let's create this"
      },
      {
        "start": 1068.61,
        "duration": 5.73,
        "text": "scatterplot here okay cool so straight"
      },
      {
        "start": 1072.15,
        "duration": 4.889,
        "text": "away we're seeing three clusters just"
      },
      {
        "start": 1074.34,
        "duration": 5.43,
        "text": "like we imagined right so here in blue"
      },
      {
        "start": 1077.039,
        "duration": 5.611,
        "text": "is vineyard one here in green and I"
      },
      {
        "start": 1079.77,
        "duration": 4.139,
        "text": "apologize to anybody if we have any"
      },
      {
        "start": 1082.65,
        "duration": 2.94,
        "text": "issues with seeing the difference"
      },
      {
        "start": 1083.909,
        "duration": 4.38,
        "text": "between a red and green because this is"
      },
      {
        "start": 1085.59,
        "duration": 5.88,
        "text": "kind of difficult to see but cluster two"
      },
      {
        "start": 1088.289,
        "duration": 5.701,
        "text": "is green here and then vineyard three is"
      },
      {
        "start": 1091.47,
        "duration": 6.089,
        "text": "in red here here at the bottom sure"
      },
      {
        "start": 1093.99,
        "duration": 6.0,
        "text": "seeing those three clusters all right so"
      },
      {
        "start": 1097.559,
        "duration": 4.201,
        "text": "let's actually see if k-means will give"
      },
      {
        "start": 1099.99,
        "duration": 4.799,
        "text": "us the same output that's what we're"
      },
      {
        "start": 1101.76,
        "duration": 4.98,
        "text": "looking for right so again it's an"
      },
      {
        "start": 1104.789,
        "duration": 4.77,
        "text": "unsupervised learning algorithm and it's"
      },
      {
        "start": 1106.74,
        "duration": 5.28,
        "text": "used to solve clustering problems so"
      },
      {
        "start": 1109.559,
        "duration": 6.151,
        "text": "what we need to do with k-means it needs"
      },
      {
        "start": 1112.02,
        "duration": 5.85,
        "text": "to have your your columns the elements"
      },
      {
        "start": 1115.71,
        "duration": 3.81,
        "text": "in the row you have when you're using it"
      },
      {
        "start": 1117.87,
        "duration": 3.72,
        "text": "with apache spark you have to make it"
      },
      {
        "start": 1119.52,
        "duration": 4.74,
        "text": "into a vector so we're gonna assemble"
      },
      {
        "start": 1121.59,
        "duration": 4.589,
        "text": "our vector here and in this case i'm"
      },
      {
        "start": 1124.26,
        "duration": 4.26,
        "text": "just gonna add all of the columns that"
      },
      {
        "start": 1126.179,
        "duration": 4.651,
        "text": "were available to me except i'm gonna"
      },
      {
        "start": 1128.52,
        "duration": 4.05,
        "text": "remove the cluster the cluster column"
      },
      {
        "start": 1130.83,
        "duration": 3.36,
        "text": "right we don't want to it doesn't mean a"
      },
      {
        "start": 1132.57,
        "duration": 4.08,
        "text": "whole lot and we don't want it swaying"
      },
      {
        "start": 1134.19,
        "duration": 5.219,
        "text": "anything in the data or in the result of"
      },
      {
        "start": 1136.65,
        "duration": 4.019,
        "text": "the data and then I removed the wine ID"
      },
      {
        "start": 1139.409,
        "duration": 2.611,
        "text": "because that's just something I"
      },
      {
        "start": 1140.669,
        "duration": 5.521,
        "text": "generated it doesn't actually have any"
      },
      {
        "start": 1142.02,
        "duration": 5.97,
        "text": "properties with the wine so then we'll"
      },
      {
        "start": 1146.19,
        "duration": 5.67,
        "text": "just create a new data frame based on it"
      },
      {
        "start": 1147.99,
        "duration": 4.74,
        "text": "now being a vector so then here this is"
      },
      {
        "start": 1151.86,
        "duration": 3.12,
        "text": "not actually where we're gonna start"
      },
      {
        "start": 1152.73,
        "duration": 5.4,
        "text": "doing the k-means and building our model"
      },
      {
        "start": 1154.98,
        "duration": 4.65,
        "text": "so we're gonna set the k here remember"
      },
      {
        "start": 1158.13,
        "duration": 4.74,
        "text": "we talked about the number of clusters"
      },
      {
        "start": 1159.63,
        "duration": 5.13,
        "text": "that we wanted to create 2/3 and then"
      },
      {
        "start": 1162.87,
        "duration": 5.07,
        "text": "we're gonna fit the data and generate"
      },
      {
        "start": 1164.76,
        "duration": 5.159,
        "text": "our model so one of the downsides of"
      },
      {
        "start": 1167.94,
        "duration": 3.51,
        "text": "k-means even though i mean look how easy"
      },
      {
        "start": 1169.919,
        "duration": 3.24,
        "text": "this is it's just two lines right at"
      },
      {
        "start": 1171.45,
        "duration": 3.39,
        "text": "building this model one of the downsides"
      },
      {
        "start": 1173.159,
        "duration": 4.171,
        "text": "is when you you have to set that"
      },
      {
        "start": 1174.84,
        "duration": 4.92,
        "text": "clustering in advance and because you do"
      },
      {
        "start": 1177.33,
        "duration": 4.5,
        "text": "that k-means is happy to just distribute"
      },
      {
        "start": 1179.76,
        "duration": 3.84,
        "text": "your data just like he told it so if i"
      },
      {
        "start": 1181.83,
        "duration": 5.03,
        "text": "said separate this data into six"
      },
      {
        "start": 1183.6,
        "duration": 5.91,
        "text": "clusters it'll do that even though i"
      },
      {
        "start": 1186.86,
        "duration": 4.0,
        "text": "know because right this is just a"
      },
      {
        "start": 1189.51,
        "duration": 4.08,
        "text": "pretend dataset that there's really only"
      },
      {
        "start": 1190.86,
        "duration": 4.289,
        "text": "three but it'll happily do that so you"
      },
      {
        "start": 1193.59,
        "duration": 3.41,
        "text": "just again data science is an iterative"
      },
      {
        "start": 1195.149,
        "duration": 3.321,
        "text": "process right"
      },
      {
        "start": 1197.0,
        "duration": 3.409,
        "text": "so you just keep going back through"
      },
      {
        "start": 1198.47,
        "duration": 4.77,
        "text": "until your results are making sense so"
      },
      {
        "start": 1200.409,
        "duration": 5.051,
        "text": "then we're just gonna transform our"
      },
      {
        "start": 1203.24,
        "duration": 4.89,
        "text": "training data set with our model and get"
      },
      {
        "start": 1205.46,
        "duration": 3.839,
        "text": "our predictions so here I can just show"
      },
      {
        "start": 1208.13,
        "duration": 5.22,
        "text": "some of the predictions let me run this"
      },
      {
        "start": 1209.299,
        "duration": 6.87,
        "text": "again okay so here we see our original"
      },
      {
        "start": 1213.35,
        "duration": 4.86,
        "text": "cluster and over here we see our"
      },
      {
        "start": 1216.169,
        "duration": 5.13,
        "text": "prediction so like in this case in this"
      },
      {
        "start": 1218.21,
        "duration": 5.699,
        "text": "first row we see cluster 3 but then"
      },
      {
        "start": 1221.299,
        "duration": 5.25,
        "text": "prediction 0 now don't be alarmed this"
      },
      {
        "start": 1223.909,
        "duration": 5.041,
        "text": "may this may be wrong and and wrongly"
      },
      {
        "start": 1226.549,
        "duration": 4.051,
        "text": "classified or like it doesn't the"
      },
      {
        "start": 1228.95,
        "duration": 4.74,
        "text": "prediction has no idea what the labels"
      },
      {
        "start": 1230.6,
        "duration": 5.55,
        "text": "are actually called so 3 could be 0 3"
      },
      {
        "start": 1233.69,
        "duration": 5.34,
        "text": "could be 2 it's a little bit confusing"
      },
      {
        "start": 1236.15,
        "duration": 6.3,
        "text": "but nevertheless so what we can do here"
      },
      {
        "start": 1239.03,
        "duration": 4.74,
        "text": "is just try to get like an eyeball on it"
      },
      {
        "start": 1242.45,
        "duration": 3.27,
        "text": "there's a couple different ways that you"
      },
      {
        "start": 1243.77,
        "duration": 4.47,
        "text": "can try to verify this when you do have"
      },
      {
        "start": 1245.72,
        "duration": 4.74,
        "text": "a supervised learning situation like"
      },
      {
        "start": 1248.24,
        "duration": 3.99,
        "text": "here you can do a confusion matrix or"
      },
      {
        "start": 1250.46,
        "duration": 3.27,
        "text": "are matching matrix in this case I'm"
      },
      {
        "start": 1252.23,
        "duration": 3.9,
        "text": "just gonna to do different things I'm"
      },
      {
        "start": 1253.73,
        "duration": 4.14,
        "text": "just gonna do a count on each one of the"
      },
      {
        "start": 1256.13,
        "duration": 3.69,
        "text": "predictions versus our original cluster"
      },
      {
        "start": 1257.87,
        "duration": 3.9,
        "text": "see if they kind of line up just by just"
      },
      {
        "start": 1259.82,
        "duration": 3.81,
        "text": "quick eyeball right it's just a demo"
      },
      {
        "start": 1261.77,
        "duration": 3.24,
        "text": "right so let's just quickly eyeball it"
      },
      {
        "start": 1263.63,
        "duration": 3.15,
        "text": "see if it looks the same then maybe we"
      },
      {
        "start": 1265.01,
        "duration": 3.779,
        "text": "got good results and then I'm gonna do"
      },
      {
        "start": 1266.78,
        "duration": 3.57,
        "text": "another scatter plot so then we can just"
      },
      {
        "start": 1268.789,
        "duration": 4.111,
        "text": "quickly see if it looks like our"
      },
      {
        "start": 1270.35,
        "duration": 4.92,
        "text": "original scatter plot so if we do a"
      },
      {
        "start": 1272.9,
        "duration": 4.409,
        "text": "group of we do a count here actually the"
      },
      {
        "start": 1275.27,
        "duration": 3.45,
        "text": "numbers are not looking not looking too"
      },
      {
        "start": 1277.309,
        "duration": 3.331,
        "text": "bad the playing it's lining up pretty"
      },
      {
        "start": 1278.72,
        "duration": 6.209,
        "text": "nicely but let's look at our scatter"
      },
      {
        "start": 1280.64,
        "duration": 6.36,
        "text": "plot and see if it's the same ok so in"
      },
      {
        "start": 1284.929,
        "duration": 4.711,
        "text": "this case we're seeing here in the green"
      },
      {
        "start": 1287.0,
        "duration": 4.98,
        "text": "which it labeled 1 which in our case was"
      },
      {
        "start": 1289.64,
        "duration": 4.769,
        "text": "vineyard 1 so they happen to have the"
      },
      {
        "start": 1291.98,
        "duration": 4.5,
        "text": "same label it seems to classify that"
      },
      {
        "start": 1294.409,
        "duration": 4.921,
        "text": "pretty well seems like it's able to"
      },
      {
        "start": 1296.48,
        "duration": 5.4,
        "text": "determine clusters from vineyard 1 the"
      },
      {
        "start": 1299.33,
        "duration": 5.459,
        "text": "wines there pretty easily but here on"
      },
      {
        "start": 1301.88,
        "duration": 3.99,
        "text": "this vineyard 2 & 3 it's kind of"
      },
      {
        "start": 1304.789,
        "duration": 2.52,
        "text": "struggling I don't know if you can see"
      },
      {
        "start": 1305.87,
        "duration": 6.27,
        "text": "that so well in the back but it's kind"
      },
      {
        "start": 1307.309,
        "duration": 7.171,
        "text": "of struggling there so again one of the"
      },
      {
        "start": 1312.14,
        "duration": 4.35,
        "text": "downsides of k-means is that the more"
      },
      {
        "start": 1314.48,
        "duration": 4.59,
        "text": "variables that you add the more"
      },
      {
        "start": 1316.49,
        "duration": 6.12,
        "text": "difficulty it has in actually"
      },
      {
        "start": 1319.07,
        "duration": 4.859,
        "text": "determining the clusters so let's see"
      },
      {
        "start": 1322.61,
        "duration": 3.059,
        "text": "how we are doing on time I've run"
      },
      {
        "start": 1323.929,
        "duration": 4.291,
        "text": "through this a couple of times but"
      },
      {
        "start": 1325.669,
        "duration": 4.45,
        "text": "basically essentially what I'm doing is"
      },
      {
        "start": 1328.22,
        "duration": 3.189,
        "text": "I'm starting to strip away"
      },
      {
        "start": 1330.119,
        "duration": 3.99,
        "text": "things that I don't think are"
      },
      {
        "start": 1331.409,
        "duration": 4.77,
        "text": "necessarily going to add to the"
      },
      {
        "start": 1334.109,
        "duration": 4.22,
        "text": "clustering right so maybe the alcohol"
      },
      {
        "start": 1336.179,
        "duration": 5.13,
        "text": "content maybe that's not going to affect"
      },
      {
        "start": 1338.329,
        "duration": 6.25,
        "text": "the wine in which plant it comes from or"
      },
      {
        "start": 1341.309,
        "duration": 4.74,
        "text": "maybe the ash or the color so I'm going"
      },
      {
        "start": 1344.579,
        "duration": 3.24,
        "text": "to remove those and then run it again"
      },
      {
        "start": 1346.049,
        "duration": 3.3,
        "text": "and see if that effects it"
      },
      {
        "start": 1347.819,
        "duration": 4.29,
        "text": "so just because I want to make sure that"
      },
      {
        "start": 1349.349,
        "duration": 5.88,
        "text": "we can get to our next demo I'm gonna"
      },
      {
        "start": 1352.109,
        "duration": 6.03,
        "text": "skip I did two of these here but the"
      },
      {
        "start": 1355.229,
        "duration": 4.53,
        "text": "results are didn't work so great so"
      },
      {
        "start": 1358.139,
        "duration": 7.64,
        "text": "let's move on to the last one here"
      },
      {
        "start": 1359.759,
        "duration": 6.02,
        "text": "because that was kind of the winner so"
      },
      {
        "start": 1368.59,
        "duration": 5.779,
        "text": "[Music]"
      },
      {
        "start": 1370.699,
        "duration": 4.93,
        "text": "okay so the very last one here I decided"
      },
      {
        "start": 1374.369,
        "duration": 2.73,
        "text": "to do and actually it's kind of funny"
      },
      {
        "start": 1375.629,
        "duration": 3.36,
        "text": "because I got that first set of results"
      },
      {
        "start": 1377.099,
        "duration": 3.21,
        "text": "I mean this one I was really doing this"
      },
      {
        "start": 1378.989,
        "duration": 2.82,
        "text": "I got that first set a result they"
      },
      {
        "start": 1380.309,
        "duration": 2.76,
        "text": "didn't look so great so that's that oh"
      },
      {
        "start": 1381.809,
        "duration": 3.0,
        "text": "you know what I'm gonna remove it all"
      },
      {
        "start": 1383.069,
        "duration": 3.3,
        "text": "the way down to just two variables I'm"
      },
      {
        "start": 1384.809,
        "duration": 2.91,
        "text": "gonna remove it to the sub variables"
      },
      {
        "start": 1386.369,
        "duration": 3.84,
        "text": "that we're actually using in the scatter"
      },
      {
        "start": 1387.719,
        "duration": 5.28,
        "text": "plot let's see what that will do well"
      },
      {
        "start": 1390.209,
        "duration": 4.8,
        "text": "that was actually our winner so if I"
      },
      {
        "start": 1392.999,
        "duration": 4.8,
        "text": "just use the alcohol content in the"
      },
      {
        "start": 1395.009,
        "duration": 6.27,
        "text": "protein in the wine and I put that into"
      },
      {
        "start": 1397.799,
        "duration": 6.35,
        "text": "my object I do a vector of that then I"
      },
      {
        "start": 1401.279,
        "duration": 7.92,
        "text": "train my model and do my predictions"
      },
      {
        "start": 1404.149,
        "duration": 6.64,
        "text": "okay and then I do my counts okay my"
      },
      {
        "start": 1409.199,
        "duration": 3.54,
        "text": "counts are lining up pretty well you"
      },
      {
        "start": 1410.789,
        "duration": 6.96,
        "text": "just kind of eyeball it and then I do my"
      },
      {
        "start": 1412.739,
        "duration": 7.4,
        "text": "scatter plot and boom straight away you"
      },
      {
        "start": 1417.749,
        "duration": 4.68,
        "text": "can see it's clustering it perfectly"
      },
      {
        "start": 1420.139,
        "duration": 4.36,
        "text": "just like how we originally had in our"
      },
      {
        "start": 1422.429,
        "duration": 4.23,
        "text": "original clusters so that was it the"
      },
      {
        "start": 1424.499,
        "duration": 4.14,
        "text": "wine the alcohol content and the protein"
      },
      {
        "start": 1426.659,
        "duration": 4.44,
        "text": "content are really the elements that"
      },
      {
        "start": 1428.639,
        "duration": 4.95,
        "text": "make it you can shoot know which wine"
      },
      {
        "start": 1431.099,
        "duration": 4.02,
        "text": "belongs to which vineyard so again just"
      },
      {
        "start": 1433.589,
        "duration": 3.3,
        "text": "kind of a reminder that data science"
      },
      {
        "start": 1435.119,
        "duration": 3.93,
        "text": "again it's an its analytics it's a"
      },
      {
        "start": 1436.889,
        "duration": 3.99,
        "text": "science it's an iterative process it's"
      },
      {
        "start": 1439.049,
        "duration": 5.67,
        "text": "all about hypothesis testing and"
      },
      {
        "start": 1440.879,
        "duration": 7.41,
        "text": "analysis and so let's talk about"
      },
      {
        "start": 1444.719,
        "duration": 5.1,
        "text": "classification so classification again"
      },
      {
        "start": 1448.289,
        "duration": 3.33,
        "text": "just like clustering it's pretty"
      },
      {
        "start": 1449.819,
        "duration": 4.59,
        "text": "straightforward of what we're trying to"
      },
      {
        "start": 1451.619,
        "duration": 6.06,
        "text": "do here we're trying to identify a set"
      },
      {
        "start": 1454.409,
        "duration": 5.58,
        "text": "of categories so if we train our dataset"
      },
      {
        "start": 1457.679,
        "duration": 4.62,
        "text": "on some known categories and how they're"
      },
      {
        "start": 1459.989,
        "duration": 3.601,
        "text": "labeled that when we have new ones we"
      },
      {
        "start": 1462.299,
        "duration": 2.821,
        "text": "can also"
      },
      {
        "start": 1463.59,
        "duration": 5.61,
        "text": "where we gonna be able to predict those"
      },
      {
        "start": 1465.12,
        "duration": 6.149,
        "text": "based on the training that we've done so"
      },
      {
        "start": 1469.2,
        "duration": 4.41,
        "text": "a good example of like classification"
      },
      {
        "start": 1471.269,
        "duration": 5.691,
        "text": "would be like in our spam in our in our"
      },
      {
        "start": 1473.61,
        "duration": 6.21,
        "text": "email inboxes right spam versus not spam"
      },
      {
        "start": 1476.96,
        "duration": 5.289,
        "text": "so it's basically a pattern recognition"
      },
      {
        "start": 1479.82,
        "duration": 5.459,
        "text": "right so sorry there's a lot of text on"
      },
      {
        "start": 1482.249,
        "duration": 5.221,
        "text": "this but nave Bayes or naive Bayes I"
      },
      {
        "start": 1485.279,
        "duration": 5.76,
        "text": "always say wrong is a simple technique"
      },
      {
        "start": 1487.47,
        "duration": 5.579,
        "text": "for constructing classifiers so you're"
      },
      {
        "start": 1491.039,
        "duration": 5.34,
        "text": "gonna you're gonna generate a model that"
      },
      {
        "start": 1493.049,
        "duration": 5.13,
        "text": "assigns class labels to various problem"
      },
      {
        "start": 1496.379,
        "duration": 3.54,
        "text": "instances that's kind of some fancy"
      },
      {
        "start": 1498.179,
        "duration": 4.74,
        "text": "language for in our case the rows of"
      },
      {
        "start": 1499.919,
        "duration": 4.321,
        "text": "data right so there's actually many"
      },
      {
        "start": 1502.919,
        "duration": 4.47,
        "text": "different algorithms that have been"
      },
      {
        "start": 1504.24,
        "duration": 5.009,
        "text": "implemented for naive Bayes but they all"
      },
      {
        "start": 1507.389,
        "duration": 4.17,
        "text": "assume kind of the same set of things"
      },
      {
        "start": 1509.249,
        "duration": 4.711,
        "text": "which is the value of each particular"
      },
      {
        "start": 1511.559,
        "duration": 4.141,
        "text": "instance feature right so in our case"
      },
      {
        "start": 1513.96,
        "duration": 3.26,
        "text": "like the columns in our row of our of"
      },
      {
        "start": 1515.7,
        "duration": 6.449,
        "text": "our item that we're trying to classify"
      },
      {
        "start": 1517.22,
        "duration": 6.73,
        "text": "are all independent okay so now we're"
      },
      {
        "start": 1522.149,
        "duration": 4.77,
        "text": "gonna go to our second demo which is"
      },
      {
        "start": 1523.95,
        "duration": 5.01,
        "text": "naive Bayes and chocolate so can I use a"
      },
      {
        "start": 1526.919,
        "duration": 5.071,
        "text": "classification machine learning"
      },
      {
        "start": 1528.96,
        "duration": 7.02,
        "text": "algorithms to find which country a candy"
      },
      {
        "start": 1531.99,
        "duration": 7.769,
        "text": "bar comes from that's the end ok let's"
      },
      {
        "start": 1535.98,
        "duration": 6.51,
        "text": "switch to that now ok so we're gonna do"
      },
      {
        "start": 1539.759,
        "duration": 3.811,
        "text": "the same thing same kind of thing so I'm"
      },
      {
        "start": 1542.49,
        "duration": 5.61,
        "text": "gonna go walk through this one a little"
      },
      {
        "start": 1543.57,
        "duration": 6.15,
        "text": "bit more quickly and so again if I have"
      },
      {
        "start": 1548.1,
        "duration": 3.39,
        "text": "some information about a chocolate bar"
      },
      {
        "start": 1549.72,
        "duration": 4.35,
        "text": "can I predict which country the data"
      },
      {
        "start": 1551.49,
        "duration": 5.689,
        "text": "that the chocolate came from so again"
      },
      {
        "start": 1554.07,
        "duration": 7.679,
        "text": "this is a real data set I found it on"
      },
      {
        "start": 1557.179,
        "duration": 5.98,
        "text": "Kegel but also it's it again it's not"
      },
      {
        "start": 1561.749,
        "duration": 3.03,
        "text": "only it's a real data set that's out"
      },
      {
        "start": 1563.159,
        "duration": 2.941,
        "text": "there on on the internet but it's"
      },
      {
        "start": 1564.779,
        "duration": 3.811,
        "text": "actually real data"
      },
      {
        "start": 1566.1,
        "duration": 4.88,
        "text": "it's the Manhattan chocolate Society"
      },
      {
        "start": 1568.59,
        "duration": 5.939,
        "text": "they actually maintain this data set"
      },
      {
        "start": 1570.98,
        "duration": 5.769,
        "text": "yeah right who knew so I just have a"
      },
      {
        "start": 1574.529,
        "duration": 4.921,
        "text": "caveat here because this is an actual"
      },
      {
        "start": 1576.749,
        "duration": 5.731,
        "text": "real data set this is not a demo data"
      },
      {
        "start": 1579.45,
        "duration": 5.459,
        "text": "set the results from this data set are"
      },
      {
        "start": 1582.48,
        "duration": 4.139,
        "text": "not crystal clear right normally when I"
      },
      {
        "start": 1584.909,
        "duration": 3.84,
        "text": "do demos like this I generate my own"
      },
      {
        "start": 1586.619,
        "duration": 5.131,
        "text": "data so that way I can get these nice"
      },
      {
        "start": 1588.749,
        "duration": 5.04,
        "text": "you know 90% accuracy right it looks"
      },
      {
        "start": 1591.75,
        "duration": 3.509,
        "text": "real slick but this is actually real so"
      },
      {
        "start": 1593.789,
        "duration": 2.581,
        "text": "we're gonna see that when you're working"
      },
      {
        "start": 1595.259,
        "duration": 2.641,
        "text": "with real data sometimes you"
      },
      {
        "start": 1596.37,
        "duration": 4.71,
        "text": "get those crystal clear answers that"
      },
      {
        "start": 1597.9,
        "duration": 8.13,
        "text": "you're looking for okay so we're just"
      },
      {
        "start": 1601.08,
        "duration": 6.3,
        "text": "gonna do our imports here and again"
      },
      {
        "start": 1606.03,
        "duration": 2.76,
        "text": "we're using data sax analytics"
      },
      {
        "start": 1607.38,
        "duration": 3.45,
        "text": "underneath under the hood as our"
      },
      {
        "start": 1608.79,
        "duration": 6.269,
        "text": "back-end we're gonna connect to our"
      },
      {
        "start": 1610.83,
        "duration": 5.7,
        "text": "database create our key space and again"
      },
      {
        "start": 1615.059,
        "duration": 3.631,
        "text": "we're gonna create a table of our"
      },
      {
        "start": 1616.53,
        "duration": 4.08,
        "text": "chocolate table same same way we're"
      },
      {
        "start": 1618.69,
        "duration": 4.11,
        "text": "gonna partition the data and Cassandra"
      },
      {
        "start": 1620.61,
        "duration": 3.69,
        "text": "by our primary key again I'm just the"
      },
      {
        "start": 1622.8,
        "duration": 5.58,
        "text": "same kind of thing I'm just making a"
      },
      {
        "start": 1624.3,
        "duration": 6.03,
        "text": "chocolate I call it chocolate ID okay"
      },
      {
        "start": 1628.38,
        "duration": 4.08,
        "text": "and so in this case we have way less"
      },
      {
        "start": 1630.33,
        "duration": 4.62,
        "text": "columns that we had on the wine we have"
      },
      {
        "start": 1632.46,
        "duration": 4.26,
        "text": "our chocolate ID we have our company so"
      },
      {
        "start": 1634.95,
        "duration": 3.63,
        "text": "that's like the company the name that"
      },
      {
        "start": 1636.72,
        "duration": 4.29,
        "text": "actually produces this chocolate bar we"
      },
      {
        "start": 1638.58,
        "duration": 3.99,
        "text": "have the bar location like we're the"
      },
      {
        "start": 1641.01,
        "duration": 4.59,
        "text": "specific bean that created this"
      },
      {
        "start": 1642.57,
        "duration": 6.3,
        "text": "chocolate where originated we have this"
      },
      {
        "start": 1645.6,
        "duration": 5.22,
        "text": "re F ID it's not really important but"
      },
      {
        "start": 1648.87,
        "duration": 3.99,
        "text": "essentially what the higher number it"
      },
      {
        "start": 1650.82,
        "duration": 4.71,
        "text": "was the more recently the chocolate bar"
      },
      {
        "start": 1652.86,
        "duration": 5.75,
        "text": "had been rated then we have the review"
      },
      {
        "start": 1655.53,
        "duration": 6.21,
        "text": "date of Wynn had been rated the coke"
      },
      {
        "start": 1658.61,
        "duration": 7.66,
        "text": "cocoa percentage so the higher that"
      },
      {
        "start": 1661.74,
        "duration": 6.18,
        "text": "value right 70% cacao 60% right then we"
      },
      {
        "start": 1666.27,
        "duration": 2.82,
        "text": "have our company location that's gonna"
      },
      {
        "start": 1667.92,
        "duration": 3.36,
        "text": "be the key that's what we're actually"
      },
      {
        "start": 1669.09,
        "duration": 4.589,
        "text": "looking for as the manufacturers based"
      },
      {
        "start": 1671.28,
        "duration": 4.29,
        "text": "company then we have a rating on that"
      },
      {
        "start": 1673.679,
        "duration": 3.75,
        "text": "chocolate bar anywhere from one to five"
      },
      {
        "start": 1675.57,
        "duration": 4.68,
        "text": "five being you know it's an awesome"
      },
      {
        "start": 1677.429,
        "duration": 4.741,
        "text": "chocolate bar we have the bean type like"
      },
      {
        "start": 1680.25,
        "duration": 4.89,
        "text": "the actually variety of the breed of the"
      },
      {
        "start": 1682.17,
        "duration": 4.62,
        "text": "plant and the bean origin so again like"
      },
      {
        "start": 1685.14,
        "duration": 4.919,
        "text": "where it's on the geolocation of the"
      },
      {
        "start": 1686.79,
        "duration": 6.72,
        "text": "world right okay so now we're gonna load"
      },
      {
        "start": 1690.059,
        "duration": 5.941,
        "text": "this CSV file so in this case this was"
      },
      {
        "start": 1693.51,
        "duration": 4.83,
        "text": "not like the wine data set this data set"
      },
      {
        "start": 1696.0,
        "duration": 4.5,
        "text": "was actually very ugly and to do quite a"
      },
      {
        "start": 1698.34,
        "duration": 4.589,
        "text": "bit of cleaning removing a bunch of"
      },
      {
        "start": 1700.5,
        "duration": 5.94,
        "text": "things it was not pretty ly comma"
      },
      {
        "start": 1702.929,
        "duration": 4.711,
        "text": "separate eliminated right so if anyone's"
      },
      {
        "start": 1706.44,
        "duration": 3.48,
        "text": "interested in this data set I actually"
      },
      {
        "start": 1707.64,
        "duration": 5.039,
        "text": "do have it all cleaned up and able to"
      },
      {
        "start": 1709.92,
        "duration": 4.17,
        "text": "use and I have it on my github so I call"
      },
      {
        "start": 1712.679,
        "duration": 3.121,
        "text": "it chocolate final because it's the"
      },
      {
        "start": 1714.09,
        "duration": 3.24,
        "text": "final one that's finally clean and you"
      },
      {
        "start": 1715.8,
        "duration": 3.57,
        "text": "can even see here I'm still doing a"
      },
      {
        "start": 1717.33,
        "duration": 4.14,
        "text": "little bit of cleaning it's still not"
      },
      {
        "start": 1719.37,
        "duration": 4.14,
        "text": "perfect but right there's only so many"
      },
      {
        "start": 1721.47,
        "duration": 4.38,
        "text": "hours in the day so when I'm gonna just"
      },
      {
        "start": 1723.51,
        "duration": 4.38,
        "text": "loop through each row and then insert it"
      },
      {
        "start": 1725.85,
        "duration": 3.6,
        "text": "into the database then I'm gonna do a"
      },
      {
        "start": 1727.89,
        "duration": 1.95,
        "text": "select star just to validate that it"
      },
      {
        "start": 1729.45,
        "duration": 4.68,
        "text": "actually"
      },
      {
        "start": 1729.84,
        "duration": 5.64,
        "text": "there which it is okay so same here what"
      },
      {
        "start": 1734.13,
        "duration": 3.3,
        "text": "we're doing is we're gonna create that"
      },
      {
        "start": 1735.48,
        "duration": 3.96,
        "text": "sparks session and then we're gonna"
      },
      {
        "start": 1737.43,
        "duration": 6.24,
        "text": "connect to our table and then create a"
      },
      {
        "start": 1739.44,
        "duration": 5.73,
        "text": "data frame from that table so we've"
      },
      {
        "start": 1743.67,
        "duration": 3.72,
        "text": "moved our data that was living in"
      },
      {
        "start": 1745.17,
        "duration": 3.63,
        "text": "Cassandra into spark into a data frame"
      },
      {
        "start": 1747.39,
        "duration": 6.3,
        "text": "so that we can use machine learning on"
      },
      {
        "start": 1748.8,
        "duration": 7.32,
        "text": "it so in this case we have 1700 rows and"
      },
      {
        "start": 1753.69,
        "duration": 4.68,
        "text": "I'm just gonna do a show here again we"
      },
      {
        "start": 1756.12,
        "duration": 4.71,
        "text": "can see the chocolate ID the company"
      },
      {
        "start": 1758.37,
        "duration": 4.11,
        "text": "location like USA New Zealand also"
      },
      {
        "start": 1760.83,
        "duration": 3.87,
        "text": "because this is a real data set you're"
      },
      {
        "start": 1762.48,
        "duration": 3.93,
        "text": "gonna see a lot of misspellings you"
      },
      {
        "start": 1764.7,
        "duration": 5.3,
        "text": "actually can't see it here but the whole"
      },
      {
        "start": 1766.41,
        "duration": 5.45,
        "text": "data set instead of France it says face"
      },
      {
        "start": 1770.0,
        "duration": 4.63,
        "text": "just go with it"
      },
      {
        "start": 1771.86,
        "duration": 4.39,
        "text": "so because we're using again this is"
      },
      {
        "start": 1774.63,
        "duration": 3.87,
        "text": "different than k-means we're using naive"
      },
      {
        "start": 1776.25,
        "duration": 4.11,
        "text": "bayes and so we need to be able to split"
      },
      {
        "start": 1778.5,
        "duration": 3.99,
        "text": "our data set into a training and to a"
      },
      {
        "start": 1780.36,
        "duration": 4.86,
        "text": "testing data set so what we're gonna do"
      },
      {
        "start": 1782.49,
        "duration": 4.32,
        "text": "is we're gonna split it 8020 so we're"
      },
      {
        "start": 1785.22,
        "duration": 5.54,
        "text": "gonna put 80% of our data into our"
      },
      {
        "start": 1786.81,
        "duration": 7.08,
        "text": "training set and 20% into our test set"
      },
      {
        "start": 1790.76,
        "duration": 7.09,
        "text": "so we do that okay so that that seems"
      },
      {
        "start": 1793.89,
        "duration": 6.93,
        "text": "like 80 80/20 split so naive Bayes again"
      },
      {
        "start": 1797.85,
        "duration": 5.91,
        "text": "is a class of our algorithm and we need"
      },
      {
        "start": 1800.82,
        "duration": 4.68,
        "text": "to predict a label we need to tell it"
      },
      {
        "start": 1803.76,
        "duration": 5.19,
        "text": "which label that we're trying to predict"
      },
      {
        "start": 1805.5,
        "duration": 5.97,
        "text": "on so that way it knows right so a"
      },
      {
        "start": 1808.95,
        "duration": 5.43,
        "text": "couple of things each of these to be"
      },
      {
        "start": 1811.47,
        "duration": 5.7,
        "text": "vectorized needs to be a float it can't"
      },
      {
        "start": 1814.38,
        "duration": 4.05,
        "text": "if we're with a spark some actually"
      },
      {
        "start": 1817.17,
        "duration": 3.18,
        "text": "naive Bayes that I've used on other"
      },
      {
        "start": 1818.43,
        "duration": 4.26,
        "text": "products actually could take in text"
      },
      {
        "start": 1820.35,
        "duration": 6.39,
        "text": "input but this cannot so because of that"
      },
      {
        "start": 1822.69,
        "duration": 6.3,
        "text": "I'm gonna need to take because the beam"
      },
      {
        "start": 1826.74,
        "duration": 5.07,
        "text": "origin like the where it was actually"
      },
      {
        "start": 1828.99,
        "duration": 6.03,
        "text": "grown that's a like text so we actually"
      },
      {
        "start": 1831.81,
        "duration": 4.35,
        "text": "need to create a float based on that so"
      },
      {
        "start": 1835.02,
        "duration": 3.36,
        "text": "what I'm gonna do is I'm going to use"
      },
      {
        "start": 1836.16,
        "duration": 4.59,
        "text": "this string indexer and it's gonna be"
      },
      {
        "start": 1838.38,
        "duration": 4.23,
        "text": "each time it sees a new instance it's"
      },
      {
        "start": 1840.75,
        "duration": 5.79,
        "text": "gonna give it an ID so it's basically"
      },
      {
        "start": 1842.61,
        "duration": 7.47,
        "text": "taking our text and making it into a"
      },
      {
        "start": 1846.54,
        "duration": 5.82,
        "text": "number write a number identifier and in"
      },
      {
        "start": 1850.08,
        "duration": 3.99,
        "text": "this case every time I see a new one so"
      },
      {
        "start": 1852.36,
        "duration": 4.56,
        "text": "after a certain point it'll say if you"
      },
      {
        "start": 1854.07,
        "duration": 4.5,
        "text": "keep seeing new instances maybe I'm just"
      },
      {
        "start": 1856.92,
        "duration": 3.89,
        "text": "saying okay that's enough I don't need I"
      },
      {
        "start": 1858.57,
        "duration": 5.28,
        "text": "don't need a thousand different indexes"
      },
      {
        "start": 1860.81,
        "duration": 5.28,
        "text": "for this but I'm saying yeah go ahead"
      },
      {
        "start": 1863.85,
        "duration": 5.52,
        "text": "do that I'll take a thousand that's fine"
      },
      {
        "start": 1866.09,
        "duration": 6.37,
        "text": "so again I'm just transforming this into"
      },
      {
        "start": 1869.37,
        "duration": 4.559,
        "text": "my data frame so I'm able also to do for"
      },
      {
        "start": 1872.46,
        "duration": 3.63,
        "text": "my company location that's the ultimate"
      },
      {
        "start": 1873.929,
        "duration": 6.691,
        "text": "label that I want it to label it with"
      },
      {
        "start": 1876.09,
        "duration": 8.49,
        "text": "and again same thing with the k-means it"
      },
      {
        "start": 1880.62,
        "duration": 6.39,
        "text": "actually needs to be vectorized okay so"
      },
      {
        "start": 1884.58,
        "duration": 4.08,
        "text": "here we can see let's see our company"
      },
      {
        "start": 1887.01,
        "duration": 4.5,
        "text": "location oh yeah now you can see it face"
      },
      {
        "start": 1888.66,
        "duration": 7.29,
        "text": "so face has been labeled as one so every"
      },
      {
        "start": 1891.51,
        "duration": 7.23,
        "text": "instance of face is now one and we need"
      },
      {
        "start": 1895.95,
        "duration": 5.729,
        "text": "to do this for our test set as well and"
      },
      {
        "start": 1898.74,
        "duration": 4.319,
        "text": "we vectorized that alright so now it's"
      },
      {
        "start": 1901.679,
        "duration": 3.391,
        "text": "actually time so now we've just been"
      },
      {
        "start": 1903.059,
        "duration": 3.36,
        "text": "doing all this prep to get into the data"
      },
      {
        "start": 1905.07,
        "duration": 4.65,
        "text": "frame now we're done now we're actually"
      },
      {
        "start": 1906.419,
        "duration": 5.37,
        "text": "able to use nave bayes so i'm gonna set"
      },
      {
        "start": 1909.72,
        "duration": 4.26,
        "text": "this up i'm gonna fit the model with my"
      },
      {
        "start": 1911.789,
        "duration": 4.02,
        "text": "training data fit the model with my"
      },
      {
        "start": 1913.98,
        "duration": 3.36,
        "text": "training data and get a model then I'm"
      },
      {
        "start": 1915.809,
        "duration": 4.201,
        "text": "gonna use that model and I'm gonna put"
      },
      {
        "start": 1917.34,
        "duration": 6.24,
        "text": "my testing data and score it off of that"
      },
      {
        "start": 1920.01,
        "duration": 6.87,
        "text": "model to get my predictions so let's run"
      },
      {
        "start": 1923.58,
        "duration": 6.81,
        "text": "this and then I'm gonna show that so"
      },
      {
        "start": 1926.88,
        "duration": 6.419,
        "text": "here we have our so in this case face so"
      },
      {
        "start": 1930.39,
        "duration": 4.35,
        "text": "so that's one so if we go over here to"
      },
      {
        "start": 1933.299,
        "duration": 3.211,
        "text": "our prediction okay"
      },
      {
        "start": 1934.74,
        "duration": 3.39,
        "text": "straight away we're seeing it was not"
      },
      {
        "start": 1936.51,
        "duration": 5.94,
        "text": "able to predict that it's giving it a"
      },
      {
        "start": 1938.13,
        "duration": 6.02,
        "text": "prediction of 12 but so let's take"
      },
      {
        "start": 1942.45,
        "duration": 5.19,
        "text": "another look here"
      },
      {
        "start": 1944.15,
        "duration": 5.86,
        "text": "so we can see in this it actually wasn't"
      },
      {
        "start": 1947.64,
        "duration": 5.88,
        "text": "able to predict it at all anywhere where"
      },
      {
        "start": 1950.01,
        "duration": 5.549,
        "text": "it was France not the same Oh actually"
      },
      {
        "start": 1953.52,
        "duration": 4.289,
        "text": "USA it was able to predict that"
      },
      {
        "start": 1955.559,
        "duration": 3.511,
        "text": "correctly okay great so now because of"
      },
      {
        "start": 1957.809,
        "duration": 3.99,
        "text": "this we can actually use a multi"
      },
      {
        "start": 1959.07,
        "duration": 4.65,
        "text": "classifier evaluator to give us our"
      },
      {
        "start": 1961.799,
        "duration": 3.87,
        "text": "percentage of how well we were actually"
      },
      {
        "start": 1963.72,
        "duration": 5.13,
        "text": "able to score this model because we do"
      },
      {
        "start": 1965.669,
        "duration": 7.801,
        "text": "have a training set right so let's run"
      },
      {
        "start": 1968.85,
        "duration": 6.99,
        "text": "that alright so we got our test accuracy"
      },
      {
        "start": 1973.47,
        "duration": 4.98,
        "text": "was about 20 percent so 20 percent of"
      },
      {
        "start": 1975.84,
        "duration": 4.98,
        "text": "the time if you know that the cacao"
      },
      {
        "start": 1978.45,
        "duration": 4.859,
        "text": "percentage where the beam was grown and"
      },
      {
        "start": 1980.82,
        "duration": 4.62,
        "text": "the rating we can figure out which"
      },
      {
        "start": 1983.309,
        "duration": 4.951,
        "text": "country of origin that the candy bar was"
      },
      {
        "start": 1985.44,
        "duration": 4.77,
        "text": "actually produced so it's not a great"
      },
      {
        "start": 1988.26,
        "duration": 4.53,
        "text": "value right you'd like to see 90 percent"
      },
      {
        "start": 1990.21,
        "duration": 4.05,
        "text": "right but this is a real data set and so"
      },
      {
        "start": 1992.79,
        "duration": 2.97,
        "text": "I wasn't able to do that actually"
      },
      {
        "start": 1994.26,
        "duration": 2.59,
        "text": "originally when I had started working"
      },
      {
        "start": 1995.76,
        "duration": 2.8,
        "text": "with this data set what"
      },
      {
        "start": 1996.85,
        "duration": 3.66,
        "text": "to show you cause I wanted to show you"
      },
      {
        "start": 1998.56,
        "duration": 3.51,
        "text": "if I knew all these things about the bar"
      },
      {
        "start": 2000.51,
        "duration": 3.57,
        "text": "the chocolate bar where it was produced"
      },
      {
        "start": 2002.07,
        "duration": 4.5,
        "text": "the rating how much chocolate you know"
      },
      {
        "start": 2004.08,
        "duration": 4.29,
        "text": "cocoa percentage that I could determine"
      },
      {
        "start": 2006.57,
        "duration": 3.0,
        "text": "the rating I thought oh that would be"
      },
      {
        "start": 2008.37,
        "duration": 2.43,
        "text": "really interesting that's how we'd know"
      },
      {
        "start": 2009.57,
        "duration": 3.93,
        "text": "if it was a good chocolate bar or not"
      },
      {
        "start": 2010.8,
        "duration": 4.68,
        "text": "but actually the that test set accuracy"
      },
      {
        "start": 2013.5,
        "duration": 3.45,
        "text": "was just it was really bad"
      },
      {
        "start": 2015.48,
        "duration": 2.52,
        "text": "it was horrible so I said okay I'm gonna"
      },
      {
        "start": 2016.95,
        "duration": 4.86,
        "text": "go with this because at least I'm"
      },
      {
        "start": 2018.0,
        "duration": 7.26,
        "text": "getting 20% here all right okay all"
      },
      {
        "start": 2021.81,
        "duration": 5.33,
        "text": "righty so wrapping it up that was"
      },
      {
        "start": 2025.26,
        "duration": 4.98,
        "text": "awesome but now what do I do"
      },
      {
        "start": 2027.14,
        "duration": 5.02,
        "text": "so again like I said this is on github"
      },
      {
        "start": 2030.24,
        "duration": 3.36,
        "text": "if you want to take a look at it if you"
      },
      {
        "start": 2032.16,
        "duration": 4.1,
        "text": "want to learn more about Cassandra or"
      },
      {
        "start": 2033.6,
        "duration": 7.35,
        "text": "spark so we have data stacks Academy"
      },
      {
        "start": 2036.26,
        "duration": 6.46,
        "text": "it's free online that you can utilize to"
      },
      {
        "start": 2040.95,
        "duration": 5.43,
        "text": "learn more about Cassandra and data sex"
      },
      {
        "start": 2042.72,
        "duration": 4.62,
        "text": "and you can follow me on Twitter I post"
      },
      {
        "start": 2046.38,
        "duration": 2.94,
        "text": "a lot of just the stuff that I'm doing"
      },
      {
        "start": 2047.34,
        "duration": 5.45,
        "text": "and sometimes I try to post interesting"
      },
      {
        "start": 2049.32,
        "duration": 9.66,
        "text": "things and thank you"
      },
      {
        "start": 2052.79,
        "duration": 6.19,
        "text": "[Applause]"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T05:04:55.842499+00:00"
}