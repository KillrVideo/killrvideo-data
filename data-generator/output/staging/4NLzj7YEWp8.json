{
  "video_id": "4NLzj7YEWp8",
  "title": "How They Built This: Gen AI Products at Instacart, Shopify, and more",
  "description": "Resources: \n➡️Vector Search for GenAI apps: https://www.datastax.com/resources/whitepaper/vector-search-for-generative-ai-apps\n➡️Recipe Chatbot Notebook:  https://colab.research.google.com/drive/15Ub81_-4ziNmay2qkHaimiA845dXBD-D\n➡️FLARE Q&A Notebook: https://colab.research.google.com/drive/1FGMmmkvy3PH7gWQdBBr05HFLTsM6PRU6\n\n\nLet’s take a deep dive into emerging architectures of Generative AI products at industry-leading companies like Instacart and Shopify.  We discuss how these companies built custom Gen AI products with their own unique datasets, using technologies like vector search, retrieval augmented generation (RAG), LangChain, and more. \n\nLearn from Alejandro Cantarero, field CTO of AI at DataStax. Alejandro built a generative AI product for support agents to rapidly answer inquiries at Shopify and designed real-time embedding pipelines to power recommendations and propensity to purchase models at Tribune Publishing and the LA Times. \n\nAlejandro uncovers:\n\n\n-Generative AI architecture and design patterns for agents, text, and retrieval augmented generation\n-Using vector search and LangChain to add context to LLMs\n-Different approaches to constructing Generative AI products through various case studies\n\n\n\n\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastaxdevs?sub_confirmation=1 \nTwitter: https://twitter.com/datastaxdevs\nTwitch: https://www.twitch.tv/datastaxdevs\n\nAbout DataStax:\nDataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra™. DataStax gives developers and enterprises the freedom to run data in any cloud, Kubernetes, hybrid or bare metal at global scale with zero downtime and zero lock-in. More than 450 of the world’s leading enterprises including Capital One, Cisco, Comcast, Delta Airlines, Macy’s, McDonald’s, Safeway, Sony, and Walmart use DataStax to build transformational data architectures for real-world outcomes. For more, visit DataStax.com and @DataS",
  "published_at": "2023-09-06T21:31:56Z",
  "thumbnail": "https://i.ytimg.com/vi/4NLzj7YEWp8/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "scalable",
    "cassandra",
    "search",
    "apache_cassandra",
    "vector",
    "nosql",
    "architecture",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=4NLzj7YEWp8",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "wait so Alejandra do you want to kick it off sure um yeah welcome everyone um let's see uh sorry jump a slide um yeah so today we're going to be talking about um some generative AI uh apps that people have probably seen in production from different companies like instacart and Shopify um little background on the speakers um I'm Alejandro contrero I'm field CTO for AI here at data Stacks I built production Ai and ml apps at a lot of companies including a Tribune publishing Los Angeles Times as well as some generative AI work at UH Shopify and then also for a number of early stage companies kind of in social media mobile apps and blockchain hi my name is Alan Hill I'm VP of product for AI at data Stacks uh engineer entrepreneur physicist by training I build AI products at Google research apogee and semiconductor industry they call PDF Solutions and today I'm going to be uh I'm so glad to be able to talk to my friend Alejandro and we're just going to talk a little bit about how to build actual production Jedi apps um so maybe you know uh Alejandra why don't you just kid off um show us a little bit about some generality of Adrian generative AI apps that are out there in production that people are using today all right let's take a look and also just so people know yeah there are some slides but we will get to some actual code and kind of show how to do this um in practice before we get to the end all right um so for Jenny Jenny apps in production let's see uh I guess the video isn't playing on this one unfortunately um instacart has a chat GPT plugin maybe folks have seen it basically you can chat with um Chachi PT say Hey you know I want to cook some Italian food tonight it'll make you some suggestions like oh there's a lot of different types of Italian food here's some things you could do and then at the end you can say hey make me a shopping list and then the instacart plug-in kicks in creates a shopping list sends you out to instacart um you can get your food delivered so um kind of simple example in the e-commerce space about actually kind of like building stuff into the company's cart which is kind of all they really care about right it's like they got to get people to add products into their cart and and then check out so kind of their whole um gender of AI flow is kind of designed around that um another one in the e-commerce space is Shopify um maybe folks have seen this Shopify has an app called the shop app um this is like an aggregation of um many of their different merchants and you can kind of browse it like you would uh an Amazon or a more central location for a lot of different products from a lot of different vendors as opposed to going directly to say a particular Merchant's site and they've added a chat bot here and it does a few things right you can talk to it you can tell it hey you know I I want to buy a hat oh I you know it shows you some options you say oh that's not what I'm looking for I want a hiking hat and then it'll kind of get more specific you could then ask a follow-up question like uh make it a different color or um you know I don't know make sure it can float in water and you know it's going to bring back options you can see it every step it kind of shows you some products at every step it gives you some recommendations about like what exam other examples you could click into um to help you navigate the search path and also again targets right from this interface you can actually purchase you know any of these products hey before you go there um and by the way everyone uh I'm gonna I'm also reading your question so if you have questions feel free to type them in uh but I got a question for you because um uh you have a lot of experience with Shopify like has this actually helped them with their conversion rates or what was some of this early feedback on this uh generative AI app for them yeah I think you know products like this are are really good because I think for anybody that's kind of worked in recommender systems before um you know one of the problems is that you often encounter is you know surfacing up products that are are maybe more unusual or things you wouldn't have thought about or you didn't necessarily think about in um the search query that you typed in right if I if I type in um uh you know skewers for cooking Japanese um food you know it's going to give me a set of skewers but it's not going to show me kind of other things it might be related to you know cooking a meal of Japanese food for some friends I'm gonna invite over uh if you type into you know the chat bot hey I want to host a dinner party where I'm going to serve Japanese food it's going to present you with a bunch of different options that could be like table settings stuff do the cooking sauce packets you can buy right so this helps you kind of discover things that you weren't even aware existed which is very hard to do with a more traditional kind of e-commerce cool all right unfortunately looks like the ones that are actual videos and not um animated gifs are are not playing so we can't kind of demo the experience but um booking.com has an AI chatbot if you open up it's in their mobile app you can go in you can click this try out button um primarily what this does is it recommends lodging so you say hey I I'm going to Macau where should I stay um then they they know they need some more information so it you know they're trained it to ask follow-up questions okay what dates are you traveling how many people are in your party you know once it's kind of collected all the information it needs it then shows you actual kind of lodging options and you can directly book them you can also talk to the chat bot and say hey um what should I do while I'm in Macau and it's going to recommend a bunch of activities that you can do however you can't book any activities and you can't uh handle you know on their website they do have you can see up top they've got flights and car rentals and things like that however the chatbot's not able to kind of execute on those options you know primarily because you know if you've used booking.com it's a lodging platform I think they have to like link out to these other sites to kind of cover these other use cases and so the chatbot itself isn't able to directly action on it and I think this is kind of interesting because you know when people thought about generative AI apps yeah it was kind of intuitive that you can do recommendations um you can do some summarization of app of text but it wasn't very clear that you know when Janae I first started that I would be able to do planning at all like you know come up with a list of things I should be doing over time and I think that's kind of very uh surprising application in gen AI uh over the last couple of years definitely all right and then we've got one last example here um I think this one's probably the one people are most familiar with is the customer support use case it's one where generative AI is being deployed a lot I also put up there as slash virtual research assistant you know um the same kind of design patterns here that work for customer service also work for um I'm a lawyer and I need to do some research on case law or I'm a medical professional and I need to get somebody you know some information from kind of the latest research papers on a particular disease or treatment solution um you know in these cases you know you know basically you've got a set of documents and you want to answer questions about those documents and you need to kind of pull in all your documents load them up into a spot where you can search over them and then answer questions based on kind of what you found and we'll kind of talk about this pattern more in a little bit and and the main question for you is that like in addition to just searching documents what about what kind of what would be some use cases where people would want to look up more real-time information uh in these kinds of scenarios yeah I mean I think you know this same pattern um also really applies into into real-time use cases you could think of if you look think about the research assistant angle um if you're using a research tool to help you make investment decisions whether to buy or sell stock um you know a lot of that depends on a very real-time information right if uh there's just a mass layoff like announced an hour ago or the CEO quit or the company just acquires announced a merger or an acquisition you know all of these factors really drive like stock price so if you're making a decision to buy or sell and your data is 24 hours out of date or a week out of date you might make a very bad decision right because you're missing really critical information yeah oh well why don't you tell me it's like what is the challenges about building some of the shortcomings possibly yeah so um just really quickly um going back to the instacart example um would you really want to cook food that shot GPT recommend it to you um and here's some examples you know from the media um the first one is probably the most honest one from Good Morning America where what they're really saying is like you you have to be a pretty decent Chef to use chat GPT for recipes because it's good at giving you inspiration but because it will create new recipes like it might have the proportions wrong it might you know have balanced the flavors wrong and the flavor profile of the food so you kind of have to know what you're doing if you're an amateur cook or someone like me who can cook well but needs a clear recipe to follow um it's probably not a good idea um the latter two are a little bit more in the realm of um you know um taking advantage of the bot like it'll recommend a recipe that creates chlorine gas it's not going to do that just like on its own they kind of prompted it to like use certain ingredients and prepare a meal and then the meal to prepare it has this like bad consequence um but you know it's you know chat is creating the recipes it's not leveraging existing recipes and so there's some real risk there um you know going back to the booking.com example and again I guess this was a real video but if you ask it about tours it'll say here's a bunch of things you can do here's activities you can do in the town you're going to visit and it's going to tell you you can say hey book me a tour to go see that ancient ruins site it's gonna say oh I don't know I can't book tours why can they not book tours because they don't have tours and activities in their platform they're not like a Trip Advisor that has that as kind of first party data or a get your guide or these other websites I kind of focus on the tour part of traveling um you know they were clever and kind of worked around by telling you we can't do that and then they do have enough information to make recommendations of oh you could go contact these tour providers like they have scraped that information and so the bot will tell you who you can go contact but they can't kind of fully automate the process that you're trying to do with their AI bot so do you think um uh these kinds of apps will be able to start taking action in the world in the future like what do you think that's on the linchpins for that yeah and you know um maybe I'll just jump on one slide here for that but I think it's to me this really boils down to your llm application needs data right if booking.com has kind of like first party data about trips and tours then all of a sudden it can recommend very specific things and it can take action right if they if their platform has the capability to book a tour then they can kind of automate that process and so I think a lot of companies are not going to be looking to as they build these assistants they build these llm tools and they find you know you're going to want to look at what questions are my customers asking about what activities are they trying to do with it and you're going to want to track this and then you find hey you know they really want to book trips and tours that's like 75 of the inquiries coming into my you know llm assistant and I can't do that you know you're gonna need to figure out how to kind of expand into that area so you can kind of increase utility for your customer base so there's a really good chance to really learn about you know there's a real real-time kind of product feedback mechanism here for your companies in in seeing like what do people actually want from us as a company or a brand like what are they asking us for in these experiences great uh and by the way everyone and please feel free to ask questions in the Q a I'm uh taking I'm constantly monitoring it for questions as Alejandro goes in uh through this presentation all right so you know we gave some examples if you look back at all those examples we just ran through like what kind of data do these experiences need well for the instacart example you need recipes for the shop app you need products booking.com uses lodging and trips and tours and then kind of these research assistants and customer support cases need documents with information so um we'll look at some kind of common design patterns here and then kind of how those can be put into practice to make these things more accurate so just at a high level um you know I'm not going to dive deep into embeddings themselves because I think we want to get to like more advanced concepts of how you kind of put these all together but um as I think many people hopefully have seen at this point you know the way you store data to make work with these systems is with embeddings and embeddings are vector representations of objects that preserve semantic meaning which is why they work with language models right so we get a mathematical representation that tells us that um hey you know all shoes are kind of related to each other all Footwear is kind of related to each other um recipes about Mexican food are all related and so are you know recipes about Italian food these things get grouped together in underlying Vector space so you can kind of find similar uh matches so when you work with Text data in particular um you know basically the processes is you take whatever your text Data sources are you need to chunk them into kind of smaller segments um chunking is you know kind of very important both from just being able to fit the data into the restrictions of the language models that generate the embeddings but also in making your embeddings useful um if you embed a 20-page PDF file as a single embedding and try to ask questions of it you're gonna have a very hard time getting back like actual facts because facts in that document are probably limited to like a couple sentences or a paragraph so you know you have to kind of look at what are what are my document sources look like what information am I trying to extract from them and kind of use that to decide was my chunking strategy again and then the next piece that I think is really important to think about is adding metadata to these Vector embeddings right so when you store Vector data you're going to want to have metadata for really for two purposes one purpose is to filter your your searches if you have a really big Vector space and you're using approximate nearest neighbor search to retrieve data you can start to see degradation and quality of the matches you're not finding the true nearest neighbors to your actual search so having some media to filter down the search space can help speed up search results because you're looking at less data increased searches result quality um also help you handle multiple use cases in one app if you've got really very data set and you want to like pre-filter down to like different areas when different types of questions are being asked and then the second part is what is your application actually doing right I mean there's gonna be a lot of data you probably need after you actually retrieve the documents that you want to use and then you need to put that back into your application so a simple example is you definitely need the text because you're going to have to put that back into the llm but a lot of times you might need other information as well maybe maybe you have information like page views on something or or click-through rate on on items that you're showing for um like the products use case with Shopify you know you have price point available quantity you know should I even show something maybe we're out of stock and I don't want to show it to people right so those are kind of pieces of information you're going to use Downstream in your application and then the last part of this is you need to store the data somewhere in some kind of vector index or vector data store so actually before you go there um that means that when you're storing um when you're storing um the information you also have to store all the information like click-throughs and all that ideally in the same database we surface that out to the user all at one time or change the ranking of your results is that right yes um I also want to address I saw one question come through on the booking.com um yeah in their app unfortunately the video didn't load but you can see they recommend exact lodging places and you're able to then click and book the lodging directly through the chatbot experience so it kind of fully handles that one use case all right so you know then let's look at how you kind of use these embeddings how you kind of improve search results and the kind of things that um I was asking about um just really quickly this is kind of like the first basic pattern so a lot of apps like that shop app experience like the recipe uh like the booking.com experience really they're just doing Vector search they don't really need to do much else right like you type in I want hats they do a vector search for hats they show you products and this is kind of this shows roughly how that works right when you do the embeddings you create a vector space things are in different locations when you make a query you take the term hats you run it back through the same embedding model and then you check for nearest neighbors and you find you know products that are hats in this example you know you find Shakespeare plays because they're next to each other in different books by different authors or in a different part of the embedding space um the you know getting to kind of a next level of complexity here is you know sometimes Vector search is not enough retrieval augmented generation very common pattern so here we're retrieving information but then we're putting it back into the llm for the llm to reason about it use it and use that to generate an actual response not just doing the um Vector search component so the way that this kind of way you think about this is you know you've generated embeddings they're stored in this database at the bottom um you know then you're gonna have a user issues a inquiry so if you take our kind of q a example from earlier maybe you ask how do I cancel my account right so that comes in that gets sent also to the embedding API you embed this query how do I cancel my account and you search all of your vector embeddings with Vector search that we kind of showed before to get back matching documents that you stored in your vector database and then you take those documents and you feed them back to the llm and you say hey llm user asked this question how do I cancel my account here's some documents I found that might answer that question and you put the text of those documents in like here's three documents I found that I think answer this question then you ask the llm consider these documents see if you can answer the question if these documents do not answer the question you know say hey I don't know what the answer is I couldn't find it you know do you want to talk to a person and we have a question on uh from the the chat is like what what is exactly an abetting sure so we'll go back to kind of this this picture here um so an embedding is a machine learning model you put in let's take the case of like text you put in text you get out a vector so the vector is just a mathematical representation of that text or the product or the booking listing from booking.com the most important thing about embeddings is that they preserve semantic similarity which means that related Concepts will be next to each other they'll be nearby to each other in the vector space so if you see this example that's running on the screen it's embedding um books say right so and it's grouping them kind of based on author in this case or kind of topic area right so Pride and Prejudice it's kind of one different piece of literature Great Expectations different author two Shakespeare plays and then when you when you do a query when you search up Vector space you search a vector space to say hey I want to find Shakespeare plays you expect the kind of all the Shakespeare plays will be nearby to each other they'll be in a cluster so that you can retrieve them and then um kind of know like you found what you're looking for so hopefully that helps thanks all right so so Alan touched on this a bit but um you know let's go one step further so now I've got Vector search that returns me similar items when I do a search so again back to that Shopify example I typed in I want hats so I embedded that query hats I searched my database of products it found a bunch of hats and then I'm going to return them to the user and show them hey here's the hatch that I found well you could do that but you know maybe we can think about well how could we improve the quality of the hats that we're showing to the customer because ultimately we want them to buy something well instead of saying maybe just returning the three closest matches on Vector similarity for hats let's pull back a hundred of the best matches for hats and then let's re-rank those results based on some Factor some information that we have so um for a lot of use cases you might have some proprietary data right so in the Shopify example uh we have order volume right um Shopify knows how much how much of each type of hat they're selling which is a good gauge for kind of popularity that particular product so you could re-rank and show your best selling top five hats right which is likely to get you much better results in actually getting people to convert if you're booking.com and you're showing properties you might want to re-rank on the reviews right like some combination of review score and kind of total count of reviews so they have at least five reviews and the total review score is above a 4.0 and then like let's sort by top review scores we'll show you kind of best properties first um you know maybe they took an information earlier on with a chat bot about your your price sensitivity and they will re-rank by pricing right you could say hey I want to see this by price like most affordable to to um least affordable right so these types of re-ranking will help you get better results because you're using information that kind of actually drives the end um action you want your customers to take right you want them to book you want them to purchase there are cases where you may not have good kind of proprietary data to put into the mix and there are some techniques for that as well so there's one called maximal marginal relevance this one will work kind of purely for text problems this will work purely on Text data by basically kind of trying to if you return a bunch of results it'll start to remove redundant information right so if you're trying to do q a and you say back to that example we gave earlier hey I want to cancel my account and you return three documents and they kind of all say exactly the same thing then you're not surfacing a lot of information to the llm to kind of figure out what it should do right you know maybe you have some different documents one that says how to cancel your account maybe there's a special case of canceling your account when you have an unpaid bill right and so that's going to be in a different document so you want to reduce that redundant information so more context comes into the model um MMR can also increase diversity in the set of information that's returned so that's kind of the same idea like I said if you're looking for cancellation information increasing diversity is like well what happens in these edge cases what happens if I have an unpaid Bill what happens if I've got um something configured in their product where like you have to go maybe delete something that you created before you're allowed to cancel the account you have to like shut down the service and then you can kind of cancel your account right so increasing that diversity can get you better results and uh actually uh just following up on that like it's kind of intuitive why you might want to use maximal marginal relevance to reduce redundant information being sent back to the end user but why does it matter for the llm again the llm just uh filter those stuff out or uh what are some of the limitations of the llm especially around context windows that um that we need to be careful about yeah excellent uh question so you know one thing you you maybe do have to be careful about is while llms so context window is kind of a combination of like how many tokens Can you put into the model and also how many tokens you need it to generate and you know you can think of token roughly as the character count for English it's it's equivalent to it it's not exactly that and it does vary for different languages but um a lot of Milestone support these very long token counts you can put in very long queries you can add a lot of documents but there's also been a lot of research that says the longer especially for retrieval augmented generation the longer these document sets get the less good it is at using the information so if you provide 20 documents and there was a paper that actually studied this called uh lost in the middle which folks could look up and they basically provided 20 documents to answer one question in the prompt and the correct answer was in like location six um and then the accuracy of the model of the answers dropped by like 30 40 percent versus if it was in like the first one two or three locations so there's a lot of need to not provide more information that's really necessary to solve the problem like you're gonna get better results if you kind of really narrow it down to like here's the the most relevant information and I only need to give you a few examples to solve very cool we actually have a question it's um it's regard from Ashanti asking following up on the embeddings of books why would vectors be focused on books by the same author for example can we search for a book that is about crime just in general like how why are they can you talk a little bit about the grouping and how those things actually occur yeah I will admit that I think that example by authors is a little unusual like you can definitely do that um but I think the graphic kind of shows how this works really nicely and um so we we kind of reuse that one but um you are correct that usually you know in many use cases you would do it the way you're describing where like crime novels are in the same spot and comedies are in the same spot and sci-fi groups together because if you look at like the text content these things are kind of talking about the same Concepts and ideas um but the interesting thing is right if you need to do something by author and that's the way you want to group um like you can do that right you could just build your text strings instead of inventing the whole documents you can embed like the titles and like the author's name as like the text string and then you would get an embedding space that's going to help you cluster by author name instead of by content of the book so you can always tweak these things based on what is the use case that you want to solve cool all right um and then here's a simple one uh you know like structured data extraction so if you think about the instacart example this is something they need to do right at the end you've got a recipe and you say build me a shopping cart well they need to go back analyze the recipe and get a list of ingredients um there's not much you need to do here because llms are particularly good at this I mean this is I this is an example literally from just chat GPT where I I wrote a little paragraph about George Banks and that he works in a bank in London and you know his age and then I just asked it in the prom just to create me a Json document that extracts name age profession and location of the primary person discussed gave them this text document and it gives us this Json um there I you can't have more complex ish um data extraction issues where you might need to do some few shot learning so you give it an input and you give it an output example here's a text here's the information I want you to extract from it give it three or four of those and you can um get some information out but if it's a pretty simple structured data extraction this will generally work and the simplest data extraction can be used to generate apis to make calls to let's say a checkout pipeline for actually doing these orderings right so yeah that's very or that that's kind of uh this is where the llms interact with the actual world right and llms are very good at producing Json and also understanding and processing Json right so they this is a good interaction you know interface between the rest of your application stack and the language model uh we actually have a question from um uh we have an extra question here and it's and it's asking can you define how the semantics uh similarity is defined which probably is asking can you define how the various uh vectors are clustered together it's like probably asking about embedding model techniques yeah so there's there's maybe two parts there um as I was saying you know one is well I guess there's there's three parts I would say right one is structuring your initial data so that you'll get the right so so data will be nearby each other for the problem you want to solve so we had the example earlier of if you want to uh cluster books by author you probably want to like have your text that you're embedding be the title of the book and the author's name if you want to Cluster books by genre you can probably just you know you would just process the actual text of the book itself and then rely on that for it to kind of group together kind of related types of books um so that's kind of one there's kind of embedding model choice there's a lot of different embedding models out there you know there's vendor ones like open AI there's open source ones um they'll embed data in slightly different ways so sometimes you might want to try different models to see if it does something better for your problem and then the last piece is like how do you tell that two vectors are similar right which would be there's various different similarity metrics that you use um some of the most common ones would be like dot products or inner products cosine similarity and euclidean distance right so liquidity and distance you can think of is really just telling you how far apart are two vectors cosine similarities more telling you what's the difference in the angle between two vectors but under the hood you use these these uh you know math models to decide like how close together are two things and then you use that to decide which Vector am I going to choose cool all right and then just kind of really quickly like how would you put all of this together so this is just an example of kind of what does like the application stack look like when you build an application that uses generative AI so starting from the left you've got your application that you're building at your company right in the middle you're going to have the movement to the right you know gonna have a service router come in you'll have your you know eight your apis the kind of interface with this and you know probably that goes down to some micro service architecture this is all kind of just standard really any application infrastructure at this point but then from that point you'll have agents agents are kind of the concept in generative AI world where the agent is kind of responsible for deciding what actions should be taken what external data do I need to fetch um how am I going to combine this in the generation steps to kind of solve my problem that often relies on proprietary data that you have so you need to kind of process that data um you know maybe it's bash data Maybe it's streaming data um you generate embeddings like we talked about so you have to have an embedding service and again there's vendor solutions for that and open source and then you feed that data back into a vector store where you maintain it and then you'll need some llms um which can be again from vendors or or open source all right so I think we want to unless there's any other questions I think maybe we want to jump into kind of seeing how we put all this together so really quickly we're going to show two examples this first one is going to be a recipes example but it uses the same patterns that booking.com and the shop app use um which is we need a set of embeddings in this case there'll be recipes we'll show how to do filtered Vector search to kind of do a not just search based on vectors but put in some other criteria that we care about to make sure we're getting good results back we'll look at re-ranking those results to get even better results from the vector search and then for there's a structured data extraction at the end right to kind of build the shopping cart um and this is the kind of pattern that booking.com Shop app and um instacart kind of all use this example will use open some open AI stuff some Lang stuff from Lang chain and for our Vector store we use a data Stacks Astra DB and then second example for kind of a q a chat bot so we'll create embeddings of some PDF files and show how you use retrieval augmented generation and flare which is like a kind of fancier version of retrieval augmented generation I'll talk about in the demo this one uses another Library called Casio that data Stacks built that abstracts away the complexities of the database just lets you kind of solve your llm problem you don't have to kind of worry about how the database works if you don't if you aren't familiar with it and then just really quickly won't spend a lot of time with this but you know why should you use astrodb um you know people may not be familiar with Astra but Astra is basically serverless version of Apache Cassandra I think a lot of people are familiar with uh Cassandra scales very well um on the serverless side where in all clouds it's very cheap to run um it's trusted by more than 90 of the Fortune 500 and more importantly even though Vector search is new for us our indexing technology is has been around for a long time we've been working on it for 10 years we have this Nifty thing called storage attached indices I won't get into that but um we we use that same technology to do our Vector surge so our Vector search is not new it leverages all this work that we've done to build kind of really effective search indexes all right uh you should be able to see um I know that the share screen I think shows up in a smaller box in in the webinar there's a button to you know make it bigger if you can't see it um if you hover over it you can full screen it or kind of make it larger also um there should be a a button on the bottom for kind of documents that we've made available if it's not showing up and if you click in that there should be links to collab versions of these notebooks if you want to run through them on your own or have access to them afterwards so actually before you jump into there do you mind going back to the to the diagram there's actually a couple of questions here from the the phone so maybe the first question is does the vector database should you keep all your proprietary data in the vector database or is there some proprietary data that's not in the vector database versus others like what where what's appropriate what proprietary data is appropriate in the vector days so that's going to depend a little bit on the database you choose one of the advantages of Astra is we are not a purely Vector database like we store all types of data so you can put all of your application data to run your application in Astra and the vectors are just another data type alongside of text and float into the other type of data you would want to store so you know we would generally recommend kind of putting all of your data in there that you're going to need to run this application so if there's other data that comes in it's not part of the generative step you can still build out those tables in Astra and then you just have one database you're working with okay and the other question is um you know where where do quote-unquote tools I mean the agent sense what are tools fit into the diagram how how how to uh LMS end up calling other apis um so maybe we'll hold on that one because I actually have an example of that in the notebook that we're going to run through so I'll talk about it when we get to that as part of our department okay a few people are reporting that the flare notebook uh is not accessible uh that the link is broken so we'll be fixing that uh after this or if Alejandro can do a quick uh fix that would also be that would also work too I wonder if we just somehow have the wrong win because uh the the viewing permissions are correct we'll double check and send it out afterwards all right so um to run this recipes workbook um you need an opening a AI API key and then you need some connect connection um credentials for astrodb you can get these on our website astro.datastacks.com and you know here's the the module using you'll see I'm not running this in collab um I this one happens via local copy but um that's fine this is just some text cleaning Pro tip a good thing for gender of models because they tend to run out of space when you you know runs off the edge of the screen when you generate the text this just fixes that um I won't talk too much about how I process the data set you guys are welcome to look at the notebook but this data set is basically a food.com set of recipes as well as user interactions and reviews that's available on kaggle um this kind of predates the popularity of llms so the data is actually very structured they already kind of pulled it all apart um in a way that you wouldn't really do with llms anymore you would just kind of process the raw text so a lot of the data prep is actually reassembling the original recipe um so you know you load and prep that data basically all this is doing is giving us a full text document of the recipe with all the ingredients the steps the instructions the title and adding in the average review score for each recipe which we'll use later on um the next Parts just connecting to Astra you need kind of two pieces for this there's a secure connect bundle you download from our website as well as a token you generate and download There's a quick connect button on a database that will just generate all these files for you and you can just upload them and then you connect to the to the data set in this case as I mentioned we've got a library called Casio that kind of abstracts a lot of the underlying database stuff but I just kind of wanted to show how it works in this example so it's pretty straightforward you know we've got a table it's got a recipe ID it's got the text of the embedding it's got the full text of the recipe because we might need that so like the recipe is broken up into different chunks but you know at some point you might want to return the full recipe so we keep the full text we keep the title the rating and then you can see we've got like a float Vector that's got 768 Dimensions which is the size from our embedding model and then you know you create an index to search this we use cosine similarity and then I also create an index so I can search on the ratings as well um the next step is you've got to generate and load data Astra create the embeddings you can do this with openai this data set has about 200 000 recipes so I didn't really want to do this on openai because I regenerated a lot of times and I didn't want to pay the money for that so I ran an open source model called the instructor embeddings they're linked here and there's actually a repo that has the embedding service that this is a restful API service that wraps that and generates the embeddings um it uses a sentence chunker instead of just uh even text splitter so it preserves sentence boundaries and then uh if you want to run all of this yourself this particular embedding service runs pretty well on M1 and M2 Max on Apple silicon on CPU if you're not on a CPU it's a little bit slow it takes a few seconds so I'd recommend like a GPU with Cuda if you're running on um like a Linux image or an Intel system okay yeah related to that like we actually have a question and it's like the question specifically said like how do you name the number types resigning the vector database in Astra and I think what is being really asked like how do you differentiate between like unstructured data versus kind of structured data and how do you query the two maybe marry them up together yeah so we'll see a little bit of that as we kind of go through this but you can see like we've got kind of text types so for the different text Fields we've got we've got a float type here because the rating is you know between zero and five and has decimals and then um the vector type and you can have as many of these types you know you can keep creating more columns on this like we support you know very wide columns so you can kind of just keep adding the data that you need to kind of make this work and I'll show how you actually filter on this a little further down cool uh so quickly to generate the data you just do uh kept two prepare statements here let's just remove one okay just a simple insert statement to put the data in I call I call the API service with the text you get back multiple embeddings so you Loop over them and you just do an insert and then we've got text so now let's look at how the chat bot itself works and so for this part I did use Lang chain this is using gpt35 Turbo from openai and I'm going to ask it hi uh recipe bot you know I wanna eat some Mexican food tonight right um and then let's let's look at the prompt so there was this question about how do you call the external tools and this kind of shows how to do this so we used a pattern there's a there's a paper called tool former it's from meta AI research and you basically tell it you know this is an example of an external API call okay so you tell it hey when you want to recommend a recipe use an API to get the recipes and you can see up here I tell it do not create your own recipes right use an API you can call the API by writing recipes description where description describes what recipe you're searching for only make one APR API call each time because otherwise they'll make like three or four which we don't want and if no food or recipe was mentioned don't make an API call right because we don't want to do that and then you then you do some few shot learning so you give it an example I would like to eat Italian food tonight say great here are some recipes for Italian food call the API with Italian food same thing for like I would like to cook some spicy Indian chicken I insert spicy Indian chicken so let's see so down here so I I ran that and you can see you know here's the result so the model says sure Mexican Cuisine is always a great choice here are some recipes for Mexican food you know and it puts in this API call yeah there's a second step here so this is a chained model so I actually made two calls the second one is basically telling the model to ask follow-up questions right so if you say I want Mexican food that's pretty generic you know maybe you want them to get more specific so ask them to ask some clarifying questions and again with few shot learning you show them how to do this right oh if you would like to eat Italian food what are you looking for maybe you want Seafood meat or pasta right or what kind if you ask I want to cook pork today what kind of pork would you like you know there's sweet recipes Savory recipes different different ethnic styles um okay so we do two generations and then the last step was a problem that just says Hey combine these two things and then at some point when you combine them and tell it hey preserve this recipes API call because that's not a standard thing in the language model right so if you look at this what do we get on the first pass we already showed this example from the vector search where it gives us you know a search thing for Mexican food on the second pass it says hey there's a lot of different kinds of variety of foods in Mexican food are you in the mood for something more specific like tacos enchiladas or a hearty soup and so then if you guys have to combine the two right you get a combined thing where it will say Mexican food is a great choice it's going to show you some Mexican recipes and it's going to ask you if you want to do something fancier um cook something more specific right so then how do you execute the API call um in this case right you just you just need to kind of grip out the um the the API call itself right so you just write you can write like a regex that just looks for API calls and then you have a lookup table that says oh if it's recipes I'm going to do this particular recipes function call if it's do something else right I can I can make other function calls they can be external apis they can be internal apis to my application and so here you see the search recipes function right so what do I have to do I take the query Mexican food I embed it I get back the vector and then here's how I search the database right so I say hey return me the title rating in full text from my recipes data limit it to ratings that are great in 3.0 I don't want low scoring ones so this does a filtering step before it does the vector search gets rid of bad recipes right off the bat okay and then look at my vectors and give me the ones closest to Mexican food limited to the top you know three um items I don't want like a hundred so it looks like if I do that you can see I get back some Yaki soft tacos I get back a peanut butter raspberry Pita which is not exactly a taco but it's kind of like a taco and I get vegetarian bean and lentil tacos um quickly the next thing I said I would show right is well let's do that search re-ranking right so instead of taking a top three let's take the top hundred and then let's re-rank based on rating right show me the top rated ones and then just return the top three like before so now I get a different set of recipes right I get this um Oaxaca beef taco I get um interesting it got a pot roast so you know that that's not great and then it got um uh a a Blanco white cheese dip um so you probably need to adjust the temperature a little bit on different runs it's been it's been better with this and then the last step that we talked about was you need to extract the ingredient list right again very simple prompt create a Json that contains a list of ingredients needed to cook the recipe below it gives you an ingredient list as a Json that's a less data structure so I have a couple questions here if you go back up to the go out back up to the part where you're doing the query like like what what why would you want to use filtered Vector search is there some advantages of that yeah so so there's there are definitely a few advantages first one let's just think from like a perspective of our product right that we're building here which is like recommending recipes and getting people to add things to their shopping cart um you know in this particular data set there are a lot of recipes that have like a 0.5 score a one out of five stars um You probably don't want to put those in front of people because you're not going to get a click right when they see if you because you probably render this in a nice card and have the title have a picture and it would have the star rating right so if you show a one star No One's Gonna click on that recipe and say hey make me a shopping list out of it so you know that's kind of improving the results of your product um from a technical standpoint this is also important because you know the recipes data set is quite is pretty large in this case there's about 200 000 recipes which results in 500 000 embeddings or something like that it's not gigantic but it's decent size when you do this filtered search first it just removes a lot of data from the data set so that makes the the a n query work better it's it it's more accurate because and we didn't touch on this too much but and a n is approximate nearest neighbor it's not true nearest neighbor right so as your data sets get bigger when you do an approximate nearest neighbor search you have more inaccuracies and did you truly find the nearest neighbor to your particular query Vector so this technique will improve that I also want to comment that uh beside a n just doing the cleaners neighbor what happens is that the accuracy of K nearest neighbor unfortunately is very very sensitive to the outliers the total number of outliers that are that is being searched over and you know outliers and data is inevitable um so by nature of having a smaller data set you're reducing your outliers so that the k n algorithm works better as well so speed performance uh sorry speed Improvement irrelevance I think these are all important factors for you know trying to constrain your search space for a n as quickly as possible prior to doing your approximate nurse name research cool um a couple other questions I saw kind of flow through one question was like why are there 500 000 vectors if there's only 200 000 recipes um that's due to the chunking stuff at the beginning so the recipe text is too long for the particular embedding models that we use they're limited to about 512 characters which is about two to three sentences of text in English so you have to chunk up the recipe into kind of different parts and then feed the put that into the vector database and then search over it if you think about longer examples let's say we're doing Q a on like a PDF file you wouldn't be able to put the whole PDF as a single embedding you would probably chunk it by paragraph or also by a few sentences uh sorry just scanning if there's any other questions um there was a question about um kind of uh indexing in Astra and elasticsearch or some of these other methods um but a Nifty thing about the way that Astra indexing works is it's a the index is built in a way where you can query it immediately so a lot of other databases that provide Vector search you have to wait for the index to First build before you can search and how long that takes kind of depends on how much data you loaded how fast you loaded it and and their their particular implementation so you know in some cases maybe you only have to wait a couple minutes but like there are cases where you might have to wait 10 or 20 minutes before the index is available for so that's kind of one spot where our technology Works a bit different than some others out there yeah I would say also too so for example if you're interacting with the chat bot and the chat bot is asking oh what are your kind of pref what kind of products do you like and they use that information to uh to recommend you items so you might type something in natural language like oh I like I like red shirts I like Etc et cetera that needs to be turned into a vector immediately uh otherwise it can't be used in the subsequent uh calls by the chat bot so that that becomes uh really important too in order to have like kind of an interactive experience with the uh with the chat Bots all right I know we're coming up on the top of the hour and I think also we were supposed to only be 45 minutes so I'll show this last one um quickly um so we talked about the other use case being this kind of you know question answering over document sets and so one way to do this is with uh so one this always works off of some flavor of retrieval augmented generation flare is a Improvement on that called forward-looking active retrieval augmented generation and um how that works is basically as the llm is generating a response when it when it sees that it's going to create a new token in the response and it's not very confident about that particular token it's going to execute additional Vector searches to pull in more data to help answer the question so in this example the task that was assigned to the llm was write a summary about Joe Biden so it writes Joe Biden attended and then when tries to plug in the next token of where he intended to realize is I don't really know the answer to that question so it does another search for Joe Biden you know School Joe Biden University pulls back a document that tells it oh hey he attended the University of Pennsylvania and then says okay great where he earned and then he realizes I don't know what degree he got so it does another search for his you know degree that he holds so this process is able to basically on the Fly generate additional questions run additional uh database queries and return more data to try to get a better answer to the question um similar setup to the last notebook so I won't kind of go through that um by the way once once you're able to access this notebook it's set up where you can bring your own data like if you create a folder in Google Drive you can upload any PDFs you want to it and then you can kind of set that PDF location here and you can just run it on your own data in this case I queued up a bunch of papers about llm research so they kind of cover prompting techniques so it covers Chain of Thought self-consistency llms can self-debug how llms can be used to generate SQL code so uh bought eight papers in this example this example uses our library Casio so you'll see you don't have to Define any Cassandra tables you don't have to do any Cassandra data modeling Casio is an integrated Upstream into many popular Frameworks for working with llm so it's integrated with line chain and it's integrated with llama index and so you can kind of see really all you have to do here is import the Cassandra Vector store from Lang chain to then kind of build this example that's really kind of all it takes so um we download the PDFs from Google Drive we create our Cassandra Vector store the only thing you really have to do is tell it what embedding model you're using you have to have initialized a connection to the database and then you need to tell what's the name of the table I want to use and it will just create the table and create all the structure you need Cassio does support creating adding metadata and doing metadata filtering as well so you can do all of that through this interface without kind of going through the you know manual kind of data modeling I did in the last example here we do a text flare that just evenly breaks up all the characters in the document to evenly sized chunks of 500 characters and with an overlap of 80 characters so you kind of want to have an overlap in case you've broken the document in a bad spot and you've kind of lost the context like if you broke it mid-sentence or mid-paragraph you might not get a good result so you want to do some overlap so you kind of carry the information context over better so you know you load the PDFs you do the splitting and then you basically just tell it to add to the vector store these text documents and then it does the embeddings and it stores it and that's really kind of all you need to do um this shows you can do like a count on the table to see how many embeddings you generated and we have about three thousand so um now let's go to the generation step of this uh here's a few queries I'm gonna run so the first one is uh my chatbot is giving incorrect instructions on how to perform tasks in the UI how can I fix this um we set up the vector store as a retriever for the retrieval augmented generation that's again one call in Lang chain um and this you know the vector store was already created with Cassandra and the prior cell and then I'm going to initially initialize just chat GPT here and then I'm going to initialize Flair that uses Cassandra for the retrieval and then I just run that question and I run it in the llm and I run it in flare uh just like just quick comment here there's a question so flare is a general architectural pattern there's papers published on it but the nice thing about Lang chain is they have a l box implementation for this pattern it's also in other Frameworks such as llama index Etc so let's look at this llm result first you see it's very general and this is what you'll see right so it doesn't know how to solve this particular problem so look at your training data collect user feedback debug logic in your code right General steps of how I do this but no like specific information um Flair is going to say let's see you know answer is a little different each time but um it says you need to debug which It also says it says you need to look at the prompt instructions and you need to use code and if you need more help it's telling you to specifically look at a specific paper all right so I was recommending a specific paper here let's run this one more time and this time I'm going to turn on some debugging so you can turn on this debugging fly or this verbosity flag in line in a lang chain to kind of get more information on what happened this dumps a lot of information but I'm really just looking for one part which should be in yellow okay here we go so this was that part about Flair generates additional questions as it runs so these are new new questions that it made right so we asked this question about there was a problem in my UI and it didn't help me answer it's so it asked what is one possible solution to fix your chatbots incorrect instructions what should you check to ensure your instructions are correct um this one is what might you need to do if checking the settings doesn't fix the issue with your chatbots instructions and what might need to do to reprogram in order to improve its instruction set right so we've basically run these uh four or five questions now gotten a lot of different results and then use them to kind of combine to get the final answer all right and we're at the we're at the top of the hour so um kind of stopped there see if there's any kind of final questions and so maybe a final question that was asked that would be good to wrap up but the question was like hey if I'm a small development shop uh do I need special skit does it make sense to hire developers with some minimal knowledge of stats or linear algebra or do I need to go up to like hire a very very high-end data scientists so you know this is the kind of one of the game changing things about generative Ai and llms is you don't necessarily you don't need a data scientist to kind of solve these problems anymore in fact you know I've worked with people that had no data background no statistics background front-end developers full stack developers who are able to build very successful um generative AI applications that were deployed into production at companies that I worked at um if you can kind of think logically and understand how to use these prompt techniques that are really common that's kind of all you need to know to do so the programming language for these things is English right that's what uh what's Andre caparthy from Tesla and open AI um has said right so you you write out so what you need to study is how does prompt engineering works right it's it's a new field accessible to any developer to kind of build these things and kind of do them on their own and you don't even necessarily need a back end for some of this stuff if you're a front-end developer and you're just going to use like open AI you can actually put your logic into your front-end code if you wanted and just call these services to kind of do a lot of the work for you so a lot of new possibilities that maybe were not so easy to do before okay uh one last question uh do you use external Libs like Facebook face to handle numeric types in the vector database no so Facebook faces like another way to do kind of what we showed right so they're kind of a pure software module that runs in memory that builds a vector index and lets you search it um you can do that same thing in Astra um you know it faces the face library is not a bad way to get started with things but um you know I I think a lot of people are reluctant to take that into production because you don't get a lot of the guarantees that you get with a database right you don't have data replication you don't have the ability to kind of scale up with that type of Library um but under the hood you're kind of doing the same thing which is providing a way to do like vector search the other thing with the face library is you have to build the whole index before you can use it it doesn't have a capability like like strut to be using the database while it's being built while the index is being built all right so I think that's all the questions so thank you everyone for attending um if you want to build an app go sign up on Astra and we're looking forward to what everyone's going to be building thank you",
    "segments": [
      {
        "start": 0.0,
        "duration": 4.319,
        "text": "wait so Alejandra do you want to kick it"
      },
      {
        "start": 2.34,
        "duration": 3.24,
        "text": "off"
      },
      {
        "start": 4.319,
        "duration": 1.98,
        "text": "sure"
      },
      {
        "start": 5.58,
        "duration": 2.46,
        "text": "um"
      },
      {
        "start": 6.299,
        "duration": 2.76,
        "text": "yeah welcome everyone"
      },
      {
        "start": 8.04,
        "duration": 4.38,
        "text": "um"
      },
      {
        "start": 9.059,
        "duration": 4.5,
        "text": "let's see uh sorry jump a slide"
      },
      {
        "start": 12.42,
        "duration": 2.82,
        "text": "um"
      },
      {
        "start": 13.559,
        "duration": 2.701,
        "text": "yeah so today we're going to be talking"
      },
      {
        "start": 15.24,
        "duration": 4.2,
        "text": "about"
      },
      {
        "start": 16.26,
        "duration": 4.62,
        "text": "um some generative AI uh apps that"
      },
      {
        "start": 19.44,
        "duration": 2.94,
        "text": "people have probably seen in production"
      },
      {
        "start": 20.88,
        "duration": 3.42,
        "text": "from different companies like instacart"
      },
      {
        "start": 22.38,
        "duration": 4.8,
        "text": "and Shopify"
      },
      {
        "start": 24.3,
        "duration": 5.58,
        "text": "um little background on the speakers"
      },
      {
        "start": 27.18,
        "duration": 5.94,
        "text": "um I'm Alejandro contrero I'm field CTO"
      },
      {
        "start": 29.88,
        "duration": 5.58,
        "text": "for AI here at data Stacks I built"
      },
      {
        "start": 33.12,
        "duration": 3.9,
        "text": "production Ai and ml apps at a lot of"
      },
      {
        "start": 35.46,
        "duration": 3.36,
        "text": "companies including a Tribune publishing"
      },
      {
        "start": 37.02,
        "duration": 4.559,
        "text": "Los Angeles Times as well as some"
      },
      {
        "start": 38.82,
        "duration": 4.44,
        "text": "generative AI work at UH Shopify and"
      },
      {
        "start": 41.579,
        "duration": 3.181,
        "text": "then also for a number of early stage"
      },
      {
        "start": 43.26,
        "duration": 4.58,
        "text": "companies kind of in social media mobile"
      },
      {
        "start": 44.76,
        "duration": 3.08,
        "text": "apps and blockchain"
      },
      {
        "start": 48.18,
        "duration": 4.92,
        "text": "hi my name is Alan Hill I'm VP of"
      },
      {
        "start": 50.28,
        "duration": 4.86,
        "text": "product for AI at data Stacks uh"
      },
      {
        "start": 53.1,
        "duration": 4.74,
        "text": "engineer entrepreneur physicist by"
      },
      {
        "start": 55.14,
        "duration": 5.7,
        "text": "training I build AI products at Google"
      },
      {
        "start": 57.84,
        "duration": 5.879,
        "text": "research apogee and semiconductor"
      },
      {
        "start": 60.84,
        "duration": 6.0,
        "text": "industry they call PDF Solutions"
      },
      {
        "start": 63.719,
        "duration": 4.741,
        "text": "and today I'm going to be uh I'm so glad"
      },
      {
        "start": 66.84,
        "duration": 4.139,
        "text": "to be able to talk to my friend"
      },
      {
        "start": 68.46,
        "duration": 4.5,
        "text": "Alejandro and we're just going to talk a"
      },
      {
        "start": 70.979,
        "duration": 4.261,
        "text": "little bit about how to build actual"
      },
      {
        "start": 72.96,
        "duration": 4.32,
        "text": "production Jedi apps"
      },
      {
        "start": 75.24,
        "duration": 4.14,
        "text": "um so maybe you know uh Alejandra why"
      },
      {
        "start": 77.28,
        "duration": 3.86,
        "text": "don't you just kid off um show us a"
      },
      {
        "start": 79.38,
        "duration": 4.26,
        "text": "little bit about some generality of"
      },
      {
        "start": 81.14,
        "duration": 4.9,
        "text": "Adrian generative AI apps that are out"
      },
      {
        "start": 83.64,
        "duration": 4.94,
        "text": "there in production that people are"
      },
      {
        "start": 86.04,
        "duration": 2.54,
        "text": "using today"
      },
      {
        "start": 88.92,
        "duration": 3.6,
        "text": "all right let's take a look and also"
      },
      {
        "start": 90.78,
        "duration": 3.72,
        "text": "just so people know yeah there are some"
      },
      {
        "start": 92.52,
        "duration": 3.779,
        "text": "slides but we will get to some actual"
      },
      {
        "start": 94.5,
        "duration": 5.22,
        "text": "code and kind of show how to do this um"
      },
      {
        "start": 96.299,
        "duration": 4.68,
        "text": "in practice before we get to the end"
      },
      {
        "start": 99.72,
        "duration": 5.16,
        "text": "all right"
      },
      {
        "start": 100.979,
        "duration": 6.301,
        "text": "um so for Jenny Jenny apps in production"
      },
      {
        "start": 104.88,
        "duration": 4.8,
        "text": "let's see uh"
      },
      {
        "start": 107.28,
        "duration": 4.019,
        "text": "I guess the video isn't playing on this"
      },
      {
        "start": 109.68,
        "duration": 2.64,
        "text": "one unfortunately"
      },
      {
        "start": 111.299,
        "duration": 3.541,
        "text": "um"
      },
      {
        "start": 112.32,
        "duration": 4.38,
        "text": "instacart has a chat GPT plugin maybe"
      },
      {
        "start": 114.84,
        "duration": 3.3,
        "text": "folks have seen it basically you can"
      },
      {
        "start": 116.7,
        "duration": 4.98,
        "text": "chat with"
      },
      {
        "start": 118.14,
        "duration": 6.839,
        "text": "um Chachi PT say Hey you know I want to"
      },
      {
        "start": 121.68,
        "duration": 4.619,
        "text": "cook some Italian food tonight it'll"
      },
      {
        "start": 124.979,
        "duration": 2.221,
        "text": "make you some suggestions like oh"
      },
      {
        "start": 126.299,
        "duration": 2.16,
        "text": "there's a lot of different types of"
      },
      {
        "start": 127.2,
        "duration": 3.66,
        "text": "Italian food here's some things you"
      },
      {
        "start": 128.459,
        "duration": 4.741,
        "text": "could do and then at the end you can say"
      },
      {
        "start": 130.86,
        "duration": 4.62,
        "text": "hey make me a shopping list and then the"
      },
      {
        "start": 133.2,
        "duration": 4.619,
        "text": "instacart plug-in kicks in creates a"
      },
      {
        "start": 135.48,
        "duration": 4.8,
        "text": "shopping list sends you out to instacart"
      },
      {
        "start": 137.819,
        "duration": 5.341,
        "text": "um you can get your food delivered"
      },
      {
        "start": 140.28,
        "duration": 4.56,
        "text": "so um kind of simple example in the"
      },
      {
        "start": 143.16,
        "duration": 3.719,
        "text": "e-commerce space about actually kind of"
      },
      {
        "start": 144.84,
        "duration": 3.78,
        "text": "like building stuff into the company's"
      },
      {
        "start": 146.879,
        "duration": 3.121,
        "text": "cart which is kind of all they really"
      },
      {
        "start": 148.62,
        "duration": 2.58,
        "text": "care about right it's like they got to"
      },
      {
        "start": 150.0,
        "duration": 3.42,
        "text": "get people to add products into their"
      },
      {
        "start": 151.2,
        "duration": 5.1,
        "text": "cart and and then check out so kind of"
      },
      {
        "start": 153.42,
        "duration": 5.72,
        "text": "their whole um gender of AI flow is kind"
      },
      {
        "start": 156.3,
        "duration": 2.84,
        "text": "of designed around that"
      },
      {
        "start": 159.42,
        "duration": 3.48,
        "text": "um another one in the e-commerce space"
      },
      {
        "start": 160.98,
        "duration": 3.539,
        "text": "is Shopify"
      },
      {
        "start": 162.9,
        "duration": 4.74,
        "text": "um maybe folks have seen this Shopify"
      },
      {
        "start": 164.519,
        "duration": 5.821,
        "text": "has an app called the shop app"
      },
      {
        "start": 167.64,
        "duration": 4.679,
        "text": "um this is like an aggregation of"
      },
      {
        "start": 170.34,
        "duration": 4.259,
        "text": "um many of their different merchants and"
      },
      {
        "start": 172.319,
        "duration": 4.741,
        "text": "you can kind of browse it like you would"
      },
      {
        "start": 174.599,
        "duration": 4.021,
        "text": "uh an Amazon or a more central location"
      },
      {
        "start": 177.06,
        "duration": 2.759,
        "text": "for a lot of different products from a"
      },
      {
        "start": 178.62,
        "duration": 2.88,
        "text": "lot of different vendors as opposed to"
      },
      {
        "start": 179.819,
        "duration": 3.661,
        "text": "going directly to say a particular"
      },
      {
        "start": 181.5,
        "duration": 5.04,
        "text": "Merchant's site"
      },
      {
        "start": 183.48,
        "duration": 4.56,
        "text": "and they've added a chat bot here and it"
      },
      {
        "start": 186.54,
        "duration": 3.24,
        "text": "does a few things right you can talk to"
      },
      {
        "start": 188.04,
        "duration": 4.74,
        "text": "it you can tell it hey you know I I want"
      },
      {
        "start": 189.78,
        "duration": 4.44,
        "text": "to buy a hat oh I you know it shows you"
      },
      {
        "start": 192.78,
        "duration": 2.7,
        "text": "some options you say oh that's not what"
      },
      {
        "start": 194.22,
        "duration": 2.7,
        "text": "I'm looking for I want a hiking hat and"
      },
      {
        "start": 195.48,
        "duration": 2.88,
        "text": "then it'll kind of get more specific you"
      },
      {
        "start": 196.92,
        "duration": 3.899,
        "text": "could then ask a follow-up question like"
      },
      {
        "start": 198.36,
        "duration": 3.239,
        "text": "uh make it a different color or um you"
      },
      {
        "start": 200.819,
        "duration": 2.521,
        "text": "know"
      },
      {
        "start": 201.599,
        "duration": 3.961,
        "text": "I don't know make sure it can float in"
      },
      {
        "start": 203.34,
        "duration": 4.56,
        "text": "water and you know it's going to bring"
      },
      {
        "start": 205.56,
        "duration": 3.959,
        "text": "back options you can see it every step"
      },
      {
        "start": 207.9,
        "duration": 2.58,
        "text": "it kind of shows you some products at"
      },
      {
        "start": 209.519,
        "duration": 3.601,
        "text": "every step it gives you some"
      },
      {
        "start": 210.48,
        "duration": 5.28,
        "text": "recommendations about like what exam"
      },
      {
        "start": 213.12,
        "duration": 5.52,
        "text": "other examples you could click into"
      },
      {
        "start": 215.76,
        "duration": 6.96,
        "text": "um to help you navigate the search path"
      },
      {
        "start": 218.64,
        "duration": 6.0,
        "text": "and also again targets right from this"
      },
      {
        "start": 222.72,
        "duration": 4.14,
        "text": "interface you can actually purchase you"
      },
      {
        "start": 224.64,
        "duration": 4.679,
        "text": "know any of these products"
      },
      {
        "start": 226.86,
        "duration": 4.739,
        "text": "hey before you go there um and by the"
      },
      {
        "start": 229.319,
        "duration": 3.84,
        "text": "way everyone uh I'm gonna I'm also"
      },
      {
        "start": 231.599,
        "duration": 3.901,
        "text": "reading your question so if you have"
      },
      {
        "start": 233.159,
        "duration": 4.44,
        "text": "questions feel free to type them in uh"
      },
      {
        "start": 235.5,
        "duration": 3.959,
        "text": "but I got a question for you because"
      },
      {
        "start": 237.599,
        "duration": 4.441,
        "text": "um uh you have a lot of experience with"
      },
      {
        "start": 239.459,
        "duration": 5.28,
        "text": "Shopify like has this actually helped"
      },
      {
        "start": 242.04,
        "duration": 5.1,
        "text": "them with their conversion rates or what"
      },
      {
        "start": 244.739,
        "duration": 4.801,
        "text": "was some of this early feedback on this"
      },
      {
        "start": 247.14,
        "duration": 5.4,
        "text": "uh generative AI app for them"
      },
      {
        "start": 249.54,
        "duration": 5.64,
        "text": "yeah I think you know products like this"
      },
      {
        "start": 252.54,
        "duration": 3.9,
        "text": "are are really good because I think for"
      },
      {
        "start": 255.18,
        "duration": 3.779,
        "text": "anybody that's kind of worked in"
      },
      {
        "start": 256.44,
        "duration": 4.799,
        "text": "recommender systems before"
      },
      {
        "start": 258.959,
        "duration": 3.901,
        "text": "um you know one of the problems is that"
      },
      {
        "start": 261.239,
        "duration": 4.68,
        "text": "you often encounter is you know"
      },
      {
        "start": 262.86,
        "duration": 5.279,
        "text": "surfacing up products that are are maybe"
      },
      {
        "start": 265.919,
        "duration": 3.361,
        "text": "more unusual or things you wouldn't have"
      },
      {
        "start": 268.139,
        "duration": 2.761,
        "text": "thought about or you didn't necessarily"
      },
      {
        "start": 269.28,
        "duration": 3.54,
        "text": "think about in"
      },
      {
        "start": 270.9,
        "duration": 4.38,
        "text": "um the search query that you typed in"
      },
      {
        "start": 272.82,
        "duration": 3.24,
        "text": "right if I if I type in"
      },
      {
        "start": 275.28,
        "duration": 4.74,
        "text": "um"
      },
      {
        "start": 276.06,
        "duration": 4.56,
        "text": "uh you know skewers for cooking Japanese"
      },
      {
        "start": 280.02,
        "duration": 2.22,
        "text": "um"
      },
      {
        "start": 280.62,
        "duration": 2.7,
        "text": "food you know it's going to give me a"
      },
      {
        "start": 282.24,
        "duration": 2.64,
        "text": "set of skewers but it's not going to"
      },
      {
        "start": 283.32,
        "duration": 2.879,
        "text": "show me kind of other things it might be"
      },
      {
        "start": 284.88,
        "duration": 3.9,
        "text": "related to"
      },
      {
        "start": 286.199,
        "duration": 4.261,
        "text": "you know cooking a meal of Japanese food"
      },
      {
        "start": 288.78,
        "duration": 3.72,
        "text": "for some friends I'm gonna invite over"
      },
      {
        "start": 290.46,
        "duration": 4.2,
        "text": "uh if you type into"
      },
      {
        "start": 292.5,
        "duration": 3.9,
        "text": "you know the chat bot hey I want to host"
      },
      {
        "start": 294.66,
        "duration": 3.3,
        "text": "a dinner party where I'm going to serve"
      },
      {
        "start": 296.4,
        "duration": 3.42,
        "text": "Japanese food it's going to present you"
      },
      {
        "start": 297.96,
        "duration": 4.56,
        "text": "with a bunch of different options that"
      },
      {
        "start": 299.82,
        "duration": 5.099,
        "text": "could be like table settings stuff do"
      },
      {
        "start": 302.52,
        "duration": 3.78,
        "text": "the cooking sauce packets you can buy"
      },
      {
        "start": 304.919,
        "duration": 3.181,
        "text": "right so this helps you kind of discover"
      },
      {
        "start": 306.3,
        "duration": 3.959,
        "text": "things that you weren't even aware"
      },
      {
        "start": 308.1,
        "duration": 5.3,
        "text": "existed which is very hard to do with a"
      },
      {
        "start": 310.259,
        "duration": 3.141,
        "text": "more traditional kind of e-commerce"
      },
      {
        "start": 313.5,
        "duration": 2.72,
        "text": "cool"
      },
      {
        "start": 318.66,
        "duration": 4.979,
        "text": "all right unfortunately looks like the"
      },
      {
        "start": 320.759,
        "duration": 5.341,
        "text": "ones that are actual videos and not um"
      },
      {
        "start": 323.639,
        "duration": 5.461,
        "text": "animated gifs are are not playing so we"
      },
      {
        "start": 326.1,
        "duration": 6.42,
        "text": "can't kind of demo the experience but"
      },
      {
        "start": 329.1,
        "duration": 5.28,
        "text": "um booking.com has an AI chatbot if you"
      },
      {
        "start": 332.52,
        "duration": 4.38,
        "text": "open up it's in their mobile app you can"
      },
      {
        "start": 334.38,
        "duration": 4.319,
        "text": "go in you can click this try out button"
      },
      {
        "start": 336.9,
        "duration": 4.859,
        "text": "um primarily what this does is it"
      },
      {
        "start": 338.699,
        "duration": 5.401,
        "text": "recommends lodging so you say hey I I'm"
      },
      {
        "start": 341.759,
        "duration": 4.44,
        "text": "going to Macau where should I stay"
      },
      {
        "start": 344.1,
        "duration": 3.84,
        "text": "um then they they know they need some"
      },
      {
        "start": 346.199,
        "duration": 3.0,
        "text": "more information so it you know they're"
      },
      {
        "start": 347.94,
        "duration": 3.24,
        "text": "trained it to ask follow-up questions"
      },
      {
        "start": 349.199,
        "duration": 3.301,
        "text": "okay what dates are you traveling how"
      },
      {
        "start": 351.18,
        "duration": 2.519,
        "text": "many people are in your party you know"
      },
      {
        "start": 352.5,
        "duration": 3.18,
        "text": "once it's kind of collected all the"
      },
      {
        "start": 353.699,
        "duration": 3.84,
        "text": "information it needs it then shows you"
      },
      {
        "start": 355.68,
        "duration": 4.38,
        "text": "actual kind of lodging options and you"
      },
      {
        "start": 357.539,
        "duration": 4.801,
        "text": "can directly book them you can also talk"
      },
      {
        "start": 360.06,
        "duration": 3.84,
        "text": "to the chat bot and say hey"
      },
      {
        "start": 362.34,
        "duration": 3.54,
        "text": "um what should I do while I'm in Macau"
      },
      {
        "start": 363.9,
        "duration": 3.78,
        "text": "and it's going to recommend a bunch of"
      },
      {
        "start": 365.88,
        "duration": 4.74,
        "text": "activities that you can do however you"
      },
      {
        "start": 367.68,
        "duration": 5.64,
        "text": "can't book any activities and you can't"
      },
      {
        "start": 370.62,
        "duration": 4.44,
        "text": "uh handle you know on their website they"
      },
      {
        "start": 373.32,
        "duration": 3.42,
        "text": "do have you can see up top they've got"
      },
      {
        "start": 375.06,
        "duration": 3.18,
        "text": "flights and car rentals and things like"
      },
      {
        "start": 376.74,
        "duration": 4.98,
        "text": "that however the chatbot's not able to"
      },
      {
        "start": 378.24,
        "duration": 6.179,
        "text": "kind of execute on those options"
      },
      {
        "start": 381.72,
        "duration": 4.56,
        "text": "you know primarily because you know if"
      },
      {
        "start": 384.419,
        "duration": 3.181,
        "text": "you've used booking.com it's a lodging"
      },
      {
        "start": 386.28,
        "duration": 2.88,
        "text": "platform I think they have to like link"
      },
      {
        "start": 387.6,
        "duration": 3.599,
        "text": "out to these other sites to kind of"
      },
      {
        "start": 389.16,
        "duration": 3.72,
        "text": "cover these other use cases and so the"
      },
      {
        "start": 391.199,
        "duration": 3.601,
        "text": "chatbot itself isn't able to directly"
      },
      {
        "start": 392.88,
        "duration": 3.539,
        "text": "action on it"
      },
      {
        "start": 394.8,
        "duration": 3.54,
        "text": "and I think this is kind of interesting"
      },
      {
        "start": 396.419,
        "duration": 5.521,
        "text": "because you know when people thought"
      },
      {
        "start": 398.34,
        "duration": 5.04,
        "text": "about generative AI apps yeah it was"
      },
      {
        "start": 401.94,
        "duration": 4.259,
        "text": "kind of intuitive that you can do"
      },
      {
        "start": 403.38,
        "duration": 4.86,
        "text": "recommendations"
      },
      {
        "start": 406.199,
        "duration": 5.881,
        "text": "um you can do some summarization of app"
      },
      {
        "start": 408.24,
        "duration": 6.78,
        "text": "of text but it wasn't very clear that"
      },
      {
        "start": 412.08,
        "duration": 4.739,
        "text": "you know when Janae I first started that"
      },
      {
        "start": 415.02,
        "duration": 3.48,
        "text": "I would be able to do planning at all"
      },
      {
        "start": 416.819,
        "duration": 3.72,
        "text": "like you know come up with a list of"
      },
      {
        "start": 418.5,
        "duration": 4.44,
        "text": "things I should be doing over time and I"
      },
      {
        "start": 420.539,
        "duration": 4.921,
        "text": "think that's kind of very uh surprising"
      },
      {
        "start": 422.94,
        "duration": 5.06,
        "text": "application in gen AI uh over the last"
      },
      {
        "start": 425.46,
        "duration": 2.54,
        "text": "couple of years"
      },
      {
        "start": 429.0,
        "duration": 3.96,
        "text": "definitely"
      },
      {
        "start": 430.919,
        "duration": 3.541,
        "text": "all right and then we've got one last"
      },
      {
        "start": 432.96,
        "duration": 2.76,
        "text": "example here"
      },
      {
        "start": 434.46,
        "duration": 2.76,
        "text": "um I think this one's probably the one"
      },
      {
        "start": 435.72,
        "duration": 3.3,
        "text": "people are most familiar with is the"
      },
      {
        "start": 437.22,
        "duration": 4.56,
        "text": "customer support use case it's one where"
      },
      {
        "start": 439.02,
        "duration": 4.86,
        "text": "generative AI is being deployed a lot I"
      },
      {
        "start": 441.78,
        "duration": 4.319,
        "text": "also put up there as slash virtual"
      },
      {
        "start": 443.88,
        "duration": 3.719,
        "text": "research assistant you know um the same"
      },
      {
        "start": 446.099,
        "duration": 4.081,
        "text": "kind of design patterns here that work"
      },
      {
        "start": 447.599,
        "duration": 4.561,
        "text": "for customer service also work for"
      },
      {
        "start": 450.18,
        "duration": 4.26,
        "text": "um I'm a lawyer and I need to do some"
      },
      {
        "start": 452.16,
        "duration": 4.259,
        "text": "research on case law or I'm a medical"
      },
      {
        "start": 454.44,
        "duration": 3.72,
        "text": "professional and I need to get somebody"
      },
      {
        "start": 456.419,
        "duration": 3.241,
        "text": "you know some information from kind of"
      },
      {
        "start": 458.16,
        "duration": 5.34,
        "text": "the latest research papers on a"
      },
      {
        "start": 459.66,
        "duration": 5.52,
        "text": "particular disease or treatment solution"
      },
      {
        "start": 463.5,
        "duration": 3.66,
        "text": "um you know in these cases"
      },
      {
        "start": 465.18,
        "duration": 3.72,
        "text": "you know you know basically you've got a"
      },
      {
        "start": 467.16,
        "duration": 3.36,
        "text": "set of documents and you want to answer"
      },
      {
        "start": 468.9,
        "duration": 2.76,
        "text": "questions about those documents and you"
      },
      {
        "start": 470.52,
        "duration": 3.36,
        "text": "need to kind of pull in all your"
      },
      {
        "start": 471.66,
        "duration": 4.14,
        "text": "documents load them up into a spot where"
      },
      {
        "start": 473.88,
        "duration": 3.18,
        "text": "you can search over them and then answer"
      },
      {
        "start": 475.8,
        "duration": 2.519,
        "text": "questions based on kind of what you"
      },
      {
        "start": 477.06,
        "duration": 4.62,
        "text": "found and we'll kind of talk about this"
      },
      {
        "start": 478.319,
        "duration": 5.341,
        "text": "pattern more in a little bit"
      },
      {
        "start": 481.68,
        "duration": 3.959,
        "text": "and and the main question for you is"
      },
      {
        "start": 483.66,
        "duration": 5.039,
        "text": "that like in addition to just searching"
      },
      {
        "start": 485.639,
        "duration": 4.62,
        "text": "documents what about what kind of what"
      },
      {
        "start": 488.699,
        "duration": 3.241,
        "text": "would be some use cases where people"
      },
      {
        "start": 490.259,
        "duration": 4.38,
        "text": "would want to look up more real-time"
      },
      {
        "start": 491.94,
        "duration": 5.039,
        "text": "information uh in these kinds of"
      },
      {
        "start": 494.639,
        "duration": 4.261,
        "text": "scenarios"
      },
      {
        "start": 496.979,
        "duration": 3.0,
        "text": "yeah I mean I think you know this same"
      },
      {
        "start": 498.9,
        "duration": 3.54,
        "text": "pattern"
      },
      {
        "start": 499.979,
        "duration": 4.381,
        "text": "um also really applies into into"
      },
      {
        "start": 502.44,
        "duration": 3.479,
        "text": "real-time use cases you could think of"
      },
      {
        "start": 504.36,
        "duration": 3.959,
        "text": "if you look think about the research"
      },
      {
        "start": 505.919,
        "duration": 4.081,
        "text": "assistant angle um if you're using a"
      },
      {
        "start": 508.319,
        "duration": 3.84,
        "text": "research tool to help you make"
      },
      {
        "start": 510.0,
        "duration": 3.539,
        "text": "investment decisions whether to buy or"
      },
      {
        "start": 512.159,
        "duration": 3.06,
        "text": "sell stock"
      },
      {
        "start": 513.539,
        "duration": 4.081,
        "text": "um you know a lot of that depends on a"
      },
      {
        "start": 515.219,
        "duration": 3.961,
        "text": "very real-time information right if uh"
      },
      {
        "start": 517.62,
        "duration": 4.14,
        "text": "there's just a mass layoff like"
      },
      {
        "start": 519.18,
        "duration": 4.38,
        "text": "announced an hour ago or the CEO quit or"
      },
      {
        "start": 521.76,
        "duration": 3.6,
        "text": "the company just acquires announced a"
      },
      {
        "start": 523.56,
        "duration": 3.54,
        "text": "merger or an acquisition you know all of"
      },
      {
        "start": 525.36,
        "duration": 3.24,
        "text": "these factors really drive like stock"
      },
      {
        "start": 527.1,
        "duration": 4.08,
        "text": "price so if you're making a decision to"
      },
      {
        "start": 528.6,
        "duration": 5.04,
        "text": "buy or sell and your data is 24 hours"
      },
      {
        "start": 531.18,
        "duration": 3.779,
        "text": "out of date or a week out of date you"
      },
      {
        "start": 533.64,
        "duration": 2.58,
        "text": "might make a very bad decision right"
      },
      {
        "start": 534.959,
        "duration": 2.88,
        "text": "because you're missing really critical"
      },
      {
        "start": 536.22,
        "duration": 4.5,
        "text": "information"
      },
      {
        "start": 537.839,
        "duration": 3.901,
        "text": "yeah oh well why don't you tell me it's"
      },
      {
        "start": 540.72,
        "duration": 3.6,
        "text": "like what is"
      },
      {
        "start": 541.74,
        "duration": 4.92,
        "text": "the challenges about building some of"
      },
      {
        "start": 544.32,
        "duration": 4.019,
        "text": "the shortcomings possibly"
      },
      {
        "start": 546.66,
        "duration": 3.299,
        "text": "yeah so"
      },
      {
        "start": 548.339,
        "duration": 3.961,
        "text": "um just really quickly"
      },
      {
        "start": 549.959,
        "duration": 3.541,
        "text": "um going back to the instacart example"
      },
      {
        "start": 552.3,
        "duration": 4.02,
        "text": "um would you really want to cook food"
      },
      {
        "start": 553.5,
        "duration": 4.56,
        "text": "that shot GPT recommend it to you"
      },
      {
        "start": 556.32,
        "duration": 3.36,
        "text": "um and here's some examples you know"
      },
      {
        "start": 558.06,
        "duration": 3.3,
        "text": "from the media"
      },
      {
        "start": 559.68,
        "duration": 3.42,
        "text": "um the first one is probably the most"
      },
      {
        "start": 561.36,
        "duration": 3.78,
        "text": "honest one from Good Morning America"
      },
      {
        "start": 563.1,
        "duration": 4.08,
        "text": "where what they're really saying is like"
      },
      {
        "start": 565.14,
        "duration": 5.16,
        "text": "you you have to be a pretty decent Chef"
      },
      {
        "start": 567.18,
        "duration": 4.86,
        "text": "to use chat GPT for recipes because it's"
      },
      {
        "start": 570.3,
        "duration": 4.2,
        "text": "good at giving you inspiration but"
      },
      {
        "start": 572.04,
        "duration": 4.62,
        "text": "because it will create new recipes like"
      },
      {
        "start": 574.5,
        "duration": 4.2,
        "text": "it might have the proportions wrong it"
      },
      {
        "start": 576.66,
        "duration": 3.9,
        "text": "might you know have balanced the flavors"
      },
      {
        "start": 578.7,
        "duration": 3.06,
        "text": "wrong and the flavor profile of the food"
      },
      {
        "start": 580.56,
        "duration": 3.54,
        "text": "so you kind of have to know what you're"
      },
      {
        "start": 581.76,
        "duration": 3.84,
        "text": "doing if you're an amateur cook or"
      },
      {
        "start": 584.1,
        "duration": 3.66,
        "text": "someone like me who can cook well but"
      },
      {
        "start": 585.6,
        "duration": 4.26,
        "text": "needs a clear recipe to follow"
      },
      {
        "start": 587.76,
        "duration": 4.38,
        "text": "um it's probably not a good idea"
      },
      {
        "start": 589.86,
        "duration": 5.46,
        "text": "um the latter two are a little bit more"
      },
      {
        "start": 592.14,
        "duration": 4.86,
        "text": "in the realm of um you know"
      },
      {
        "start": 595.32,
        "duration": 3.36,
        "text": "um taking advantage of the bot like"
      },
      {
        "start": 597.0,
        "duration": 3.12,
        "text": "it'll recommend a recipe that creates"
      },
      {
        "start": 598.68,
        "duration": 2.94,
        "text": "chlorine gas it's not going to do that"
      },
      {
        "start": 600.12,
        "duration": 3.06,
        "text": "just like on its own they kind of"
      },
      {
        "start": 601.62,
        "duration": 3.06,
        "text": "prompted it to like use certain"
      },
      {
        "start": 603.18,
        "duration": 3.12,
        "text": "ingredients and prepare a meal and then"
      },
      {
        "start": 604.68,
        "duration": 3.48,
        "text": "the meal to prepare it has this like bad"
      },
      {
        "start": 606.3,
        "duration": 4.92,
        "text": "consequence"
      },
      {
        "start": 608.16,
        "duration": 4.98,
        "text": "um but you know it's you know chat is"
      },
      {
        "start": 611.22,
        "duration": 3.6,
        "text": "creating the recipes it's not leveraging"
      },
      {
        "start": 613.14,
        "duration": 4.379,
        "text": "existing recipes and so there's some"
      },
      {
        "start": 614.82,
        "duration": 3.3,
        "text": "real risk there"
      },
      {
        "start": 617.519,
        "duration": 1.741,
        "text": "um"
      },
      {
        "start": 618.12,
        "duration": 3.32,
        "text": "you know"
      },
      {
        "start": 619.26,
        "duration": 4.44,
        "text": "going back to the booking.com example"
      },
      {
        "start": 621.44,
        "duration": 5.62,
        "text": "and again I guess this was a real video"
      },
      {
        "start": 623.7,
        "duration": 4.74,
        "text": "but if you ask it about tours it'll say"
      },
      {
        "start": 627.06,
        "duration": 3.0,
        "text": "here's a bunch of things you can do"
      },
      {
        "start": 628.44,
        "duration": 3.72,
        "text": "here's activities you can do in the town"
      },
      {
        "start": 630.06,
        "duration": 4.02,
        "text": "you're going to visit and"
      },
      {
        "start": 632.16,
        "duration": 3.72,
        "text": "it's going to tell you you can say hey"
      },
      {
        "start": 634.08,
        "duration": 3.0,
        "text": "book me a tour to go see that ancient"
      },
      {
        "start": 635.88,
        "duration": 4.26,
        "text": "ruins site it's gonna say oh I don't"
      },
      {
        "start": 637.08,
        "duration": 5.22,
        "text": "know I can't book tours why can they not"
      },
      {
        "start": 640.14,
        "duration": 4.02,
        "text": "book tours because they don't have tours"
      },
      {
        "start": 642.3,
        "duration": 3.659,
        "text": "and activities in their platform they're"
      },
      {
        "start": 644.16,
        "duration": 3.6,
        "text": "not like a Trip Advisor that has that as"
      },
      {
        "start": 645.959,
        "duration": 3.361,
        "text": "kind of first party data or a get your"
      },
      {
        "start": 647.76,
        "duration": 4.62,
        "text": "guide or these other websites I kind of"
      },
      {
        "start": 649.32,
        "duration": 4.259,
        "text": "focus on the tour part of traveling"
      },
      {
        "start": 652.38,
        "duration": 2.579,
        "text": "um you know they were clever and kind of"
      },
      {
        "start": 653.579,
        "duration": 3.061,
        "text": "worked around by telling you we can't do"
      },
      {
        "start": 654.959,
        "duration": 3.181,
        "text": "that and then they do have enough"
      },
      {
        "start": 656.64,
        "duration": 3.24,
        "text": "information to make recommendations of"
      },
      {
        "start": 658.14,
        "duration": 4.02,
        "text": "oh you could go contact these tour"
      },
      {
        "start": 659.88,
        "duration": 3.899,
        "text": "providers like they have scraped that"
      },
      {
        "start": 662.16,
        "duration": 3.299,
        "text": "information and so the bot will tell you"
      },
      {
        "start": 663.779,
        "duration": 3.361,
        "text": "who you can go contact but they can't"
      },
      {
        "start": 665.459,
        "duration": 5.461,
        "text": "kind of fully automate the process that"
      },
      {
        "start": 667.14,
        "duration": 5.639,
        "text": "you're trying to do with their AI bot"
      },
      {
        "start": 670.92,
        "duration": 2.659,
        "text": "so do you think"
      },
      {
        "start": 672.779,
        "duration": 3.721,
        "text": "um"
      },
      {
        "start": 673.579,
        "duration": 5.2,
        "text": "uh these kinds of apps will be able to"
      },
      {
        "start": 676.5,
        "duration": 3.72,
        "text": "start taking action in the world in the"
      },
      {
        "start": 678.779,
        "duration": 4.141,
        "text": "future like what do you think that's on"
      },
      {
        "start": 680.22,
        "duration": 4.859,
        "text": "the linchpins for that"
      },
      {
        "start": 682.92,
        "duration": 4.38,
        "text": "yeah and you know"
      },
      {
        "start": 685.079,
        "duration": 4.26,
        "text": "um maybe I'll just jump on one slide"
      },
      {
        "start": 687.3,
        "duration": 4.68,
        "text": "here for that but I think it's to me"
      },
      {
        "start": 689.339,
        "duration": 5.401,
        "text": "this really boils down to your llm"
      },
      {
        "start": 691.98,
        "duration": 5.16,
        "text": "application needs data right if"
      },
      {
        "start": 694.74,
        "duration": 5.039,
        "text": "booking.com has kind of like first party"
      },
      {
        "start": 697.14,
        "duration": 4.139,
        "text": "data about trips and tours then all of a"
      },
      {
        "start": 699.779,
        "duration": 3.961,
        "text": "sudden it can recommend very specific"
      },
      {
        "start": 701.279,
        "duration": 4.141,
        "text": "things and it can take action right if"
      },
      {
        "start": 703.74,
        "duration": 3.719,
        "text": "they if their platform has the"
      },
      {
        "start": 705.42,
        "duration": 4.02,
        "text": "capability to book a tour then they can"
      },
      {
        "start": 707.459,
        "duration": 3.901,
        "text": "kind of automate that process and so I"
      },
      {
        "start": 709.44,
        "duration": 5.22,
        "text": "think a lot of companies are not going"
      },
      {
        "start": 711.36,
        "duration": 5.039,
        "text": "to be looking to as they build these"
      },
      {
        "start": 714.66,
        "duration": 3.72,
        "text": "assistants they build these llm tools"
      },
      {
        "start": 716.399,
        "duration": 3.18,
        "text": "and they find you know you're going to"
      },
      {
        "start": 718.38,
        "duration": 2.639,
        "text": "want to look at what questions are my"
      },
      {
        "start": 719.579,
        "duration": 2.7,
        "text": "customers asking about what activities"
      },
      {
        "start": 721.019,
        "duration": 2.76,
        "text": "are they trying to do with it and you're"
      },
      {
        "start": 722.279,
        "duration": 3.721,
        "text": "going to want to track this and then you"
      },
      {
        "start": 723.779,
        "duration": 5.461,
        "text": "find hey you know they really want to"
      },
      {
        "start": 726.0,
        "duration": 4.92,
        "text": "book trips and tours that's like 75 of"
      },
      {
        "start": 729.24,
        "duration": 4.38,
        "text": "the inquiries coming into my you know"
      },
      {
        "start": 730.92,
        "duration": 3.599,
        "text": "llm assistant and I can't do that you"
      },
      {
        "start": 733.62,
        "duration": 2.82,
        "text": "know you're gonna need to figure out how"
      },
      {
        "start": 734.519,
        "duration": 3.421,
        "text": "to kind of expand into that area so you"
      },
      {
        "start": 736.44,
        "duration": 3.54,
        "text": "can kind of increase utility for your"
      },
      {
        "start": 737.94,
        "duration": 4.26,
        "text": "customer base so there's a really good"
      },
      {
        "start": 739.98,
        "duration": 4.38,
        "text": "chance to really learn about you know"
      },
      {
        "start": 742.2,
        "duration": 3.96,
        "text": "there's a real real-time kind of product"
      },
      {
        "start": 744.36,
        "duration": 3.719,
        "text": "feedback mechanism here for your"
      },
      {
        "start": 746.16,
        "duration": 3.66,
        "text": "companies in in seeing like what do"
      },
      {
        "start": 748.079,
        "duration": 3.061,
        "text": "people actually want from us as a"
      },
      {
        "start": 749.82,
        "duration": 5.22,
        "text": "company or a brand like what are they"
      },
      {
        "start": 751.14,
        "duration": 6.18,
        "text": "asking us for in these experiences"
      },
      {
        "start": 755.04,
        "duration": 4.32,
        "text": "great uh and by the way everyone and"
      },
      {
        "start": 757.32,
        "duration": 4.38,
        "text": "please feel free to ask questions in the"
      },
      {
        "start": 759.36,
        "duration": 4.86,
        "text": "Q a I'm uh taking I'm constantly"
      },
      {
        "start": 761.7,
        "duration": 7.04,
        "text": "monitoring it for questions as Alejandro"
      },
      {
        "start": 764.22,
        "duration": 4.52,
        "text": "goes in uh through this presentation"
      },
      {
        "start": 769.38,
        "duration": 2.88,
        "text": "all right so you know we gave some"
      },
      {
        "start": 771.0,
        "duration": 2.82,
        "text": "examples if you look back at all those"
      },
      {
        "start": 772.26,
        "duration": 4.199,
        "text": "examples we just ran through like what"
      },
      {
        "start": 773.82,
        "duration": 4.8,
        "text": "kind of data do these experiences need"
      },
      {
        "start": 776.459,
        "duration": 4.801,
        "text": "well for the instacart example you need"
      },
      {
        "start": 778.62,
        "duration": 5.399,
        "text": "recipes for the shop app you need"
      },
      {
        "start": 781.26,
        "duration": 4.56,
        "text": "products booking.com uses lodging and"
      },
      {
        "start": 784.019,
        "duration": 3.481,
        "text": "trips and tours and then kind of these"
      },
      {
        "start": 785.82,
        "duration": 5.22,
        "text": "research assistants and customer support"
      },
      {
        "start": 787.5,
        "duration": 5.16,
        "text": "cases need documents with information"
      },
      {
        "start": 791.04,
        "duration": 3.359,
        "text": "so"
      },
      {
        "start": 792.66,
        "duration": 3.54,
        "text": "um we'll look at some kind of common"
      },
      {
        "start": 794.399,
        "duration": 4.38,
        "text": "design patterns here and then kind of"
      },
      {
        "start": 796.2,
        "duration": 5.4,
        "text": "how those can be put into practice to"
      },
      {
        "start": 798.779,
        "duration": 4.381,
        "text": "make these things more accurate so"
      },
      {
        "start": 801.6,
        "duration": 3.06,
        "text": "just at a high level"
      },
      {
        "start": 803.16,
        "duration": 3.9,
        "text": "um you know I'm not going to dive deep"
      },
      {
        "start": 804.66,
        "duration": 4.02,
        "text": "into embeddings themselves because I"
      },
      {
        "start": 807.06,
        "duration": 2.94,
        "text": "think we want to get to like more"
      },
      {
        "start": 808.68,
        "duration": 3.599,
        "text": "advanced concepts of how you kind of put"
      },
      {
        "start": 810.0,
        "duration": 4.74,
        "text": "these all together but"
      },
      {
        "start": 812.279,
        "duration": 3.781,
        "text": "um as I think many people hopefully have"
      },
      {
        "start": 814.74,
        "duration": 3.06,
        "text": "seen at this point you know the way you"
      },
      {
        "start": 816.06,
        "duration": 3.66,
        "text": "store data to make work with these"
      },
      {
        "start": 817.8,
        "duration": 4.08,
        "text": "systems is with embeddings and"
      },
      {
        "start": 819.72,
        "duration": 4.38,
        "text": "embeddings are vector representations of"
      },
      {
        "start": 821.88,
        "duration": 3.36,
        "text": "objects that preserve semantic meaning"
      },
      {
        "start": 824.1,
        "duration": 3.179,
        "text": "which is why they work with language"
      },
      {
        "start": 825.24,
        "duration": 4.2,
        "text": "models right so we get a mathematical"
      },
      {
        "start": 827.279,
        "duration": 3.721,
        "text": "representation that tells us that"
      },
      {
        "start": 829.44,
        "duration": 4.139,
        "text": "um hey you know all shoes are kind of"
      },
      {
        "start": 831.0,
        "duration": 4.5,
        "text": "related to each other all Footwear is"
      },
      {
        "start": 833.579,
        "duration": 3.601,
        "text": "kind of related to each other"
      },
      {
        "start": 835.5,
        "duration": 3.0,
        "text": "um recipes about Mexican food are all"
      },
      {
        "start": 837.18,
        "duration": 3.54,
        "text": "related and so are you know recipes"
      },
      {
        "start": 838.5,
        "duration": 3.72,
        "text": "about Italian food these things get"
      },
      {
        "start": 840.72,
        "duration": 3.9,
        "text": "grouped together in underlying Vector"
      },
      {
        "start": 842.22,
        "duration": 3.6,
        "text": "space so you can kind of find similar uh"
      },
      {
        "start": 844.62,
        "duration": 3.18,
        "text": "matches"
      },
      {
        "start": 845.82,
        "duration": 3.6,
        "text": "so when you work with Text data in"
      },
      {
        "start": 847.8,
        "duration": 3.06,
        "text": "particular"
      },
      {
        "start": 849.42,
        "duration": 3.0,
        "text": "um you know basically the processes is"
      },
      {
        "start": 850.86,
        "duration": 3.479,
        "text": "you take whatever your text Data sources"
      },
      {
        "start": 852.42,
        "duration": 3.719,
        "text": "are you need to chunk them into kind of"
      },
      {
        "start": 854.339,
        "duration": 3.781,
        "text": "smaller segments"
      },
      {
        "start": 856.139,
        "duration": 4.44,
        "text": "um chunking is you know kind of very"
      },
      {
        "start": 858.12,
        "duration": 4.26,
        "text": "important both from just being able to"
      },
      {
        "start": 860.579,
        "duration": 2.94,
        "text": "fit the data into the restrictions of"
      },
      {
        "start": 862.38,
        "duration": 2.699,
        "text": "the language models that generate the"
      },
      {
        "start": 863.519,
        "duration": 3.06,
        "text": "embeddings but also in making your"
      },
      {
        "start": 865.079,
        "duration": 4.56,
        "text": "embeddings useful"
      },
      {
        "start": 866.579,
        "duration": 4.081,
        "text": "um if you embed a 20-page PDF file as a"
      },
      {
        "start": 869.639,
        "duration": 2.281,
        "text": "single embedding and try to ask"
      },
      {
        "start": 870.66,
        "duration": 3.359,
        "text": "questions of it you're gonna have a very"
      },
      {
        "start": 871.92,
        "duration": 3.84,
        "text": "hard time getting back like actual facts"
      },
      {
        "start": 874.019,
        "duration": 2.88,
        "text": "because facts in that document are"
      },
      {
        "start": 875.76,
        "duration": 2.819,
        "text": "probably limited to like a couple"
      },
      {
        "start": 876.899,
        "duration": 3.721,
        "text": "sentences or a paragraph"
      },
      {
        "start": 878.579,
        "duration": 4.38,
        "text": "so you know you have to kind of look at"
      },
      {
        "start": 880.62,
        "duration": 4.32,
        "text": "what are what are my document sources"
      },
      {
        "start": 882.959,
        "duration": 3.721,
        "text": "look like what information am I trying"
      },
      {
        "start": 884.94,
        "duration": 3.72,
        "text": "to extract from them and kind of use"
      },
      {
        "start": 886.68,
        "duration": 2.94,
        "text": "that to decide was my chunking strategy"
      },
      {
        "start": 888.66,
        "duration": 3.66,
        "text": "again"
      },
      {
        "start": 889.62,
        "duration": 3.899,
        "text": "and then the next piece that I think is"
      },
      {
        "start": 892.32,
        "duration": 3.06,
        "text": "really important to think about is"
      },
      {
        "start": 893.519,
        "duration": 4.081,
        "text": "adding metadata to these Vector"
      },
      {
        "start": 895.38,
        "duration": 4.139,
        "text": "embeddings right so when you store"
      },
      {
        "start": 897.6,
        "duration": 3.78,
        "text": "Vector data you're going to want to have"
      },
      {
        "start": 899.519,
        "duration": 4.5,
        "text": "metadata for really for two purposes one"
      },
      {
        "start": 901.38,
        "duration": 4.259,
        "text": "purpose is to filter your your searches"
      },
      {
        "start": 904.019,
        "duration": 2.88,
        "text": "if you have a really big Vector space"
      },
      {
        "start": 905.639,
        "duration": 3.421,
        "text": "and you're using approximate nearest"
      },
      {
        "start": 906.899,
        "duration": 3.961,
        "text": "neighbor search to retrieve data you can"
      },
      {
        "start": 909.06,
        "duration": 3.719,
        "text": "start to see degradation and quality of"
      },
      {
        "start": 910.86,
        "duration": 3.479,
        "text": "the matches you're not finding the true"
      },
      {
        "start": 912.779,
        "duration": 3.36,
        "text": "nearest neighbors to your actual search"
      },
      {
        "start": 914.339,
        "duration": 3.841,
        "text": "so having some media to filter down the"
      },
      {
        "start": 916.139,
        "duration": 3.241,
        "text": "search space can help speed up search"
      },
      {
        "start": 918.18,
        "duration": 5.219,
        "text": "results because you're looking at less"
      },
      {
        "start": 919.38,
        "duration": 5.819,
        "text": "data increased searches result quality"
      },
      {
        "start": 923.399,
        "duration": 3.24,
        "text": "um also help you handle multiple use"
      },
      {
        "start": 925.199,
        "duration": 3.061,
        "text": "cases in one app if you've got really"
      },
      {
        "start": 926.639,
        "duration": 3.361,
        "text": "very data set and you want to like"
      },
      {
        "start": 928.26,
        "duration": 3.0,
        "text": "pre-filter down to like different areas"
      },
      {
        "start": 930.0,
        "duration": 3.12,
        "text": "when different types of questions are"
      },
      {
        "start": 931.26,
        "duration": 3.24,
        "text": "being asked and then the second part is"
      },
      {
        "start": 933.12,
        "duration": 3.06,
        "text": "what is your application actually doing"
      },
      {
        "start": 934.5,
        "duration": 3.42,
        "text": "right I mean there's gonna be a lot of"
      },
      {
        "start": 936.18,
        "duration": 4.019,
        "text": "data you probably need after you"
      },
      {
        "start": 937.92,
        "duration": 4.68,
        "text": "actually retrieve the documents that you"
      },
      {
        "start": 940.199,
        "duration": 3.961,
        "text": "want to use and then you need to put"
      },
      {
        "start": 942.6,
        "duration": 3.359,
        "text": "that back into your application so a"
      },
      {
        "start": 944.16,
        "duration": 3.72,
        "text": "simple example is you definitely need"
      },
      {
        "start": 945.959,
        "duration": 3.601,
        "text": "the text because you're going to have to"
      },
      {
        "start": 947.88,
        "duration": 2.699,
        "text": "put that back into the llm but a lot of"
      },
      {
        "start": 949.56,
        "duration": 3.719,
        "text": "times you might need other information"
      },
      {
        "start": 950.579,
        "duration": 4.981,
        "text": "as well maybe maybe you have information"
      },
      {
        "start": 953.279,
        "duration": 4.5,
        "text": "like page views on something or or"
      },
      {
        "start": 955.56,
        "duration": 4.8,
        "text": "click-through rate on on items that"
      },
      {
        "start": 957.779,
        "duration": 4.381,
        "text": "you're showing for"
      },
      {
        "start": 960.36,
        "duration": 3.779,
        "text": "um like the products use case with"
      },
      {
        "start": 962.16,
        "duration": 3.6,
        "text": "Shopify you know you have price point"
      },
      {
        "start": 964.139,
        "duration": 3.06,
        "text": "available quantity you know should I"
      },
      {
        "start": 965.76,
        "duration": 3.18,
        "text": "even show something maybe we're out of"
      },
      {
        "start": 967.199,
        "duration": 4.26,
        "text": "stock and I don't want to show it to"
      },
      {
        "start": 968.94,
        "duration": 3.42,
        "text": "people right so those are kind of pieces"
      },
      {
        "start": 971.459,
        "duration": 3.301,
        "text": "of information you're going to use"
      },
      {
        "start": 972.36,
        "duration": 3.779,
        "text": "Downstream in your application"
      },
      {
        "start": 974.76,
        "duration": 2.939,
        "text": "and then the last part of this is you"
      },
      {
        "start": 976.139,
        "duration": 3.361,
        "text": "need to store the data somewhere in some"
      },
      {
        "start": 977.699,
        "duration": 3.421,
        "text": "kind of vector index or vector data"
      },
      {
        "start": 979.5,
        "duration": 3.779,
        "text": "store"
      },
      {
        "start": 981.12,
        "duration": 6.6,
        "text": "so actually before you go there"
      },
      {
        "start": 983.279,
        "duration": 6.721,
        "text": "um that means that when you're storing"
      },
      {
        "start": 987.72,
        "duration": 4.5,
        "text": "um when you're storing"
      },
      {
        "start": 990.0,
        "duration": 4.139,
        "text": "um the information you also have to"
      },
      {
        "start": 992.22,
        "duration": 3.6,
        "text": "store all the information like"
      },
      {
        "start": 994.139,
        "duration": 3.601,
        "text": "click-throughs and all that ideally in"
      },
      {
        "start": 995.82,
        "duration": 4.5,
        "text": "the same database we surface that out to"
      },
      {
        "start": 997.74,
        "duration": 5.88,
        "text": "the user all at one time or change the"
      },
      {
        "start": 1000.32,
        "duration": 4.5,
        "text": "ranking of your results is that right"
      },
      {
        "start": 1003.62,
        "duration": 3.24,
        "text": "yes"
      },
      {
        "start": 1004.82,
        "duration": 4.079,
        "text": "um I also want to address I saw one"
      },
      {
        "start": 1006.86,
        "duration": 4.5,
        "text": "question come through on the booking.com"
      },
      {
        "start": 1008.899,
        "duration": 4.5,
        "text": "um yeah in their app unfortunately the"
      },
      {
        "start": 1011.36,
        "duration": 3.539,
        "text": "video didn't load but you can see they"
      },
      {
        "start": 1013.399,
        "duration": 2.88,
        "text": "recommend exact lodging places and"
      },
      {
        "start": 1014.899,
        "duration": 3.12,
        "text": "you're able to then click and book the"
      },
      {
        "start": 1016.279,
        "duration": 4.201,
        "text": "lodging directly through the chatbot"
      },
      {
        "start": 1018.019,
        "duration": 5.361,
        "text": "experience so it kind of fully handles"
      },
      {
        "start": 1020.48,
        "duration": 2.9,
        "text": "that one use case"
      },
      {
        "start": 1024.74,
        "duration": 4.079,
        "text": "all right so you know then let's look at"
      },
      {
        "start": 1026.66,
        "duration": 4.08,
        "text": "how you kind of use these embeddings how"
      },
      {
        "start": 1028.819,
        "duration": 3.841,
        "text": "you kind of improve search results and"
      },
      {
        "start": 1030.74,
        "duration": 5.0,
        "text": "the kind of things that um"
      },
      {
        "start": 1032.66,
        "duration": 3.08,
        "text": "I was asking about"
      },
      {
        "start": 1036.799,
        "duration": 2.28,
        "text": "um"
      },
      {
        "start": 1037.52,
        "duration": 3.48,
        "text": "just really quickly this is kind of like"
      },
      {
        "start": 1039.079,
        "duration": 4.681,
        "text": "the first basic pattern so a lot of apps"
      },
      {
        "start": 1041.0,
        "duration": 4.86,
        "text": "like that shop app experience like the"
      },
      {
        "start": 1043.76,
        "duration": 4.86,
        "text": "recipe uh like the booking.com"
      },
      {
        "start": 1045.86,
        "duration": 4.319,
        "text": "experience really they're just doing"
      },
      {
        "start": 1048.62,
        "duration": 3.48,
        "text": "Vector search they don't really need to"
      },
      {
        "start": 1050.179,
        "duration": 3.301,
        "text": "do much else right like you type in I"
      },
      {
        "start": 1052.1,
        "duration": 3.6,
        "text": "want hats they do a vector search for"
      },
      {
        "start": 1053.48,
        "duration": 3.84,
        "text": "hats they show you products and this is"
      },
      {
        "start": 1055.7,
        "duration": 3.18,
        "text": "kind of this shows roughly how that"
      },
      {
        "start": 1057.32,
        "duration": 3.18,
        "text": "works right when you do the embeddings"
      },
      {
        "start": 1058.88,
        "duration": 4.74,
        "text": "you create a vector space things are in"
      },
      {
        "start": 1060.5,
        "duration": 5.16,
        "text": "different locations when you make a"
      },
      {
        "start": 1063.62,
        "duration": 3.36,
        "text": "query you take the term hats you run it"
      },
      {
        "start": 1065.66,
        "duration": 3.3,
        "text": "back through the same embedding model"
      },
      {
        "start": 1066.98,
        "duration": 3.72,
        "text": "and then you check for nearest neighbors"
      },
      {
        "start": 1068.96,
        "duration": 3.48,
        "text": "and you find you know"
      },
      {
        "start": 1070.7,
        "duration": 3.599,
        "text": "products that are hats in this example"
      },
      {
        "start": 1072.44,
        "duration": 3.119,
        "text": "you know you find Shakespeare plays"
      },
      {
        "start": 1074.299,
        "duration": 2.401,
        "text": "because they're next to each other in"
      },
      {
        "start": 1075.559,
        "duration": 2.281,
        "text": "different books by different authors or"
      },
      {
        "start": 1076.7,
        "duration": 3.32,
        "text": "in a different part of the embedding"
      },
      {
        "start": 1077.84,
        "duration": 2.18,
        "text": "space"
      },
      {
        "start": 1080.24,
        "duration": 2.12,
        "text": "um"
      },
      {
        "start": 1082.4,
        "duration": 4.56,
        "text": "the you know getting to kind of a next"
      },
      {
        "start": 1084.799,
        "duration": 3.981,
        "text": "level of complexity here is you know"
      },
      {
        "start": 1086.96,
        "duration": 4.68,
        "text": "sometimes Vector search is not enough"
      },
      {
        "start": 1088.78,
        "duration": 5.92,
        "text": "retrieval augmented generation very"
      },
      {
        "start": 1091.64,
        "duration": 4.8,
        "text": "common pattern so here we're retrieving"
      },
      {
        "start": 1094.7,
        "duration": 4.2,
        "text": "information but then we're putting it"
      },
      {
        "start": 1096.44,
        "duration": 5.099,
        "text": "back into the llm for the llm to reason"
      },
      {
        "start": 1098.9,
        "duration": 5.58,
        "text": "about it use it and use that to generate"
      },
      {
        "start": 1101.539,
        "duration": 7.201,
        "text": "an actual response not just"
      },
      {
        "start": 1104.48,
        "duration": 6.18,
        "text": "doing the um Vector search component"
      },
      {
        "start": 1108.74,
        "duration": 3.66,
        "text": "so the way that this kind of way you"
      },
      {
        "start": 1110.66,
        "duration": 3.3,
        "text": "think about this is you know you've"
      },
      {
        "start": 1112.4,
        "duration": 3.6,
        "text": "generated embeddings they're stored in"
      },
      {
        "start": 1113.96,
        "duration": 4.14,
        "text": "this database at the bottom"
      },
      {
        "start": 1116.0,
        "duration": 5.34,
        "text": "um you know then you're gonna have a"
      },
      {
        "start": 1118.1,
        "duration": 6.12,
        "text": "user issues a inquiry so if you take our"
      },
      {
        "start": 1121.34,
        "duration": 5.52,
        "text": "kind of q a example from earlier maybe"
      },
      {
        "start": 1124.22,
        "duration": 4.199,
        "text": "you ask how do I cancel my account right"
      },
      {
        "start": 1126.86,
        "duration": 4.08,
        "text": "so that comes in"
      },
      {
        "start": 1128.419,
        "duration": 5.161,
        "text": "that gets sent also to the embedding API"
      },
      {
        "start": 1130.94,
        "duration": 4.14,
        "text": "you embed this query how do I cancel my"
      },
      {
        "start": 1133.58,
        "duration": 3.3,
        "text": "account and you search all of your"
      },
      {
        "start": 1135.08,
        "duration": 3.78,
        "text": "vector embeddings with Vector search"
      },
      {
        "start": 1136.88,
        "duration": 4.74,
        "text": "that we kind of showed before to get"
      },
      {
        "start": 1138.86,
        "duration": 4.62,
        "text": "back matching documents that you stored"
      },
      {
        "start": 1141.62,
        "duration": 3.299,
        "text": "in your vector database"
      },
      {
        "start": 1143.48,
        "duration": 3.12,
        "text": "and then you take those documents and"
      },
      {
        "start": 1144.919,
        "duration": 2.521,
        "text": "you feed them back to the llm and you"
      },
      {
        "start": 1146.6,
        "duration": 3.18,
        "text": "say"
      },
      {
        "start": 1147.44,
        "duration": 4.32,
        "text": "hey llm user asked this question how do"
      },
      {
        "start": 1149.78,
        "duration": 3.779,
        "text": "I cancel my account here's some"
      },
      {
        "start": 1151.76,
        "duration": 3.18,
        "text": "documents I found that might answer that"
      },
      {
        "start": 1153.559,
        "duration": 3.181,
        "text": "question and you put the text of those"
      },
      {
        "start": 1154.94,
        "duration": 3.0,
        "text": "documents in like here's three documents"
      },
      {
        "start": 1156.74,
        "duration": 3.48,
        "text": "I found that I think answer this"
      },
      {
        "start": 1157.94,
        "duration": 4.5,
        "text": "question then you ask the llm consider"
      },
      {
        "start": 1160.22,
        "duration": 4.38,
        "text": "these documents see if you can answer"
      },
      {
        "start": 1162.44,
        "duration": 3.9,
        "text": "the question if these documents do not"
      },
      {
        "start": 1164.6,
        "duration": 3.06,
        "text": "answer the question you know say hey I"
      },
      {
        "start": 1166.34,
        "duration": 2.94,
        "text": "don't know what the answer is I couldn't"
      },
      {
        "start": 1167.66,
        "duration": 2.82,
        "text": "find it you know do you want to talk to"
      },
      {
        "start": 1169.28,
        "duration": 3.779,
        "text": "a person"
      },
      {
        "start": 1170.48,
        "duration": 4.86,
        "text": "and we have a question on uh from the"
      },
      {
        "start": 1173.059,
        "duration": 4.761,
        "text": "the chat is like what what is exactly an"
      },
      {
        "start": 1175.34,
        "duration": 2.48,
        "text": "abetting"
      },
      {
        "start": 1180.08,
        "duration": 3.9,
        "text": "sure so we'll go back to kind of this"
      },
      {
        "start": 1182.36,
        "duration": 2.88,
        "text": "this picture here"
      },
      {
        "start": 1183.98,
        "duration": 4.199,
        "text": "um so"
      },
      {
        "start": 1185.24,
        "duration": 5.1,
        "text": "an embedding is a machine learning model"
      },
      {
        "start": 1188.179,
        "duration": 4.681,
        "text": "you put in let's take the case of like"
      },
      {
        "start": 1190.34,
        "duration": 5.88,
        "text": "text you put in text"
      },
      {
        "start": 1192.86,
        "duration": 5.16,
        "text": "you get out a vector so the vector is"
      },
      {
        "start": 1196.22,
        "duration": 5.04,
        "text": "just a mathematical representation of"
      },
      {
        "start": 1198.02,
        "duration": 5.7,
        "text": "that text or the product or the booking"
      },
      {
        "start": 1201.26,
        "duration": 4.98,
        "text": "listing from booking.com the most"
      },
      {
        "start": 1203.72,
        "duration": 4.74,
        "text": "important thing about embeddings is that"
      },
      {
        "start": 1206.24,
        "duration": 5.28,
        "text": "they preserve semantic similarity which"
      },
      {
        "start": 1208.46,
        "duration": 4.74,
        "text": "means that related Concepts will be next"
      },
      {
        "start": 1211.52,
        "duration": 3.72,
        "text": "to each other they'll be nearby to each"
      },
      {
        "start": 1213.2,
        "duration": 3.719,
        "text": "other in the vector space so if you see"
      },
      {
        "start": 1215.24,
        "duration": 4.26,
        "text": "this example that's running on the"
      },
      {
        "start": 1216.919,
        "duration": 6.241,
        "text": "screen it's embedding um"
      },
      {
        "start": 1219.5,
        "duration": 7.38,
        "text": "books say right so and it's grouping"
      },
      {
        "start": 1223.16,
        "duration": 5.82,
        "text": "them kind of based on author in this"
      },
      {
        "start": 1226.88,
        "duration": 4.02,
        "text": "case or kind of topic area right so"
      },
      {
        "start": 1228.98,
        "duration": 3.66,
        "text": "Pride and Prejudice it's kind of one"
      },
      {
        "start": 1230.9,
        "duration": 3.84,
        "text": "different piece of literature Great"
      },
      {
        "start": 1232.64,
        "duration": 4.44,
        "text": "Expectations different author two"
      },
      {
        "start": 1234.74,
        "duration": 4.5,
        "text": "Shakespeare plays and then when you when"
      },
      {
        "start": 1237.08,
        "duration": 3.36,
        "text": "you do a query when you search up Vector"
      },
      {
        "start": 1239.24,
        "duration": 3.54,
        "text": "space you search a vector space to say"
      },
      {
        "start": 1240.44,
        "duration": 3.599,
        "text": "hey I want to find Shakespeare plays you"
      },
      {
        "start": 1242.78,
        "duration": 2.94,
        "text": "expect the kind of all the Shakespeare"
      },
      {
        "start": 1244.039,
        "duration": 3.301,
        "text": "plays will be nearby to each other"
      },
      {
        "start": 1245.72,
        "duration": 3.9,
        "text": "they'll be in a cluster so that you can"
      },
      {
        "start": 1247.34,
        "duration": 5.24,
        "text": "retrieve them and then um kind of know"
      },
      {
        "start": 1249.62,
        "duration": 6.059,
        "text": "like you found what you're looking for"
      },
      {
        "start": 1252.58,
        "duration": 5.32,
        "text": "so hopefully that helps"
      },
      {
        "start": 1255.679,
        "duration": 4.62,
        "text": "thanks"
      },
      {
        "start": 1257.9,
        "duration": 4.92,
        "text": "all right so so Alan touched on this a"
      },
      {
        "start": 1260.299,
        "duration": 4.921,
        "text": "bit but um you know let's go one step"
      },
      {
        "start": 1262.82,
        "duration": 5.52,
        "text": "further so now I've got Vector search"
      },
      {
        "start": 1265.22,
        "duration": 6.18,
        "text": "that returns me similar items when I do"
      },
      {
        "start": 1268.34,
        "duration": 6.3,
        "text": "a search so again back to that Shopify"
      },
      {
        "start": 1271.4,
        "duration": 5.399,
        "text": "example I typed in I want hats so I"
      },
      {
        "start": 1274.64,
        "duration": 4.14,
        "text": "embedded that query hats I searched my"
      },
      {
        "start": 1276.799,
        "duration": 3.781,
        "text": "database of products it found a bunch of"
      },
      {
        "start": 1278.78,
        "duration": 3.12,
        "text": "hats and then I'm going to return them"
      },
      {
        "start": 1280.58,
        "duration": 2.94,
        "text": "to the user and show them hey here's the"
      },
      {
        "start": 1281.9,
        "duration": 3.899,
        "text": "hatch that I found"
      },
      {
        "start": 1283.52,
        "duration": 4.019,
        "text": "well you could do that but you know"
      },
      {
        "start": 1285.799,
        "duration": 3.781,
        "text": "maybe we can think about well how could"
      },
      {
        "start": 1287.539,
        "duration": 3.061,
        "text": "we improve the quality of the hats that"
      },
      {
        "start": 1289.58,
        "duration": 2.76,
        "text": "we're showing to the customer because"
      },
      {
        "start": 1290.6,
        "duration": 3.54,
        "text": "ultimately we want them to buy something"
      },
      {
        "start": 1292.34,
        "duration": 4.14,
        "text": "well instead of saying maybe just"
      },
      {
        "start": 1294.14,
        "duration": 5.34,
        "text": "returning the three closest matches on"
      },
      {
        "start": 1296.48,
        "duration": 5.04,
        "text": "Vector similarity for hats let's pull"
      },
      {
        "start": 1299.48,
        "duration": 3.6,
        "text": "back a hundred of the best matches for"
      },
      {
        "start": 1301.52,
        "duration": 3.539,
        "text": "hats and then let's re-rank those"
      },
      {
        "start": 1303.08,
        "duration": 5.219,
        "text": "results based on some Factor some"
      },
      {
        "start": 1305.059,
        "duration": 4.681,
        "text": "information that we have so"
      },
      {
        "start": 1308.299,
        "duration": 3.24,
        "text": "um for a lot of use cases you might have"
      },
      {
        "start": 1309.74,
        "duration": 4.14,
        "text": "some proprietary data right so in the"
      },
      {
        "start": 1311.539,
        "duration": 3.601,
        "text": "Shopify example uh we have order volume"
      },
      {
        "start": 1313.88,
        "duration": 3.24,
        "text": "right"
      },
      {
        "start": 1315.14,
        "duration": 3.419,
        "text": "um Shopify knows how much how much of"
      },
      {
        "start": 1317.12,
        "duration": 3.0,
        "text": "each type of hat they're selling which"
      },
      {
        "start": 1318.559,
        "duration": 3.301,
        "text": "is a good gauge for kind of popularity"
      },
      {
        "start": 1320.12,
        "duration": 4.08,
        "text": "that particular product so you could"
      },
      {
        "start": 1321.86,
        "duration": 5.22,
        "text": "re-rank and show your best selling top"
      },
      {
        "start": 1324.2,
        "duration": 5.58,
        "text": "five hats right which is likely to get"
      },
      {
        "start": 1327.08,
        "duration": 4.8,
        "text": "you much better results in actually"
      },
      {
        "start": 1329.78,
        "duration": 4.139,
        "text": "getting people to convert"
      },
      {
        "start": 1331.88,
        "duration": 4.32,
        "text": "if you're booking.com"
      },
      {
        "start": 1333.919,
        "duration": 5.401,
        "text": "and you're showing properties you might"
      },
      {
        "start": 1336.2,
        "duration": 4.8,
        "text": "want to re-rank on the reviews right"
      },
      {
        "start": 1339.32,
        "duration": 4.14,
        "text": "like some combination of review score"
      },
      {
        "start": 1341.0,
        "duration": 4.799,
        "text": "and kind of total count of reviews so"
      },
      {
        "start": 1343.46,
        "duration": 4.56,
        "text": "they have at least five reviews and the"
      },
      {
        "start": 1345.799,
        "duration": 3.841,
        "text": "total review score is above a 4.0 and"
      },
      {
        "start": 1348.02,
        "duration": 3.0,
        "text": "then like let's sort by top review"
      },
      {
        "start": 1349.64,
        "duration": 3.3,
        "text": "scores we'll show you kind of best"
      },
      {
        "start": 1351.02,
        "duration": 3.24,
        "text": "properties first"
      },
      {
        "start": 1352.94,
        "duration": 3.06,
        "text": "um you know maybe they took an"
      },
      {
        "start": 1354.26,
        "duration": 3.899,
        "text": "information earlier on with a chat bot"
      },
      {
        "start": 1356.0,
        "duration": 3.9,
        "text": "about your your price sensitivity and"
      },
      {
        "start": 1358.159,
        "duration": 3.0,
        "text": "they will re-rank by pricing right you"
      },
      {
        "start": 1359.9,
        "duration": 5.04,
        "text": "could say hey I want to see this by"
      },
      {
        "start": 1361.159,
        "duration": 5.341,
        "text": "price like most affordable to to"
      },
      {
        "start": 1364.94,
        "duration": 4.08,
        "text": "um least affordable"
      },
      {
        "start": 1366.5,
        "duration": 4.14,
        "text": "right so these types of re-ranking will"
      },
      {
        "start": 1369.02,
        "duration": 3.24,
        "text": "help you get better results because"
      },
      {
        "start": 1370.64,
        "duration": 4.62,
        "text": "you're using information that kind of"
      },
      {
        "start": 1372.26,
        "duration": 4.5,
        "text": "actually drives the end um action you"
      },
      {
        "start": 1375.26,
        "duration": 3.24,
        "text": "want your customers to take right you"
      },
      {
        "start": 1376.76,
        "duration": 3.06,
        "text": "want them to book you want them to"
      },
      {
        "start": 1378.5,
        "duration": 3.6,
        "text": "purchase"
      },
      {
        "start": 1379.82,
        "duration": 4.26,
        "text": "there are cases where"
      },
      {
        "start": 1382.1,
        "duration": 4.14,
        "text": "you may not have good kind of"
      },
      {
        "start": 1384.08,
        "duration": 3.36,
        "text": "proprietary data to put into the mix and"
      },
      {
        "start": 1386.24,
        "duration": 2.7,
        "text": "there are some techniques for that as"
      },
      {
        "start": 1387.44,
        "duration": 3.2,
        "text": "well so there's one called maximal"
      },
      {
        "start": 1388.94,
        "duration": 4.5,
        "text": "marginal relevance"
      },
      {
        "start": 1390.64,
        "duration": 4.84,
        "text": "this one will work kind of purely for"
      },
      {
        "start": 1393.44,
        "duration": 3.66,
        "text": "text problems this will work purely on"
      },
      {
        "start": 1395.48,
        "duration": 4.439,
        "text": "Text data"
      },
      {
        "start": 1397.1,
        "duration": 4.559,
        "text": "by basically kind of trying to if you"
      },
      {
        "start": 1399.919,
        "duration": 4.5,
        "text": "return a bunch of results it'll start to"
      },
      {
        "start": 1401.659,
        "duration": 4.801,
        "text": "remove redundant information right so if"
      },
      {
        "start": 1404.419,
        "duration": 3.361,
        "text": "you're trying to do q a and you say back"
      },
      {
        "start": 1406.46,
        "duration": 2.76,
        "text": "to that example we gave earlier hey I"
      },
      {
        "start": 1407.78,
        "duration": 3.42,
        "text": "want to cancel my account"
      },
      {
        "start": 1409.22,
        "duration": 3.839,
        "text": "and you return three documents and they"
      },
      {
        "start": 1411.2,
        "duration": 4.26,
        "text": "kind of all say exactly the same thing"
      },
      {
        "start": 1413.059,
        "duration": 4.321,
        "text": "then you're not surfacing a lot of"
      },
      {
        "start": 1415.46,
        "duration": 3.54,
        "text": "information to the llm to kind of figure"
      },
      {
        "start": 1417.38,
        "duration": 3.36,
        "text": "out what it should do right you know"
      },
      {
        "start": 1419.0,
        "duration": 3.179,
        "text": "maybe you have some different documents"
      },
      {
        "start": 1420.74,
        "duration": 3.54,
        "text": "one that says how to cancel your account"
      },
      {
        "start": 1422.179,
        "duration": 3.781,
        "text": "maybe there's a special case of"
      },
      {
        "start": 1424.28,
        "duration": 3.84,
        "text": "canceling your account when you have an"
      },
      {
        "start": 1425.96,
        "duration": 3.78,
        "text": "unpaid bill right and so that's going to"
      },
      {
        "start": 1428.12,
        "duration": 3.72,
        "text": "be in a different document so you want"
      },
      {
        "start": 1429.74,
        "duration": 5.16,
        "text": "to reduce that redundant information so"
      },
      {
        "start": 1431.84,
        "duration": 6.36,
        "text": "more context comes into the model"
      },
      {
        "start": 1434.9,
        "duration": 5.22,
        "text": "um MMR can also increase diversity in"
      },
      {
        "start": 1438.2,
        "duration": 3.9,
        "text": "the set of information that's returned"
      },
      {
        "start": 1440.12,
        "duration": 3.299,
        "text": "so that's kind of the same idea like I"
      },
      {
        "start": 1442.1,
        "duration": 2.939,
        "text": "said if you're looking for cancellation"
      },
      {
        "start": 1443.419,
        "duration": 2.88,
        "text": "information increasing diversity is like"
      },
      {
        "start": 1445.039,
        "duration": 3.02,
        "text": "well what happens in these edge cases"
      },
      {
        "start": 1446.299,
        "duration": 5.341,
        "text": "what happens if I have an unpaid Bill"
      },
      {
        "start": 1448.059,
        "duration": 4.961,
        "text": "what happens if I've got"
      },
      {
        "start": 1451.64,
        "duration": 3.0,
        "text": "um something configured in their product"
      },
      {
        "start": 1453.02,
        "duration": 3.3,
        "text": "where like you have to go maybe delete"
      },
      {
        "start": 1454.64,
        "duration": 3.0,
        "text": "something that you created before you're"
      },
      {
        "start": 1456.32,
        "duration": 2.88,
        "text": "allowed to cancel the account you have"
      },
      {
        "start": 1457.64,
        "duration": 3.3,
        "text": "to like shut down the service and then"
      },
      {
        "start": 1459.2,
        "duration": 4.14,
        "text": "you can kind of cancel your account"
      },
      {
        "start": 1460.94,
        "duration": 5.9,
        "text": "right so increasing that diversity can"
      },
      {
        "start": 1463.34,
        "duration": 3.5,
        "text": "get you better results"
      },
      {
        "start": 1467.36,
        "duration": 5.58,
        "text": "and uh actually uh just following up on"
      },
      {
        "start": 1469.64,
        "duration": 5.34,
        "text": "that like it's kind of intuitive why you"
      },
      {
        "start": 1472.94,
        "duration": 3.479,
        "text": "might want to use maximal marginal"
      },
      {
        "start": 1474.98,
        "duration": 3.36,
        "text": "relevance to reduce redundant"
      },
      {
        "start": 1476.419,
        "duration": 5.101,
        "text": "information being sent back to the end"
      },
      {
        "start": 1478.34,
        "duration": 5.94,
        "text": "user but why does it matter for the llm"
      },
      {
        "start": 1481.52,
        "duration": 3.96,
        "text": "again the llm just uh filter those stuff"
      },
      {
        "start": 1484.28,
        "duration": 3.779,
        "text": "out or"
      },
      {
        "start": 1485.48,
        "duration": 4.319,
        "text": "uh what are some of the limitations of"
      },
      {
        "start": 1488.059,
        "duration": 4.261,
        "text": "the llm especially around context"
      },
      {
        "start": 1489.799,
        "duration": 4.141,
        "text": "windows that um that we need to be"
      },
      {
        "start": 1492.32,
        "duration": 4.859,
        "text": "careful about"
      },
      {
        "start": 1493.94,
        "duration": 4.739,
        "text": "yeah excellent uh question so you know"
      },
      {
        "start": 1497.179,
        "duration": 5.461,
        "text": "one thing you you maybe do have to be"
      },
      {
        "start": 1498.679,
        "duration": 5.641,
        "text": "careful about is while llms so context"
      },
      {
        "start": 1502.64,
        "duration": 4.14,
        "text": "window is kind of a combination of like"
      },
      {
        "start": 1504.32,
        "duration": 4.5,
        "text": "how many tokens Can you put into the"
      },
      {
        "start": 1506.78,
        "duration": 5.34,
        "text": "model and also how many tokens you need"
      },
      {
        "start": 1508.82,
        "duration": 5.82,
        "text": "it to generate and you know you can"
      },
      {
        "start": 1512.12,
        "duration": 5.039,
        "text": "think of token roughly as the character"
      },
      {
        "start": 1514.64,
        "duration": 4.5,
        "text": "count for English it's it's equivalent"
      },
      {
        "start": 1517.159,
        "duration": 4.921,
        "text": "to it it's not exactly that and it does"
      },
      {
        "start": 1519.14,
        "duration": 4.56,
        "text": "vary for different languages but"
      },
      {
        "start": 1522.08,
        "duration": 3.0,
        "text": "um a lot of Milestone support these very"
      },
      {
        "start": 1523.7,
        "duration": 2.88,
        "text": "long token counts you can put in very"
      },
      {
        "start": 1525.08,
        "duration": 3.18,
        "text": "long queries you can add a lot of"
      },
      {
        "start": 1526.58,
        "duration": 3.24,
        "text": "documents but there's also been a lot of"
      },
      {
        "start": 1528.26,
        "duration": 4.08,
        "text": "research that says"
      },
      {
        "start": 1529.82,
        "duration": 4.02,
        "text": "the longer especially for retrieval"
      },
      {
        "start": 1532.34,
        "duration": 3.719,
        "text": "augmented generation the longer these"
      },
      {
        "start": 1533.84,
        "duration": 3.78,
        "text": "document sets get the less good it is at"
      },
      {
        "start": 1536.059,
        "duration": 3.061,
        "text": "using the information so if you provide"
      },
      {
        "start": 1537.62,
        "duration": 2.88,
        "text": "20 documents"
      },
      {
        "start": 1539.12,
        "duration": 2.939,
        "text": "and there was a paper that actually"
      },
      {
        "start": 1540.5,
        "duration": 3.72,
        "text": "studied this called uh lost in the"
      },
      {
        "start": 1542.059,
        "duration": 4.561,
        "text": "middle which folks could look up and"
      },
      {
        "start": 1544.22,
        "duration": 4.8,
        "text": "they basically provided 20 documents to"
      },
      {
        "start": 1546.62,
        "duration": 3.72,
        "text": "answer one question in the prompt and"
      },
      {
        "start": 1549.02,
        "duration": 2.519,
        "text": "the correct answer was in like location"
      },
      {
        "start": 1550.34,
        "duration": 3.54,
        "text": "six"
      },
      {
        "start": 1551.539,
        "duration": 3.661,
        "text": "um and then the accuracy of the model of"
      },
      {
        "start": 1553.88,
        "duration": 3.779,
        "text": "the answers"
      },
      {
        "start": 1555.2,
        "duration": 4.5,
        "text": "dropped by like 30 40 percent"
      },
      {
        "start": 1557.659,
        "duration": 5.161,
        "text": "versus if it was in like the first one"
      },
      {
        "start": 1559.7,
        "duration": 4.44,
        "text": "two or three locations so there's a lot"
      },
      {
        "start": 1562.82,
        "duration": 3.06,
        "text": "of need to"
      },
      {
        "start": 1564.14,
        "duration": 3.12,
        "text": "not provide more information that's"
      },
      {
        "start": 1565.88,
        "duration": 3.24,
        "text": "really necessary to solve the problem"
      },
      {
        "start": 1567.26,
        "duration": 3.12,
        "text": "like you're gonna get better results if"
      },
      {
        "start": 1569.12,
        "duration": 3.0,
        "text": "you kind of really narrow it down to"
      },
      {
        "start": 1570.38,
        "duration": 3.06,
        "text": "like here's the the most relevant"
      },
      {
        "start": 1572.12,
        "duration": 4.159,
        "text": "information and I only need to give you"
      },
      {
        "start": 1573.44,
        "duration": 2.839,
        "text": "a few examples to solve"
      },
      {
        "start": 1576.62,
        "duration": 6.72,
        "text": "very cool we actually have a question"
      },
      {
        "start": 1578.779,
        "duration": 7.38,
        "text": "it's um it's regard from Ashanti"
      },
      {
        "start": 1583.34,
        "duration": 5.219,
        "text": "asking following up on the embeddings of"
      },
      {
        "start": 1586.159,
        "duration": 5.281,
        "text": "books why would vectors be focused on"
      },
      {
        "start": 1588.559,
        "duration": 5.461,
        "text": "books by the same author for example can"
      },
      {
        "start": 1591.44,
        "duration": 4.38,
        "text": "we search for a book that is about crime"
      },
      {
        "start": 1594.02,
        "duration": 2.88,
        "text": "just in general like how why are they"
      },
      {
        "start": 1595.82,
        "duration": 2.52,
        "text": "can you talk a little bit about the"
      },
      {
        "start": 1596.9,
        "duration": 3.48,
        "text": "grouping and how those things actually"
      },
      {
        "start": 1598.34,
        "duration": 4.199,
        "text": "occur"
      },
      {
        "start": 1600.38,
        "duration": 4.919,
        "text": "yeah I will admit that I think that"
      },
      {
        "start": 1602.539,
        "duration": 5.161,
        "text": "example by authors is a little unusual"
      },
      {
        "start": 1605.299,
        "duration": 3.781,
        "text": "like you can definitely do that um but I"
      },
      {
        "start": 1607.7,
        "duration": 3.9,
        "text": "think the graphic kind of shows how this"
      },
      {
        "start": 1609.08,
        "duration": 4.62,
        "text": "works really nicely and um so we we kind"
      },
      {
        "start": 1611.6,
        "duration": 4.98,
        "text": "of reuse that one but um you are correct"
      },
      {
        "start": 1613.7,
        "duration": 3.66,
        "text": "that usually you know in many use cases"
      },
      {
        "start": 1616.58,
        "duration": 2.76,
        "text": "you would do it the way you're"
      },
      {
        "start": 1617.36,
        "duration": 3.78,
        "text": "describing where like crime novels are"
      },
      {
        "start": 1619.34,
        "duration": 4.699,
        "text": "in the same spot and comedies are in the"
      },
      {
        "start": 1621.14,
        "duration": 5.1,
        "text": "same spot and sci-fi groups together"
      },
      {
        "start": 1624.039,
        "duration": 3.461,
        "text": "because if you look at like the text"
      },
      {
        "start": 1626.24,
        "duration": 4.38,
        "text": "content these things are kind of talking"
      },
      {
        "start": 1627.5,
        "duration": 5.039,
        "text": "about the same Concepts and ideas"
      },
      {
        "start": 1630.62,
        "duration": 3.96,
        "text": "um but the interesting thing is right if"
      },
      {
        "start": 1632.539,
        "duration": 4.081,
        "text": "you need to do something by author and"
      },
      {
        "start": 1634.58,
        "duration": 3.719,
        "text": "that's the way you want to group"
      },
      {
        "start": 1636.62,
        "duration": 3.059,
        "text": "um like you can do that right you could"
      },
      {
        "start": 1638.299,
        "duration": 2.701,
        "text": "just build your text strings instead of"
      },
      {
        "start": 1639.679,
        "duration": 2.761,
        "text": "inventing the whole documents you can"
      },
      {
        "start": 1641.0,
        "duration": 3.0,
        "text": "embed like the titles and like the"
      },
      {
        "start": 1642.44,
        "duration": 2.76,
        "text": "author's name as like the text string"
      },
      {
        "start": 1644.0,
        "duration": 3.299,
        "text": "and then you would get an embedding"
      },
      {
        "start": 1645.2,
        "duration": 4.979,
        "text": "space that's going to help you cluster"
      },
      {
        "start": 1647.299,
        "duration": 4.801,
        "text": "by author name instead of by content of"
      },
      {
        "start": 1650.179,
        "duration": 4.081,
        "text": "the book so you can always tweak these"
      },
      {
        "start": 1652.1,
        "duration": 5.939,
        "text": "things based on what is the use case"
      },
      {
        "start": 1654.26,
        "duration": 7.399,
        "text": "that you want to solve"
      },
      {
        "start": 1658.039,
        "duration": 3.62,
        "text": "cool all right"
      },
      {
        "start": 1661.82,
        "duration": 3.599,
        "text": "um and then here's a simple one uh you"
      },
      {
        "start": 1663.62,
        "duration": 3.36,
        "text": "know like structured data extraction so"
      },
      {
        "start": 1665.419,
        "duration": 2.88,
        "text": "if you think about the instacart example"
      },
      {
        "start": 1666.98,
        "duration": 3.0,
        "text": "this is something they need to do right"
      },
      {
        "start": 1668.299,
        "duration": 3.541,
        "text": "at the end you've got a recipe and you"
      },
      {
        "start": 1669.98,
        "duration": 4.679,
        "text": "say build me a shopping cart well they"
      },
      {
        "start": 1671.84,
        "duration": 4.56,
        "text": "need to go back analyze the recipe and"
      },
      {
        "start": 1674.659,
        "duration": 2.461,
        "text": "get a list of ingredients"
      },
      {
        "start": 1676.4,
        "duration": 2.04,
        "text": "um"
      },
      {
        "start": 1677.12,
        "duration": 2.939,
        "text": "there's not much you need to do here"
      },
      {
        "start": 1678.44,
        "duration": 3.96,
        "text": "because llms are particularly good at"
      },
      {
        "start": 1680.059,
        "duration": 4.62,
        "text": "this I mean this is I this is an example"
      },
      {
        "start": 1682.4,
        "duration": 3.6,
        "text": "literally from just chat GPT where I I"
      },
      {
        "start": 1684.679,
        "duration": 3.6,
        "text": "wrote a little paragraph about George"
      },
      {
        "start": 1686.0,
        "duration": 4.679,
        "text": "Banks and that he works in a bank in"
      },
      {
        "start": 1688.279,
        "duration": 3.721,
        "text": "London and you know his age and then I"
      },
      {
        "start": 1690.679,
        "duration": 3.0,
        "text": "just asked it in the prom just to create"
      },
      {
        "start": 1692.0,
        "duration": 3.0,
        "text": "me a Json document that extracts name"
      },
      {
        "start": 1693.679,
        "duration": 3.421,
        "text": "age profession and location of the"
      },
      {
        "start": 1695.0,
        "duration": 4.799,
        "text": "primary person discussed gave them this"
      },
      {
        "start": 1697.1,
        "duration": 5.4,
        "text": "text document and it gives us this Json"
      },
      {
        "start": 1699.799,
        "duration": 6.0,
        "text": "um there I you can't have more complex"
      },
      {
        "start": 1702.5,
        "duration": 4.74,
        "text": "ish um data extraction issues where you"
      },
      {
        "start": 1705.799,
        "duration": 3.0,
        "text": "might need to do some few shot learning"
      },
      {
        "start": 1707.24,
        "duration": 4.2,
        "text": "so you give it an input and you give it"
      },
      {
        "start": 1708.799,
        "duration": 3.781,
        "text": "an output example here's a text here's"
      },
      {
        "start": 1711.44,
        "duration": 3.0,
        "text": "the information I want you to extract"
      },
      {
        "start": 1712.58,
        "duration": 4.8,
        "text": "from it give it three or four of those"
      },
      {
        "start": 1714.44,
        "duration": 4.38,
        "text": "and you can um get some information out"
      },
      {
        "start": 1717.38,
        "duration": 4.26,
        "text": "but if it's a pretty simple structured"
      },
      {
        "start": 1718.82,
        "duration": 5.88,
        "text": "data extraction this will generally work"
      },
      {
        "start": 1721.64,
        "duration": 6.18,
        "text": "and the simplest data extraction can be"
      },
      {
        "start": 1724.7,
        "duration": 6.18,
        "text": "used to generate apis to make calls to"
      },
      {
        "start": 1727.82,
        "duration": 5.04,
        "text": "let's say a checkout pipeline for"
      },
      {
        "start": 1730.88,
        "duration": 4.62,
        "text": "actually doing these orderings right so"
      },
      {
        "start": 1732.86,
        "duration": 5.34,
        "text": "yeah that's very or that that's kind of"
      },
      {
        "start": 1735.5,
        "duration": 4.26,
        "text": "uh this is where the llms interact with"
      },
      {
        "start": 1738.2,
        "duration": 3.78,
        "text": "the actual world right"
      },
      {
        "start": 1739.76,
        "duration": 3.96,
        "text": "and llms are very good at producing Json"
      },
      {
        "start": 1741.98,
        "duration": 3.9,
        "text": "and also understanding and processing"
      },
      {
        "start": 1743.72,
        "duration": 4.86,
        "text": "Json right so they this is a good"
      },
      {
        "start": 1745.88,
        "duration": 4.679,
        "text": "interaction you know interface between"
      },
      {
        "start": 1748.58,
        "duration": 4.44,
        "text": "the rest of your application stack and"
      },
      {
        "start": 1750.559,
        "duration": 6.0,
        "text": "the language model"
      },
      {
        "start": 1753.02,
        "duration": 5.639,
        "text": "uh we actually have a question from"
      },
      {
        "start": 1756.559,
        "duration": 5.701,
        "text": "um uh we have an extra question here and"
      },
      {
        "start": 1758.659,
        "duration": 7.14,
        "text": "it's and it's asking can you define how"
      },
      {
        "start": 1762.26,
        "duration": 5.94,
        "text": "the semantics uh similarity is defined"
      },
      {
        "start": 1765.799,
        "duration": 7.021,
        "text": "which probably is asking can you define"
      },
      {
        "start": 1768.2,
        "duration": 7.14,
        "text": "how the various uh vectors are clustered"
      },
      {
        "start": 1772.82,
        "duration": 5.219,
        "text": "together it's like probably asking about"
      },
      {
        "start": 1775.34,
        "duration": 4.68,
        "text": "embedding model techniques"
      },
      {
        "start": 1778.039,
        "duration": 3.0,
        "text": "yeah so there's there's maybe two parts"
      },
      {
        "start": 1780.02,
        "duration": 3.12,
        "text": "there"
      },
      {
        "start": 1781.039,
        "duration": 3.661,
        "text": "um as I was saying you know one is"
      },
      {
        "start": 1783.14,
        "duration": 4.56,
        "text": "well I guess there's there's three parts"
      },
      {
        "start": 1784.7,
        "duration": 4.92,
        "text": "I would say right one is structuring"
      },
      {
        "start": 1787.7,
        "duration": 4.32,
        "text": "your initial data so that you'll get the"
      },
      {
        "start": 1789.62,
        "duration": 3.84,
        "text": "right so so data will be nearby each"
      },
      {
        "start": 1792.02,
        "duration": 3.899,
        "text": "other for the problem you want to solve"
      },
      {
        "start": 1793.46,
        "duration": 6.0,
        "text": "so we had the example earlier of if you"
      },
      {
        "start": 1795.919,
        "duration": 6.301,
        "text": "want to uh cluster"
      },
      {
        "start": 1799.46,
        "duration": 4.38,
        "text": "books by author you probably want to"
      },
      {
        "start": 1802.22,
        "duration": 3.36,
        "text": "like have your text that you're"
      },
      {
        "start": 1803.84,
        "duration": 4.199,
        "text": "embedding be the title of the book and"
      },
      {
        "start": 1805.58,
        "duration": 6.0,
        "text": "the author's name if you want to Cluster"
      },
      {
        "start": 1808.039,
        "duration": 5.041,
        "text": "books by genre you can probably just you"
      },
      {
        "start": 1811.58,
        "duration": 3.479,
        "text": "know you would just process the actual"
      },
      {
        "start": 1813.08,
        "duration": 3.24,
        "text": "text of the book itself and then rely on"
      },
      {
        "start": 1815.059,
        "duration": 4.261,
        "text": "that for it to kind of group together"
      },
      {
        "start": 1816.32,
        "duration": 5.219,
        "text": "kind of related types of books"
      },
      {
        "start": 1819.32,
        "duration": 4.079,
        "text": "um so that's kind of one there's kind of"
      },
      {
        "start": 1821.539,
        "duration": 3.421,
        "text": "embedding model choice there's a lot of"
      },
      {
        "start": 1823.399,
        "duration": 3.241,
        "text": "different embedding models out there you"
      },
      {
        "start": 1824.96,
        "duration": 4.079,
        "text": "know there's vendor ones like open AI"
      },
      {
        "start": 1826.64,
        "duration": 4.68,
        "text": "there's open source ones"
      },
      {
        "start": 1829.039,
        "duration": 3.781,
        "text": "um they'll embed data in slightly"
      },
      {
        "start": 1831.32,
        "duration": 3.359,
        "text": "different ways so sometimes you might"
      },
      {
        "start": 1832.82,
        "duration": 3.18,
        "text": "want to try different models to see if"
      },
      {
        "start": 1834.679,
        "duration": 3.781,
        "text": "it does something better for your"
      },
      {
        "start": 1836.0,
        "duration": 4.2,
        "text": "problem and then the last piece is like"
      },
      {
        "start": 1838.46,
        "duration": 2.699,
        "text": "how do you tell that two vectors are"
      },
      {
        "start": 1840.2,
        "duration": 3.12,
        "text": "similar"
      },
      {
        "start": 1841.159,
        "duration": 3.721,
        "text": "right which would be there's various"
      },
      {
        "start": 1843.32,
        "duration": 2.64,
        "text": "different similarity metrics that you"
      },
      {
        "start": 1844.88,
        "duration": 2.519,
        "text": "use"
      },
      {
        "start": 1845.96,
        "duration": 3.36,
        "text": "um some of the most common ones would be"
      },
      {
        "start": 1847.399,
        "duration": 4.461,
        "text": "like dot products or inner products"
      },
      {
        "start": 1849.32,
        "duration": 5.04,
        "text": "cosine similarity and euclidean distance"
      },
      {
        "start": 1851.86,
        "duration": 4.36,
        "text": "right so liquidity and distance you can"
      },
      {
        "start": 1854.36,
        "duration": 4.02,
        "text": "think of is really just telling you how"
      },
      {
        "start": 1856.22,
        "duration": 3.839,
        "text": "far apart are two vectors cosine"
      },
      {
        "start": 1858.38,
        "duration": 3.419,
        "text": "similarities more telling you what's the"
      },
      {
        "start": 1860.059,
        "duration": 3.12,
        "text": "difference in the angle between two"
      },
      {
        "start": 1861.799,
        "duration": 4.38,
        "text": "vectors but"
      },
      {
        "start": 1863.179,
        "duration": 5.461,
        "text": "under the hood you use these these uh"
      },
      {
        "start": 1866.179,
        "duration": 4.5,
        "text": "you know math models to decide like how"
      },
      {
        "start": 1868.64,
        "duration": 4.019,
        "text": "close together are two things and then"
      },
      {
        "start": 1870.679,
        "duration": 4.461,
        "text": "you use that to decide which Vector am I"
      },
      {
        "start": 1872.659,
        "duration": 2.481,
        "text": "going to choose"
      },
      {
        "start": 1875.36,
        "duration": 3.66,
        "text": "cool"
      },
      {
        "start": 1877.34,
        "duration": 3.36,
        "text": "all right and then just kind of really"
      },
      {
        "start": 1879.02,
        "duration": 3.3,
        "text": "quickly like how would you put all of"
      },
      {
        "start": 1880.7,
        "duration": 3.06,
        "text": "this together so this is just an example"
      },
      {
        "start": 1882.32,
        "duration": 3.12,
        "text": "of kind of what does like the"
      },
      {
        "start": 1883.76,
        "duration": 3.419,
        "text": "application stack look like when you"
      },
      {
        "start": 1885.44,
        "duration": 4.14,
        "text": "build an application that uses"
      },
      {
        "start": 1887.179,
        "duration": 3.961,
        "text": "generative AI so starting from the left"
      },
      {
        "start": 1889.58,
        "duration": 3.959,
        "text": "you've got your application that you're"
      },
      {
        "start": 1891.14,
        "duration": 4.44,
        "text": "building at your company right in the"
      },
      {
        "start": 1893.539,
        "duration": 4.201,
        "text": "middle you're going to have the movement"
      },
      {
        "start": 1895.58,
        "duration": 3.719,
        "text": "to the right you know gonna have a"
      },
      {
        "start": 1897.74,
        "duration": 3.72,
        "text": "service router come in you'll have your"
      },
      {
        "start": 1899.299,
        "duration": 3.301,
        "text": "you know eight your apis the kind of"
      },
      {
        "start": 1901.46,
        "duration": 2.16,
        "text": "interface with this and you know"
      },
      {
        "start": 1902.6,
        "duration": 2.819,
        "text": "probably that goes down to some micro"
      },
      {
        "start": 1903.62,
        "duration": 3.779,
        "text": "service architecture this is all kind of"
      },
      {
        "start": 1905.419,
        "duration": 4.081,
        "text": "just standard really any application"
      },
      {
        "start": 1907.399,
        "duration": 4.201,
        "text": "infrastructure at this point but then"
      },
      {
        "start": 1909.5,
        "duration": 5.0,
        "text": "from that point you'll have agents"
      },
      {
        "start": 1911.6,
        "duration": 5.34,
        "text": "agents are kind of the concept in"
      },
      {
        "start": 1914.5,
        "duration": 4.24,
        "text": "generative AI world where the agent is"
      },
      {
        "start": 1916.94,
        "duration": 4.32,
        "text": "kind of responsible for deciding what"
      },
      {
        "start": 1918.74,
        "duration": 4.38,
        "text": "actions should be taken what external"
      },
      {
        "start": 1921.26,
        "duration": 3.299,
        "text": "data do I need to fetch"
      },
      {
        "start": 1923.12,
        "duration": 3.12,
        "text": "um how am I going to combine this in the"
      },
      {
        "start": 1924.559,
        "duration": 4.98,
        "text": "generation steps to kind of solve my"
      },
      {
        "start": 1926.24,
        "duration": 4.86,
        "text": "problem that often relies on proprietary"
      },
      {
        "start": 1929.539,
        "duration": 3.661,
        "text": "data that you have so you need to kind"
      },
      {
        "start": 1931.1,
        "duration": 5.22,
        "text": "of process that data um you know maybe"
      },
      {
        "start": 1933.2,
        "duration": 4.56,
        "text": "it's bash data Maybe it's streaming data"
      },
      {
        "start": 1936.32,
        "duration": 2.52,
        "text": "um you generate embeddings like we"
      },
      {
        "start": 1937.76,
        "duration": 3.0,
        "text": "talked about so you have to have an"
      },
      {
        "start": 1938.84,
        "duration": 3.48,
        "text": "embedding service and again there's"
      },
      {
        "start": 1940.76,
        "duration": 3.299,
        "text": "vendor solutions for that and open"
      },
      {
        "start": 1942.32,
        "duration": 4.14,
        "text": "source and then you feed that data back"
      },
      {
        "start": 1944.059,
        "duration": 5.521,
        "text": "into a vector store where you maintain"
      },
      {
        "start": 1946.46,
        "duration": 6.06,
        "text": "it and then you'll need some llms"
      },
      {
        "start": 1949.58,
        "duration": 5.339,
        "text": "um which can be again from vendors or or"
      },
      {
        "start": 1952.52,
        "duration": 5.06,
        "text": "open source"
      },
      {
        "start": 1954.919,
        "duration": 2.661,
        "text": "all right"
      },
      {
        "start": 1957.86,
        "duration": 3.12,
        "text": "so I think we want to unless there's any"
      },
      {
        "start": 1959.72,
        "duration": 2.52,
        "text": "other questions I think maybe we want to"
      },
      {
        "start": 1960.98,
        "duration": 3.679,
        "text": "jump into kind of seeing how we put all"
      },
      {
        "start": 1962.24,
        "duration": 2.419,
        "text": "this together"
      },
      {
        "start": 1965.539,
        "duration": 3.781,
        "text": "so really quickly we're going to show"
      },
      {
        "start": 1967.34,
        "duration": 4.319,
        "text": "two examples this first one is going to"
      },
      {
        "start": 1969.32,
        "duration": 4.2,
        "text": "be a recipes example but it uses the"
      },
      {
        "start": 1971.659,
        "duration": 3.541,
        "text": "same patterns that booking.com and the"
      },
      {
        "start": 1973.52,
        "duration": 3.72,
        "text": "shop app use"
      },
      {
        "start": 1975.2,
        "duration": 4.44,
        "text": "um which is we need a set of embeddings"
      },
      {
        "start": 1977.24,
        "duration": 4.74,
        "text": "in this case there'll be recipes"
      },
      {
        "start": 1979.64,
        "duration": 4.62,
        "text": "we'll show how to do filtered Vector"
      },
      {
        "start": 1981.98,
        "duration": 4.199,
        "text": "search to kind of do a not just search"
      },
      {
        "start": 1984.26,
        "duration": 3.419,
        "text": "based on vectors but put in some other"
      },
      {
        "start": 1986.179,
        "duration": 3.661,
        "text": "criteria that we care about to make sure"
      },
      {
        "start": 1987.679,
        "duration": 4.261,
        "text": "we're getting good results back we'll"
      },
      {
        "start": 1989.84,
        "duration": 3.839,
        "text": "look at re-ranking those results to get"
      },
      {
        "start": 1991.94,
        "duration": 4.14,
        "text": "even better results from the vector"
      },
      {
        "start": 1993.679,
        "duration": 3.781,
        "text": "search and then for there's a structured"
      },
      {
        "start": 1996.08,
        "duration": 3.42,
        "text": "data extraction at the end right to kind"
      },
      {
        "start": 1997.46,
        "duration": 2.64,
        "text": "of build the shopping cart"
      },
      {
        "start": 1999.5,
        "duration": 2.159,
        "text": "um"
      },
      {
        "start": 2000.1,
        "duration": 3.9,
        "text": "and this is the kind of pattern that"
      },
      {
        "start": 2001.659,
        "duration": 5.341,
        "text": "booking.com Shop app and"
      },
      {
        "start": 2004.0,
        "duration": 5.34,
        "text": "um instacart kind of all use this"
      },
      {
        "start": 2007.0,
        "duration": 4.44,
        "text": "example will use open some open AI stuff"
      },
      {
        "start": 2009.34,
        "duration": 4.02,
        "text": "some Lang stuff from Lang chain and for"
      },
      {
        "start": 2011.44,
        "duration": 4.56,
        "text": "our Vector store we use a data Stacks"
      },
      {
        "start": 2013.36,
        "duration": 5.699,
        "text": "Astra DB and then"
      },
      {
        "start": 2016.0,
        "duration": 4.679,
        "text": "second example for kind of a q a chat"
      },
      {
        "start": 2019.059,
        "duration": 3.6,
        "text": "bot so we'll create embeddings of some"
      },
      {
        "start": 2020.679,
        "duration": 4.74,
        "text": "PDF files and show how you use retrieval"
      },
      {
        "start": 2022.659,
        "duration": 4.321,
        "text": "augmented generation and flare which is"
      },
      {
        "start": 2025.419,
        "duration": 3.12,
        "text": "like a kind of fancier version of"
      },
      {
        "start": 2026.98,
        "duration": 4.02,
        "text": "retrieval augmented generation I'll talk"
      },
      {
        "start": 2028.539,
        "duration": 5.341,
        "text": "about in the demo this one uses another"
      },
      {
        "start": 2031.0,
        "duration": 4.5,
        "text": "Library called Casio that data Stacks"
      },
      {
        "start": 2033.88,
        "duration": 3.299,
        "text": "built that abstracts away the"
      },
      {
        "start": 2035.5,
        "duration": 3.899,
        "text": "complexities of the database just lets"
      },
      {
        "start": 2037.179,
        "duration": 3.301,
        "text": "you kind of solve your llm problem you"
      },
      {
        "start": 2039.399,
        "duration": 2.581,
        "text": "don't have to kind of worry about how"
      },
      {
        "start": 2040.48,
        "duration": 4.02,
        "text": "the database works if you don't if you"
      },
      {
        "start": 2041.98,
        "duration": 4.199,
        "text": "aren't familiar with it"
      },
      {
        "start": 2044.5,
        "duration": 3.24,
        "text": "and then just really quickly won't spend"
      },
      {
        "start": 2046.179,
        "duration": 4.021,
        "text": "a lot of time with this but"
      },
      {
        "start": 2047.74,
        "duration": 4.619,
        "text": "you know why should you use astrodb"
      },
      {
        "start": 2050.2,
        "duration": 4.219,
        "text": "um you know people may not be familiar"
      },
      {
        "start": 2052.359,
        "duration": 5.581,
        "text": "with Astra but Astra is basically"
      },
      {
        "start": 2054.419,
        "duration": 4.661,
        "text": "serverless version of Apache Cassandra I"
      },
      {
        "start": 2057.94,
        "duration": 4.62,
        "text": "think a lot of people are familiar with"
      },
      {
        "start": 2059.08,
        "duration": 5.519,
        "text": "uh Cassandra scales very well"
      },
      {
        "start": 2062.56,
        "duration": 4.74,
        "text": "um on the serverless side where in all"
      },
      {
        "start": 2064.599,
        "duration": 4.32,
        "text": "clouds it's very cheap to run"
      },
      {
        "start": 2067.3,
        "duration": 4.02,
        "text": "um it's trusted by more than 90 of the"
      },
      {
        "start": 2068.919,
        "duration": 4.92,
        "text": "Fortune 500 and"
      },
      {
        "start": 2071.32,
        "duration": 4.74,
        "text": "more importantly even though Vector"
      },
      {
        "start": 2073.839,
        "duration": 4.56,
        "text": "search is new for us our indexing"
      },
      {
        "start": 2076.06,
        "duration": 3.66,
        "text": "technology is has been around for a long"
      },
      {
        "start": 2078.399,
        "duration": 3.0,
        "text": "time we've been working on it for 10"
      },
      {
        "start": 2079.72,
        "duration": 3.3,
        "text": "years we have this Nifty thing called"
      },
      {
        "start": 2081.399,
        "duration": 4.561,
        "text": "storage attached indices I won't get"
      },
      {
        "start": 2083.02,
        "duration": 5.339,
        "text": "into that but um we we use that same"
      },
      {
        "start": 2085.96,
        "duration": 4.139,
        "text": "technology to do our Vector surge so our"
      },
      {
        "start": 2088.359,
        "duration": 3.3,
        "text": "Vector search is not new it leverages"
      },
      {
        "start": 2090.099,
        "duration": 5.421,
        "text": "all this work that we've done to build"
      },
      {
        "start": 2091.659,
        "duration": 3.861,
        "text": "kind of really effective search indexes"
      },
      {
        "start": 2096.159,
        "duration": 2.601,
        "text": "all right"
      },
      {
        "start": 2103.359,
        "duration": 3.901,
        "text": "uh you should be able to see"
      },
      {
        "start": 2105.64,
        "duration": 3.18,
        "text": "um"
      },
      {
        "start": 2107.26,
        "duration": 3.9,
        "text": "I know that the share screen I think"
      },
      {
        "start": 2108.82,
        "duration": 4.86,
        "text": "shows up in a smaller box in in the"
      },
      {
        "start": 2111.16,
        "duration": 4.86,
        "text": "webinar there's a button to you know"
      },
      {
        "start": 2113.68,
        "duration": 4.08,
        "text": "make it bigger if you can't see it um if"
      },
      {
        "start": 2116.02,
        "duration": 4.5,
        "text": "you hover over it you can full screen it"
      },
      {
        "start": 2117.76,
        "duration": 4.56,
        "text": "or kind of make it larger also"
      },
      {
        "start": 2120.52,
        "duration": 3.66,
        "text": "um there should be a a button on the"
      },
      {
        "start": 2122.32,
        "duration": 3.24,
        "text": "bottom for kind of documents that we've"
      },
      {
        "start": 2124.18,
        "duration": 2.7,
        "text": "made available if it's not showing up"
      },
      {
        "start": 2125.56,
        "duration": 3.24,
        "text": "and if you click in that there should be"
      },
      {
        "start": 2126.88,
        "duration": 3.06,
        "text": "links to collab versions of these"
      },
      {
        "start": 2128.8,
        "duration": 2.64,
        "text": "notebooks"
      },
      {
        "start": 2129.94,
        "duration": 3.78,
        "text": "if you want to run through them on your"
      },
      {
        "start": 2131.44,
        "duration": 5.22,
        "text": "own or have access to them afterwards"
      },
      {
        "start": 2133.72,
        "duration": 5.1,
        "text": "so actually before you jump into there"
      },
      {
        "start": 2136.66,
        "duration": 3.54,
        "text": "do you mind going back to the to the"
      },
      {
        "start": 2138.82,
        "duration": 3.42,
        "text": "diagram there's actually a couple of"
      },
      {
        "start": 2140.2,
        "duration": 4.56,
        "text": "questions here"
      },
      {
        "start": 2142.24,
        "duration": 4.8,
        "text": "from the the phone so maybe the first"
      },
      {
        "start": 2144.76,
        "duration": 4.44,
        "text": "question is does the vector database"
      },
      {
        "start": 2147.04,
        "duration": 4.14,
        "text": "should you keep all your proprietary"
      },
      {
        "start": 2149.2,
        "duration": 4.379,
        "text": "data in the vector database or is there"
      },
      {
        "start": 2151.18,
        "duration": 4.2,
        "text": "some proprietary data that's not in the"
      },
      {
        "start": 2153.579,
        "duration": 3.301,
        "text": "vector database versus others like what"
      },
      {
        "start": 2155.38,
        "duration": 3.12,
        "text": "where what's appropriate what"
      },
      {
        "start": 2156.88,
        "duration": 3.6,
        "text": "proprietary data is appropriate in the"
      },
      {
        "start": 2158.5,
        "duration": 4.079,
        "text": "vector days"
      },
      {
        "start": 2160.48,
        "duration": 4.5,
        "text": "so that's going to depend a little bit"
      },
      {
        "start": 2162.579,
        "duration": 5.401,
        "text": "on the database you choose one of the"
      },
      {
        "start": 2164.98,
        "duration": 6.0,
        "text": "advantages of Astra is we are not a"
      },
      {
        "start": 2167.98,
        "duration": 4.859,
        "text": "purely Vector database like we store all"
      },
      {
        "start": 2170.98,
        "duration": 3.42,
        "text": "types of data so you can put all of your"
      },
      {
        "start": 2172.839,
        "duration": 4.561,
        "text": "application data to run your application"
      },
      {
        "start": 2174.4,
        "duration": 5.58,
        "text": "in Astra and the vectors are just"
      },
      {
        "start": 2177.4,
        "duration": 4.74,
        "text": "another data type alongside of text and"
      },
      {
        "start": 2179.98,
        "duration": 4.379,
        "text": "float into the other type of data you"
      },
      {
        "start": 2182.14,
        "duration": 3.719,
        "text": "would want to store so you know we would"
      },
      {
        "start": 2184.359,
        "duration": 2.76,
        "text": "generally recommend kind of putting all"
      },
      {
        "start": 2185.859,
        "duration": 3.061,
        "text": "of your data in there that you're going"
      },
      {
        "start": 2187.119,
        "duration": 3.541,
        "text": "to need to run this application so if"
      },
      {
        "start": 2188.92,
        "duration": 3.78,
        "text": "there's other data that comes in it's"
      },
      {
        "start": 2190.66,
        "duration": 3.66,
        "text": "not part of the generative step you can"
      },
      {
        "start": 2192.7,
        "duration": 3.0,
        "text": "still build out those tables in Astra"
      },
      {
        "start": 2194.32,
        "duration": 3.86,
        "text": "and then you just have one database"
      },
      {
        "start": 2195.7,
        "duration": 2.48,
        "text": "you're working with"
      },
      {
        "start": 2199.359,
        "duration": 5.641,
        "text": "okay and the other question is"
      },
      {
        "start": 2202.96,
        "duration": 4.379,
        "text": "um you know where where do quote-unquote"
      },
      {
        "start": 2205.0,
        "duration": 5.18,
        "text": "tools I mean the agent sense what are"
      },
      {
        "start": 2207.339,
        "duration": 8.76,
        "text": "tools fit into the diagram how how how"
      },
      {
        "start": 2210.18,
        "duration": 7.3,
        "text": "to uh LMS end up calling other apis"
      },
      {
        "start": 2216.099,
        "duration": 2.581,
        "text": "um so maybe we'll hold on that one"
      },
      {
        "start": 2217.48,
        "duration": 2.82,
        "text": "because I actually have an example of"
      },
      {
        "start": 2218.68,
        "duration": 3.3,
        "text": "that in the notebook that we're going to"
      },
      {
        "start": 2220.3,
        "duration": 4.7,
        "text": "run through so I'll talk about it when"
      },
      {
        "start": 2221.98,
        "duration": 3.02,
        "text": "we get to that as part of"
      },
      {
        "start": 2225.339,
        "duration": 4.5,
        "text": "our department okay a few people are"
      },
      {
        "start": 2227.14,
        "duration": 4.8,
        "text": "reporting that the flare notebook uh is"
      },
      {
        "start": 2229.839,
        "duration": 4.561,
        "text": "not accessible uh that the link is"
      },
      {
        "start": 2231.94,
        "duration": 6.12,
        "text": "broken so we'll be fixing that uh after"
      },
      {
        "start": 2234.4,
        "duration": 5.58,
        "text": "this or if Alejandro can do a quick uh"
      },
      {
        "start": 2238.06,
        "duration": 3.9,
        "text": "fix that would also be that would also"
      },
      {
        "start": 2239.98,
        "duration": 3.66,
        "text": "work too"
      },
      {
        "start": 2241.96,
        "duration": 3.54,
        "text": "I wonder if we just somehow have the"
      },
      {
        "start": 2243.64,
        "duration": 3.3,
        "text": "wrong win because uh the the viewing"
      },
      {
        "start": 2245.5,
        "duration": 4.88,
        "text": "permissions are correct we'll double"
      },
      {
        "start": 2246.94,
        "duration": 3.44,
        "text": "check and send it out afterwards"
      },
      {
        "start": 2254.079,
        "duration": 3.481,
        "text": "all right"
      },
      {
        "start": 2256.18,
        "duration": 3.6,
        "text": "so"
      },
      {
        "start": 2257.56,
        "duration": 5.46,
        "text": "um to run this recipes workbook"
      },
      {
        "start": 2259.78,
        "duration": 6.0,
        "text": "um you need an opening a AI API key and"
      },
      {
        "start": 2263.02,
        "duration": 3.54,
        "text": "then you need some connect connection"
      },
      {
        "start": 2265.78,
        "duration": 3.059,
        "text": "um"
      },
      {
        "start": 2266.56,
        "duration": 3.059,
        "text": "credentials for astrodb you can get"
      },
      {
        "start": 2268.839,
        "duration": 3.78,
        "text": "these on our website"
      },
      {
        "start": 2269.619,
        "duration": 4.681,
        "text": "astro.datastacks.com"
      },
      {
        "start": 2272.619,
        "duration": 3.421,
        "text": "and"
      },
      {
        "start": 2274.3,
        "duration": 2.76,
        "text": "you know here's the the module using"
      },
      {
        "start": 2276.04,
        "duration": 2.039,
        "text": "you'll see I'm not running this in"
      },
      {
        "start": 2277.06,
        "duration": 3.72,
        "text": "collab"
      },
      {
        "start": 2278.079,
        "duration": 3.78,
        "text": "um I this one happens via local copy but"
      },
      {
        "start": 2280.78,
        "duration": 2.819,
        "text": "um"
      },
      {
        "start": 2281.859,
        "duration": 4.081,
        "text": "that's fine this is just some text"
      },
      {
        "start": 2283.599,
        "duration": 3.901,
        "text": "cleaning Pro tip a good thing for gender"
      },
      {
        "start": 2285.94,
        "duration": 3.24,
        "text": "of models because they tend to run out"
      },
      {
        "start": 2287.5,
        "duration": 3.0,
        "text": "of space when you you know runs off the"
      },
      {
        "start": 2289.18,
        "duration": 4.439,
        "text": "edge of the screen when you generate the"
      },
      {
        "start": 2290.5,
        "duration": 4.619,
        "text": "text this just fixes that"
      },
      {
        "start": 2293.619,
        "duration": 2.821,
        "text": "um I won't talk too much about how I"
      },
      {
        "start": 2295.119,
        "duration": 3.361,
        "text": "process the data set you guys are"
      },
      {
        "start": 2296.44,
        "duration": 4.98,
        "text": "welcome to look at the notebook but this"
      },
      {
        "start": 2298.48,
        "duration": 4.92,
        "text": "data set is basically a food.com set of"
      },
      {
        "start": 2301.42,
        "duration": 4.38,
        "text": "recipes as well as user interactions and"
      },
      {
        "start": 2303.4,
        "duration": 4.38,
        "text": "reviews that's available on kaggle"
      },
      {
        "start": 2305.8,
        "duration": 4.14,
        "text": "um this kind of predates the popularity"
      },
      {
        "start": 2307.78,
        "duration": 3.42,
        "text": "of llms so the data is actually very"
      },
      {
        "start": 2309.94,
        "duration": 3.0,
        "text": "structured they already kind of pulled"
      },
      {
        "start": 2311.2,
        "duration": 3.24,
        "text": "it all apart um in a way that you"
      },
      {
        "start": 2312.94,
        "duration": 2.76,
        "text": "wouldn't really do with llms anymore you"
      },
      {
        "start": 2314.44,
        "duration": 2.88,
        "text": "would just kind of process the raw text"
      },
      {
        "start": 2315.7,
        "duration": 5.48,
        "text": "so a lot of the data prep is actually"
      },
      {
        "start": 2317.32,
        "duration": 3.86,
        "text": "reassembling the original recipe"
      },
      {
        "start": 2321.28,
        "duration": 5.04,
        "text": "um"
      },
      {
        "start": 2322.5,
        "duration": 6.04,
        "text": "so you know you load and prep that data"
      },
      {
        "start": 2326.32,
        "duration": 5.22,
        "text": "basically all this is doing is giving us"
      },
      {
        "start": 2328.54,
        "duration": 4.26,
        "text": "a full text document of the recipe with"
      },
      {
        "start": 2331.54,
        "duration": 3.66,
        "text": "all the ingredients the steps the"
      },
      {
        "start": 2332.8,
        "duration": 4.38,
        "text": "instructions the title and adding in the"
      },
      {
        "start": 2335.2,
        "duration": 4.74,
        "text": "average review score for each recipe"
      },
      {
        "start": 2337.18,
        "duration": 5.04,
        "text": "which we'll use later on"
      },
      {
        "start": 2339.94,
        "duration": 4.08,
        "text": "um the next Parts just connecting to"
      },
      {
        "start": 2342.22,
        "duration": 3.72,
        "text": "Astra you need kind of two pieces for"
      },
      {
        "start": 2344.02,
        "duration": 4.14,
        "text": "this there's a secure connect bundle you"
      },
      {
        "start": 2345.94,
        "duration": 4.5,
        "text": "download from our website as well as a"
      },
      {
        "start": 2348.16,
        "duration": 4.08,
        "text": "token you generate and download There's"
      },
      {
        "start": 2350.44,
        "duration": 3.36,
        "text": "a quick connect button on a database"
      },
      {
        "start": 2352.24,
        "duration": 3.66,
        "text": "that will just generate all these files"
      },
      {
        "start": 2353.8,
        "duration": 4.08,
        "text": "for you and you can just upload them"
      },
      {
        "start": 2355.9,
        "duration": 2.76,
        "text": "and then you connect to the to the data"
      },
      {
        "start": 2357.88,
        "duration": 3.06,
        "text": "set"
      },
      {
        "start": 2358.66,
        "duration": 3.66,
        "text": "in this case as I mentioned we've got a"
      },
      {
        "start": 2360.94,
        "duration": 2.76,
        "text": "library called Casio that kind of"
      },
      {
        "start": 2362.32,
        "duration": 2.94,
        "text": "abstracts a lot of the underlying"
      },
      {
        "start": 2363.7,
        "duration": 4.26,
        "text": "database stuff but I just kind of wanted"
      },
      {
        "start": 2365.26,
        "duration": 3.96,
        "text": "to show how it works in this example so"
      },
      {
        "start": 2367.96,
        "duration": 3.54,
        "text": "it's pretty straightforward you know"
      },
      {
        "start": 2369.22,
        "duration": 3.6,
        "text": "we've got a table it's got a recipe ID"
      },
      {
        "start": 2371.5,
        "duration": 2.88,
        "text": "it's got"
      },
      {
        "start": 2372.82,
        "duration": 3.06,
        "text": "the text of the embedding it's got the"
      },
      {
        "start": 2374.38,
        "duration": 3.36,
        "text": "full text of the recipe because we might"
      },
      {
        "start": 2375.88,
        "duration": 3.78,
        "text": "need that so like the recipe is broken"
      },
      {
        "start": 2377.74,
        "duration": 2.94,
        "text": "up into different chunks but you know at"
      },
      {
        "start": 2379.66,
        "duration": 3.0,
        "text": "some point you might want to return the"
      },
      {
        "start": 2380.68,
        "duration": 3.36,
        "text": "full recipe so we keep the full text we"
      },
      {
        "start": 2382.66,
        "duration": 3.0,
        "text": "keep the title"
      },
      {
        "start": 2384.04,
        "duration": 3.84,
        "text": "the rating and then you can see we've"
      },
      {
        "start": 2385.66,
        "duration": 5.22,
        "text": "got like a float Vector that's got 768"
      },
      {
        "start": 2387.88,
        "duration": 6.54,
        "text": "Dimensions which is the size from our"
      },
      {
        "start": 2390.88,
        "duration": 5.64,
        "text": "embedding model and then you know you"
      },
      {
        "start": 2394.42,
        "duration": 3.72,
        "text": "create an index to search this we use"
      },
      {
        "start": 2396.52,
        "duration": 3.839,
        "text": "cosine similarity and then I also create"
      },
      {
        "start": 2398.14,
        "duration": 4.76,
        "text": "an index so I can search on the ratings"
      },
      {
        "start": 2400.359,
        "duration": 2.541,
        "text": "as well"
      },
      {
        "start": 2403.66,
        "duration": 6.179,
        "text": "um the next step is you've got to"
      },
      {
        "start": 2405.9,
        "duration": 5.74,
        "text": "generate and load data Astra create the"
      },
      {
        "start": 2409.839,
        "duration": 4.26,
        "text": "embeddings you can do this with openai"
      },
      {
        "start": 2411.64,
        "duration": 4.02,
        "text": "this data set has about 200 000 recipes"
      },
      {
        "start": 2414.099,
        "duration": 3.48,
        "text": "so I didn't really want to do this on"
      },
      {
        "start": 2415.66,
        "duration": 3.06,
        "text": "openai because I regenerated a lot of"
      },
      {
        "start": 2417.579,
        "duration": 3.841,
        "text": "times and I didn't want to pay the money"
      },
      {
        "start": 2418.72,
        "duration": 4.44,
        "text": "for that so I ran an open source model"
      },
      {
        "start": 2421.42,
        "duration": 3.3,
        "text": "called the instructor embeddings they're"
      },
      {
        "start": 2423.16,
        "duration": 3.78,
        "text": "linked here and there's actually a repo"
      },
      {
        "start": 2424.72,
        "duration": 4.2,
        "text": "that has the embedding service that this"
      },
      {
        "start": 2426.94,
        "duration": 4.44,
        "text": "is a restful API service that wraps that"
      },
      {
        "start": 2428.92,
        "duration": 4.86,
        "text": "and generates the embeddings"
      },
      {
        "start": 2431.38,
        "duration": 4.5,
        "text": "um it uses a sentence chunker instead of"
      },
      {
        "start": 2433.78,
        "duration": 4.22,
        "text": "just uh even text splitter so it"
      },
      {
        "start": 2435.88,
        "duration": 4.26,
        "text": "preserves sentence boundaries and then"
      },
      {
        "start": 2438.0,
        "duration": 3.28,
        "text": "uh if you want to run all of this"
      },
      {
        "start": 2440.14,
        "duration": 3.36,
        "text": "yourself this particular embedding"
      },
      {
        "start": 2441.28,
        "duration": 4.559,
        "text": "service runs pretty well on M1 and M2"
      },
      {
        "start": 2443.5,
        "duration": 4.92,
        "text": "Max on Apple silicon on CPU if you're"
      },
      {
        "start": 2445.839,
        "duration": 5.341,
        "text": "not on a CPU it's a little bit slow it"
      },
      {
        "start": 2448.42,
        "duration": 4.439,
        "text": "takes a few seconds so I'd recommend"
      },
      {
        "start": 2451.18,
        "duration": 4.26,
        "text": "like a GPU with Cuda if you're running"
      },
      {
        "start": 2452.859,
        "duration": 4.321,
        "text": "on um like a Linux image or an Intel"
      },
      {
        "start": 2455.44,
        "duration": 4.139,
        "text": "system"
      },
      {
        "start": 2457.18,
        "duration": 4.8,
        "text": "okay yeah related to that like we"
      },
      {
        "start": 2459.579,
        "duration": 5.101,
        "text": "actually have a question and it's like"
      },
      {
        "start": 2461.98,
        "duration": 4.859,
        "text": "the question specifically said like how"
      },
      {
        "start": 2464.68,
        "duration": 4.62,
        "text": "do you name the number types resigning"
      },
      {
        "start": 2466.839,
        "duration": 4.921,
        "text": "the vector database in Astra and I think"
      },
      {
        "start": 2469.3,
        "duration": 3.84,
        "text": "what is being really asked like how do"
      },
      {
        "start": 2471.76,
        "duration": 3.66,
        "text": "you differentiate between like"
      },
      {
        "start": 2473.14,
        "duration": 4.14,
        "text": "unstructured data versus kind of"
      },
      {
        "start": 2475.42,
        "duration": 5.88,
        "text": "structured data and how do you query the"
      },
      {
        "start": 2477.28,
        "duration": 5.76,
        "text": "two maybe marry them up together"
      },
      {
        "start": 2481.3,
        "duration": 3.48,
        "text": "yeah so we'll see a little bit of that"
      },
      {
        "start": 2483.04,
        "duration": 3.299,
        "text": "as we kind of go through this but you"
      },
      {
        "start": 2484.78,
        "duration": 4.559,
        "text": "can see like we've got kind of text"
      },
      {
        "start": 2486.339,
        "duration": 4.5,
        "text": "types so for the different text Fields"
      },
      {
        "start": 2489.339,
        "duration": 2.76,
        "text": "we've got we've got a float type here"
      },
      {
        "start": 2490.839,
        "duration": 5.401,
        "text": "because the rating is you know between"
      },
      {
        "start": 2492.099,
        "duration": 6.181,
        "text": "zero and five and has decimals and then"
      },
      {
        "start": 2496.24,
        "duration": 3.42,
        "text": "um the vector type and you can have as"
      },
      {
        "start": 2498.28,
        "duration": 3.24,
        "text": "many of these types you know you can"
      },
      {
        "start": 2499.66,
        "duration": 4.56,
        "text": "keep creating more columns on this like"
      },
      {
        "start": 2501.52,
        "duration": 3.72,
        "text": "we support you know very wide columns so"
      },
      {
        "start": 2504.22,
        "duration": 2.52,
        "text": "you can kind of just keep adding the"
      },
      {
        "start": 2505.24,
        "duration": 2.52,
        "text": "data that you need to kind of make this"
      },
      {
        "start": 2506.74,
        "duration": 2.879,
        "text": "work"
      },
      {
        "start": 2507.76,
        "duration": 4.44,
        "text": "and I'll show how you actually filter on"
      },
      {
        "start": 2509.619,
        "duration": 3.96,
        "text": "this a little further down"
      },
      {
        "start": 2512.2,
        "duration": 3.78,
        "text": "cool"
      },
      {
        "start": 2513.579,
        "duration": 4.201,
        "text": "uh so quickly to generate the data you"
      },
      {
        "start": 2515.98,
        "duration": 3.599,
        "text": "just do uh"
      },
      {
        "start": 2517.78,
        "duration": 3.839,
        "text": "kept two prepare statements here let's"
      },
      {
        "start": 2519.579,
        "duration": 4.381,
        "text": "just remove one okay just a simple"
      },
      {
        "start": 2521.619,
        "duration": 4.321,
        "text": "insert statement to put the data in I"
      },
      {
        "start": 2523.96,
        "duration": 4.08,
        "text": "call I call the API service with the"
      },
      {
        "start": 2525.94,
        "duration": 3.419,
        "text": "text you get back multiple embeddings so"
      },
      {
        "start": 2528.04,
        "duration": 4.22,
        "text": "you Loop over them and you just do an"
      },
      {
        "start": 2529.359,
        "duration": 5.461,
        "text": "insert and then we've got text"
      },
      {
        "start": 2532.26,
        "duration": 4.359,
        "text": "so now let's look at how the chat bot"
      },
      {
        "start": 2534.82,
        "duration": 5.279,
        "text": "itself works and so for this part I did"
      },
      {
        "start": 2536.619,
        "duration": 5.101,
        "text": "use Lang chain this is using gpt35 Turbo"
      },
      {
        "start": 2540.099,
        "duration": 5.341,
        "text": "from openai"
      },
      {
        "start": 2541.72,
        "duration": 6.18,
        "text": "and I'm going to ask it hi uh recipe bot"
      },
      {
        "start": 2545.44,
        "duration": 3.84,
        "text": "you know I wanna eat some Mexican food"
      },
      {
        "start": 2547.9,
        "duration": 2.219,
        "text": "tonight"
      },
      {
        "start": 2549.28,
        "duration": 2.1,
        "text": "right"
      },
      {
        "start": 2550.119,
        "duration": 2.341,
        "text": "um and then let's let's look at the"
      },
      {
        "start": 2551.38,
        "duration": 3.42,
        "text": "prompt so there was this question about"
      },
      {
        "start": 2552.46,
        "duration": 4.379,
        "text": "how do you call the external tools"
      },
      {
        "start": 2554.8,
        "duration": 3.539,
        "text": "and this kind of shows how to do this so"
      },
      {
        "start": 2556.839,
        "duration": 3.24,
        "text": "we used a pattern there's a there's a"
      },
      {
        "start": 2558.339,
        "duration": 3.24,
        "text": "paper called tool former it's from meta"
      },
      {
        "start": 2560.079,
        "duration": 4.801,
        "text": "AI research"
      },
      {
        "start": 2561.579,
        "duration": 5.04,
        "text": "and you basically tell it you know this"
      },
      {
        "start": 2564.88,
        "duration": 4.32,
        "text": "is an example of an external API call"
      },
      {
        "start": 2566.619,
        "duration": 4.681,
        "text": "okay so you tell it hey when you want to"
      },
      {
        "start": 2569.2,
        "duration": 4.08,
        "text": "recommend a recipe use an API to get the"
      },
      {
        "start": 2571.3,
        "duration": 3.6,
        "text": "recipes and you can see up here I tell"
      },
      {
        "start": 2573.28,
        "duration": 3.78,
        "text": "it do not create your own recipes right"
      },
      {
        "start": 2574.9,
        "duration": 4.56,
        "text": "use an API you can call the API by"
      },
      {
        "start": 2577.06,
        "duration": 4.14,
        "text": "writing recipes description where"
      },
      {
        "start": 2579.46,
        "duration": 5.1,
        "text": "description describes what recipe you're"
      },
      {
        "start": 2581.2,
        "duration": 5.58,
        "text": "searching for only make one APR API call"
      },
      {
        "start": 2584.56,
        "duration": 4.039,
        "text": "each time because otherwise they'll make"
      },
      {
        "start": 2586.78,
        "duration": 4.38,
        "text": "like three or four which we don't want"
      },
      {
        "start": 2588.599,
        "duration": 4.301,
        "text": "and if no food or recipe was mentioned"
      },
      {
        "start": 2591.16,
        "duration": 3.0,
        "text": "don't make an API call right because we"
      },
      {
        "start": 2592.9,
        "duration": 2.82,
        "text": "don't want to do that and then you then"
      },
      {
        "start": 2594.16,
        "duration": 3.72,
        "text": "you do some few shot learning so you"
      },
      {
        "start": 2595.72,
        "duration": 4.5,
        "text": "give it an example I would like to eat"
      },
      {
        "start": 2597.88,
        "duration": 4.08,
        "text": "Italian food tonight say great here are"
      },
      {
        "start": 2600.22,
        "duration": 3.599,
        "text": "some recipes for Italian food call the"
      },
      {
        "start": 2601.96,
        "duration": 3.42,
        "text": "API with Italian food"
      },
      {
        "start": 2603.819,
        "duration": 4.561,
        "text": "same thing for like I would like to cook"
      },
      {
        "start": 2605.38,
        "duration": 4.56,
        "text": "some spicy Indian chicken I insert spicy"
      },
      {
        "start": 2608.38,
        "duration": 5.52,
        "text": "Indian chicken"
      },
      {
        "start": 2609.94,
        "duration": 6.54,
        "text": "so let's see so down here so I I ran"
      },
      {
        "start": 2613.9,
        "duration": 4.62,
        "text": "that and you can see you know here's the"
      },
      {
        "start": 2616.48,
        "duration": 4.02,
        "text": "result so the model says sure Mexican"
      },
      {
        "start": 2618.52,
        "duration": 3.9,
        "text": "Cuisine is always a great choice here"
      },
      {
        "start": 2620.5,
        "duration": 4.5,
        "text": "are some recipes for Mexican food"
      },
      {
        "start": 2622.42,
        "duration": 3.72,
        "text": "you know and it puts in this API call"
      },
      {
        "start": 2625.0,
        "duration": 3.06,
        "text": "yeah"
      },
      {
        "start": 2626.14,
        "duration": 3.42,
        "text": "there's a second step here so this is a"
      },
      {
        "start": 2628.06,
        "duration": 3.72,
        "text": "chained model so I actually made two"
      },
      {
        "start": 2629.56,
        "duration": 3.84,
        "text": "calls the second one is basically"
      },
      {
        "start": 2631.78,
        "duration": 3.299,
        "text": "telling the model to ask follow-up"
      },
      {
        "start": 2633.4,
        "duration": 3.36,
        "text": "questions right so if you say I want"
      },
      {
        "start": 2635.079,
        "duration": 2.821,
        "text": "Mexican food that's pretty generic you"
      },
      {
        "start": 2636.76,
        "duration": 3.0,
        "text": "know maybe you want them to get more"
      },
      {
        "start": 2637.9,
        "duration": 3.6,
        "text": "specific so ask them to ask some"
      },
      {
        "start": 2639.76,
        "duration": 3.0,
        "text": "clarifying questions and again with few"
      },
      {
        "start": 2641.5,
        "duration": 1.98,
        "text": "shot learning you show them how to do"
      },
      {
        "start": 2642.76,
        "duration": 2.76,
        "text": "this"
      },
      {
        "start": 2643.48,
        "duration": 3.359,
        "text": "right oh if you would like to eat"
      },
      {
        "start": 2645.52,
        "duration": 3.599,
        "text": "Italian food what are you looking for"
      },
      {
        "start": 2646.839,
        "duration": 4.561,
        "text": "maybe you want Seafood meat or pasta"
      },
      {
        "start": 2649.119,
        "duration": 3.421,
        "text": "right or what kind if you ask I want to"
      },
      {
        "start": 2651.4,
        "duration": 2.52,
        "text": "cook pork today"
      },
      {
        "start": 2652.54,
        "duration": 3.18,
        "text": "what kind of pork would you like you"
      },
      {
        "start": 2653.92,
        "duration": 3.84,
        "text": "know there's sweet recipes Savory"
      },
      {
        "start": 2655.72,
        "duration": 4.56,
        "text": "recipes different different ethnic"
      },
      {
        "start": 2657.76,
        "duration": 4.559,
        "text": "styles"
      },
      {
        "start": 2660.28,
        "duration": 3.36,
        "text": "um okay so we do two generations and"
      },
      {
        "start": 2662.319,
        "duration": 2.821,
        "text": "then the last step"
      },
      {
        "start": 2663.64,
        "duration": 3.24,
        "text": "was a problem that just says Hey combine"
      },
      {
        "start": 2665.14,
        "duration": 2.76,
        "text": "these two things and then at some point"
      },
      {
        "start": 2666.88,
        "duration": 2.88,
        "text": "when you combine them and tell it hey"
      },
      {
        "start": 2667.9,
        "duration": 3.84,
        "text": "preserve this recipes API call because"
      },
      {
        "start": 2669.76,
        "duration": 3.3,
        "text": "that's not a standard thing in the"
      },
      {
        "start": 2671.74,
        "duration": 3.96,
        "text": "language model right"
      },
      {
        "start": 2673.06,
        "duration": 4.08,
        "text": "so if you look at this what do we get on"
      },
      {
        "start": 2675.7,
        "duration": 3.06,
        "text": "the first pass we already showed this"
      },
      {
        "start": 2677.14,
        "duration": 3.84,
        "text": "example from the vector search where it"
      },
      {
        "start": 2678.76,
        "duration": 4.98,
        "text": "gives us you know a search thing for"
      },
      {
        "start": 2680.98,
        "duration": 4.619,
        "text": "Mexican food on the second pass it says"
      },
      {
        "start": 2683.74,
        "duration": 3.54,
        "text": "hey there's a lot of different kinds of"
      },
      {
        "start": 2685.599,
        "duration": 2.821,
        "text": "variety of foods in Mexican food are you"
      },
      {
        "start": 2687.28,
        "duration": 3.78,
        "text": "in the mood for something more specific"
      },
      {
        "start": 2688.42,
        "duration": 4.26,
        "text": "like tacos enchiladas or a hearty soup"
      },
      {
        "start": 2691.06,
        "duration": 3.72,
        "text": "and so then if you guys have to combine"
      },
      {
        "start": 2692.68,
        "duration": 3.72,
        "text": "the two right you get a combined thing"
      },
      {
        "start": 2694.78,
        "duration": 3.0,
        "text": "where it will say Mexican food is a"
      },
      {
        "start": 2696.4,
        "duration": 2.939,
        "text": "great choice it's going to show you some"
      },
      {
        "start": 2697.78,
        "duration": 4.14,
        "text": "Mexican recipes and it's going to ask"
      },
      {
        "start": 2699.339,
        "duration": 4.26,
        "text": "you if you want to do something fancier"
      },
      {
        "start": 2701.92,
        "duration": 3.54,
        "text": "um cook something more specific"
      },
      {
        "start": 2703.599,
        "duration": 3.121,
        "text": "right so then how do you execute the API"
      },
      {
        "start": 2705.46,
        "duration": 2.879,
        "text": "call"
      },
      {
        "start": 2706.72,
        "duration": 5.04,
        "text": "um in this case right you just you just"
      },
      {
        "start": 2708.339,
        "duration": 5.76,
        "text": "need to kind of grip out the um the the"
      },
      {
        "start": 2711.76,
        "duration": 3.839,
        "text": "API call itself right so you just write"
      },
      {
        "start": 2714.099,
        "duration": 3.541,
        "text": "you can write like a regex that just"
      },
      {
        "start": 2715.599,
        "duration": 3.72,
        "text": "looks for API calls"
      },
      {
        "start": 2717.64,
        "duration": 3.84,
        "text": "and then you have a lookup table that"
      },
      {
        "start": 2719.319,
        "duration": 4.02,
        "text": "says oh if it's recipes I'm going to do"
      },
      {
        "start": 2721.48,
        "duration": 3.78,
        "text": "this particular recipes function call if"
      },
      {
        "start": 2723.339,
        "duration": 3.421,
        "text": "it's do something else right I can I can"
      },
      {
        "start": 2725.26,
        "duration": 3.48,
        "text": "make other function calls they can be"
      },
      {
        "start": 2726.76,
        "duration": 4.26,
        "text": "external apis they can be internal apis"
      },
      {
        "start": 2728.74,
        "duration": 3.66,
        "text": "to my application"
      },
      {
        "start": 2731.02,
        "duration": 2.88,
        "text": "and so here you see the search recipes"
      },
      {
        "start": 2732.4,
        "duration": 3.419,
        "text": "function"
      },
      {
        "start": 2733.9,
        "duration": 4.459,
        "text": "right so what do I have to do I take the"
      },
      {
        "start": 2735.819,
        "duration": 5.941,
        "text": "query Mexican food I embed it"
      },
      {
        "start": 2738.359,
        "duration": 5.381,
        "text": "I get back the vector and then here's"
      },
      {
        "start": 2741.76,
        "duration": 3.839,
        "text": "how I search the database right so I say"
      },
      {
        "start": 2743.74,
        "duration": 5.099,
        "text": "hey return me the title rating in full"
      },
      {
        "start": 2745.599,
        "duration": 5.101,
        "text": "text from my recipes data limit it to"
      },
      {
        "start": 2748.839,
        "duration": 3.901,
        "text": "ratings that are great in 3.0 I don't"
      },
      {
        "start": 2750.7,
        "duration": 3.78,
        "text": "want low scoring ones so this does a"
      },
      {
        "start": 2752.74,
        "duration": 3.839,
        "text": "filtering step before it does the vector"
      },
      {
        "start": 2754.48,
        "duration": 5.16,
        "text": "search gets rid of bad recipes right off"
      },
      {
        "start": 2756.579,
        "duration": 5.221,
        "text": "the bat okay and then look at my vectors"
      },
      {
        "start": 2759.64,
        "duration": 5.64,
        "text": "and give me the ones closest to Mexican"
      },
      {
        "start": 2761.8,
        "duration": 5.64,
        "text": "food limited to the top you know three"
      },
      {
        "start": 2765.28,
        "duration": 5.22,
        "text": "um items I don't want like a hundred"
      },
      {
        "start": 2767.44,
        "duration": 5.28,
        "text": "so it looks like if I do that"
      },
      {
        "start": 2770.5,
        "duration": 4.02,
        "text": "you can see I get back some Yaki soft"
      },
      {
        "start": 2772.72,
        "duration": 3.78,
        "text": "tacos I get back a peanut butter"
      },
      {
        "start": 2774.52,
        "duration": 4.38,
        "text": "raspberry Pita which is not exactly a"
      },
      {
        "start": 2776.5,
        "duration": 3.18,
        "text": "taco but it's kind of like a taco and I"
      },
      {
        "start": 2778.9,
        "duration": 3.84,
        "text": "get"
      },
      {
        "start": 2779.68,
        "duration": 4.679,
        "text": "vegetarian bean and lentil tacos"
      },
      {
        "start": 2782.74,
        "duration": 3.9,
        "text": "um quickly the next thing I said I would"
      },
      {
        "start": 2784.359,
        "duration": 3.841,
        "text": "show right is well let's do that search"
      },
      {
        "start": 2786.64,
        "duration": 3.719,
        "text": "re-ranking right so instead of taking a"
      },
      {
        "start": 2788.2,
        "duration": 4.619,
        "text": "top three let's take the top hundred"
      },
      {
        "start": 2790.359,
        "duration": 5.281,
        "text": "and then let's re-rank based on rating"
      },
      {
        "start": 2792.819,
        "duration": 4.26,
        "text": "right show me the top rated ones and"
      },
      {
        "start": 2795.64,
        "duration": 3.12,
        "text": "then just return the top three like"
      },
      {
        "start": 2797.079,
        "duration": 4.26,
        "text": "before so now I get a different set of"
      },
      {
        "start": 2798.76,
        "duration": 5.52,
        "text": "recipes right I get this"
      },
      {
        "start": 2801.339,
        "duration": 5.641,
        "text": "um Oaxaca beef taco I get"
      },
      {
        "start": 2804.28,
        "duration": 4.62,
        "text": "um interesting it got a pot roast so you"
      },
      {
        "start": 2806.98,
        "duration": 6.06,
        "text": "know that that's not great and then it"
      },
      {
        "start": 2808.9,
        "duration": 5.219,
        "text": "got um uh a a Blanco white cheese dip"
      },
      {
        "start": 2813.04,
        "duration": 2.1,
        "text": "um so you probably need to adjust the"
      },
      {
        "start": 2814.119,
        "duration": 2.941,
        "text": "temperature a little bit on different"
      },
      {
        "start": 2815.14,
        "duration": 3.479,
        "text": "runs it's been it's been better with"
      },
      {
        "start": 2817.06,
        "duration": 2.88,
        "text": "this and then the last step that we"
      },
      {
        "start": 2818.619,
        "duration": 2.401,
        "text": "talked about was you need to extract the"
      },
      {
        "start": 2819.94,
        "duration": 3.24,
        "text": "ingredient list"
      },
      {
        "start": 2821.02,
        "duration": 3.599,
        "text": "right again very simple prompt create a"
      },
      {
        "start": 2823.18,
        "duration": 3.54,
        "text": "Json that contains a list of ingredients"
      },
      {
        "start": 2824.619,
        "duration": 4.321,
        "text": "needed to cook the recipe below it gives"
      },
      {
        "start": 2826.72,
        "duration": 5.599,
        "text": "you an ingredient list as a Json that's"
      },
      {
        "start": 2828.94,
        "duration": 3.379,
        "text": "a less data structure"
      },
      {
        "start": 2832.78,
        "duration": 5.579,
        "text": "so I have a couple questions here if you"
      },
      {
        "start": 2835.359,
        "duration": 5.281,
        "text": "go back up to the go out back up to the"
      },
      {
        "start": 2838.359,
        "duration": 4.681,
        "text": "part where you're doing the query like"
      },
      {
        "start": 2840.64,
        "duration": 4.26,
        "text": "like what what why would you want to use"
      },
      {
        "start": 2843.04,
        "duration": 4.98,
        "text": "filtered Vector search is there some"
      },
      {
        "start": 2844.9,
        "duration": 5.219,
        "text": "advantages of that"
      },
      {
        "start": 2848.02,
        "duration": 4.319,
        "text": "yeah so so there's there are definitely"
      },
      {
        "start": 2850.119,
        "duration": 4.381,
        "text": "a few advantages first one let's just"
      },
      {
        "start": 2852.339,
        "duration": 3.841,
        "text": "think from like a perspective of our"
      },
      {
        "start": 2854.5,
        "duration": 4.079,
        "text": "product right that we're building here"
      },
      {
        "start": 2856.18,
        "duration": 3.6,
        "text": "which is like recommending recipes and"
      },
      {
        "start": 2858.579,
        "duration": 2.581,
        "text": "getting people to add things to their"
      },
      {
        "start": 2859.78,
        "duration": 3.24,
        "text": "shopping cart"
      },
      {
        "start": 2861.16,
        "duration": 3.24,
        "text": "um you know in this particular data set"
      },
      {
        "start": 2863.02,
        "duration": 2.28,
        "text": "there are a lot of recipes that have"
      },
      {
        "start": 2864.4,
        "duration": 5.34,
        "text": "like"
      },
      {
        "start": 2865.3,
        "duration": 5.819,
        "text": "a 0.5 score a one out of five stars"
      },
      {
        "start": 2869.74,
        "duration": 2.94,
        "text": "um You probably don't want to put those"
      },
      {
        "start": 2871.119,
        "duration": 3.061,
        "text": "in front of people because you're not"
      },
      {
        "start": 2872.68,
        "duration": 3.48,
        "text": "going to get a click right when they see"
      },
      {
        "start": 2874.18,
        "duration": 3.78,
        "text": "if you because you probably render this"
      },
      {
        "start": 2876.16,
        "duration": 2.88,
        "text": "in a nice card and have the title have a"
      },
      {
        "start": 2877.96,
        "duration": 2.639,
        "text": "picture and it would have the star"
      },
      {
        "start": 2879.04,
        "duration": 3.299,
        "text": "rating right so if you show a one star"
      },
      {
        "start": 2880.599,
        "duration": 3.121,
        "text": "No One's Gonna click on that recipe and"
      },
      {
        "start": 2882.339,
        "duration": 2.401,
        "text": "say hey make me a shopping list out of"
      },
      {
        "start": 2883.72,
        "duration": 3.78,
        "text": "it"
      },
      {
        "start": 2884.74,
        "duration": 4.44,
        "text": "so you know that's kind of improving the"
      },
      {
        "start": 2887.5,
        "duration": 3.96,
        "text": "results of your product"
      },
      {
        "start": 2889.18,
        "duration": 4.02,
        "text": "um from a technical standpoint this is"
      },
      {
        "start": 2891.46,
        "duration": 4.74,
        "text": "also important because you know the"
      },
      {
        "start": 2893.2,
        "duration": 5.28,
        "text": "recipes data set is quite is pretty"
      },
      {
        "start": 2896.2,
        "duration": 4.619,
        "text": "large in this case there's about 200 000"
      },
      {
        "start": 2898.48,
        "duration": 4.26,
        "text": "recipes which results in 500 000"
      },
      {
        "start": 2900.819,
        "duration": 4.741,
        "text": "embeddings or something like that it's"
      },
      {
        "start": 2902.74,
        "duration": 4.14,
        "text": "not gigantic but it's decent size when"
      },
      {
        "start": 2905.56,
        "duration": 3.539,
        "text": "you do this"
      },
      {
        "start": 2906.88,
        "duration": 4.14,
        "text": "filtered search first it just removes a"
      },
      {
        "start": 2909.099,
        "duration": 5.641,
        "text": "lot of data from the data set so that"
      },
      {
        "start": 2911.02,
        "duration": 6.78,
        "text": "makes the the a n query work better it's"
      },
      {
        "start": 2914.74,
        "duration": 4.98,
        "text": "it it's more accurate because"
      },
      {
        "start": 2917.8,
        "duration": 4.559,
        "text": "and we didn't touch on this too much but"
      },
      {
        "start": 2919.72,
        "duration": 5.04,
        "text": "and a n is approximate nearest neighbor"
      },
      {
        "start": 2922.359,
        "duration": 4.561,
        "text": "it's not true nearest neighbor right so"
      },
      {
        "start": 2924.76,
        "duration": 3.54,
        "text": "as your data sets get bigger when you do"
      },
      {
        "start": 2926.92,
        "duration": 4.08,
        "text": "an approximate nearest neighbor search"
      },
      {
        "start": 2928.3,
        "duration": 4.319,
        "text": "you have more inaccuracies and did you"
      },
      {
        "start": 2931.0,
        "duration": 3.119,
        "text": "truly find the nearest neighbor to your"
      },
      {
        "start": 2932.619,
        "duration": 4.2,
        "text": "particular query Vector so this"
      },
      {
        "start": 2934.119,
        "duration": 5.641,
        "text": "technique will improve that"
      },
      {
        "start": 2936.819,
        "duration": 6.061,
        "text": "I also want to comment that uh beside a"
      },
      {
        "start": 2939.76,
        "duration": 6.24,
        "text": "n just doing the cleaners neighbor"
      },
      {
        "start": 2942.88,
        "duration": 5.58,
        "text": "what happens is that the accuracy of K"
      },
      {
        "start": 2946.0,
        "duration": 5.76,
        "text": "nearest neighbor unfortunately is very"
      },
      {
        "start": 2948.46,
        "duration": 5.04,
        "text": "very sensitive to the outliers the total"
      },
      {
        "start": 2951.76,
        "duration": 3.18,
        "text": "number of outliers that are that is"
      },
      {
        "start": 2953.5,
        "duration": 4.26,
        "text": "being searched over"
      },
      {
        "start": 2954.94,
        "duration": 4.62,
        "text": "and you know outliers and data is"
      },
      {
        "start": 2957.76,
        "duration": 5.16,
        "text": "inevitable"
      },
      {
        "start": 2959.56,
        "duration": 6.059,
        "text": "um so by nature of having a smaller data"
      },
      {
        "start": 2962.92,
        "duration": 4.86,
        "text": "set you're reducing your outliers so"
      },
      {
        "start": 2965.619,
        "duration": 7.321,
        "text": "that the k n algorithm works better as"
      },
      {
        "start": 2967.78,
        "duration": 7.02,
        "text": "well so speed performance uh sorry speed"
      },
      {
        "start": 2972.94,
        "duration": 4.8,
        "text": "Improvement irrelevance I think these"
      },
      {
        "start": 2974.8,
        "duration": 5.039,
        "text": "are all important factors for you know"
      },
      {
        "start": 2977.74,
        "duration": 3.96,
        "text": "trying to constrain your search space"
      },
      {
        "start": 2979.839,
        "duration": 5.461,
        "text": "for a n"
      },
      {
        "start": 2981.7,
        "duration": 6.24,
        "text": "as quickly as possible prior to"
      },
      {
        "start": 2985.3,
        "duration": 5.12,
        "text": "doing your approximate nurse name"
      },
      {
        "start": 2987.94,
        "duration": 2.48,
        "text": "research"
      },
      {
        "start": 2992.98,
        "duration": 3.48,
        "text": "cool"
      },
      {
        "start": 2994.24,
        "duration": 3.359,
        "text": "um a couple other questions I saw kind"
      },
      {
        "start": 2996.46,
        "duration": 2.879,
        "text": "of flow through one question was like"
      },
      {
        "start": 2997.599,
        "duration": 4.441,
        "text": "why are there 500 000 vectors if there's"
      },
      {
        "start": 2999.339,
        "duration": 4.5,
        "text": "only 200 000 recipes"
      },
      {
        "start": 3002.04,
        "duration": 4.38,
        "text": "um that's due to the chunking stuff at"
      },
      {
        "start": 3003.839,
        "duration": 5.041,
        "text": "the beginning so the recipe text is too"
      },
      {
        "start": 3006.42,
        "duration": 4.32,
        "text": "long for the particular embedding models"
      },
      {
        "start": 3008.88,
        "duration": 3.479,
        "text": "that we use they're limited to about 512"
      },
      {
        "start": 3010.74,
        "duration": 4.44,
        "text": "characters which is about two to three"
      },
      {
        "start": 3012.359,
        "duration": 4.921,
        "text": "sentences of text in English so you have"
      },
      {
        "start": 3015.18,
        "duration": 4.919,
        "text": "to chunk up the recipe into kind of"
      },
      {
        "start": 3017.28,
        "duration": 4.14,
        "text": "different parts and then feed the put"
      },
      {
        "start": 3020.099,
        "duration": 4.081,
        "text": "that into the vector database and then"
      },
      {
        "start": 3021.42,
        "duration": 4.679,
        "text": "search over it if you think about longer"
      },
      {
        "start": 3024.18,
        "duration": 3.84,
        "text": "examples let's say we're doing Q a on"
      },
      {
        "start": 3026.099,
        "duration": 4.02,
        "text": "like a PDF file you wouldn't be able to"
      },
      {
        "start": 3028.02,
        "duration": 3.42,
        "text": "put the whole PDF as a single embedding"
      },
      {
        "start": 3030.119,
        "duration": 4.941,
        "text": "you would probably chunk it by paragraph"
      },
      {
        "start": 3031.44,
        "duration": 3.62,
        "text": "or also by a few sentences"
      },
      {
        "start": 3038.119,
        "duration": 3.821,
        "text": "uh sorry just scanning if there's any"
      },
      {
        "start": 3040.319,
        "duration": 2.641,
        "text": "other questions"
      },
      {
        "start": 3041.94,
        "duration": 3.0,
        "text": "um"
      },
      {
        "start": 3042.96,
        "duration": 5.3,
        "text": "there was a question about"
      },
      {
        "start": 3044.94,
        "duration": 5.04,
        "text": "um kind of uh indexing in Astra and"
      },
      {
        "start": 3048.26,
        "duration": 3.22,
        "text": "elasticsearch or some of these other"
      },
      {
        "start": 3049.98,
        "duration": 3.24,
        "text": "methods um"
      },
      {
        "start": 3051.48,
        "duration": 4.56,
        "text": "but a Nifty thing about the way that"
      },
      {
        "start": 3053.22,
        "duration": 4.44,
        "text": "Astra indexing works is it's a the index"
      },
      {
        "start": 3056.04,
        "duration": 4.44,
        "text": "is built in a way where you can query it"
      },
      {
        "start": 3057.66,
        "duration": 5.459,
        "text": "immediately so a lot of"
      },
      {
        "start": 3060.48,
        "duration": 5.04,
        "text": "other databases that provide Vector"
      },
      {
        "start": 3063.119,
        "duration": 4.381,
        "text": "search you have to wait for the index to"
      },
      {
        "start": 3065.52,
        "duration": 3.42,
        "text": "First build before you can search and"
      },
      {
        "start": 3067.5,
        "duration": 3.359,
        "text": "how long that takes kind of depends on"
      },
      {
        "start": 3068.94,
        "duration": 3.78,
        "text": "how much data you loaded how fast you"
      },
      {
        "start": 3070.859,
        "duration": 2.881,
        "text": "loaded it and and their their particular"
      },
      {
        "start": 3072.72,
        "duration": 3.3,
        "text": "implementation"
      },
      {
        "start": 3073.74,
        "duration": 4.02,
        "text": "so you know in some cases maybe you only"
      },
      {
        "start": 3076.02,
        "duration": 3.18,
        "text": "have to wait a couple minutes but like"
      },
      {
        "start": 3077.76,
        "duration": 3.359,
        "text": "there are cases where you might have to"
      },
      {
        "start": 3079.2,
        "duration": 3.06,
        "text": "wait 10 or 20 minutes before the index"
      },
      {
        "start": 3081.119,
        "duration": 3.661,
        "text": "is available for"
      },
      {
        "start": 3082.26,
        "duration": 3.9,
        "text": "so that's kind of one spot where our"
      },
      {
        "start": 3084.78,
        "duration": 4.26,
        "text": "technology Works a bit different than"
      },
      {
        "start": 3086.16,
        "duration": 5.28,
        "text": "some others out there"
      },
      {
        "start": 3089.04,
        "duration": 5.059,
        "text": "yeah I would say also too so for example"
      },
      {
        "start": 3091.44,
        "duration": 6.659,
        "text": "if you're interacting with the chat bot"
      },
      {
        "start": 3094.099,
        "duration": 4.841,
        "text": "and the chat bot is asking oh what are"
      },
      {
        "start": 3098.099,
        "duration": 2.881,
        "text": "your"
      },
      {
        "start": 3098.94,
        "duration": 3.54,
        "text": "kind of pref what kind of products do"
      },
      {
        "start": 3100.98,
        "duration": 6.359,
        "text": "you like"
      },
      {
        "start": 3102.48,
        "duration": 7.139,
        "text": "and they use that information to uh to"
      },
      {
        "start": 3107.339,
        "duration": 4.441,
        "text": "recommend you items so you might type"
      },
      {
        "start": 3109.619,
        "duration": 4.98,
        "text": "something in natural language like oh I"
      },
      {
        "start": 3111.78,
        "duration": 5.039,
        "text": "like I like red shirts I like Etc et"
      },
      {
        "start": 3114.599,
        "duration": 5.341,
        "text": "cetera that needs to be turned into a"
      },
      {
        "start": 3116.819,
        "duration": 5.941,
        "text": "vector immediately uh otherwise it can't"
      },
      {
        "start": 3119.94,
        "duration": 5.159,
        "text": "be used in the subsequent uh calls by"
      },
      {
        "start": 3122.76,
        "duration": 4.2,
        "text": "the chat bot so that that becomes uh"
      },
      {
        "start": 3125.099,
        "duration": 3.601,
        "text": "really important too in order to have"
      },
      {
        "start": 3126.96,
        "duration": 6.379,
        "text": "like kind of an interactive experience"
      },
      {
        "start": 3128.7,
        "duration": 4.639,
        "text": "with the uh with the chat Bots"
      },
      {
        "start": 3133.7,
        "duration": 5.08,
        "text": "all right I know we're coming up on the"
      },
      {
        "start": 3136.44,
        "duration": 4.139,
        "text": "top of the hour and I think also we were"
      },
      {
        "start": 3138.78,
        "duration": 3.6,
        "text": "supposed to only be 45 minutes so I'll"
      },
      {
        "start": 3140.579,
        "duration": 2.52,
        "text": "show this last one"
      },
      {
        "start": 3142.38,
        "duration": 3.32,
        "text": "um"
      },
      {
        "start": 3143.099,
        "duration": 2.601,
        "text": "quickly"
      },
      {
        "start": 3145.74,
        "duration": 3.18,
        "text": "um"
      },
      {
        "start": 3146.94,
        "duration": 3.659,
        "text": "so we talked about the other use case"
      },
      {
        "start": 3148.92,
        "duration": 4.139,
        "text": "being this kind of you know question"
      },
      {
        "start": 3150.599,
        "duration": 5.701,
        "text": "answering over document sets and so one"
      },
      {
        "start": 3153.059,
        "duration": 4.681,
        "text": "way to do this is with uh so one this"
      },
      {
        "start": 3156.3,
        "duration": 4.14,
        "text": "always works off of some flavor of"
      },
      {
        "start": 3157.74,
        "duration": 4.02,
        "text": "retrieval augmented generation flare is"
      },
      {
        "start": 3160.44,
        "duration": 2.94,
        "text": "a Improvement on that called"
      },
      {
        "start": 3161.76,
        "duration": 3.66,
        "text": "forward-looking active retrieval"
      },
      {
        "start": 3163.38,
        "duration": 6.239,
        "text": "augmented generation"
      },
      {
        "start": 3165.42,
        "duration": 7.199,
        "text": "and um how that works is basically as"
      },
      {
        "start": 3169.619,
        "duration": 5.041,
        "text": "the llm is generating a response when it"
      },
      {
        "start": 3172.619,
        "duration": 3.841,
        "text": "when it sees that it's going to create a"
      },
      {
        "start": 3174.66,
        "duration": 3.36,
        "text": "new token in the response and it's not"
      },
      {
        "start": 3176.46,
        "duration": 3.96,
        "text": "very confident about that particular"
      },
      {
        "start": 3178.02,
        "duration": 6.0,
        "text": "token it's going to execute additional"
      },
      {
        "start": 3180.42,
        "duration": 5.1,
        "text": "Vector searches to pull in more data to"
      },
      {
        "start": 3184.02,
        "duration": 3.539,
        "text": "help answer the question"
      },
      {
        "start": 3185.52,
        "duration": 3.96,
        "text": "so in this example"
      },
      {
        "start": 3187.559,
        "duration": 4.5,
        "text": "the task that was assigned to the llm"
      },
      {
        "start": 3189.48,
        "duration": 4.26,
        "text": "was write a summary about Joe Biden so"
      },
      {
        "start": 3192.059,
        "duration": 3.3,
        "text": "it writes Joe Biden attended and then"
      },
      {
        "start": 3193.74,
        "duration": 3.06,
        "text": "when tries to plug in the next token of"
      },
      {
        "start": 3195.359,
        "duration": 2.641,
        "text": "where he intended to realize is I don't"
      },
      {
        "start": 3196.8,
        "duration": 3.72,
        "text": "really know the answer to that question"
      },
      {
        "start": 3198.0,
        "duration": 4.4,
        "text": "so it does another search for Joe Biden"
      },
      {
        "start": 3200.52,
        "duration": 4.2,
        "text": "you know School Joe Biden University"
      },
      {
        "start": 3202.4,
        "duration": 3.699,
        "text": "pulls back a document that tells it oh"
      },
      {
        "start": 3204.72,
        "duration": 2.879,
        "text": "hey he attended the University of"
      },
      {
        "start": 3206.099,
        "duration": 3.361,
        "text": "Pennsylvania and then says okay great"
      },
      {
        "start": 3207.599,
        "duration": 3.301,
        "text": "where he earned and then he realizes I"
      },
      {
        "start": 3209.46,
        "duration": 3.84,
        "text": "don't know what degree he got so it does"
      },
      {
        "start": 3210.9,
        "duration": 4.439,
        "text": "another search for his you know degree"
      },
      {
        "start": 3213.3,
        "duration": 4.68,
        "text": "that he holds"
      },
      {
        "start": 3215.339,
        "duration": 4.381,
        "text": "so this process is able to basically on"
      },
      {
        "start": 3217.98,
        "duration": 6.24,
        "text": "the Fly generate additional questions"
      },
      {
        "start": 3219.72,
        "duration": 6.359,
        "text": "run additional uh database queries and"
      },
      {
        "start": 3224.22,
        "duration": 4.399,
        "text": "return more data to try to get a better"
      },
      {
        "start": 3226.079,
        "duration": 2.54,
        "text": "answer to the question"
      },
      {
        "start": 3229.14,
        "duration": 4.74,
        "text": "um similar setup to the last notebook so"
      },
      {
        "start": 3231.359,
        "duration": 3.96,
        "text": "I won't kind of go through that"
      },
      {
        "start": 3233.88,
        "duration": 3.0,
        "text": "um"
      },
      {
        "start": 3235.319,
        "duration": 3.061,
        "text": "by the way once once you're able to"
      },
      {
        "start": 3236.88,
        "duration": 3.3,
        "text": "access this notebook it's set up where"
      },
      {
        "start": 3238.38,
        "duration": 3.9,
        "text": "you can bring your own data like if you"
      },
      {
        "start": 3240.18,
        "duration": 4.139,
        "text": "create a folder in Google Drive you can"
      },
      {
        "start": 3242.28,
        "duration": 3.9,
        "text": "upload any PDFs you want to it and then"
      },
      {
        "start": 3244.319,
        "duration": 3.841,
        "text": "you can kind of set that PDF location"
      },
      {
        "start": 3246.18,
        "duration": 4.74,
        "text": "here and you can just run it on your own"
      },
      {
        "start": 3248.16,
        "duration": 5.64,
        "text": "data in this case I queued up a bunch of"
      },
      {
        "start": 3250.92,
        "duration": 5.34,
        "text": "papers about llm research so they kind"
      },
      {
        "start": 3253.8,
        "duration": 5.299,
        "text": "of cover prompting techniques so it"
      },
      {
        "start": 3256.26,
        "duration": 6.48,
        "text": "covers Chain of Thought self-consistency"
      },
      {
        "start": 3259.099,
        "duration": 7.061,
        "text": "llms can self-debug how llms can be used"
      },
      {
        "start": 3262.74,
        "duration": 6.359,
        "text": "to generate SQL code"
      },
      {
        "start": 3266.16,
        "duration": 4.459,
        "text": "so uh bought eight papers in this"
      },
      {
        "start": 3269.099,
        "duration": 4.681,
        "text": "example"
      },
      {
        "start": 3270.619,
        "duration": 5.341,
        "text": "this example uses our library Casio so"
      },
      {
        "start": 3273.78,
        "duration": 4.02,
        "text": "you'll see you don't have to Define any"
      },
      {
        "start": 3275.96,
        "duration": 3.84,
        "text": "Cassandra tables you don't have to do"
      },
      {
        "start": 3277.8,
        "duration": 4.799,
        "text": "any Cassandra data modeling"
      },
      {
        "start": 3279.8,
        "duration": 5.68,
        "text": "Casio is an integrated Upstream into"
      },
      {
        "start": 3282.599,
        "duration": 4.561,
        "text": "many popular Frameworks for working with"
      },
      {
        "start": 3285.48,
        "duration": 4.2,
        "text": "llm so it's integrated with line chain"
      },
      {
        "start": 3287.16,
        "duration": 3.959,
        "text": "and it's integrated with llama index and"
      },
      {
        "start": 3289.68,
        "duration": 3.96,
        "text": "so you can kind of see really all you"
      },
      {
        "start": 3291.119,
        "duration": 4.98,
        "text": "have to do here is import the Cassandra"
      },
      {
        "start": 3293.64,
        "duration": 3.959,
        "text": "Vector store from Lang chain to then"
      },
      {
        "start": 3296.099,
        "duration": 4.201,
        "text": "kind of build this example that's really"
      },
      {
        "start": 3297.599,
        "duration": 5.041,
        "text": "kind of all it takes so"
      },
      {
        "start": 3300.3,
        "duration": 4.019,
        "text": "um we download the PDFs from Google"
      },
      {
        "start": 3302.64,
        "duration": 4.8,
        "text": "Drive"
      },
      {
        "start": 3304.319,
        "duration": 5.101,
        "text": "we create our Cassandra Vector store the"
      },
      {
        "start": 3307.44,
        "duration": 3.96,
        "text": "only thing you really have to do is tell"
      },
      {
        "start": 3309.42,
        "duration": 4.26,
        "text": "it what embedding model you're using"
      },
      {
        "start": 3311.4,
        "duration": 4.5,
        "text": "you have to have initialized a"
      },
      {
        "start": 3313.68,
        "duration": 3.179,
        "text": "connection to the database and then you"
      },
      {
        "start": 3315.9,
        "duration": 2.699,
        "text": "need to tell what's the name of the"
      },
      {
        "start": 3316.859,
        "duration": 3.24,
        "text": "table I want to use and it will just"
      },
      {
        "start": 3318.599,
        "duration": 3.901,
        "text": "create the table and create all the"
      },
      {
        "start": 3320.099,
        "duration": 4.801,
        "text": "structure you need Cassio does support"
      },
      {
        "start": 3322.5,
        "duration": 4.26,
        "text": "creating adding metadata and doing"
      },
      {
        "start": 3324.9,
        "duration": 3.06,
        "text": "metadata filtering as well so you can do"
      },
      {
        "start": 3326.76,
        "duration": 3.359,
        "text": "all of that through this interface"
      },
      {
        "start": 3327.96,
        "duration": 3.599,
        "text": "without kind of going through the you"
      },
      {
        "start": 3330.119,
        "duration": 4.021,
        "text": "know manual kind of data modeling I did"
      },
      {
        "start": 3331.559,
        "duration": 4.861,
        "text": "in the last example here we do a text"
      },
      {
        "start": 3334.14,
        "duration": 3.84,
        "text": "flare that just evenly breaks up all the"
      },
      {
        "start": 3336.42,
        "duration": 4.5,
        "text": "characters in the document to evenly"
      },
      {
        "start": 3337.98,
        "duration": 5.579,
        "text": "sized chunks of 500 characters and with"
      },
      {
        "start": 3340.92,
        "duration": 3.899,
        "text": "an overlap of 80 characters so you kind"
      },
      {
        "start": 3343.559,
        "duration": 3.06,
        "text": "of want to have an overlap in case"
      },
      {
        "start": 3344.819,
        "duration": 3.841,
        "text": "you've broken the document in a bad spot"
      },
      {
        "start": 3346.619,
        "duration": 3.301,
        "text": "and you've kind of lost the context like"
      },
      {
        "start": 3348.66,
        "duration": 2.939,
        "text": "if you broke it mid-sentence or"
      },
      {
        "start": 3349.92,
        "duration": 3.54,
        "text": "mid-paragraph you might not get a good"
      },
      {
        "start": 3351.599,
        "duration": 4.101,
        "text": "result so you want to do some overlap so"
      },
      {
        "start": 3353.46,
        "duration": 4.98,
        "text": "you kind of carry the information"
      },
      {
        "start": 3355.7,
        "duration": 5.379,
        "text": "context over better"
      },
      {
        "start": 3358.44,
        "duration": 4.619,
        "text": "so you know you load the PDFs you do the"
      },
      {
        "start": 3361.079,
        "duration": 4.26,
        "text": "splitting and then you basically just"
      },
      {
        "start": 3363.059,
        "duration": 3.961,
        "text": "tell it to add to the vector store these"
      },
      {
        "start": 3365.339,
        "duration": 3.48,
        "text": "text documents and then it does the"
      },
      {
        "start": 3367.02,
        "duration": 4.38,
        "text": "embeddings and it stores it and that's"
      },
      {
        "start": 3368.819,
        "duration": 4.561,
        "text": "really kind of all you need to do"
      },
      {
        "start": 3371.4,
        "duration": 3.3,
        "text": "um this shows you can do like a count on"
      },
      {
        "start": 3373.38,
        "duration": 2.76,
        "text": "the table to see how many embeddings you"
      },
      {
        "start": 3374.7,
        "duration": 3.06,
        "text": "generated and we have about three"
      },
      {
        "start": 3376.14,
        "duration": 3.66,
        "text": "thousand"
      },
      {
        "start": 3377.76,
        "duration": 3.48,
        "text": "so"
      },
      {
        "start": 3379.8,
        "duration": 3.9,
        "text": "um now let's go to the generation step"
      },
      {
        "start": 3381.24,
        "duration": 4.14,
        "text": "of this uh here's a few queries I'm"
      },
      {
        "start": 3383.7,
        "duration": 3.359,
        "text": "gonna run so the first one is uh my"
      },
      {
        "start": 3385.38,
        "duration": 4.02,
        "text": "chatbot is giving incorrect instructions"
      },
      {
        "start": 3387.059,
        "duration": 4.201,
        "text": "on how to perform tasks in the UI how"
      },
      {
        "start": 3389.4,
        "duration": 3.959,
        "text": "can I fix this"
      },
      {
        "start": 3391.26,
        "duration": 3.78,
        "text": "um we set up the vector store as a"
      },
      {
        "start": 3393.359,
        "duration": 3.96,
        "text": "retriever for the retrieval augmented"
      },
      {
        "start": 3395.04,
        "duration": 3.36,
        "text": "generation that's again one call in Lang"
      },
      {
        "start": 3397.319,
        "duration": 1.681,
        "text": "chain"
      },
      {
        "start": 3398.4,
        "duration": 2.28,
        "text": "um"
      },
      {
        "start": 3399.0,
        "duration": 2.94,
        "text": "and this you know the vector store was"
      },
      {
        "start": 3400.68,
        "duration": 2.76,
        "text": "already created with Cassandra and the"
      },
      {
        "start": 3401.94,
        "duration": 2.58,
        "text": "prior cell"
      },
      {
        "start": 3403.44,
        "duration": 3.899,
        "text": "and then I'm going to initially"
      },
      {
        "start": 3404.52,
        "duration": 5.46,
        "text": "initialize just chat GPT here and then"
      },
      {
        "start": 3407.339,
        "duration": 4.441,
        "text": "I'm going to initialize Flair that uses"
      },
      {
        "start": 3409.98,
        "duration": 5.16,
        "text": "Cassandra for the retrieval and then I"
      },
      {
        "start": 3411.78,
        "duration": 7.92,
        "text": "just run that question and I run it in"
      },
      {
        "start": 3415.14,
        "duration": 6.84,
        "text": "the llm and I run it in flare"
      },
      {
        "start": 3419.7,
        "duration": 4.02,
        "text": "uh just like just quick comment here"
      },
      {
        "start": 3421.98,
        "duration": 4.92,
        "text": "there's a question"
      },
      {
        "start": 3423.72,
        "duration": 5.099,
        "text": "so flare is a general architectural"
      },
      {
        "start": 3426.9,
        "duration": 3.719,
        "text": "pattern there's papers published on it"
      },
      {
        "start": 3428.819,
        "duration": 3.601,
        "text": "but the nice thing about Lang chain is"
      },
      {
        "start": 3430.619,
        "duration": 3.72,
        "text": "they have a l box implementation for"
      },
      {
        "start": 3432.42,
        "duration": 6.26,
        "text": "this pattern it's also in other"
      },
      {
        "start": 3434.339,
        "duration": 4.341,
        "text": "Frameworks such as llama index Etc"
      },
      {
        "start": 3439.2,
        "duration": 4.56,
        "text": "so let's look at this llm result first"
      },
      {
        "start": 3441.3,
        "duration": 3.539,
        "text": "you see it's very general and this is"
      },
      {
        "start": 3443.76,
        "duration": 2.7,
        "text": "what you'll see right so it doesn't know"
      },
      {
        "start": 3444.839,
        "duration": 4.081,
        "text": "how to solve this particular problem so"
      },
      {
        "start": 3446.46,
        "duration": 5.94,
        "text": "look at your training data collect user"
      },
      {
        "start": 3448.92,
        "duration": 5.399,
        "text": "feedback debug logic in your code right"
      },
      {
        "start": 3452.4,
        "duration": 4.26,
        "text": "General steps of how I do this but no"
      },
      {
        "start": 3454.319,
        "duration": 6.3,
        "text": "like specific information"
      },
      {
        "start": 3456.66,
        "duration": 4.98,
        "text": "um Flair is going to say let's see you"
      },
      {
        "start": 3460.619,
        "duration": 3.72,
        "text": "know answer is a little different each"
      },
      {
        "start": 3461.64,
        "duration": 4.199,
        "text": "time but"
      },
      {
        "start": 3464.339,
        "duration": 3.541,
        "text": "um it says you need to debug which It"
      },
      {
        "start": 3465.839,
        "duration": 3.961,
        "text": "also says it says you need to look at"
      },
      {
        "start": 3467.88,
        "duration": 4.56,
        "text": "the prompt instructions"
      },
      {
        "start": 3469.8,
        "duration": 4.44,
        "text": "and you need to use code"
      },
      {
        "start": 3472.44,
        "duration": 3.0,
        "text": "and if you need more help it's telling"
      },
      {
        "start": 3474.24,
        "duration": 2.7,
        "text": "you to specifically look at a specific"
      },
      {
        "start": 3475.44,
        "duration": 3.84,
        "text": "paper all right so I was recommending a"
      },
      {
        "start": 3476.94,
        "duration": 4.44,
        "text": "specific paper here"
      },
      {
        "start": 3479.28,
        "duration": 3.6,
        "text": "let's run this one more time"
      },
      {
        "start": 3481.38,
        "duration": 3.179,
        "text": "and this time I'm going to turn on some"
      },
      {
        "start": 3482.88,
        "duration": 4.739,
        "text": "debugging so you can turn on this"
      },
      {
        "start": 3484.559,
        "duration": 4.8,
        "text": "debugging fly or this verbosity flag in"
      },
      {
        "start": 3487.619,
        "duration": 3.661,
        "text": "line in a lang chain to kind of get more"
      },
      {
        "start": 3489.359,
        "duration": 3.361,
        "text": "information on what happened this dumps"
      },
      {
        "start": 3491.28,
        "duration": 3.839,
        "text": "a lot of information but I'm really just"
      },
      {
        "start": 3492.72,
        "duration": 5.16,
        "text": "looking for one part"
      },
      {
        "start": 3495.119,
        "duration": 4.861,
        "text": "which should be in yellow okay here we"
      },
      {
        "start": 3497.88,
        "duration": 4.62,
        "text": "go so this was that part about Flair"
      },
      {
        "start": 3499.98,
        "duration": 5.7,
        "text": "generates additional questions as it"
      },
      {
        "start": 3502.5,
        "duration": 4.859,
        "text": "runs so these are new new questions that"
      },
      {
        "start": 3505.68,
        "duration": 3.96,
        "text": "it made right so we asked this question"
      },
      {
        "start": 3507.359,
        "duration": 3.96,
        "text": "about there was a problem in my UI and"
      },
      {
        "start": 3509.64,
        "duration": 3.959,
        "text": "it didn't help me answer it's so it"
      },
      {
        "start": 3511.319,
        "duration": 3.961,
        "text": "asked what is one possible solution to"
      },
      {
        "start": 3513.599,
        "duration": 3.661,
        "text": "fix your chatbots incorrect instructions"
      },
      {
        "start": 3515.28,
        "duration": 4.079,
        "text": "what should you check to ensure your"
      },
      {
        "start": 3517.26,
        "duration": 3.059,
        "text": "instructions are correct"
      },
      {
        "start": 3519.359,
        "duration": 3.601,
        "text": "um"
      },
      {
        "start": 3520.319,
        "duration": 3.901,
        "text": "this one is what might you need to do if"
      },
      {
        "start": 3522.96,
        "duration": 2.7,
        "text": "checking the settings doesn't fix the"
      },
      {
        "start": 3524.22,
        "duration": 3.0,
        "text": "issue with your chatbots instructions"
      },
      {
        "start": 3525.66,
        "duration": 3.48,
        "text": "and what might need to do to reprogram"
      },
      {
        "start": 3527.22,
        "duration": 4.04,
        "text": "in order to improve its instruction set"
      },
      {
        "start": 3529.14,
        "duration": 4.8,
        "text": "right so we've basically run"
      },
      {
        "start": 3531.26,
        "duration": 4.9,
        "text": "these uh four or five questions now"
      },
      {
        "start": 3533.94,
        "duration": 3.96,
        "text": "gotten a lot of different results and"
      },
      {
        "start": 3536.16,
        "duration": 4.34,
        "text": "then use them to kind of combine to get"
      },
      {
        "start": 3537.9,
        "duration": 2.6,
        "text": "the final answer"
      },
      {
        "start": 3542.52,
        "duration": 4.26,
        "text": "all right and we're at the we're at the"
      },
      {
        "start": 3544.02,
        "duration": 3.9,
        "text": "top of the hour so um kind of stopped"
      },
      {
        "start": 3546.78,
        "duration": 3.62,
        "text": "there see if there's any kind of final"
      },
      {
        "start": 3547.92,
        "duration": 2.48,
        "text": "questions"
      },
      {
        "start": 3550.559,
        "duration": 4.441,
        "text": "and so maybe a final question that was"
      },
      {
        "start": 3552.72,
        "duration": 4.139,
        "text": "asked that would be good to wrap up but"
      },
      {
        "start": 3555.0,
        "duration": 5.7,
        "text": "the question was like hey if I'm a small"
      },
      {
        "start": 3556.859,
        "duration": 5.341,
        "text": "development shop uh do I need special"
      },
      {
        "start": 3560.7,
        "duration": 4.56,
        "text": "skit does it make sense to hire"
      },
      {
        "start": 3562.2,
        "duration": 5.159,
        "text": "developers with some minimal knowledge"
      },
      {
        "start": 3565.26,
        "duration": 3.96,
        "text": "of stats or linear algebra or do I need"
      },
      {
        "start": 3567.359,
        "duration": 5.101,
        "text": "to go up to like hire a very very"
      },
      {
        "start": 3569.22,
        "duration": 6.119,
        "text": "high-end data scientists"
      },
      {
        "start": 3572.46,
        "duration": 4.08,
        "text": "so you know this is the kind of one of"
      },
      {
        "start": 3575.339,
        "duration": 4.141,
        "text": "the game changing things about"
      },
      {
        "start": 3576.54,
        "duration": 4.799,
        "text": "generative Ai and llms is you don't"
      },
      {
        "start": 3579.48,
        "duration": 4.139,
        "text": "necessarily you don't need a data"
      },
      {
        "start": 3581.339,
        "duration": 3.72,
        "text": "scientist to kind of solve these"
      },
      {
        "start": 3583.619,
        "duration": 3.841,
        "text": "problems anymore in fact you know I've"
      },
      {
        "start": 3585.059,
        "duration": 3.78,
        "text": "worked with people that had no data"
      },
      {
        "start": 3587.46,
        "duration": 2.879,
        "text": "background no statistics background"
      },
      {
        "start": 3588.839,
        "duration": 3.24,
        "text": "front-end developers full stack"
      },
      {
        "start": 3590.339,
        "duration": 2.941,
        "text": "developers who are able to build very"
      },
      {
        "start": 3592.079,
        "duration": 2.941,
        "text": "successful"
      },
      {
        "start": 3593.28,
        "duration": 3.18,
        "text": "um generative AI applications that were"
      },
      {
        "start": 3595.02,
        "duration": 3.18,
        "text": "deployed into production at companies"
      },
      {
        "start": 3596.46,
        "duration": 4.04,
        "text": "that I worked at"
      },
      {
        "start": 3598.2,
        "duration": 5.22,
        "text": "um if you can kind of think logically"
      },
      {
        "start": 3600.5,
        "duration": 6.28,
        "text": "and understand how to use these prompt"
      },
      {
        "start": 3603.42,
        "duration": 6.24,
        "text": "techniques that are really common that's"
      },
      {
        "start": 3606.78,
        "duration": 4.2,
        "text": "kind of all you need to know to do so"
      },
      {
        "start": 3609.66,
        "duration": 2.88,
        "text": "the programming language for these"
      },
      {
        "start": 3610.98,
        "duration": 4.319,
        "text": "things is English right that's what uh"
      },
      {
        "start": 3612.54,
        "duration": 5.519,
        "text": "what's Andre caparthy from Tesla and"
      },
      {
        "start": 3615.299,
        "duration": 4.5,
        "text": "open AI um has said right so you you"
      },
      {
        "start": 3618.059,
        "duration": 3.361,
        "text": "write out so what you need to study is"
      },
      {
        "start": 3619.799,
        "duration": 4.381,
        "text": "how does prompt engineering works right"
      },
      {
        "start": 3621.42,
        "duration": 4.919,
        "text": "it's it's a new field accessible to any"
      },
      {
        "start": 3624.18,
        "duration": 4.5,
        "text": "developer to kind of build these things"
      },
      {
        "start": 3626.339,
        "duration": 3.96,
        "text": "and kind of do them on their own and you"
      },
      {
        "start": 3628.68,
        "duration": 2.7,
        "text": "don't even necessarily need a back end"
      },
      {
        "start": 3630.299,
        "duration": 2.701,
        "text": "for some of this stuff if you're a"
      },
      {
        "start": 3631.38,
        "duration": 3.3,
        "text": "front-end developer and you're just"
      },
      {
        "start": 3633.0,
        "duration": 3.839,
        "text": "going to use like open AI you can"
      },
      {
        "start": 3634.68,
        "duration": 3.72,
        "text": "actually put your logic into your"
      },
      {
        "start": 3636.839,
        "duration": 3.301,
        "text": "front-end code if you wanted and just"
      },
      {
        "start": 3638.4,
        "duration": 3.899,
        "text": "call these services to kind of do a lot"
      },
      {
        "start": 3640.14,
        "duration": 4.32,
        "text": "of the work for you"
      },
      {
        "start": 3642.299,
        "duration": 5.241,
        "text": "so a lot of new possibilities that maybe"
      },
      {
        "start": 3644.46,
        "duration": 3.08,
        "text": "were not so easy to do before"
      },
      {
        "start": 3648.54,
        "duration": 7.799,
        "text": "okay uh one last question uh do you use"
      },
      {
        "start": 3653.16,
        "duration": 4.86,
        "text": "external Libs like Facebook face to"
      },
      {
        "start": 3656.339,
        "duration": 3.361,
        "text": "handle numeric types in the vector"
      },
      {
        "start": 3658.02,
        "duration": 5.819,
        "text": "database"
      },
      {
        "start": 3659.7,
        "duration": 6.18,
        "text": "no so Facebook faces like another way to"
      },
      {
        "start": 3663.839,
        "duration": 4.201,
        "text": "do kind of what we showed right so"
      },
      {
        "start": 3665.88,
        "duration": 4.979,
        "text": "they're kind of a pure software module"
      },
      {
        "start": 3668.04,
        "duration": 4.68,
        "text": "that runs in memory that builds a vector"
      },
      {
        "start": 3670.859,
        "duration": 4.681,
        "text": "index and lets you search it"
      },
      {
        "start": 3672.72,
        "duration": 3.78,
        "text": "um you can do that same thing in Astra"
      },
      {
        "start": 3675.54,
        "duration": 3.24,
        "text": "um"
      },
      {
        "start": 3676.5,
        "duration": 4.859,
        "text": "you know"
      },
      {
        "start": 3678.78,
        "duration": 4.68,
        "text": "it faces the face library is not a bad"
      },
      {
        "start": 3681.359,
        "duration": 2.7,
        "text": "way to get started with things but"
      },
      {
        "start": 3683.46,
        "duration": 2.46,
        "text": "um"
      },
      {
        "start": 3684.059,
        "duration": 3.24,
        "text": "you know I I think a lot of people are"
      },
      {
        "start": 3685.92,
        "duration": 2.699,
        "text": "reluctant to take that into production"
      },
      {
        "start": 3687.299,
        "duration": 3.06,
        "text": "because you don't get a lot of the"
      },
      {
        "start": 3688.619,
        "duration": 3.48,
        "text": "guarantees that you get with a database"
      },
      {
        "start": 3690.359,
        "duration": 3.72,
        "text": "right you don't have data replication"
      },
      {
        "start": 3692.099,
        "duration": 4.921,
        "text": "you don't have the ability to kind of"
      },
      {
        "start": 3694.079,
        "duration": 5.22,
        "text": "scale up with that type of Library"
      },
      {
        "start": 3697.02,
        "duration": 3.48,
        "text": "um but under the hood you're kind of"
      },
      {
        "start": 3699.299,
        "duration": 3.121,
        "text": "doing the same thing which is providing"
      },
      {
        "start": 3700.5,
        "duration": 3.299,
        "text": "a way to do like vector search the other"
      },
      {
        "start": 3702.42,
        "duration": 2.76,
        "text": "thing with the face library is you have"
      },
      {
        "start": 3703.799,
        "duration": 3.661,
        "text": "to build the whole index before you can"
      },
      {
        "start": 3705.18,
        "duration": 4.26,
        "text": "use it it doesn't have a capability like"
      },
      {
        "start": 3707.46,
        "duration": 4.2,
        "text": "like strut to be using the database"
      },
      {
        "start": 3709.44,
        "duration": 4.94,
        "text": "while it's being built while the index"
      },
      {
        "start": 3711.66,
        "duration": 2.72,
        "text": "is being built"
      },
      {
        "start": 3714.42,
        "duration": 3.48,
        "text": "all right so I think that's all the"
      },
      {
        "start": 3716.099,
        "duration": 2.881,
        "text": "questions so thank you everyone for"
      },
      {
        "start": 3717.9,
        "duration": 2.88,
        "text": "attending"
      },
      {
        "start": 3718.98,
        "duration": 4.26,
        "text": "um if you want to build an app go sign"
      },
      {
        "start": 3720.78,
        "duration": 3.66,
        "text": "up on Astra and we're looking forward to"
      },
      {
        "start": 3723.24,
        "duration": 4.04,
        "text": "what everyone's going to be building"
      },
      {
        "start": 3724.44,
        "duration": 2.84,
        "text": "thank you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T17:39:39.598447+00:00"
}