{
  "video_id": "l_LmXJPz_Ro",
  "title": "DS320.31 Spark Streaming: Architecture | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.31 Spark Streaming: Architecture\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:31:32Z",
  "thumbnail": "https://i.ytimg.com/vi/l_LmXJPz_Ro/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "architecture",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=l_LmXJPz_Ro",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's talk about the architecture of spark streaming just a little bit it builds on the architecture of spark itself so all this should look very familiar if you've seen the previous material on spark architecture but let's have a look at the diagram and see what's different just like in regular spark we have a client that has a driver and that's the program that's actually got our spark streaming application in it just like spark after all this is spark we have a master node and we have worker nodes those workers have executors that have some cache and tasks running in them just like they would in conventional spark what we add is of course the stream source there is stream data coming in that's some source out in the world again it's twitter it's kafka it's whatever we get data from and that input stream comes in to a receiver now that receiver is just a task it's a long running task that will run for the duration of the spark streaming application it will always be responsible for receiving the data from the stream source and again whether it does that by having data pushed to it on a socket or it goes out somewhere and reads from some resource totally depends on the details of the receiver that's not something that we can determine a priori just from the architecture diagram that can work either way also notice in the spark client the driver has not just a spark context but a spark streaming context and that streaming context is now the object that we're going to be programming as we build our spark application what exactly is a data stream source well let me give you some examples and some categories that they break down into you've got the basic sources which are file systems and sockets and akka actors then the more advanced sources that have some kind of protocol to deal with like if it's a twitter source there is an actual twitter api that that receiver is going to have to implement if it's kafka we're going to have to be speaking the kafka binary protocol from that receiver to get those messages in and of course there are apis we can implement to make our own custom receivers if you have your own sources of events if you're say an internet of things company and your devices have their own api out there on the web then you can implement that api and the receiver and bring that data into spark streaming yourself now receivers can be either reliable or unreliable a reliable receiver will acknowledge data that has been received so if the data source is able to receive an acknowledgement and resend data that hasn't been acknowledged then a reliable receiver will participate in that protocol it'll send the acknowledgement after it's received data and replicated it an unreliable receiver of course will still be replicating but doesn't have the capability of sending an acknowledgement when data comes in which one of these you use depends on whether you need to enforce reliability and of course whether the data source can uphold its end of the bargain whether it's able to do re-transmission and it's able to process acknowledgements itself",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.399,
        "duration": 2.881,
        "text": "let's talk about the architecture of"
      },
      {
        "start": 7.6,
        "duration": 3.52,
        "text": "spark streaming just a little bit"
      },
      {
        "start": 9.28,
        "duration": 3.359,
        "text": "it builds on the architecture of spark"
      },
      {
        "start": 11.12,
        "duration": 2.16,
        "text": "itself so all this should look very"
      },
      {
        "start": 12.639,
        "duration": 2.241,
        "text": "familiar"
      },
      {
        "start": 13.28,
        "duration": 3.52,
        "text": "if you've seen the previous material on"
      },
      {
        "start": 14.88,
        "duration": 4.159,
        "text": "spark architecture but let's have a look"
      },
      {
        "start": 16.8,
        "duration": 4.239,
        "text": "at the diagram and see what's different"
      },
      {
        "start": 19.039,
        "duration": 3.921,
        "text": "just like in regular spark we have a"
      },
      {
        "start": 21.039,
        "duration": 3.601,
        "text": "client that has a driver and that's the"
      },
      {
        "start": 22.96,
        "duration": 3.52,
        "text": "program that's actually got"
      },
      {
        "start": 24.64,
        "duration": 3.68,
        "text": "our spark streaming application in it"
      },
      {
        "start": 26.48,
        "duration": 4.0,
        "text": "just like spark after all this is"
      },
      {
        "start": 28.32,
        "duration": 3.199,
        "text": "spark we have a master node and we have"
      },
      {
        "start": 30.48,
        "duration": 3.36,
        "text": "worker nodes"
      },
      {
        "start": 31.519,
        "duration": 3.2,
        "text": "those workers have executors that have"
      },
      {
        "start": 33.84,
        "duration": 2.879,
        "text": "some cache"
      },
      {
        "start": 34.719,
        "duration": 3.84,
        "text": "and tasks running in them just like they"
      },
      {
        "start": 36.719,
        "duration": 4.321,
        "text": "would in conventional spark"
      },
      {
        "start": 38.559,
        "duration": 4.401,
        "text": "what we add is of course the stream"
      },
      {
        "start": 41.04,
        "duration": 3.839,
        "text": "source there is stream data coming in"
      },
      {
        "start": 42.96,
        "duration": 3.2,
        "text": "that's some source out in the world"
      },
      {
        "start": 44.879,
        "duration": 3.601,
        "text": "again it's twitter"
      },
      {
        "start": 46.16,
        "duration": 4.719,
        "text": "it's kafka it's whatever we get data"
      },
      {
        "start": 48.48,
        "duration": 5.52,
        "text": "from and that input stream comes in"
      },
      {
        "start": 50.879,
        "duration": 5.761,
        "text": "to a receiver now that receiver is"
      },
      {
        "start": 54.0,
        "duration": 3.039,
        "text": "just a task it's a long running task"
      },
      {
        "start": 56.64,
        "duration": 2.719,
        "text": "that"
      },
      {
        "start": 57.039,
        "duration": 4.32,
        "text": "will run for the duration of the spark"
      },
      {
        "start": 59.359,
        "duration": 4.401,
        "text": "streaming application it will always be"
      },
      {
        "start": 61.359,
        "duration": 4.481,
        "text": "responsible for receiving the data from"
      },
      {
        "start": 63.76,
        "duration": 4.48,
        "text": "the stream source and again whether it"
      },
      {
        "start": 65.84,
        "duration": 4.72,
        "text": "does that by having data pushed to it"
      },
      {
        "start": 68.24,
        "duration": 4.32,
        "text": "on a socket or it goes out somewhere and"
      },
      {
        "start": 70.56,
        "duration": 4.0,
        "text": "reads from some resource totally depends"
      },
      {
        "start": 72.56,
        "duration": 3.36,
        "text": "on the details of the receiver that's"
      },
      {
        "start": 74.56,
        "duration": 2.8,
        "text": "not something that we can determine a"
      },
      {
        "start": 75.92,
        "duration": 2.16,
        "text": "priori just from the architecture"
      },
      {
        "start": 77.36,
        "duration": 3.2,
        "text": "diagram"
      },
      {
        "start": 78.08,
        "duration": 3.84,
        "text": "that can work either way also notice in"
      },
      {
        "start": 80.56,
        "duration": 3.919,
        "text": "the spark client"
      },
      {
        "start": 81.92,
        "duration": 3.44,
        "text": "the driver has not just a spark context"
      },
      {
        "start": 84.479,
        "duration": 3.201,
        "text": "but a spark"
      },
      {
        "start": 85.36,
        "duration": 4.32,
        "text": "streaming context and that streaming"
      },
      {
        "start": 87.68,
        "duration": 3.36,
        "text": "context is now the object"
      },
      {
        "start": 89.68,
        "duration": 3.6,
        "text": "that we're going to be programming as we"
      },
      {
        "start": 91.04,
        "duration": 3.92,
        "text": "build our spark application what exactly"
      },
      {
        "start": 93.28,
        "duration": 2.64,
        "text": "is a data stream source well let me give"
      },
      {
        "start": 94.96,
        "duration": 2.799,
        "text": "you some examples"
      },
      {
        "start": 95.92,
        "duration": 3.839,
        "text": "and some categories that they break down"
      },
      {
        "start": 97.759,
        "duration": 4.081,
        "text": "into you've got the basic sources which"
      },
      {
        "start": 99.759,
        "duration": 4.481,
        "text": "are file systems and sockets and"
      },
      {
        "start": 101.84,
        "duration": 3.04,
        "text": "akka actors then the more advanced"
      },
      {
        "start": 104.24,
        "duration": 3.28,
        "text": "sources"
      },
      {
        "start": 104.88,
        "duration": 4.239,
        "text": "that have some kind of protocol to deal"
      },
      {
        "start": 107.52,
        "duration": 4.239,
        "text": "with like if it's a twitter source"
      },
      {
        "start": 109.119,
        "duration": 4.481,
        "text": "there is an actual twitter api that that"
      },
      {
        "start": 111.759,
        "duration": 3.601,
        "text": "receiver is going to have to implement"
      },
      {
        "start": 113.6,
        "duration": 4.0,
        "text": "if it's kafka we're going to have to be"
      },
      {
        "start": 115.36,
        "duration": 4.16,
        "text": "speaking the kafka binary protocol"
      },
      {
        "start": 117.6,
        "duration": 3.519,
        "text": "from that receiver to get those messages"
      },
      {
        "start": 119.52,
        "duration": 3.199,
        "text": "in and of course there are apis we can"
      },
      {
        "start": 121.119,
        "duration": 3.28,
        "text": "implement to make our own custom"
      },
      {
        "start": 122.719,
        "duration": 3.601,
        "text": "receivers if you have your own"
      },
      {
        "start": 124.399,
        "duration": 3.761,
        "text": "sources of events if you're say an"
      },
      {
        "start": 126.32,
        "duration": 3.68,
        "text": "internet of things company and your"
      },
      {
        "start": 128.16,
        "duration": 3.68,
        "text": "devices have their own api"
      },
      {
        "start": 130.0,
        "duration": 3.52,
        "text": "out there on the web then you can"
      },
      {
        "start": 131.84,
        "duration": 3.36,
        "text": "implement that api and the receiver"
      },
      {
        "start": 133.52,
        "duration": 3.439,
        "text": "and bring that data into spark streaming"
      },
      {
        "start": 135.2,
        "duration": 2.64,
        "text": "yourself now receivers can be either"
      },
      {
        "start": 136.959,
        "duration": 3.761,
        "text": "reliable or"
      },
      {
        "start": 137.84,
        "duration": 3.68,
        "text": "unreliable a reliable receiver will"
      },
      {
        "start": 140.72,
        "duration": 2.879,
        "text": "acknowledge"
      },
      {
        "start": 141.52,
        "duration": 3.68,
        "text": "data that has been received so if the"
      },
      {
        "start": 143.599,
        "duration": 3.441,
        "text": "data source is able to receive an"
      },
      {
        "start": 145.2,
        "duration": 3.039,
        "text": "acknowledgement and resend data that"
      },
      {
        "start": 147.04,
        "duration": 2.8,
        "text": "hasn't been acknowledged"
      },
      {
        "start": 148.239,
        "duration": 3.121,
        "text": "then a reliable receiver will"
      },
      {
        "start": 149.84,
        "duration": 3.6,
        "text": "participate in that protocol"
      },
      {
        "start": 151.36,
        "duration": 3.12,
        "text": "it'll send the acknowledgement after"
      },
      {
        "start": 153.44,
        "duration": 3.92,
        "text": "it's received data"
      },
      {
        "start": 154.48,
        "duration": 4.08,
        "text": "and replicated it an unreliable receiver"
      },
      {
        "start": 157.36,
        "duration": 2.959,
        "text": "of course will still be"
      },
      {
        "start": 158.56,
        "duration": 3.759,
        "text": "replicating but doesn't have the"
      },
      {
        "start": 160.319,
        "duration": 3.121,
        "text": "capability of sending an acknowledgement"
      },
      {
        "start": 162.319,
        "duration": 3.441,
        "text": "when data comes in"
      },
      {
        "start": 163.44,
        "duration": 4.32,
        "text": "which one of these you use depends on"
      },
      {
        "start": 165.76,
        "duration": 3.759,
        "text": "whether you need to enforce reliability"
      },
      {
        "start": 167.76,
        "duration": 3.52,
        "text": "and of course whether the data source"
      },
      {
        "start": 169.519,
        "duration": 3.36,
        "text": "can uphold its end of the bargain"
      },
      {
        "start": 171.28,
        "duration": 4.16,
        "text": "whether it's able to do"
      },
      {
        "start": 172.879,
        "duration": 5.281,
        "text": "re-transmission and it's able to process"
      },
      {
        "start": 175.44,
        "duration": 2.72,
        "text": "acknowledgements"
      },
      {
        "start": 182.68,
        "duration": 3.0,
        "text": "itself"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:25:26.094917+00:00"
}