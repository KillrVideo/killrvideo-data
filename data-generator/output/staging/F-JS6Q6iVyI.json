{
  "video_id": "F-JS6Q6iVyI",
  "title": "DS210.26 DSE DSBulk | Operations with Apache Cassandra",
  "description": "#DataStaxAcademy #DS210\nDS210.26 DSE DSBULK\nDatsStax Enterprise DsBulk utility has a lot of power. It is an awesome import tool, specifically built for DataStax Enterprise. Learn more about DsBulk in this unit.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-12T01:28:27Z",
  "thumbnail": "https://i.ytimg.com/vi/F-JS6Q6iVyI/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "dse",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=F-JS6Q6iVyI",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's talk about datastax enterprise ds bulk utility so ds bulk what is it it's a funny name right but it has a lot of power so what is ds bulk it's pretty much a big import tool a really good import tool specifically built for data stacks enterprise so you could take csv files json and import that data into a table or multiple tables and it's all a command line interface tool which makes it really easy to use when you're using it say a cron job or something like that it was built out of this idea that we need to get data into this database how do you do it efficiently so if you've ever used any other database load tool in datastax enterprise you know that this can be really difficult things like ss table loader can be really difficult to work with over time sometimes you just have a file and you want to get it into the database and taking data out can be hard too using the copy command isn't exactly the best for the largest bits of data so if you have terabytes of data you're not going to use copy if you use gigabytes of data you're not going to use copy so what do you use well a lot of folks use ss table loader like i said but it's not the best way to do this bulk loader solves a lot of those problems and it is built around this idea that you will have to load a lot of data so what are some of the use cases if you have a lot of files the pile of files if you will that happens a lot you get data from some data source that may be ordered by day and you have multiple files how do you get them all into the database how about that one one-time load where you need to bring something into production from maybe a migration also a developers if you need to load data from a different data source to start working with it it's a good tool for that and then even like it's a backup so let's say you have a table that you want to just quickly back up you can unload that table into a file and store it away maybe it's static data or something like that that's a good use case and then finally what if you have a change in data model changing data models means that you need to reload your data so you unload it from one data model and then you can load it back into a different data model it gives you the facilities to do that so if you see the command syntax here it's very much oriented around the command line there is no graphic utility for this ds bulk takes lots of different flags at its most simple you can tell it to load or unload and give it a file name in this example you can see i'm going to load a file from a csv and i'm going to load it into a key space 1 table one that's it really easy it'll take that csv file from file one csv loaded into my table the mapping specifications are stored in the application.comp file and you can map out some really complicated things or do it pretty simply the nice thing is that we follow some standards for the mapping format it's the familiar hokan format which you can go out and find in a lot of places it's not just specific to ds bulk so this should be a good overview of how to use it it's a pretty simple tool but very powerful it should be in your toolbox when it comes to developing code with datasets enterprise",
    "segments": [
      {
        "start": 1.43,
        "duration": 5.33,
        "text": "[Music]"
      },
      {
        "start": 7.6,
        "duration": 5.6,
        "text": "let's talk about datastax enterprise"
      },
      {
        "start": 9.92,
        "duration": 5.839,
        "text": "ds bulk utility so ds bulk what is it"
      },
      {
        "start": 13.2,
        "duration": 3.52,
        "text": "it's a funny name right but it has a lot"
      },
      {
        "start": 15.759,
        "duration": 3.52,
        "text": "of power"
      },
      {
        "start": 16.72,
        "duration": 3.84,
        "text": "so what is ds bulk it's pretty much a"
      },
      {
        "start": 19.279,
        "duration": 4.16,
        "text": "big import tool"
      },
      {
        "start": 20.56,
        "duration": 4.559,
        "text": "a really good import tool specifically"
      },
      {
        "start": 23.439,
        "duration": 4.801,
        "text": "built for data stacks enterprise"
      },
      {
        "start": 25.119,
        "duration": 5.761,
        "text": "so you could take csv files json"
      },
      {
        "start": 28.24,
        "duration": 3.999,
        "text": "and import that data into a table or"
      },
      {
        "start": 30.88,
        "duration": 3.6,
        "text": "multiple tables"
      },
      {
        "start": 32.239,
        "duration": 4.801,
        "text": "and it's all a command line interface"
      },
      {
        "start": 34.48,
        "duration": 4.48,
        "text": "tool which makes it really easy to use"
      },
      {
        "start": 37.04,
        "duration": 3.519,
        "text": "when you're using it say a cron job or"
      },
      {
        "start": 38.96,
        "duration": 3.52,
        "text": "something like that"
      },
      {
        "start": 40.559,
        "duration": 3.84,
        "text": "it was built out of this idea that we"
      },
      {
        "start": 42.48,
        "duration": 3.759,
        "text": "need to get data into this database"
      },
      {
        "start": 44.399,
        "duration": 4.16,
        "text": "how do you do it efficiently so if"
      },
      {
        "start": 46.239,
        "duration": 3.921,
        "text": "you've ever used any other database load"
      },
      {
        "start": 48.559,
        "duration": 3.121,
        "text": "tool in datastax enterprise you know"
      },
      {
        "start": 50.16,
        "duration": 3.28,
        "text": "that this can be really difficult"
      },
      {
        "start": 51.68,
        "duration": 3.92,
        "text": "things like ss table loader can be"
      },
      {
        "start": 53.44,
        "duration": 3.68,
        "text": "really difficult to work with over time"
      },
      {
        "start": 55.6,
        "duration": 3.119,
        "text": "sometimes you just have a file and you"
      },
      {
        "start": 57.12,
        "duration": 3.68,
        "text": "want to get it into the database and"
      },
      {
        "start": 58.719,
        "duration": 4.64,
        "text": "taking data out can be hard too"
      },
      {
        "start": 60.8,
        "duration": 4.319,
        "text": "using the copy command isn't exactly the"
      },
      {
        "start": 63.359,
        "duration": 3.12,
        "text": "best for the largest bits of data so if"
      },
      {
        "start": 65.119,
        "duration": 3.201,
        "text": "you have terabytes of data"
      },
      {
        "start": 66.479,
        "duration": 3.121,
        "text": "you're not going to use copy if you use"
      },
      {
        "start": 68.32,
        "duration": 2.08,
        "text": "gigabytes of data you're not going to"
      },
      {
        "start": 69.6,
        "duration": 3.36,
        "text": "use copy"
      },
      {
        "start": 70.4,
        "duration": 4.719,
        "text": "so what do you use well a lot of folks"
      },
      {
        "start": 72.96,
        "duration": 4.72,
        "text": "use ss table loader like i said"
      },
      {
        "start": 75.119,
        "duration": 4.161,
        "text": "but it's not the best way to do this"
      },
      {
        "start": 77.68,
        "duration": 2.479,
        "text": "bulk loader solves a lot of those"
      },
      {
        "start": 79.28,
        "duration": 3.199,
        "text": "problems"
      },
      {
        "start": 80.159,
        "duration": 4.401,
        "text": "and it is built around this idea that"
      },
      {
        "start": 82.479,
        "duration": 4.481,
        "text": "you will have to load a lot of data"
      },
      {
        "start": 84.56,
        "duration": 4.0,
        "text": "so what are some of the use cases if you"
      },
      {
        "start": 86.96,
        "duration": 4.0,
        "text": "have a lot of files"
      },
      {
        "start": 88.56,
        "duration": 3.36,
        "text": "the pile of files if you will that"
      },
      {
        "start": 90.96,
        "duration": 2.799,
        "text": "happens a lot"
      },
      {
        "start": 91.92,
        "duration": 3.28,
        "text": "you get data from some data source that"
      },
      {
        "start": 93.759,
        "duration": 2.641,
        "text": "may be ordered by day and you have"
      },
      {
        "start": 95.2,
        "duration": 2.559,
        "text": "multiple files"
      },
      {
        "start": 96.4,
        "duration": 2.96,
        "text": "how do you get them all into the"
      },
      {
        "start": 97.759,
        "duration": 3.121,
        "text": "database how about that one one-time"
      },
      {
        "start": 99.36,
        "duration": 2.64,
        "text": "load where you need to bring something"
      },
      {
        "start": 100.88,
        "duration": 4.0,
        "text": "into production from"
      },
      {
        "start": 102.0,
        "duration": 4.079,
        "text": "maybe a migration also a developers if"
      },
      {
        "start": 104.88,
        "duration": 3.04,
        "text": "you need to load data from a different"
      },
      {
        "start": 106.079,
        "duration": 4.161,
        "text": "data source to start working with it"
      },
      {
        "start": 107.92,
        "duration": 3.519,
        "text": "it's a good tool for that and then even"
      },
      {
        "start": 110.24,
        "duration": 2.64,
        "text": "like it's a backup"
      },
      {
        "start": 111.439,
        "duration": 3.121,
        "text": "so let's say you have a table that you"
      },
      {
        "start": 112.88,
        "duration": 3.519,
        "text": "want to just quickly back up"
      },
      {
        "start": 114.56,
        "duration": 4.08,
        "text": "you can unload that table into a file"
      },
      {
        "start": 116.399,
        "duration": 3.04,
        "text": "and store it away maybe it's static data"
      },
      {
        "start": 118.64,
        "duration": 3.36,
        "text": "or something like that"
      },
      {
        "start": 119.439,
        "duration": 4.72,
        "text": "that's a good use case and then finally"
      },
      {
        "start": 122.0,
        "duration": 3.68,
        "text": "what if you have a change in data model"
      },
      {
        "start": 124.159,
        "duration": 3.201,
        "text": "changing data models means that you need"
      },
      {
        "start": 125.68,
        "duration": 3.84,
        "text": "to reload your data so you"
      },
      {
        "start": 127.36,
        "duration": 3.76,
        "text": "unload it from one data model and then"
      },
      {
        "start": 129.52,
        "duration": 3.439,
        "text": "you can load it back into a different"
      },
      {
        "start": 131.12,
        "duration": 2.96,
        "text": "data model it gives you the facilities"
      },
      {
        "start": 132.959,
        "duration": 3.28,
        "text": "to do that"
      },
      {
        "start": 134.08,
        "duration": 3.04,
        "text": "so if you see the command syntax here"
      },
      {
        "start": 136.239,
        "duration": 3.121,
        "text": "it's very much"
      },
      {
        "start": 137.12,
        "duration": 4.24,
        "text": "oriented around the command line there"
      },
      {
        "start": 139.36,
        "duration": 4.8,
        "text": "is no graphic utility for this"
      },
      {
        "start": 141.36,
        "duration": 4.0,
        "text": "ds bulk takes lots of different flags at"
      },
      {
        "start": 144.16,
        "duration": 3.2,
        "text": "its most simple"
      },
      {
        "start": 145.36,
        "duration": 3.28,
        "text": "you can tell it to load or unload and"
      },
      {
        "start": 147.36,
        "duration": 3.04,
        "text": "give it a file name"
      },
      {
        "start": 148.64,
        "duration": 4.4,
        "text": "in this example you can see i'm going to"
      },
      {
        "start": 150.4,
        "duration": 3.839,
        "text": "load a file from a csv"
      },
      {
        "start": 153.04,
        "duration": 3.199,
        "text": "and i'm going to load it into a key"
      },
      {
        "start": 154.239,
        "duration": 4.481,
        "text": "space 1 table one"
      },
      {
        "start": 156.239,
        "duration": 3.041,
        "text": "that's it really easy it'll take that"
      },
      {
        "start": 158.72,
        "duration": 2.879,
        "text": "csv"
      },
      {
        "start": 159.28,
        "duration": 4.56,
        "text": "file from file one csv loaded into my"
      },
      {
        "start": 161.599,
        "duration": 2.801,
        "text": "table the mapping specifications are"
      },
      {
        "start": 163.84,
        "duration": 3.28,
        "text": "stored"
      },
      {
        "start": 164.4,
        "duration": 4.32,
        "text": "in the application.comp file and you can"
      },
      {
        "start": 167.12,
        "duration": 2.88,
        "text": "map out some really complicated things"
      },
      {
        "start": 168.72,
        "duration": 2.799,
        "text": "or do it pretty simply"
      },
      {
        "start": 170.0,
        "duration": 3.76,
        "text": "the nice thing is that we follow some"
      },
      {
        "start": 171.519,
        "duration": 5.041,
        "text": "standards for the mapping format"
      },
      {
        "start": 173.76,
        "duration": 4.72,
        "text": "it's the familiar hokan format which you"
      },
      {
        "start": 176.56,
        "duration": 4.56,
        "text": "can go out and find in a lot of places"
      },
      {
        "start": 178.48,
        "duration": 4.08,
        "text": "it's not just specific to ds bulk so"
      },
      {
        "start": 181.12,
        "duration": 2.16,
        "text": "this should be a good overview of how to"
      },
      {
        "start": 182.56,
        "duration": 2.48,
        "text": "use it"
      },
      {
        "start": 183.28,
        "duration": 4.08,
        "text": "it's a pretty simple tool but very"
      },
      {
        "start": 185.04,
        "duration": 4.4,
        "text": "powerful it should be in your toolbox"
      },
      {
        "start": 187.36,
        "duration": 8.159,
        "text": "when it comes to developing code with"
      },
      {
        "start": 189.44,
        "duration": 6.079,
        "text": "datasets enterprise"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T01:19:21.032353+00:00"
}