{
  "video_id": "bcd1V_uEtyk",
  "title": "Get Your Data AI Ready with Unstructured and DataStax",
  "description": "You've probably heard that there is no AI without data - but how do you easily tap into unimaginable large troves of data, spread across multiple data sources and formats? Developers often need to use different types of unstructured enterprise data to build AI apps using RAG, from PDFs to Salesforce data to Google and Microsoft Office Docs.\r\n\r\nJoin DataStax and Unstructured for a 45-minute livestream to learn how we can help get your unstructured data ready to use in your AI-enabled applications. We’ll be joined by the Unstructured team alongside Tejas Kumar and Eric Hare from DataStax for a live coding demonstration of how you can:\r\n--Ingest unstructured data into a vector database\r\n--Think about unlocking that data in an application using RAG\r\n--Build a simple AI application using Langflow\n\nRESOURCES:\n\r\nSign up for Astra: https://astra.datastax.com/signup\nSign up for Langflow: https://astra.datastax.com/signup?type=langflow\nColab: From Documents to Data Pipelines - https://colab.research.google.com/drive/1jq9VVUT8DiLgY6v0A7CgmCNgoSgE9kjN?usp=sharing \nColab: Unstructured- https://colab.research.google.com/drive/1ZlEmWcZMxsRhQwIo3uHMyuHEF-e8oks2?usp=sharing\nUnstructured API Key: https://unstructured.io/api-key-hosted \nBlog Post: https://www.datastax.com/blog/two-new-unstructured-integrations-langflow-astra-data-loader\r\n\n\nAbout DataStax Developer:\r\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache Cassandra©.  Visit https://datastax.com/dev for more free learning resources.",
  "published_at": "2024-09-20T06:02:47Z",
  "thumbnail": "https://i.ytimg.com/vi/bcd1V_uEtyk/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "tutorial",
    "astra",
    "demo",
    "search",
    "datastax",
    "vector",
    "apache_cassandra",
    "database",
    "cassandra",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=bcd1V_uEtyk",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "share today how you can get your data AI ready with unstructured and data stacks and today we're joined by Maria kosova from unstructured staff developer Advocate and Eric hair my colleague at data Stacks welcome thank you great well it's great to to see you all um we were just talking before the show began and let's maybe start really high level with some introductions and then let's I want to dive straight into what it even means for data to be AI ready so maybe we can start with um Maria can you tell us a little bit about yourself the unstructured and the work that you're doing there absolutely my name is Maria HOSA and I'm a Staff developer Advocate at unstructured at unstructured we fuss over data so unstructured data specifically we pre-process data for all kinds of AI applications mostly uh rag applications and there are a lot of challenges associated with that and we help solve those challenges fantastic um and I can't wait to dive into what what that means in in a little bit more detail um but before that of course Eric um would you um give us a short introduction to yourself and the work we're doing in data Stacks yeah um hi everyone really excited to be here so I'm a software engineer at data stacks and um one of the primary focuses that I've had is on building Integrations between Astro DB our our database offering and platforms like unstructured so um making it really easy basically to leverage unstructured when you're building rag applications that Target you know a vector database like Astra fantastic thanks I'm so excited to dive into all of this now one unifying thing that both of you mentioned was rag applications and when I mentioned AI Readiness um Maria you said unstructured helps with this particularly also with building rag applications because there's some complexity there um when we talk about rag at events um I just got back from a conference yesterday and you know we did this thing from the stage where we asked the attendees um how many of you have heard of rag in in an audience of 3,000 two hands went up and so I think it's very important that we that we actually take a few minutes maybe not too many but just Define rag for the um listeners here and then we can dive into specifically unstructured role there Maria would you help us um just by explaining rag in detail of course so rag stands for retrieval augmented generation and it sounds complex and sounds very sciency but it's really a very simple idea uh that you give some context to your llm so llm does the generative part it generates text and the issue with the off-the-shelf llms is that they have been trained on massive amounts of data from the internet but they have never seen your personal data or your company's data about your internal policies or customer or any kind of proprietary data they have never been exposed to that kind of data and they're also Limited in in um uh in time because they have a knowledge cut off date so past certain date they don't know about any of the events that happened after that and rag is a way to give additional context to an llm uh to help supplement some information and to help the llm to generate a useful response to whatever user query can be fantastic thank you for that yeah that's so helpful um for the vast majority people because even here in the comments Kevin U mentions he says rag that's what I use to clean my kitchen counter indeed we're not talking about a cloth here we're talking about a technique um to bring real-time context into larger language models and also um correct me if I'm wrong here but the right context right because like um a number of folks have reported with Google's AI overview feature they they ask questions um for example how many rocks should want each per day and the answer is hallucinated with three or four or something like this but that's because the training data often includes like jokes and so the the right answer isn't often given because of this and rag can help with that um Eric is that accurate at all yeah absolutely um you know it it it becomes kind of the case that that rag is a way of like um augmenting the information that is out there with the information that sometimes you need to pull in from sources that that might not be indexed in llms by default for various reasons uh you know proprietary um brand new information and so combining those two concepts uh you can really like build something pretty cool with it excellent I'm excited to find out um what exactly we're going to do um I understand we have some demos and some really great things to share with the attendees here um before we get into it I want to be very clear that um this is very much not just a trialogue as you may be watching on the live stream but um for those leaving comments we read all of them and we're more than happy to take your questions if and when you have any um with that Maria let's dive into unstructured you mentioned unstructured helps with AI Readiness um specifically with rag applications what is unstructured and how does it do that I have a short answer and a long answer to that so the short answer is um that unstructured is a startup that has built tools to make um the most frustrating step of building R applications easier and that step is pre-processing unstructured data and making it rag ready and the longer answer is for the for the longer answer it's important to understand that you really do need to have good data to have a useful rag applications because um data fuels AI data fuels rag applications or even fine tuning uh use cases and the most abandoned type of data in any organization any company any business is unstructured up to 80% of data created on daily basis is unstructured so these are PDFs Word documents PowerPoint presentations markdown HTML so on and so forth so if you think about um contract policies legal document marketing documents field reports and so on and so forth all of these are uh stored in unstructured formats they encapsulate a lot of important information about uh the business itself the customers the products the kind of insights that are often not available in structured d uh structured data and so that's why you often want to leverage those documents but it's not easy to use them because they're not very usable in their raw format so you have to pre-process them first and that is complex because a you have to first aggregate them from a whole bunch of different silos that they are scattered across you have to figure out how to parse all these different Native formats uh in certain cases like with PDFs images PowerPoints you will have to figure out how to use uh possibly OCR models or document understanding models or VMS to extract content content from those documents um and then of course you will you need to understand how you want your data model to look like so this is uniform across different sources different Native formats and then of course you need to um figure out where you're going to store them how you going to upload to your destination and so this is what unru structured takes care of all the steps from the ingestion up to uh uploading the rag ready results into a vector store so we don't orchestrate rag or we don't prescribe what llm you should be using unstructured focuses on one and one thing only is making your raw unstructured data usable and ready for whatever rag application you want to build fantastic thank you so much so if if we understand that together and and by we I mean all the people um in the comment section as well unstructured can help get data become rag ready um by essentially converting it from whatever format the data is originally in into an alternate format um and this alternate format can then be used for rag workflows is this alternate format text um because like if if we consider rag right rag is dependent on prompt engineering meaning you get context you injected into your prompt and then you ask your question um is the final destination of unstructured processing text or is is there some intermediary layer that then an application would take forward for a rag use case so you're correct that when you pass a document to an llm as context you pass text however when we ex uh process unstructured documents we return Json because we want to add metadata to that text as well documents don't exist in vacuum they can refer to they they can describe different products they can uh you know you there are different file types different sources different languages and you want to have as much metadata as possible uh with your um results so that you can apply maybe metadata pre-filtering or maybe you want to curate your data and you know drop some parts of it before you use it um so for that reason it's not just the text that we extract but we also try to get get as much metadata as we possibly can from all of those documents and so that results in adjacent document that we then load into uh a vecture store incredible okay so one would even could perhaps argue that unstructured processing step helps take data from unstructured formats and prepare them to even be structured because if you're getting Json you can then group um certain Json properties together form even model relationships with them perhaps build a Knowledge Graph and store that somewhere as well is is that close to something possible yeah you can absolutely do that yeah awesome that's so great okay so now we start to understand um why the partnership between data stacks unstructured is so synergistic because unstructured is the pre-processing step where the next obvious destination is a vector store um where you store the unstructured data that you can then use for similarity search um we have some questions in the comments but since we're following the pipeline for rag applications all the way from the unstructured beginning let's talk a little bit about the sync or the destination of that process data which is the vector store Eric would you be willing to um expand there Eric you're muted sorry about that yeah definitely willing to so basically um unstructured has this incredibly powerful framework what of what they call Destination connectors they actually have Source connectors too and Maria if I say anything incorrect here just interrupt me but um but basically the power of the vector store component is that you we already built this destination connector that supports Astra and what that means is essentially there's one really simple script to Define your pipeline which essentially that pipeline is going from your unstructured document which as Maria talked about could be all kinds of formats PowerPoints Etc and going through all the stages needing to make it AI ready and rag ready and then immediately getting back a collection in astb where the text from that document and other components of that document have their vectors automatically computed so you can essentially start quering and doing similarity surge you know with with really minimal effort it's mostly just getting API keys in order and it's it's pretty powerful what they built that's incredible um and so astrab being this destination connector would then take that forward and be used to build a rag flow how exactly would a develop um then take the output of unstructured and build a rag application yeah so there there's a lot of options here and um some some that might more low code and some that are more for the season developer but um one option is if you're either familiar or willing to get your feet wet with some of the more common Frameworks for um llms and AI Lang chain llama index Etc um basically once you've done the step of storing your unstructured data into your asra DB Vector store um it's again almost like a oneline connect to your as DB collection and start doing similarity searches and start building your rag pipeline so you can imagine pretty much the whole Suite of functionality that's in Lang chain for example is now at your fingertips and um you know you began simply from a PowerPoint or from a collection of PowerPoints in order to start building that fantastic I can't wait to see all of this in practice I think we have some stuff prepared for that um one question before we go further from the demo is for Maria somebody asked here in the audience Jimmy Burns is asking if PDF and spreadsheets are also transformed into these he mentioned text in the comments but we already established it's a Json um is that accurate so unstructured can take PDFs spreadsheets and other formats and transform them into something rag ready yes so the way we do it we support 25 plus different file types that includes PDFs and uh Excel and HTML markdown many more what we do is we uh take the document in and then we partition it and we split it into individual document elements we call them document elements so document elements this can be a title a paragraph which we call narrative text it can be a table an image a list item footer header all those different categories of um logical elements of a document uh and this is helpful when we further transform those uh documents and chunk them and embed them it is specifically helpful for chunking because we understand this way the document structure and we Chunk in a smart way so that the content of different topics are not mixed within the same chunk which helps further down the the stream with the retrieval process because you want to retrieve the most relevant information and you don't want to retrieve a chunk that partially contains something about topic a and partially contains information about the topic B so that is one of the reasons why we split those documents so the Json that you receive um contains these document elements and every element contains text uh Cate or type and additional metadata and if you do the embedding step as well then that document element will also have embedding there I hope this answers the question that definitely does out of curiosity and this is maybe a little bit silly but you mentioned so many different file types what's the most unexpected one um for example can unstructured um can like process like a movie and give you a big Json a of each line from the movie or something um not yet but um maybe soon fantastic but I guess more questions yeah if you're asking about unexpected files I think one of them can be an email uh we can process emails as well and then for example for emails in the metadata you can have the recipients the senders this kind of information attachments to the email can be processed as well so I guess this could be more of an unusual type fantastic that's actually gives me already some ideas I can use to build Vector search for my email uh client in case um there's so many questions Hey listen I want to take a moment and really appreciate the audience here because um often live streams you never know how they're going to go sometimes a lot of people want to interact and other times it's just us but since the audience is being so engaged I'd love to actually pay more attention to the audience and and just answer their questions I understand we do have some demos prepared but if we can prioritize the audience here I think it would be a better service to them and then we can handle these in in follow-up discussions so with that um there's a question from Sophia Kino asking um the security measures um and how we can avoid for example taking like confidential documents passing them through unstructured processing and then feeding them to an llm where the llm can then steal that data or other otherwise use it to train itself or have the vendors of the llms ReUse them in the fine-tuning processes um I think an an maybe a a question to extrapolate from this is does unstructured in its processing pipeline offer ways to strip or redact certain data from formats that go through its processing so there are several aspects to this question first uh the security um unstructured is a sock compliant and it's Hippa compliant you can uh process your documents with unstructured hosted API but we also offer unstructured on Azure and AWS Marketplace so that you can have it in your own VPC infrastructure so that the data stays there never goes anywhere um that's one of the question questions as for stripping certain parts of content uh you can set up your pipeline to First maybe partition the documents and once you have the document elements you can curate the results so you can go through those Json documents however you want remove um information that you don't want to be uh used for your application and then you can use another pipeline to do the rest of the steps for example chunking and embedding and uploading you can do that so there's a lot of flexibility um you can maybe integrate some Pi Pi reduction step in there so personal personally identifiable information that that is pi you can remove that somewhere along the process uh as for training llms if you don't want to use an llm that is provided by uh you know open AI or somebody else you can host your own llm somewhere and then you have full control over happens to that llm what goes to it and and um you can be sure that you are not giving your data to any of the other companies fantastic yeah and for for to make it really practical Lama is is a great inference engine you can use for example where you run your llm locally including l. CPP and VM there's a number of things that's a separate discussion thank you so much very thoughtful answer and I hope that answered um Sophia's question there was a question by David Edwards um that I think is on the minds of many um attendees which is unstructured can as you mentioned handle multiple file formats um I think a strong use case for really most SAS businesses that came up a few years ago was just we're creating a nicer way to work with company data that isn't Google Sheets or some type of spreadsheet solution right um and nowadays there's a lot of um work still being done with spreadsheets and so David's question is how good or reliable is unstructured working with spreadsheets um he mentioned specifically ex Excel spreadsheets but I assume all spreadsheets with unpredictable formats meaning um merged cells or empty cells or cells that are that are just purely formatting that don't really some some Excel spreadsheets have like background colors and text colors that don't really mean much um how how does how can one maybe even fine-tune the data pulled out from these spreadsheets with unstructured I'm not sure what you mean exactly by fine-tuning the data pulled out uh but uh when it comes to spreadsheets we treat them as tables and for the tables we have very strong mechanisms for layout detection and understanding when they're merged cells uh different uh earch key nested tables all that we have seen all kinds of nasty tables I have to say and so this you know all kinds of weird tables are not going to be used to us but I'm not sure what you mean by fine-tuning the data usually you fine-tune a model on something and maybe cleaning the data correct yeah just um so you get back a bunch of data that some of it makes no sense and others are just formatting and so um by fine mean like filtering out the useless stuff yeah once you have Json you have so many different ways to to filter that out um there are many options to do that excellent okay let's let's make it really practical because Sammy is asking for a start to finish scenario and exam example of taking unstructured data for a company to to the Finish end results meaning a working rag llm application I think this is this is like prime time to show not tell um so what if what if we did this what if we showed unstructured ingesting some data process not ingesting that's the wrong word but um reading data and processing it and then outputting this Json um as as part A and I think we can do that um with with you Maria and then as Part B we can look at Lang flow and building a rag app with Eric um we can maybe start at the beginning with with unstructured absolutely let me just share um a notebook with you okay I'm going to zoom in a little bit so um oops there we go I think this should be let me know if you want me to zoom zoom in a bit more that looks great okay so um in the interest of time and just to live some time to Eric I'm going to run the whole notebook but I'm going to explain how you can build a pipeline with unstructured in just a very few steps first you're going to need a key uh the API key that you can get here and it comes with a 14-day trial which is kept at 1,000 pages per day there are many ways to use unstructured API so you can use uh we have a python SDK you can um send direct request requests to the API endpoint there is a javascri script SDK as well but my favorite way of using unstructured is with unstructured in just Library so this is what we are installing here it's a python Library so if you're allergic to python I'm sorry but I'm going to use Python here um and when you're installing unstructured inest you need to specify def dependencies uh so for your Source sources destinations um uh embedding providers and uh types of some types of files so here for example I'm going to be oh I actually don't need S3 because in this example I'm uh loading from a local source so this is this was unnecessary so PDF for a PDF document ASB is going to be the destination here and hugging face is used for generating embeddings um because we're going to be loading into asro DB you're going to have to set up your asrb database and collection and you're going to have to have your um authentication credentials obviously I'm not showing mine here you're going to need to choose which embedding model you're going to use for the final step of transformation and when you have embedding models they um output vectors of different dimensions so you will need to know the dimension of the vector that your model outputs you can usually find that information in the um the documentation of the model on the embedding providers website so if you go to huging face model card there will contain that information if you go to open thei the documentation will have that we're going to do some imports and then in this example I'm loading from a local directory so I need to know where uh my files are located uh this is the path I'm going to use work directory is um optional but I like to set it because this is where I can save the caches for each and every step so if I want to look into what happened after partitioning after chunking after embedding I I can go into this directory and see the results un structure will save them even if you don't specify the working directory it's just going to be a path and will be somewhat obscure and uh maybe not as easy to find and so when you build a pipeline from a source to the destination you build it from a bunch of configurations so think of it as building a tower with Lego brakes it's that easy so this is all it takes this is the whole Pipeline and let me explain what these configs are so you specify pipeline do from configs the first one context uh and this is a this is called a processor config it controls General Behavior verbosity the working Direction number of processes the general behavior of the pipeline next you specify the location of your documents in this case it's a local directory so this is where we're going to be indexing the files from downloading the files from uh and where we going to be connecting to you can replace these three lines with a different connect ctor for example S3 Azure blob storage you can connect to SharePoint or Confluence uh and there are 20 different sources that are supported so if your source is different if you're not doing this with local documents that these are the connect uh the configurations that you will need to replace and for example here I no I cannot switch to a different tab but you can find documentation for all the source conectors uh in unstructured docs once the data is ingested next step is partitioning the files into those document elements and this is what partitioner config controls this is where I decide that I want to use the API and I provide my credentials we have different strategies for partitioning the document you have fast strategy and high resolution strategy essentially if you're processing markdown or HTML files you can use fast to um use rule based parsers to quickly get the results and hes strategy is tailored for image-based documents such as PDF power points images and so on the cool thing is that if your PDFs only contain text and nothing else then fast strategy will also work for them and it's going to be very very fast so you may be able to use it too um this block of parameters just makes uh PDF processing a little bit faster by setting concurrency levels and things like that and that's it at that step your documents are partitioned uh split into document elements they have metadata they are in Json format and normally if you want to build a rag application next you have to chunk documents and you have to generate settings for them so the next logical step in the pipeline is uh the chunkin step which is configured with the chunker config un structure offers several different chunkin strategies um that you can use to split your documents into chunks that will fit into the context window of your embedding models and are small enough to contain precise information and enough context but you might be thinking that wait a second we already split the document into elements why do I need to chunk them aren't they already chunked and the answer to to this is they're not really chunked because you may have a very very long paragraph which is a single document element which will not fit your maximum limit so that element will need to be split into smaller elements and then you may have a whole bunch of small elements for example a list will uh come out as a bunch of list item elements so every list item might be quite small and you may want to merge those elements into a bigger chunk so this is uh why you still need the chunking step and the chunking strategies control how smaller elements are merged so by title strategy make sure that you don't merge content from different sections this helps with the retrieval performance down the line but there are other strategies that take into account other aspects like for example whether two elements are similar or not whether we can merge them into one chunk based on similarity after the documents are chunked next step is to embed the documents and unstructured doesn't uh host embedding models but we integrate with a number of embedding model providers here I'm using hugging phase but you can also use embedding models from open AI uh verx AI octo Ai and a few others depending on your provider you may need to provide your open sorry your key like open AI key for example uh for hugging phas we just need to give it the name of the embedding model and fantastic um sorry to interrupt you but there's a question here from automate Labs that um they say it's the perfect time to ask um which is can I define so they want to know can can they Define some elements themselves like some titles so the chunking will respect their you know quote unquote options of titles and chunking by titles for example uh no so the chunking strategy by title it's um the title means just a section so we we def we identify which document ele elements are section titles and then we respect that structure and hierarchy of the document uh you would have to um no I don't think there's a straightforward way to to modify that perfect thank you sorry again sorry to interrupt but they This was um the timing thank you I'm happy to answer any more questions if if uh if you have if not I'll just continue yeah let let's continue um because we also want to um see the the destination of this um from from yeah and that's the next step so once the documents are embedded and by the way chunkin step and embedding step are optional so if you just want to pre-process a whole bunch of PDFs and then load them somewhere and sech through them if rag is not your use case you don't have to do chunking and embedding but these are essential steps for a rag pipeline specifically and we can load the results into Aster DB and for that we need these three configs connection config where you set your authentication options upload Stager uh will check that everything is okay with your data and then ASB uploader config this is where your specify the name space and the collection uh where you want to upload the results and that's it so it may look intimidating at first but this is all it takes to build a full end to end ETL pipeline from any of the 20 sources to uh the destination of your choice with uh partitioning and chunking and embedding um all taken care of so um takes a moment to get used to but when you see it as a bunch of Lego breaks it becomes much easier fantastic thank you so much for that presentation and it's you know it I don't even know python um but that that looks approachable to me which which is just a testament of how intuitive um it really is so thank you again for that um there's a question by by James here in the comments who's asking so the embedding step is really just the creation of the VOR and James the answer is correct there's a specialized class of machine learning models that um they literally embed they embed natural language in high-dimensional space and so they just tend to group semantic meaning words closer to each other in that space than words that are further away um but yes they just embed words in space and the output of those is Vector embeddings which is just a big list of numbers um usually the models have a deterministic length um so um thank you so much for that Maria and and it's a really one question I have and this is not in the comments it's just me being curious mainly because I'm um kind of a novice at python is there an alternate are there different language sdks for unstructured meaning if I wanted to do this in JavaScript could I or yet yes you can you uh well not for ingest ingest pipelines are uh only in Python you can process files one by one with a JavaScript SDK we have a JavaScript SDK for that uh and you can also send files one by one to uh the endpoint directly there's uh you can just send post requests um I personally prefer the injust pipeline because a I like Python and B it's just the simplest way you have this very easy way to build the whole pipeline by just uh configuring the steps essentially where do you start what do you do and where do you land and um it's just the the the most um userfriendly way for me to to build these pipelines fantastic great thank you so much and that everybody is how practically you can use unstructured to go from a bunch of PDFs or Excel sheets or soon we were told movies um to embeddings to then a vector database and rag um and again as as Maria said if if if rag is not your use case you could just cut it off at the at the step where you have a bunch of Json and process that however you want um great so now that the things uploaded into ASB let's say with that step um I'd love to invoke the Eric uh my my colleague and and pick it up from there and and turn it into an actual endtoend um rag application so we extracted a bunch of data from PDFs we followed this Pipeline and data is now in astb um Eric how might we then use this to finish our rag application yeah yeah so um if if if it's okay with with everyone what I'll do is I'll show maybe two two kind of um I guess demo ingest examples that that are kind of you know once you have your data in astrab you've gone through this pipeline that Maria showed um building a rag application off of that and one is a code based solution and one is a I guess what we'd call a low code based solution so um so let me go ahead and share could we um can I can I ask for can I express a preference for which one we do first you have a plan oh great um if we could do the the low code first just because we can start at the most accessible to the most people assuming vast majority of people don't write a lot of code and then we can go to the code one provided there's enough time cool so I'm getting a message that says the screen is actively being shared ask the presenter to stop sharing um is is there any sharing going on there's no one sharing um at least that I can see um but crowdcast maybe is having some issues um no I I don't think anyone's sharing right now okay then maybe I'll hop back on briefly and see if I can get that working yeah that's that's okay we appreciate that you guys want to talk for 30 he just like left yes there's a question in in the audience we can actually this is the perfect time to take some of those um there was a question about um using a domain specific is it better to use and this is a question from eera I hope I'm not rooting this name or mispronouncing it is it better to use a domain specific AI model as an embed embedded I assume they mean embedding the model um I'm not sure what I understand the question because I don't know if I don't know what a domain specific AI model is Maria do you have an answer to this yes I believe uh the question is so when you have embedding metal embedding models you can go especially for open source models for example you can go to uh there's a litter board by hugging face that shows the metrics on academic benchmarks how they perform and you may there are a lot of general purpose embedding models that were trained on um just text from the internet essentially but let's say if you're trying to use that kind of model on legal documents there can be a lot of very specific terminology abbreviations and it may not necessarily do a generic model may not necessarily do as well on those documents so ideally if you have an embedding model that was tailored or fine-tuned to a domain it will perform better yes fantastic thank you so much and look we have Eric's screen share finally was perfect way to to fill some some downtime hey Eric um yeah let's go how do we finish the application yeah so um I I want to um emphasize by the way that a lot of what you're about to see is under very heavy development so this is kind of like a preview of of a way you can do it it already is working end to end in a way that I think will be impressive but um but there's still work to do to make it like handle more cases with that said um what you're looking at here is what's called Lang Lang flow um if you're unfamiliar with it think of L AI pipeline a rag pipeline really a a pretty significant um number of components that they have available and chaining the different outputs together such that you're building that pipeline almost in a graphical way that's like a really highlevel way of saying what it does um but I'll show you this little pipeline this is actually like by the standards of some of the ones that that exist in Lang flow maybe on the simpler side but I think it's how we can do this end to end step that um that basically uh is is what what what we're trying to do to build this rag application so we have an unstructured component um what this component is going to do is it's abstracting away that python code and instead what it's asking for is a file and an API key kind of like what Maria was showing you um the components of the pipeline are kind of hidden right now like that's an advanced feature so it's and obviously that can be exposed but what happens is that file gets passed in um a number of files get passed in and this data then gets stored into astrab in a vector store so basically we have all these uh you know what what we're looking at here is those different narrative texts um you can view the actual text of it um unstructured is doing all the processing of this um and then what we can do is we can start to actually build a rag pipeline so this one is taking the data that was just outputed by unstructured and saying Hey I want to do some kind of template transformation on that data in this case we're keeping it as simple as possible all we're saying is we want to pull the text field from that data but you could imagine you can construct a template that does all kinds of interesting things like maybe it's tagged by some parameter of the got extracted um which by the way could be far more I guess like indepth in this declaration of independence example that I'm showing um but right now we're just going to extract the text so we're saying unstructured Parts is the data stores it into Astra and we're going to build a template that's just the text that gets extracted so you can see what we get out of that is essentially here's the context for some queries that you might want to run doesn't have to just be the full text of the document it can be a lot more interesting than that what we can do is we can start to build the rag application so we're going to pipe that text in to the document that is going to underline The Prompt template that we're going to create so once again we have all kinds of flexibility and what we build here this is a really simple one we're just instructing the llm answer the users's question based on the document provided here's the user's question give me your answer again we can make that much more interesting if we wanted to um where the power really starts to come into play so now um I can type the question and I said just what is this document about at a high level so just to give it some creative freedom to answer a question and now this prompt template what we're do is we're GNA pipe that in to open AI basically what we're doing is taking the question using my open a a API key open AI key to then generate the embeddings and compare it to what unstructured produced in order to help answer that question and then the last step is we get an output so um all those are chained together let's see what happens we get a response from the model and it says this document is the Declaration of Independence adopted in 1776 and you'll notice it's very specific to say at a high level here's some of the things that it says so so the more what we did here is we started with literally just uploading a file and we built a pipeline where you can in this case it's a really simple question answer prompt but you can imagine there's so much we can do here to make it more interesting if we wanted to and that's all done graphically by chaining together these inputs in Lang flow um I didn't have to do any code I just had to kind of gather my API keys and put them in the right place right so um that's what we think is like you know you can build a code-based solution but you don't have to if something like this is more friendly to you to start building your pipeline you basically have at your fingertips a chat bot that we just created from almost no code at all right thanks to the power of both Astra and unstructured that's a that's a great point you just make um as as a result of the power of Astra and unstructured in this langlow diagram um what it looks like we're doing is reading a text file and then adding the entire text file to The Prompt um in a true rag application correct me if I'm wrong we would retrieve chunks that are ranked based on similarity to a user's query from a vector store which we don't have here um and then feed just the highest ranked chunks aha there it is okay so there's the AST okay this is what I was asking where's the astb where's the vector where's the similar he was the chunking you you you're exactly right and um good for calling me out on this because this is a little bit for sake of just making the the demo a little easier but the the truth is you're right we wouldn't just want to pull all the text we would have a big database so instead we would pipe this to Astra and then do some the filter right here and do a retrieve basically this Retriever with search results that then get piped down to this so we filter basically the inputs to the most relevant um rows from the astb table fantastic okay and that's that's how so with unstructured we've uploaded the chunks of let's say the Declaration of Independence um maybe chunked by every thousand characters or so and we then ingested them into Astra and we could search for something like um Liberty and so when when a user types this word that's converted into a vector and it's compared for what vectors in the database what chunks are closest to this word and we get maybe the top five we put them in a prompt and say here um Mr or Mrs or non-binary llm um here's the context answer the question is is that representative of a true rag flow that's absolutely right and um while I I probably can't run it end to end I can show you the for the way that you interact with langlow to do what what Tes just said is essentially you say okay well I'm outputting data here I I know what the format of that data is I know what I can connect it to to build drag and drop a connection and say okay instead of par going directly to the parse text from unstructured We're first going to ingest it and then do some search query like liberty before we pass those results to the next stage of the pipeline fantastic great um and I hope for for the there was the question above that this is exactly the one um we answered I'm going to see if I can find it here in the comments um because it was so perfect um and I'm also going to highlight it here um where they asked practically what this would look like and we just did that right we we captured practically how you would ingest a document with unstructured and then expose it to an llm with Lang flow um we have about 10 minutes left um Eric you did mention you had two examples um can we take a look at the other one yeah the the more code focused version yep correct um I'm gonna again it'll just take 10 seconds because for whatever reason it doesn't let me reshare after someone's been sharing including myself so I'll be right no problem um while let's let's do this dance again Maria um while Eric comes back there was a question um from David Edwards which I think is more for langlow which maybe I could answer but I I genuinely don't know he's asking can you connect two documents to The Prompt for example RFI response and evaluation criteria now unstructured would give us would take all the documents and just put them in Astra so Astra here would be responsible for retrieving any number of related documents to the prom exactly so let's say you uh you can have thousands or millions of documents and then you can have an a pro pre-processing pipeline that ingests all of that uh transforms them stores them and then when you retrieve uh in Astra and you use similarity search you can absolutely retrieve chunks from different documents if they're relevant to the query if they're similar to the query I should say because similarity search finds a similar document um and yeah you can have chunks from different documents added as context to your llm in the prompt perfect thank you so much and Sean Shan Duffy says um Eric had to leave and come back because it's the Clark Kent to Superman shift that's funny um yeah excellent Superman will you show us the the second um code heavy variant yes um is my screen visible right now it is awesome fantastic so um this notebook I think will be shared um by the way and a lot of it's really what Maria covered so I'm going to kind of skip ahead here but what I'm doing basically is setting up in Google collaba U the unstructured dependencies and I'm parsing uh this paper which some of you may be familiar with if you you know read about llms attention is all you need it was kind of one of the more like it's a pretty famous paper talking about some of of this stuff and what's happening is we're parsing unstructured and um I'm I'm not yet building the full pipeline this is just an example of parsing it um but here we get into kind of what Maria was showing which is I build that full end end pipeline which is starting with a partition you know chunking and bedding and then storing into Astra just like she showed um that runs that stores into Astra and just to give you like a little preview of what you get out of that um you can see little pieces of that paper according to the chunking strategy get stored as content and of course every row of this table has an Associated V I think I use open AI in fettings here with my key so um so here's the part I want to show you um given that we have that so what I'm actually doing this is totally outside of the realm of um unstructured unstructured basically orchestrated the step of getting this table set up the way we want it which is really the most important part and now we're trying to build a rag application and this is a super super simple example of course but it shows you how easy it is so um all I'm doing is I'm using langing chain um and I'm importing two uh basically two packages um uh classes from those packages so the astb vector store where we just basically tell it what embeddings are we using open AI um this is the name of my collection and then the parameters that are required for authentication with Astra so really simple line to connect to seeing right here it's just you know this right here um and then all I have page is with one simple call in Python and there by the way there's plenty more you can do other than just a similarity search with score um but what I'm doing is I'm searching for the word German translation because there's some paragraph that talks about um a test that they use to basically test English to German translation and I'm saying just return one result for this particular example um it shows you the similarity score you'll notice that this kind of goes back a little bit to what Maria was talking about which is um just just because we're splitting documents in I should say partitioning documents into particular chunks uh you still have a chunking strategy after to make sure it stays within your context window for the llm that you're using so this is actually like a portion of a paragraph that's why it starts out a mid sentence but what it found was the most similar result to that query it says more paralyzable and then achieves 28.4 Blu on the English to German translation test improving over the existing best result by over two um that should all be um with yeah so that actually came right from the abstract here so that's a portion that got pulled from that document that's a row in astb with a particular vector and the only thing I needed to do to start building um you know a really simple in this case application um is connect to that Vector store that unstructured created for us and stored the data for us and then just print some results that come direct from Lang chain you can use Lang chain you can use l index you can use all kinds of other even custom Solutions if you so choose um as long as you already have you know and this the data is already there that's why it's so simple just one little block of code to start building some excellent um that that's so we've what we've done is we've taken the data from un structured uploaded into Astra and we've we've looked at how you can finish the application um both with a visual loo tool and with um an actual extension of The Notebook um I I want to um pause here and ask the both of you if if you have anything um extra to share any calls to action anything to plug any links any socials um and then and then we'll wrap up as well this is a call to action for the the people here um in the comments there are some questions but we're almost at time so if you want them to have somewhere to to find you and ask questions whether it's a Discord Community or slack or maybe X formerly known as Twitter now is the time yeah I just want to encourage people to try the collab notebooks and we're going to share them as well probably in the description for the videos and in the recordings and if you go to unstructured documentation uh we have an example code tab there and we have a whole bunch of different notebooks that you can try to build rag which is very basic and a little bit more complicated with different sources different types of documents so um I think the best way to learn anything thing is to start building something and I just hope that you have time and and desire to learn and build R rag applications and I hope that our notebooks can be helpful fantastic Eric yeah um I was just going to make a quick men um that that that that that's a spot on and I wanted to say too that from the perspective of someone who's working at data stacks on Astra um I want to say how why un structured like to me is so invaluable and so amazing and um before I worked at data Stacks I was a statistical consultant and I I can tell you that having worked with clients big and small it's constant that they say oh I have some data oh what's that data look like handwritten notes or it's you know scraps it's nothing structured and um of course everything that we do in Astra and everything we do with these rag applications assumes we have these nice tables where we have text that that's been vectorized and chunked Etc and so having that link between data that exists in the real world and data that we need to build the rag applications is not only like important but like fundamental and that's why I think this is so cool absolutely um well listen I know it's um it's it's been a quite a discussion we we covered a lot um and it's it's no small feat to show up live and share your and just walk through your content so um on behalf of everyone in the audience um and myself thank you so much for coming here and and sharing all of this with everybody thank you for those in the comments this discussion in the comments was so valuable I've never actually never had a live stream um where everybody has been so engaged with so many questions thank you Kevin um and Maria and Ilia and Mars and um all Sean Spyros David all the people who posed such great questions thank you thank you thank you",
    "segments": [
      {
        "start": 0.199,
        "duration": 4.6,
        "text": "share today how you can get your data AI"
      },
      {
        "start": 2.84,
        "duration": 4.28,
        "text": "ready with unstructured and data stacks"
      },
      {
        "start": 4.799,
        "duration": 3.681,
        "text": "and today we're joined by Maria kosova"
      },
      {
        "start": 7.12,
        "duration": 3.28,
        "text": "from unstructured staff developer"
      },
      {
        "start": 8.48,
        "duration": 4.48,
        "text": "Advocate and Eric hair my colleague at"
      },
      {
        "start": 10.4,
        "duration": 2.56,
        "text": "data Stacks"
      },
      {
        "start": 13.36,
        "duration": 4.999,
        "text": "welcome thank you great well it's great"
      },
      {
        "start": 16.64,
        "duration": 3.76,
        "text": "to to see you all um we were just"
      },
      {
        "start": 18.359,
        "duration": 3.721,
        "text": "talking before the show began and let's"
      },
      {
        "start": 20.4,
        "duration": 3.799,
        "text": "maybe start really high level with some"
      },
      {
        "start": 22.08,
        "duration": 4.72,
        "text": "introductions and then let's I want to"
      },
      {
        "start": 24.199,
        "duration": 4.48,
        "text": "dive straight into what it even means"
      },
      {
        "start": 26.8,
        "duration": 3.399,
        "text": "for data to be AI ready so maybe we can"
      },
      {
        "start": 28.679,
        "duration": 2.92,
        "text": "start with um Maria can you tell us a"
      },
      {
        "start": 30.199,
        "duration": 2.601,
        "text": "little bit about yourself the"
      },
      {
        "start": 31.599,
        "duration": 2.241,
        "text": "unstructured and the work that you're"
      },
      {
        "start": 32.8,
        "duration": 4.599,
        "text": "doing"
      },
      {
        "start": 33.84,
        "duration": 6.28,
        "text": "there absolutely my name is Maria HOSA"
      },
      {
        "start": 37.399,
        "duration": 5.081,
        "text": "and I'm a Staff developer Advocate at"
      },
      {
        "start": 40.12,
        "duration": 4.4,
        "text": "unstructured at unstructured we fuss"
      },
      {
        "start": 42.48,
        "duration": 5.12,
        "text": "over data so unstructured data"
      },
      {
        "start": 44.52,
        "duration": 6.64,
        "text": "specifically we pre-process data for all"
      },
      {
        "start": 47.6,
        "duration": 5.799,
        "text": "kinds of AI applications mostly uh rag"
      },
      {
        "start": 51.16,
        "duration": 4.919,
        "text": "applications and there are a lot of"
      },
      {
        "start": 53.399,
        "duration": 4.84,
        "text": "challenges associated with that and we"
      },
      {
        "start": 56.079,
        "duration": 4.64,
        "text": "help solve those"
      },
      {
        "start": 58.239,
        "duration": 4.48,
        "text": "challenges fantastic um and I can't wait"
      },
      {
        "start": 60.719,
        "duration": 3.961,
        "text": "to dive into what what that means in in"
      },
      {
        "start": 62.719,
        "duration": 4.121,
        "text": "a little bit more detail um but before"
      },
      {
        "start": 64.68,
        "duration": 3.72,
        "text": "that of course Eric um would you um give"
      },
      {
        "start": 66.84,
        "duration": 4.4,
        "text": "us a short introduction to yourself and"
      },
      {
        "start": 68.4,
        "duration": 2.84,
        "text": "the work we're doing in data"
      },
      {
        "start": 71.72,
        "duration": 4.24,
        "text": "Stacks yeah um hi everyone really"
      },
      {
        "start": 74.119,
        "duration": 4.32,
        "text": "excited to be here so I'm a software"
      },
      {
        "start": 75.96,
        "duration": 4.76,
        "text": "engineer at data stacks and um one of"
      },
      {
        "start": 78.439,
        "duration": 5.601,
        "text": "the primary focuses that I've had is on"
      },
      {
        "start": 80.72,
        "duration": 6.359,
        "text": "building Integrations between Astro DB"
      },
      {
        "start": 84.04,
        "duration": 6.039,
        "text": "our our database offering and platforms"
      },
      {
        "start": 87.079,
        "duration": 4.961,
        "text": "like unstructured so um making it really"
      },
      {
        "start": 90.079,
        "duration": 4.241,
        "text": "easy basically to leverage unstructured"
      },
      {
        "start": 92.04,
        "duration": 5.24,
        "text": "when you're building rag applications"
      },
      {
        "start": 94.32,
        "duration": 4.479,
        "text": "that Target you know a vector database"
      },
      {
        "start": 97.28,
        "duration": 4.28,
        "text": "like"
      },
      {
        "start": 98.799,
        "duration": 4.32,
        "text": "Astra fantastic thanks I'm so excited to"
      },
      {
        "start": 101.56,
        "duration": 4.12,
        "text": "dive into all of this now one unifying"
      },
      {
        "start": 103.119,
        "duration": 4.881,
        "text": "thing that both of you mentioned was rag"
      },
      {
        "start": 105.68,
        "duration": 5.28,
        "text": "applications and when I mentioned AI"
      },
      {
        "start": 108.0,
        "duration": 4.799,
        "text": "Readiness um Maria you said unstructured"
      },
      {
        "start": 110.96,
        "duration": 3.439,
        "text": "helps with this particularly also with"
      },
      {
        "start": 112.799,
        "duration": 4.401,
        "text": "building rag applications because"
      },
      {
        "start": 114.399,
        "duration": 5.0,
        "text": "there's some complexity there um when we"
      },
      {
        "start": 117.2,
        "duration": 4.12,
        "text": "talk about rag at events um I just got"
      },
      {
        "start": 119.399,
        "duration": 3.32,
        "text": "back from a conference yesterday and you"
      },
      {
        "start": 121.32,
        "duration": 4.0,
        "text": "know we did this thing from the stage"
      },
      {
        "start": 122.719,
        "duration": 4.961,
        "text": "where we asked the attendees um how many"
      },
      {
        "start": 125.32,
        "duration": 5.919,
        "text": "of you have heard of rag in in an"
      },
      {
        "start": 127.68,
        "duration": 5.16,
        "text": "audience of 3,000 two hands went up and"
      },
      {
        "start": 131.239,
        "duration": 3.521,
        "text": "so I think it's very important that we"
      },
      {
        "start": 132.84,
        "duration": 4.479,
        "text": "that we actually take a few minutes"
      },
      {
        "start": 134.76,
        "duration": 4.28,
        "text": "maybe not too many but just Define rag"
      },
      {
        "start": 137.319,
        "duration": 3.64,
        "text": "for the um listeners here and then we"
      },
      {
        "start": 139.04,
        "duration": 3.32,
        "text": "can dive into specifically unstructured"
      },
      {
        "start": 140.959,
        "duration": 3.601,
        "text": "role there Maria would you help us um"
      },
      {
        "start": 142.36,
        "duration": 5.72,
        "text": "just by explaining rag in"
      },
      {
        "start": 144.56,
        "duration": 6.08,
        "text": "detail of course so rag stands for"
      },
      {
        "start": 148.08,
        "duration": 5.439,
        "text": "retrieval augmented generation"
      },
      {
        "start": 150.64,
        "duration": 5.319,
        "text": "and it sounds complex and sounds very"
      },
      {
        "start": 153.519,
        "duration": 5.321,
        "text": "sciency but it's really a very simple"
      },
      {
        "start": 155.959,
        "duration": 5.64,
        "text": "idea uh that you give some context to"
      },
      {
        "start": 158.84,
        "duration": 6.119,
        "text": "your llm so llm does the generative part"
      },
      {
        "start": 161.599,
        "duration": 5.72,
        "text": "it generates text and the issue with the"
      },
      {
        "start": 164.959,
        "duration": 5.481,
        "text": "off-the-shelf llms is that they have"
      },
      {
        "start": 167.319,
        "duration": 5.881,
        "text": "been trained on massive amounts of data"
      },
      {
        "start": 170.44,
        "duration": 5.799,
        "text": "from the internet but they have never"
      },
      {
        "start": 173.2,
        "duration": 5.119,
        "text": "seen your personal data or your"
      },
      {
        "start": 176.239,
        "duration": 5.561,
        "text": "company's data about your internal"
      },
      {
        "start": 178.319,
        "duration": 5.961,
        "text": "policies or customer or any kind of"
      },
      {
        "start": 181.8,
        "duration": 4.48,
        "text": "proprietary data they have never been"
      },
      {
        "start": 184.28,
        "duration": 7.0,
        "text": "exposed to that kind of data and they're"
      },
      {
        "start": 186.28,
        "duration": 7.039,
        "text": "also Limited in in um uh in time because"
      },
      {
        "start": 191.28,
        "duration": 5.08,
        "text": "they have a knowledge cut off date so"
      },
      {
        "start": 193.319,
        "duration": 4.64,
        "text": "past certain date they don't know about"
      },
      {
        "start": 196.36,
        "duration": 6.64,
        "text": "any of the events that happened after"
      },
      {
        "start": 197.959,
        "duration": 8.721,
        "text": "that and rag is a way to give additional"
      },
      {
        "start": 203.0,
        "duration": 6.36,
        "text": "context to an llm uh to help supplement"
      },
      {
        "start": 206.68,
        "duration": 4.759,
        "text": "some information and to help the llm to"
      },
      {
        "start": 209.36,
        "duration": 4.48,
        "text": "generate a useful response to whatever"
      },
      {
        "start": 211.439,
        "duration": 4.601,
        "text": "user query can"
      },
      {
        "start": 213.84,
        "duration": 3.72,
        "text": "be fantastic thank you for that yeah"
      },
      {
        "start": 216.04,
        "duration": 2.68,
        "text": "that's so helpful um for the vast"
      },
      {
        "start": 217.56,
        "duration": 3.52,
        "text": "majority people because even here in the"
      },
      {
        "start": 218.72,
        "duration": 3.599,
        "text": "comments Kevin U mentions he says rag"
      },
      {
        "start": 221.08,
        "duration": 2.799,
        "text": "that's what I use to clean my kitchen"
      },
      {
        "start": 222.319,
        "duration": 2.961,
        "text": "counter indeed we're not talking about a"
      },
      {
        "start": 223.879,
        "duration": 3.241,
        "text": "cloth here we're talking about a"
      },
      {
        "start": 225.28,
        "duration": 4.599,
        "text": "technique um to bring real-time context"
      },
      {
        "start": 227.12,
        "duration": 4.08,
        "text": "into larger language models and also um"
      },
      {
        "start": 229.879,
        "duration": 3.521,
        "text": "correct me if I'm wrong here but the"
      },
      {
        "start": 231.2,
        "duration": 3.84,
        "text": "right context right because like um a"
      },
      {
        "start": 233.4,
        "duration": 3.8,
        "text": "number of folks have reported with"
      },
      {
        "start": 235.04,
        "duration": 3.96,
        "text": "Google's AI overview feature they they"
      },
      {
        "start": 237.2,
        "duration": 3.64,
        "text": "ask questions um for example how many"
      },
      {
        "start": 239.0,
        "duration": 3.519,
        "text": "rocks should want each per day and the"
      },
      {
        "start": 240.84,
        "duration": 3.36,
        "text": "answer is hallucinated with three or"
      },
      {
        "start": 242.519,
        "duration": 4.0,
        "text": "four or something like this but that's"
      },
      {
        "start": 244.2,
        "duration": 5.36,
        "text": "because the training data often includes"
      },
      {
        "start": 246.519,
        "duration": 4.841,
        "text": "like jokes and so the the right answer"
      },
      {
        "start": 249.56,
        "duration": 3.84,
        "text": "isn't often given because of this and"
      },
      {
        "start": 251.36,
        "duration": 3.239,
        "text": "rag can help with that um Eric is that"
      },
      {
        "start": 253.4,
        "duration": 5.0,
        "text": "accurate at"
      },
      {
        "start": 254.599,
        "duration": 6.92,
        "text": "all yeah absolutely um you know it it it"
      },
      {
        "start": 258.4,
        "duration": 6.16,
        "text": "becomes kind of the case that that rag"
      },
      {
        "start": 261.519,
        "duration": 5.161,
        "text": "is a way of like um augmenting the"
      },
      {
        "start": 264.56,
        "duration": 5.32,
        "text": "information that"
      },
      {
        "start": 266.68,
        "duration": 4.519,
        "text": "is out there with the information that"
      },
      {
        "start": 269.88,
        "duration": 3.72,
        "text": "sometimes you need to pull in from"
      },
      {
        "start": 271.199,
        "duration": 5.801,
        "text": "sources that that might not be indexed"
      },
      {
        "start": 273.6,
        "duration": 6.56,
        "text": "in llms by default for various reasons"
      },
      {
        "start": 277.0,
        "duration": 5.28,
        "text": "uh you know proprietary um brand new"
      },
      {
        "start": 280.16,
        "duration": 4.319,
        "text": "information and so combining those two"
      },
      {
        "start": 282.28,
        "duration": 5.0,
        "text": "concepts uh you can really like build"
      },
      {
        "start": 284.479,
        "duration": 5.801,
        "text": "something pretty cool with"
      },
      {
        "start": 287.28,
        "duration": 5.24,
        "text": "it excellent I'm excited to find out um"
      },
      {
        "start": 290.28,
        "duration": 3.6,
        "text": "what exactly we're going to do um I"
      },
      {
        "start": 292.52,
        "duration": 2.64,
        "text": "understand we have some demos and some"
      },
      {
        "start": 293.88,
        "duration": 3.68,
        "text": "really great things to share with the"
      },
      {
        "start": 295.16,
        "duration": 4.68,
        "text": "attendees here um before we get into it"
      },
      {
        "start": 297.56,
        "duration": 4.72,
        "text": "I want to be very clear that um this is"
      },
      {
        "start": 299.84,
        "duration": 4.079,
        "text": "very much not just a trialogue as you"
      },
      {
        "start": 302.28,
        "duration": 3.72,
        "text": "may be watching on the live stream but"
      },
      {
        "start": 303.919,
        "duration": 3.321,
        "text": "um for those leaving comments we read"
      },
      {
        "start": 306.0,
        "duration": 2.68,
        "text": "all of them and we're more than happy to"
      },
      {
        "start": 307.24,
        "duration": 3.92,
        "text": "take your questions if and when you have"
      },
      {
        "start": 308.68,
        "duration": 4.48,
        "text": "any um with that Maria let's dive into"
      },
      {
        "start": 311.16,
        "duration": 4.84,
        "text": "unstructured you mentioned unstructured"
      },
      {
        "start": 313.16,
        "duration": 4.64,
        "text": "helps with AI Readiness um specifically"
      },
      {
        "start": 316.0,
        "duration": 4.28,
        "text": "with rag applications what is"
      },
      {
        "start": 317.8,
        "duration": 5.239,
        "text": "unstructured and how does it do"
      },
      {
        "start": 320.28,
        "duration": 6.0,
        "text": "that I have a short answer and a long"
      },
      {
        "start": 323.039,
        "duration": 5.121,
        "text": "answer to that so the short answer is um"
      },
      {
        "start": 326.28,
        "duration": 4.4,
        "text": "that unstructured is a startup that has"
      },
      {
        "start": 328.16,
        "duration": 5.8,
        "text": "built tools to make"
      },
      {
        "start": 330.68,
        "duration": 6.12,
        "text": "um the most frustrating step of building"
      },
      {
        "start": 333.96,
        "duration": 4.88,
        "text": "R applications easier and that step is"
      },
      {
        "start": 336.8,
        "duration": 4.959,
        "text": "pre-processing unstructured data and"
      },
      {
        "start": 338.84,
        "duration": 4.639,
        "text": "making it rag ready and the longer"
      },
      {
        "start": 341.759,
        "duration": 4.121,
        "text": "answer"
      },
      {
        "start": 343.479,
        "duration": 5.361,
        "text": "is for the for the longer answer it's"
      },
      {
        "start": 345.88,
        "duration": 5.8,
        "text": "important to understand that you really"
      },
      {
        "start": 348.84,
        "duration": 6.359,
        "text": "do need to have good data to have a"
      },
      {
        "start": 351.68,
        "duration": 6.6,
        "text": "useful rag applications because um data"
      },
      {
        "start": 355.199,
        "duration": 5.881,
        "text": "fuels AI data fuels rag applications or"
      },
      {
        "start": 358.28,
        "duration": 5.84,
        "text": "even fine tuning uh use cases and the"
      },
      {
        "start": 361.08,
        "duration": 5.959,
        "text": "most abandoned type of data in any"
      },
      {
        "start": 364.12,
        "duration": 7.0,
        "text": "organization any company any business is"
      },
      {
        "start": 367.039,
        "duration": 7.041,
        "text": "unstructured up to 80% of data created"
      },
      {
        "start": 371.12,
        "duration": 5.84,
        "text": "on daily basis is unstructured so these"
      },
      {
        "start": 374.08,
        "duration": 6.559,
        "text": "are PDFs Word documents PowerPoint"
      },
      {
        "start": 376.96,
        "duration": 7.76,
        "text": "presentations markdown HTML so on and so"
      },
      {
        "start": 380.639,
        "duration": 6.4,
        "text": "forth so if you think about um contract"
      },
      {
        "start": 384.72,
        "duration": 4.64,
        "text": "policies legal document marketing"
      },
      {
        "start": 387.039,
        "duration": 5.321,
        "text": "documents field reports and so on and so"
      },
      {
        "start": 389.36,
        "duration": 4.6,
        "text": "forth all of these are uh stored in"
      },
      {
        "start": 392.36,
        "duration": 5.839,
        "text": "unstructured"
      },
      {
        "start": 393.96,
        "duration": 7.72,
        "text": "formats they encapsulate a lot of"
      },
      {
        "start": 398.199,
        "duration": 5.361,
        "text": "important information about uh the"
      },
      {
        "start": 401.68,
        "duration": 4.959,
        "text": "business itself the customers the"
      },
      {
        "start": 403.56,
        "duration": 6.359,
        "text": "products the kind of insights that are"
      },
      {
        "start": 406.639,
        "duration": 7.241,
        "text": "often not available in structured d uh"
      },
      {
        "start": 409.919,
        "duration": 7.801,
        "text": "structured data and so that's why you"
      },
      {
        "start": 413.88,
        "duration": 6.159,
        "text": "often want to leverage those documents"
      },
      {
        "start": 417.72,
        "duration": 4.12,
        "text": "but it's not easy"
      },
      {
        "start": 420.039,
        "duration": 3.681,
        "text": "to use them because they're not very"
      },
      {
        "start": 421.84,
        "duration": 6.12,
        "text": "usable in their raw format so you have"
      },
      {
        "start": 423.72,
        "duration": 7.8,
        "text": "to pre-process them first and that is"
      },
      {
        "start": 427.96,
        "duration": 5.519,
        "text": "complex because a you have to first"
      },
      {
        "start": 431.52,
        "duration": 4.399,
        "text": "aggregate them from a whole bunch of"
      },
      {
        "start": 433.479,
        "duration": 5.241,
        "text": "different silos that they are scattered"
      },
      {
        "start": 435.919,
        "duration": 6.96,
        "text": "across you have to figure out how to"
      },
      {
        "start": 438.72,
        "duration": 6.4,
        "text": "parse all these different Native formats"
      },
      {
        "start": 442.879,
        "duration": 5.32,
        "text": "uh in certain cases like with PDFs"
      },
      {
        "start": 445.12,
        "duration": 6.519,
        "text": "images PowerPoints you will have to"
      },
      {
        "start": 448.199,
        "duration": 5.961,
        "text": "figure out how to use uh possibly OCR"
      },
      {
        "start": 451.639,
        "duration": 5.761,
        "text": "models or document understanding models"
      },
      {
        "start": 454.16,
        "duration": 5.759,
        "text": "or VMS to extract content content from"
      },
      {
        "start": 457.4,
        "duration": 5.12,
        "text": "those documents um and then of course"
      },
      {
        "start": 459.919,
        "duration": 5.161,
        "text": "you will you need to understand how you"
      },
      {
        "start": 462.52,
        "duration": 6.079,
        "text": "want your data model to look like so"
      },
      {
        "start": 465.08,
        "duration": 5.44,
        "text": "this is uniform across different sources"
      },
      {
        "start": 468.599,
        "duration": 4.72,
        "text": "different Native"
      },
      {
        "start": 470.52,
        "duration": 4.399,
        "text": "formats and then of course you need to"
      },
      {
        "start": 473.319,
        "duration": 3.201,
        "text": "um figure out where you're going to"
      },
      {
        "start": 474.919,
        "duration": 4.12,
        "text": "store them how you going to upload to"
      },
      {
        "start": 476.52,
        "duration": 5.959,
        "text": "your destination and so this is what"
      },
      {
        "start": 479.039,
        "duration": 6.921,
        "text": "unru structured takes care of all the"
      },
      {
        "start": 482.479,
        "duration": 6.12,
        "text": "steps from the ingestion up to uh"
      },
      {
        "start": 485.96,
        "duration": 5.76,
        "text": "uploading the rag ready results into a"
      },
      {
        "start": 488.599,
        "duration": 5.921,
        "text": "vector store so we don't orchestrate rag"
      },
      {
        "start": 491.72,
        "duration": 6.28,
        "text": "or we don't prescribe what llm you"
      },
      {
        "start": 494.52,
        "duration": 6.639,
        "text": "should be using unstructured focuses on"
      },
      {
        "start": 498.0,
        "duration": 6.759,
        "text": "one and one thing only is making your"
      },
      {
        "start": 501.159,
        "duration": 6.561,
        "text": "raw unstructured data usable and ready"
      },
      {
        "start": 504.759,
        "duration": 5.681,
        "text": "for whatever rag application you want to"
      },
      {
        "start": 507.72,
        "duration": 4.52,
        "text": "build fantastic thank you so much so if"
      },
      {
        "start": 510.44,
        "duration": 3.839,
        "text": "if we understand that together and and"
      },
      {
        "start": 512.24,
        "duration": 4.44,
        "text": "by we I mean all the people um in the"
      },
      {
        "start": 514.279,
        "duration": 7.12,
        "text": "comment section as well unstructured can"
      },
      {
        "start": 516.68,
        "duration": 6.919,
        "text": "help get data become rag ready um by"
      },
      {
        "start": 521.399,
        "duration": 5.201,
        "text": "essentially converting it from whatever"
      },
      {
        "start": 523.599,
        "duration": 5.161,
        "text": "format the data is originally in into an"
      },
      {
        "start": 526.6,
        "duration": 3.64,
        "text": "alternate format um and this alternate"
      },
      {
        "start": 528.76,
        "duration": 3.8,
        "text": "format can then be used for rag"
      },
      {
        "start": 530.24,
        "duration": 5.279,
        "text": "workflows is this alternate"
      },
      {
        "start": 532.56,
        "duration": 7.32,
        "text": "format text um because like if if we"
      },
      {
        "start": 535.519,
        "duration": 5.961,
        "text": "consider rag right rag is dependent on"
      },
      {
        "start": 539.88,
        "duration": 3.24,
        "text": "prompt engineering meaning you get"
      },
      {
        "start": 541.48,
        "duration": 5.2,
        "text": "context you injected into your prompt"
      },
      {
        "start": 543.12,
        "duration": 5.8,
        "text": "and then you ask your question um is the"
      },
      {
        "start": 546.68,
        "duration": 5.88,
        "text": "final destination of"
      },
      {
        "start": 548.92,
        "duration": 5.56,
        "text": "unstructured processing text or is is"
      },
      {
        "start": 552.56,
        "duration": 3.56,
        "text": "there some intermediary layer that then"
      },
      {
        "start": 554.48,
        "duration": 3.039,
        "text": "an application would take forward for a"
      },
      {
        "start": 556.12,
        "duration": 5.399,
        "text": "rag use"
      },
      {
        "start": 557.519,
        "duration": 7.601,
        "text": "case so you're correct that when you"
      },
      {
        "start": 561.519,
        "duration": 8.521,
        "text": "pass a document to an llm as context you"
      },
      {
        "start": 565.12,
        "duration": 8.48,
        "text": "pass text however when we ex uh process"
      },
      {
        "start": 570.04,
        "duration": 7.4,
        "text": "unstructured documents we return Json"
      },
      {
        "start": 573.6,
        "duration": 6.679,
        "text": "because we want to add metadata to that"
      },
      {
        "start": 577.44,
        "duration": 5.92,
        "text": "text as well documents don't exist in"
      },
      {
        "start": 580.279,
        "duration": 5.68,
        "text": "vacuum they can refer to they they can"
      },
      {
        "start": 583.36,
        "duration": 4.08,
        "text": "describe different products they can uh"
      },
      {
        "start": 585.959,
        "duration": 4.281,
        "text": "you know you there are different file"
      },
      {
        "start": 587.44,
        "duration": 5.28,
        "text": "types different sources different"
      },
      {
        "start": 590.24,
        "duration": 5.32,
        "text": "languages and you want to have as much"
      },
      {
        "start": 592.72,
        "duration": 7.04,
        "text": "metadata as possible"
      },
      {
        "start": 595.56,
        "duration": 7.12,
        "text": "uh with your um results so that you can"
      },
      {
        "start": 599.76,
        "duration": 5.199,
        "text": "apply maybe metadata pre-filtering or"
      },
      {
        "start": 602.68,
        "duration": 4.76,
        "text": "maybe you want to curate your data and"
      },
      {
        "start": 604.959,
        "duration": 5.721,
        "text": "you know drop some parts of it before"
      },
      {
        "start": 607.44,
        "duration": 5.68,
        "text": "you use it um so for that reason it's"
      },
      {
        "start": 610.68,
        "duration": 5.64,
        "text": "not just the text that we extract but we"
      },
      {
        "start": 613.12,
        "duration": 5.08,
        "text": "also try to get get as much metadata as"
      },
      {
        "start": 616.32,
        "duration": 5.079,
        "text": "we possibly can from all of those"
      },
      {
        "start": 618.2,
        "duration": 6.079,
        "text": "documents and so that results in"
      },
      {
        "start": 621.399,
        "duration": 6.321,
        "text": "adjacent document that we then load into"
      },
      {
        "start": 624.279,
        "duration": 6.441,
        "text": "uh a vecture store incredible okay so"
      },
      {
        "start": 627.72,
        "duration": 5.44,
        "text": "one would even could perhaps argue that"
      },
      {
        "start": 630.72,
        "duration": 5.679,
        "text": "unstructured processing step helps take"
      },
      {
        "start": 633.16,
        "duration": 5.2,
        "text": "data from unstructured formats and"
      },
      {
        "start": 636.399,
        "duration": 3.401,
        "text": "prepare them to even be structured"
      },
      {
        "start": 638.36,
        "duration": 3.56,
        "text": "because if you're getting Json you can"
      },
      {
        "start": 639.8,
        "duration": 4.32,
        "text": "then group um certain Json properties"
      },
      {
        "start": 641.92,
        "duration": 3.56,
        "text": "together form even model relationships"
      },
      {
        "start": 644.12,
        "duration": 4.6,
        "text": "with them perhaps build a Knowledge"
      },
      {
        "start": 645.48,
        "duration": 5.64,
        "text": "Graph and store that somewhere as well"
      },
      {
        "start": 648.72,
        "duration": 4.48,
        "text": "is is that close to something possible"
      },
      {
        "start": 651.12,
        "duration": 4.2,
        "text": "yeah you can absolutely do that yeah"
      },
      {
        "start": 653.2,
        "duration": 4.68,
        "text": "awesome that's so great okay so now we"
      },
      {
        "start": 655.32,
        "duration": 4.04,
        "text": "start to understand um why the"
      },
      {
        "start": 657.88,
        "duration": 3.88,
        "text": "partnership between data stacks"
      },
      {
        "start": 659.36,
        "duration": 4.32,
        "text": "unstructured is so synergistic because"
      },
      {
        "start": 661.76,
        "duration": 4.759,
        "text": "unstructured is the pre-processing step"
      },
      {
        "start": 663.68,
        "duration": 5.32,
        "text": "where the next obvious destination is a"
      },
      {
        "start": 666.519,
        "duration": 3.961,
        "text": "vector store um where you store the"
      },
      {
        "start": 669.0,
        "duration": 4.0,
        "text": "unstructured data that you can then use"
      },
      {
        "start": 670.48,
        "duration": 4.32,
        "text": "for similarity search um we have some"
      },
      {
        "start": 673.0,
        "duration": 4.0,
        "text": "questions in the comments but since"
      },
      {
        "start": 674.8,
        "duration": 3.8,
        "text": "we're following the pipeline for rag"
      },
      {
        "start": 677.0,
        "duration": 3.519,
        "text": "applications all the way from the"
      },
      {
        "start": 678.6,
        "duration": 3.239,
        "text": "unstructured beginning let's talk a"
      },
      {
        "start": 680.519,
        "duration": 3.56,
        "text": "little bit about the sync or the"
      },
      {
        "start": 681.839,
        "duration": 3.68,
        "text": "destination of that process data which"
      },
      {
        "start": 684.079,
        "duration": 5.56,
        "text": "is the vector store Eric would you be"
      },
      {
        "start": 685.519,
        "duration": 4.12,
        "text": "willing to um expand there"
      },
      {
        "start": 691.279,
        "duration": 2.521,
        "text": "Eric you're"
      },
      {
        "start": 694.36,
        "duration": 4.599,
        "text": "muted sorry about that yeah definitely"
      },
      {
        "start": 697.36,
        "duration": 5.56,
        "text": "willing to"
      },
      {
        "start": 698.959,
        "duration": 7.12,
        "text": "so basically um unstructured has this"
      },
      {
        "start": 702.92,
        "duration": 5.24,
        "text": "incredibly powerful framework what of"
      },
      {
        "start": 706.079,
        "duration": 3.841,
        "text": "what they call Destination connectors"
      },
      {
        "start": 708.16,
        "duration": 4.04,
        "text": "they actually have Source connectors too"
      },
      {
        "start": 709.92,
        "duration": 4.719,
        "text": "and Maria if I say anything incorrect"
      },
      {
        "start": 712.2,
        "duration": 5.04,
        "text": "here just interrupt me but um but"
      },
      {
        "start": 714.639,
        "duration": 5.401,
        "text": "basically the power of the vector store"
      },
      {
        "start": 717.24,
        "duration": 5.68,
        "text": "component is that you we already built"
      },
      {
        "start": 720.04,
        "duration": 5.32,
        "text": "this destination connector that supports"
      },
      {
        "start": 722.92,
        "duration": 4.719,
        "text": "Astra and what that means is essentially"
      },
      {
        "start": 725.36,
        "duration": 4.719,
        "text": "there's one really simple script to"
      },
      {
        "start": 727.639,
        "duration": 4.521,
        "text": "Define your pipeline which essentially"
      },
      {
        "start": 730.079,
        "duration": 4.521,
        "text": "that pipeline is going from your"
      },
      {
        "start": 732.16,
        "duration": 4.44,
        "text": "unstructured document which as Maria"
      },
      {
        "start": 734.6,
        "duration": 4.679,
        "text": "talked about could be all kinds of"
      },
      {
        "start": 736.6,
        "duration": 4.32,
        "text": "formats PowerPoints Etc and going"
      },
      {
        "start": 739.279,
        "duration": 4.881,
        "text": "through all the stages needing to make"
      },
      {
        "start": 740.92,
        "duration": 5.56,
        "text": "it AI ready and rag ready and then"
      },
      {
        "start": 744.16,
        "duration": 5.039,
        "text": "immediately getting back a collection in"
      },
      {
        "start": 746.48,
        "duration": 5.08,
        "text": "astb where the text from that document"
      },
      {
        "start": 749.199,
        "duration": 4.481,
        "text": "and other components of that document"
      },
      {
        "start": 751.56,
        "duration": 4.36,
        "text": "have their vectors automatically"
      },
      {
        "start": 753.68,
        "duration": 4.64,
        "text": "computed so you can essentially start"
      },
      {
        "start": 755.92,
        "duration": 4.599,
        "text": "quering and doing similarity surge you"
      },
      {
        "start": 758.32,
        "duration": 4.92,
        "text": "know with with really minimal effort"
      },
      {
        "start": 760.519,
        "duration": 4.801,
        "text": "it's mostly just getting API keys in"
      },
      {
        "start": 763.24,
        "duration": 4.32,
        "text": "order and it's it's pretty powerful what"
      },
      {
        "start": 765.32,
        "duration": 2.24,
        "text": "they"
      },
      {
        "start": 769.399,
        "duration": 5.88,
        "text": "built that's incredible um and so astrab"
      },
      {
        "start": 773.44,
        "duration": 3.639,
        "text": "being this destination connector would"
      },
      {
        "start": 775.279,
        "duration": 3.521,
        "text": "then take that forward and be used to"
      },
      {
        "start": 777.079,
        "duration": 2.76,
        "text": "build a rag flow how exactly would a"
      },
      {
        "start": 778.8,
        "duration": 2.96,
        "text": "develop"
      },
      {
        "start": 779.839,
        "duration": 4.401,
        "text": "um then take the output of unstructured"
      },
      {
        "start": 781.76,
        "duration": 4.639,
        "text": "and build a rag application yeah so"
      },
      {
        "start": 784.24,
        "duration": 5.76,
        "text": "there there's a lot of options here and"
      },
      {
        "start": 786.399,
        "duration": 5.68,
        "text": "um some some that might more low code"
      },
      {
        "start": 790.0,
        "duration": 5.44,
        "text": "and some that are more for the season"
      },
      {
        "start": 792.079,
        "duration": 4.88,
        "text": "developer but um one option is if you're"
      },
      {
        "start": 795.44,
        "duration": 3.8,
        "text": "either familiar or willing to get your"
      },
      {
        "start": 796.959,
        "duration": 6.281,
        "text": "feet wet with some of the more common"
      },
      {
        "start": 799.24,
        "duration": 7.92,
        "text": "Frameworks for um llms and AI Lang chain"
      },
      {
        "start": 803.24,
        "duration": 6.08,
        "text": "llama index Etc um basically once you've"
      },
      {
        "start": 807.16,
        "duration": 4.44,
        "text": "done the step of storing your"
      },
      {
        "start": 809.32,
        "duration": 6.04,
        "text": "unstructured data into your asra DB"
      },
      {
        "start": 811.6,
        "duration": 7.12,
        "text": "Vector store um it's again almost like a"
      },
      {
        "start": 815.36,
        "duration": 5.4,
        "text": "oneline connect to your as DB collection"
      },
      {
        "start": 818.72,
        "duration": 4.4,
        "text": "and start doing similarity searches and"
      },
      {
        "start": 820.76,
        "duration": 4.639,
        "text": "start building your rag pipeline so you"
      },
      {
        "start": 823.12,
        "duration": 4.68,
        "text": "can imagine pretty much the whole Suite"
      },
      {
        "start": 825.399,
        "duration": 6.24,
        "text": "of functionality that's in Lang chain"
      },
      {
        "start": 827.8,
        "duration": 6.8,
        "text": "for example is now at your fingertips"
      },
      {
        "start": 831.639,
        "duration": 4.64,
        "text": "and um you know you began simply from a"
      },
      {
        "start": 834.6,
        "duration": 3.96,
        "text": "PowerPoint or from a collection of"
      },
      {
        "start": 836.279,
        "duration": 4.601,
        "text": "PowerPoints in order to start building"
      },
      {
        "start": 838.56,
        "duration": 3.959,
        "text": "that"
      },
      {
        "start": 840.88,
        "duration": 2.879,
        "text": "fantastic I can't wait to see all of"
      },
      {
        "start": 842.519,
        "duration": 4.041,
        "text": "this in practice I think we have some"
      },
      {
        "start": 843.759,
        "duration": 5.241,
        "text": "stuff prepared for that um one question"
      },
      {
        "start": 846.56,
        "duration": 4.56,
        "text": "before we go further from the demo is"
      },
      {
        "start": 849.0,
        "duration": 4.32,
        "text": "for Maria somebody asked here in the"
      },
      {
        "start": 851.12,
        "duration": 4.079,
        "text": "audience Jimmy Burns is asking if PDF"
      },
      {
        "start": 853.32,
        "duration": 4.36,
        "text": "and spreadsheets are also transformed"
      },
      {
        "start": 855.199,
        "duration": 4.08,
        "text": "into these he mentioned text in the"
      },
      {
        "start": 857.68,
        "duration": 3.519,
        "text": "comments but we already established it's"
      },
      {
        "start": 859.279,
        "duration": 3.8,
        "text": "a Json um is that accurate so"
      },
      {
        "start": 861.199,
        "duration": 3.801,
        "text": "unstructured can take PDFs spreadsheets"
      },
      {
        "start": 863.079,
        "duration": 3.76,
        "text": "and other formats and transform them"
      },
      {
        "start": 865.0,
        "duration": 5.12,
        "text": "into something rag"
      },
      {
        "start": 866.839,
        "duration": 5.641,
        "text": "ready yes so the way we do it we support"
      },
      {
        "start": 870.12,
        "duration": 6.279,
        "text": "25 plus different file types that"
      },
      {
        "start": 872.48,
        "duration": 9.88,
        "text": "includes PDFs and uh Excel and HTML"
      },
      {
        "start": 876.399,
        "duration": 7.921,
        "text": "markdown many more what we do is we uh"
      },
      {
        "start": 882.36,
        "duration": 4.399,
        "text": "take the document in and then we"
      },
      {
        "start": 884.32,
        "duration": 4.48,
        "text": "partition it and we split it into"
      },
      {
        "start": 886.759,
        "duration": 4.601,
        "text": "individual document elements we call"
      },
      {
        "start": 888.8,
        "duration": 5.599,
        "text": "them document elements so document"
      },
      {
        "start": 891.36,
        "duration": 5.039,
        "text": "elements this can be a title a paragraph"
      },
      {
        "start": 894.399,
        "duration": 5.521,
        "text": "which we call narrative text it can be a"
      },
      {
        "start": 896.399,
        "duration": 6.921,
        "text": "table an image a list item footer header"
      },
      {
        "start": 899.92,
        "duration": 5.839,
        "text": "all those different categories of um"
      },
      {
        "start": 903.32,
        "duration": 6.48,
        "text": "logical elements of a"
      },
      {
        "start": 905.759,
        "duration": 6.921,
        "text": "document uh and this is helpful when we"
      },
      {
        "start": 909.8,
        "duration": 4.88,
        "text": "further transform those uh documents and"
      },
      {
        "start": 912.68,
        "duration": 4.36,
        "text": "chunk them and embed them it is"
      },
      {
        "start": 914.68,
        "duration": 4.399,
        "text": "specifically helpful for chunking"
      },
      {
        "start": 917.04,
        "duration": 3.159,
        "text": "because we understand this way the"
      },
      {
        "start": 919.079,
        "duration": 2.841,
        "text": "document"
      },
      {
        "start": 920.199,
        "duration": 5.56,
        "text": "structure"
      },
      {
        "start": 921.92,
        "duration": 6.56,
        "text": "and we Chunk in a smart way so that the"
      },
      {
        "start": 925.759,
        "duration": 4.681,
        "text": "content of different topics are not"
      },
      {
        "start": 928.48,
        "duration": 4.0,
        "text": "mixed within the same chunk which helps"
      },
      {
        "start": 930.44,
        "duration": 3.839,
        "text": "further down the the stream with the"
      },
      {
        "start": 932.48,
        "duration": 4.799,
        "text": "retrieval process because you want to"
      },
      {
        "start": 934.279,
        "duration": 4.761,
        "text": "retrieve the most relevant information"
      },
      {
        "start": 937.279,
        "duration": 3.601,
        "text": "and you don't want to retrieve a chunk"
      },
      {
        "start": 939.04,
        "duration": 3.52,
        "text": "that partially contains something about"
      },
      {
        "start": 940.88,
        "duration": 4.68,
        "text": "topic a and partially contains"
      },
      {
        "start": 942.56,
        "duration": 6.04,
        "text": "information about the topic B so that is"
      },
      {
        "start": 945.56,
        "duration": 6.32,
        "text": "one of the reasons why we split those"
      },
      {
        "start": 948.6,
        "duration": 6.96,
        "text": "documents so the Json that you receive"
      },
      {
        "start": 951.88,
        "duration": 7.6,
        "text": "um contains these document elements and"
      },
      {
        "start": 955.56,
        "duration": 7.16,
        "text": "every element contains text uh Cate"
      },
      {
        "start": 959.48,
        "duration": 6.159,
        "text": "or type and additional metadata and if"
      },
      {
        "start": 962.72,
        "duration": 6.16,
        "text": "you do the embedding step as well then"
      },
      {
        "start": 965.639,
        "duration": 4.76,
        "text": "that document element will also have"
      },
      {
        "start": 968.88,
        "duration": 4.48,
        "text": "embedding"
      },
      {
        "start": 970.399,
        "duration": 5.041,
        "text": "there I hope this answers the question"
      },
      {
        "start": 973.36,
        "duration": 3.36,
        "text": "that definitely does out of curiosity"
      },
      {
        "start": 975.44,
        "duration": 3.28,
        "text": "and this is maybe a little bit silly but"
      },
      {
        "start": 976.72,
        "duration": 5.08,
        "text": "you mentioned so many different file"
      },
      {
        "start": 978.72,
        "duration": 6.32,
        "text": "types what's the most unexpected one um"
      },
      {
        "start": 981.8,
        "duration": 6.479,
        "text": "for example can unstructured um can like"
      },
      {
        "start": 985.04,
        "duration": 6.2,
        "text": "process like a movie and give you a big"
      },
      {
        "start": 988.279,
        "duration": 7.961,
        "text": "Json a of each line from the movie or"
      },
      {
        "start": 991.24,
        "duration": 7.279,
        "text": "something um not yet but"
      },
      {
        "start": 996.24,
        "duration": 4.48,
        "text": "um maybe"
      },
      {
        "start": 998.519,
        "duration": 4.521,
        "text": "soon fantastic but I guess more"
      },
      {
        "start": 1000.72,
        "duration": 5.4,
        "text": "questions yeah if you're asking about"
      },
      {
        "start": 1003.04,
        "duration": 4.68,
        "text": "unexpected files I think one of them can"
      },
      {
        "start": 1006.12,
        "duration": 5.48,
        "text": "be an"
      },
      {
        "start": 1007.72,
        "duration": 5.599,
        "text": "email uh we can process emails as well"
      },
      {
        "start": 1011.6,
        "duration": 5.28,
        "text": "and then for example for emails in the"
      },
      {
        "start": 1013.319,
        "duration": 5.921,
        "text": "metadata you can have the recipients the"
      },
      {
        "start": 1016.88,
        "duration": 4.199,
        "text": "senders this kind of information"
      },
      {
        "start": 1019.24,
        "duration": 3.76,
        "text": "attachments to the email can be"
      },
      {
        "start": 1021.079,
        "duration": 5.081,
        "text": "processed as well so I guess this could"
      },
      {
        "start": 1023.0,
        "duration": 6.039,
        "text": "be more of an unusual"
      },
      {
        "start": 1026.16,
        "duration": 4.679,
        "text": "type fantastic that's actually gives me"
      },
      {
        "start": 1029.039,
        "duration": 5.081,
        "text": "already some ideas I can use to build"
      },
      {
        "start": 1030.839,
        "duration": 5.12,
        "text": "Vector search for my email uh client in"
      },
      {
        "start": 1034.12,
        "duration": 2.839,
        "text": "case um there's so many questions Hey"
      },
      {
        "start": 1035.959,
        "duration": 2.84,
        "text": "listen I want to take a moment and"
      },
      {
        "start": 1036.959,
        "duration": 4.521,
        "text": "really appreciate the audience here"
      },
      {
        "start": 1038.799,
        "duration": 4.12,
        "text": "because um often live streams you never"
      },
      {
        "start": 1041.48,
        "duration": 2.839,
        "text": "know how they're going to go sometimes a"
      },
      {
        "start": 1042.919,
        "duration": 3.201,
        "text": "lot of people want to interact and other"
      },
      {
        "start": 1044.319,
        "duration": 4.12,
        "text": "times it's just us but since the"
      },
      {
        "start": 1046.12,
        "duration": 3.679,
        "text": "audience is being so engaged I'd love to"
      },
      {
        "start": 1048.439,
        "duration": 3.041,
        "text": "actually pay more attention to the"
      },
      {
        "start": 1049.799,
        "duration": 3.201,
        "text": "audience and and just answer their"
      },
      {
        "start": 1051.48,
        "duration": 3.36,
        "text": "questions I understand we do have some"
      },
      {
        "start": 1053.0,
        "duration": 2.799,
        "text": "demos prepared but if we can prioritize"
      },
      {
        "start": 1054.84,
        "duration": 2.68,
        "text": "the audience here I think it would be a"
      },
      {
        "start": 1055.799,
        "duration": 3.801,
        "text": "better service to them and then we can"
      },
      {
        "start": 1057.52,
        "duration": 4.64,
        "text": "handle these in in follow-up discussions"
      },
      {
        "start": 1059.6,
        "duration": 5.64,
        "text": "so with that um there's a question from"
      },
      {
        "start": 1062.16,
        "duration": 7.6,
        "text": "Sophia Kino asking um the security"
      },
      {
        "start": 1065.24,
        "duration": 6.04,
        "text": "measures um and how we can avoid for"
      },
      {
        "start": 1069.76,
        "duration": 2.799,
        "text": "example taking like confidential"
      },
      {
        "start": 1071.28,
        "duration": 4.12,
        "text": "documents passing them through"
      },
      {
        "start": 1072.559,
        "duration": 5.041,
        "text": "unstructured processing and then feeding"
      },
      {
        "start": 1075.4,
        "duration": 3.84,
        "text": "them to an llm where the llm can then"
      },
      {
        "start": 1077.6,
        "duration": 4.68,
        "text": "steal that data or other otherwise use"
      },
      {
        "start": 1079.24,
        "duration": 4.319,
        "text": "it to train itself or have the vendors"
      },
      {
        "start": 1082.28,
        "duration": 4.8,
        "text": "of the llms ReUse them in the"
      },
      {
        "start": 1083.559,
        "duration": 5.561,
        "text": "fine-tuning processes um I think an an"
      },
      {
        "start": 1087.08,
        "duration": 3.959,
        "text": "maybe a a question to extrapolate from"
      },
      {
        "start": 1089.12,
        "duration": 4.559,
        "text": "this is does unstructured in its"
      },
      {
        "start": 1091.039,
        "duration": 5.241,
        "text": "processing pipeline offer ways to strip"
      },
      {
        "start": 1093.679,
        "duration": 4.201,
        "text": "or redact certain data from formats that"
      },
      {
        "start": 1096.28,
        "duration": 5.279,
        "text": "go through its"
      },
      {
        "start": 1097.88,
        "duration": 7.36,
        "text": "processing so there are several aspects"
      },
      {
        "start": 1101.559,
        "duration": 7.281,
        "text": "to this question first uh the security"
      },
      {
        "start": 1105.24,
        "duration": 8.2,
        "text": "um unstructured is a sock compliant and"
      },
      {
        "start": 1108.84,
        "duration": 7.959,
        "text": "it's Hippa compliant you can uh process"
      },
      {
        "start": 1113.44,
        "duration": 6.44,
        "text": "your documents with unstructured hosted"
      },
      {
        "start": 1116.799,
        "duration": 6.721,
        "text": "API but we also offer unstructured on"
      },
      {
        "start": 1119.88,
        "duration": 6.0,
        "text": "Azure and AWS Marketplace so that you"
      },
      {
        "start": 1123.52,
        "duration": 4.12,
        "text": "can have it in your own VPC"
      },
      {
        "start": 1125.88,
        "duration": 3.279,
        "text": "infrastructure so that the data stays"
      },
      {
        "start": 1127.64,
        "duration": 4.24,
        "text": "there never goes"
      },
      {
        "start": 1129.159,
        "duration": 7.161,
        "text": "anywhere um that's one of the question"
      },
      {
        "start": 1131.88,
        "duration": 5.56,
        "text": "questions as for stripping certain parts"
      },
      {
        "start": 1136.32,
        "duration": 5.16,
        "text": "of"
      },
      {
        "start": 1137.44,
        "duration": 6.08,
        "text": "content uh you can set up your pipeline"
      },
      {
        "start": 1141.48,
        "duration": 3.88,
        "text": "to First maybe partition the documents"
      },
      {
        "start": 1143.52,
        "duration": 5.36,
        "text": "and once you have the document elements"
      },
      {
        "start": 1145.36,
        "duration": 5.319,
        "text": "you can curate the results so you can go"
      },
      {
        "start": 1148.88,
        "duration": 5.48,
        "text": "through those Json documents however you"
      },
      {
        "start": 1150.679,
        "duration": 7.921,
        "text": "want remove um information that you"
      },
      {
        "start": 1154.36,
        "duration": 7.439,
        "text": "don't want to be uh used for your"
      },
      {
        "start": 1158.6,
        "duration": 5.72,
        "text": "application and then you can use another"
      },
      {
        "start": 1161.799,
        "duration": 4.281,
        "text": "pipeline to do the rest of the steps for"
      },
      {
        "start": 1164.32,
        "duration": 4.16,
        "text": "example chunking and embedding and"
      },
      {
        "start": 1166.08,
        "duration": 6.24,
        "text": "uploading you can do that so there's a"
      },
      {
        "start": 1168.48,
        "duration": 7.16,
        "text": "lot of flexibility um you can maybe"
      },
      {
        "start": 1172.32,
        "duration": 5.359,
        "text": "integrate some Pi Pi reduction step in"
      },
      {
        "start": 1175.64,
        "duration": 5.44,
        "text": "there so personal personally"
      },
      {
        "start": 1177.679,
        "duration": 6.041,
        "text": "identifiable information that that is pi"
      },
      {
        "start": 1181.08,
        "duration": 5.92,
        "text": "you can remove that somewhere along the"
      },
      {
        "start": 1183.72,
        "duration": 6.959,
        "text": "process uh as for training llms if you"
      },
      {
        "start": 1187.0,
        "duration": 7.52,
        "text": "don't want to use an llm that is"
      },
      {
        "start": 1190.679,
        "duration": 6.321,
        "text": "provided by uh you know open AI or"
      },
      {
        "start": 1194.52,
        "duration": 5.24,
        "text": "somebody else you can host your own llm"
      },
      {
        "start": 1197.0,
        "duration": 5.64,
        "text": "somewhere and then you have full control"
      },
      {
        "start": 1199.76,
        "duration": 7.0,
        "text": "over happens to that llm what goes to it"
      },
      {
        "start": 1202.64,
        "duration": 8.399,
        "text": "and and um you can be sure that you are"
      },
      {
        "start": 1206.76,
        "duration": 6.919,
        "text": "not giving your data to any of the other"
      },
      {
        "start": 1211.039,
        "duration": 4.88,
        "text": "companies fantastic yeah and for for to"
      },
      {
        "start": 1213.679,
        "duration": 3.601,
        "text": "make it really practical Lama is is a"
      },
      {
        "start": 1215.919,
        "duration": 3.441,
        "text": "great inference engine you can use for"
      },
      {
        "start": 1217.28,
        "duration": 5.0,
        "text": "example where you run your llm locally"
      },
      {
        "start": 1219.36,
        "duration": 4.439,
        "text": "including l. CPP and VM there's a number"
      },
      {
        "start": 1222.28,
        "duration": 3.04,
        "text": "of things that's a separate discussion"
      },
      {
        "start": 1223.799,
        "duration": 3.561,
        "text": "thank you so much very thoughtful answer"
      },
      {
        "start": 1225.32,
        "duration": 4.0,
        "text": "and I hope that answered um Sophia's"
      },
      {
        "start": 1227.36,
        "duration": 4.76,
        "text": "question there was a question by David"
      },
      {
        "start": 1229.32,
        "duration": 4.52,
        "text": "Edwards um that I think is on the minds"
      },
      {
        "start": 1232.12,
        "duration": 4.12,
        "text": "of many um attendees which is"
      },
      {
        "start": 1233.84,
        "duration": 5.319,
        "text": "unstructured can as you mentioned handle"
      },
      {
        "start": 1236.24,
        "duration": 4.799,
        "text": "multiple file formats um I think a"
      },
      {
        "start": 1239.159,
        "duration": 4.681,
        "text": "strong use case for really most SAS"
      },
      {
        "start": 1241.039,
        "duration": 5.12,
        "text": "businesses that came up a few years ago"
      },
      {
        "start": 1243.84,
        "duration": 4.4,
        "text": "was just we're creating a nicer way to"
      },
      {
        "start": 1246.159,
        "duration": 3.64,
        "text": "work with company data that isn't Google"
      },
      {
        "start": 1248.24,
        "duration": 4.24,
        "text": "Sheets or some type of spreadsheet"
      },
      {
        "start": 1249.799,
        "duration": 4.801,
        "text": "solution right um and nowadays there's a"
      },
      {
        "start": 1252.48,
        "duration": 4.12,
        "text": "lot of um work still being done with"
      },
      {
        "start": 1254.6,
        "duration": 4.24,
        "text": "spreadsheets and so David's question is"
      },
      {
        "start": 1256.6,
        "duration": 4.24,
        "text": "how good or reliable is unstructured"
      },
      {
        "start": 1258.84,
        "duration": 3.52,
        "text": "working with spreadsheets um he"
      },
      {
        "start": 1260.84,
        "duration": 2.52,
        "text": "mentioned specifically ex Excel"
      },
      {
        "start": 1262.36,
        "duration": 3.04,
        "text": "spreadsheets but I assume all"
      },
      {
        "start": 1263.36,
        "duration": 5.16,
        "text": "spreadsheets with unpredictable formats"
      },
      {
        "start": 1265.4,
        "duration": 5.04,
        "text": "meaning um merged cells or empty cells"
      },
      {
        "start": 1268.52,
        "duration": 3.519,
        "text": "or cells that are that are just purely"
      },
      {
        "start": 1270.44,
        "duration": 3.44,
        "text": "formatting that don't really some some"
      },
      {
        "start": 1272.039,
        "duration": 3.161,
        "text": "Excel spreadsheets have like background"
      },
      {
        "start": 1273.88,
        "duration": 4.679,
        "text": "colors and text colors that don't really"
      },
      {
        "start": 1275.2,
        "duration": 5.16,
        "text": "mean much um how how does how can one"
      },
      {
        "start": 1278.559,
        "duration": 4.72,
        "text": "maybe even fine-tune the data pulled out"
      },
      {
        "start": 1280.36,
        "duration": 2.919,
        "text": "from these spreadsheets with"
      },
      {
        "start": 1284.0,
        "duration": 5.159,
        "text": "unstructured I'm not sure what you mean"
      },
      {
        "start": 1286.08,
        "duration": 6.28,
        "text": "exactly by fine-tuning the data pulled"
      },
      {
        "start": 1289.159,
        "duration": 6.76,
        "text": "out uh but uh when it comes to"
      },
      {
        "start": 1292.36,
        "duration": 5.559,
        "text": "spreadsheets we treat them as tables and"
      },
      {
        "start": 1295.919,
        "duration": 4.12,
        "text": "for the tables we have very strong"
      },
      {
        "start": 1297.919,
        "duration": 5.441,
        "text": "mechanisms for layout detection and"
      },
      {
        "start": 1300.039,
        "duration": 7.481,
        "text": "understanding when they're merged cells"
      },
      {
        "start": 1303.36,
        "duration": 6.76,
        "text": "uh different uh earch key nested tables"
      },
      {
        "start": 1307.52,
        "duration": 6.639,
        "text": "all that we have seen all kinds of nasty"
      },
      {
        "start": 1310.12,
        "duration": 5.76,
        "text": "tables I have to say and so this you"
      },
      {
        "start": 1314.159,
        "duration": 4.76,
        "text": "know all kinds of weird tables are not"
      },
      {
        "start": 1315.88,
        "duration": 4.799,
        "text": "going to be used to us but I'm not sure"
      },
      {
        "start": 1318.919,
        "duration": 3.401,
        "text": "what you mean by fine-tuning the data"
      },
      {
        "start": 1320.679,
        "duration": 5.321,
        "text": "usually you fine-tune a model on"
      },
      {
        "start": 1322.32,
        "duration": 5.599,
        "text": "something and maybe cleaning the data"
      },
      {
        "start": 1326.0,
        "duration": 3.279,
        "text": "correct yeah just um so you get back a"
      },
      {
        "start": 1327.919,
        "duration": 2.921,
        "text": "bunch of data that some of it makes no"
      },
      {
        "start": 1329.279,
        "duration": 3.841,
        "text": "sense and others are just formatting and"
      },
      {
        "start": 1330.84,
        "duration": 3.48,
        "text": "so um by fine mean like filtering out"
      },
      {
        "start": 1333.12,
        "duration": 3.72,
        "text": "the useless"
      },
      {
        "start": 1334.32,
        "duration": 4.52,
        "text": "stuff yeah once you have Json you have"
      },
      {
        "start": 1336.84,
        "duration": 5.16,
        "text": "so many different ways to to filter that"
      },
      {
        "start": 1338.84,
        "duration": 5.4,
        "text": "out um there are many options to do that"
      },
      {
        "start": 1342.0,
        "duration": 4.36,
        "text": "excellent okay let's let's make it"
      },
      {
        "start": 1344.24,
        "duration": 4.28,
        "text": "really practical because Sammy is asking"
      },
      {
        "start": 1346.36,
        "duration": 4.04,
        "text": "for a start to finish scenario and exam"
      },
      {
        "start": 1348.52,
        "duration": 4.0,
        "text": "example of taking unstructured data for"
      },
      {
        "start": 1350.4,
        "duration": 5.08,
        "text": "a company to to the Finish end results"
      },
      {
        "start": 1352.52,
        "duration": 4.88,
        "text": "meaning a working rag llm application I"
      },
      {
        "start": 1355.48,
        "duration": 4.12,
        "text": "think this is this is like prime time to"
      },
      {
        "start": 1357.4,
        "duration": 4.92,
        "text": "show not tell um so what if what if we"
      },
      {
        "start": 1359.6,
        "duration": 4.4,
        "text": "did this what if we showed unstructured"
      },
      {
        "start": 1362.32,
        "duration": 3.28,
        "text": "ingesting some data process not"
      },
      {
        "start": 1364.0,
        "duration": 3.24,
        "text": "ingesting that's the wrong word but um"
      },
      {
        "start": 1365.6,
        "duration": 4.559,
        "text": "reading data and processing it and then"
      },
      {
        "start": 1367.24,
        "duration": 4.52,
        "text": "outputting this Json um as as part A and"
      },
      {
        "start": 1370.159,
        "duration": 3.281,
        "text": "I think we can do that um with with you"
      },
      {
        "start": 1371.76,
        "duration": 3.519,
        "text": "Maria and then as Part B we can look at"
      },
      {
        "start": 1373.44,
        "duration": 3.44,
        "text": "Lang flow and building a rag app with"
      },
      {
        "start": 1375.279,
        "duration": 4.4,
        "text": "Eric um we can maybe start at the"
      },
      {
        "start": 1376.88,
        "duration": 5.039,
        "text": "beginning with with unstructured"
      },
      {
        "start": 1379.679,
        "duration": 4.88,
        "text": "absolutely let me just share um a"
      },
      {
        "start": 1381.919,
        "duration": 2.64,
        "text": "notebook with"
      },
      {
        "start": 1387.6,
        "duration": 3.8,
        "text": "you okay I'm going to zoom in a little"
      },
      {
        "start": 1389.919,
        "duration": 3.921,
        "text": "bit so"
      },
      {
        "start": 1391.4,
        "duration": 5.56,
        "text": "um"
      },
      {
        "start": 1393.84,
        "duration": 5.12,
        "text": "oops there we go I think this should be"
      },
      {
        "start": 1396.96,
        "duration": 3.199,
        "text": "let me know if you want me to zoom zoom"
      },
      {
        "start": 1398.96,
        "duration": 6.0,
        "text": "in a bit"
      },
      {
        "start": 1400.159,
        "duration": 6.601,
        "text": "more that looks great okay so um in the"
      },
      {
        "start": 1404.96,
        "duration": 4.44,
        "text": "interest of time and just to live some"
      },
      {
        "start": 1406.76,
        "duration": 4.32,
        "text": "time to Eric I'm going to run the whole"
      },
      {
        "start": 1409.4,
        "duration": 3.08,
        "text": "notebook but I'm going to explain how"
      },
      {
        "start": 1411.08,
        "duration": 4.4,
        "text": "you can build a pipeline with"
      },
      {
        "start": 1412.48,
        "duration": 5.48,
        "text": "unstructured in just a very few steps"
      },
      {
        "start": 1415.48,
        "duration": 4.92,
        "text": "first you're going to need a key uh the"
      },
      {
        "start": 1417.96,
        "duration": 4.52,
        "text": "API key that you can get here and it"
      },
      {
        "start": 1420.4,
        "duration": 4.72,
        "text": "comes with a 14-day trial which is kept"
      },
      {
        "start": 1422.48,
        "duration": 4.96,
        "text": "at 1,000 pages per day there are many"
      },
      {
        "start": 1425.12,
        "duration": 6.64,
        "text": "ways to use unstructured API so you can"
      },
      {
        "start": 1427.44,
        "duration": 9.119,
        "text": "use uh we have a python SDK you can um"
      },
      {
        "start": 1431.76,
        "duration": 7.88,
        "text": "send direct request requests to the API"
      },
      {
        "start": 1436.559,
        "duration": 5.081,
        "text": "endpoint there is a javascri script SDK"
      },
      {
        "start": 1439.64,
        "duration": 4.24,
        "text": "as well but my favorite way of using"
      },
      {
        "start": 1441.64,
        "duration": 4.159,
        "text": "unstructured is with unstructured in"
      },
      {
        "start": 1443.88,
        "duration": 4.799,
        "text": "just Library so this is what we are"
      },
      {
        "start": 1445.799,
        "duration": 5.401,
        "text": "installing here it's a python Library so"
      },
      {
        "start": 1448.679,
        "duration": 5.88,
        "text": "if you're allergic to python I'm sorry"
      },
      {
        "start": 1451.2,
        "duration": 4.839,
        "text": "but I'm going to use Python here um and"
      },
      {
        "start": 1454.559,
        "duration": 3.281,
        "text": "when you're installing unstructured"
      },
      {
        "start": 1456.039,
        "duration": 4.841,
        "text": "inest you need to specify def"
      },
      {
        "start": 1457.84,
        "duration": 6.839,
        "text": "dependencies uh so for your Source"
      },
      {
        "start": 1460.88,
        "duration": 7.159,
        "text": "sources destinations um uh embedding"
      },
      {
        "start": 1464.679,
        "duration": 6.24,
        "text": "providers and uh types of some types of"
      },
      {
        "start": 1468.039,
        "duration": 5.441,
        "text": "files so here for example I'm going to"
      },
      {
        "start": 1470.919,
        "duration": 4.601,
        "text": "be oh I actually don't need S3 because"
      },
      {
        "start": 1473.48,
        "duration": 3.72,
        "text": "in this example I'm uh loading from a"
      },
      {
        "start": 1475.52,
        "duration": 4.72,
        "text": "local source so this is this was"
      },
      {
        "start": 1477.2,
        "duration": 5.32,
        "text": "unnecessary so PDF for a PDF document"
      },
      {
        "start": 1480.24,
        "duration": 5.88,
        "text": "ASB is going to be the destination here"
      },
      {
        "start": 1482.52,
        "duration": 6.36,
        "text": "and hugging face is used for generating"
      },
      {
        "start": 1486.12,
        "duration": 4.64,
        "text": "embeddings um because we're going to be"
      },
      {
        "start": 1488.88,
        "duration": 5.039,
        "text": "loading"
      },
      {
        "start": 1490.76,
        "duration": 6.24,
        "text": "into asro DB you're going to have to set"
      },
      {
        "start": 1493.919,
        "duration": 6.401,
        "text": "up your asrb database and"
      },
      {
        "start": 1497.0,
        "duration": 6.679,
        "text": "collection and you're going to have to"
      },
      {
        "start": 1500.32,
        "duration": 6.12,
        "text": "have your um authentication credentials"
      },
      {
        "start": 1503.679,
        "duration": 4.081,
        "text": "obviously I'm not showing mine here"
      },
      {
        "start": 1506.44,
        "duration": 3.16,
        "text": "you're going to need to choose which"
      },
      {
        "start": 1507.76,
        "duration": 3.399,
        "text": "embedding model you're going to use for"
      },
      {
        "start": 1509.6,
        "duration": 4.0,
        "text": "the final step of"
      },
      {
        "start": 1511.159,
        "duration": 6.12,
        "text": "transformation and when you have"
      },
      {
        "start": 1513.6,
        "duration": 6.0,
        "text": "embedding models they um output vectors"
      },
      {
        "start": 1517.279,
        "duration": 5.161,
        "text": "of different dimensions so you will need"
      },
      {
        "start": 1519.6,
        "duration": 4.799,
        "text": "to know the dimension of the vector that"
      },
      {
        "start": 1522.44,
        "duration": 4.88,
        "text": "your model outputs you can usually find"
      },
      {
        "start": 1524.399,
        "duration": 5.121,
        "text": "that information in the um the"
      },
      {
        "start": 1527.32,
        "duration": 4.239,
        "text": "documentation of the model on the"
      },
      {
        "start": 1529.52,
        "duration": 4.24,
        "text": "embedding providers website so if you go"
      },
      {
        "start": 1531.559,
        "duration": 3.84,
        "text": "to huging face model card there will"
      },
      {
        "start": 1533.76,
        "duration": 4.96,
        "text": "contain that information if you go to"
      },
      {
        "start": 1535.399,
        "duration": 6.16,
        "text": "open thei the documentation will have"
      },
      {
        "start": 1538.72,
        "duration": 6.199,
        "text": "that we're going to do some imports and"
      },
      {
        "start": 1541.559,
        "duration": 6.72,
        "text": "then in this example I'm loading from a"
      },
      {
        "start": 1544.919,
        "duration": 5.841,
        "text": "local directory so I need to know where"
      },
      {
        "start": 1548.279,
        "duration": 5.801,
        "text": "uh my files are"
      },
      {
        "start": 1550.76,
        "duration": 7.32,
        "text": "located uh this is the path I'm going to"
      },
      {
        "start": 1554.08,
        "duration": 7.199,
        "text": "use work directory is um optional but I"
      },
      {
        "start": 1558.08,
        "duration": 6.959,
        "text": "like to set it because this is where I"
      },
      {
        "start": 1561.279,
        "duration": 6.12,
        "text": "can save the caches for each and every"
      },
      {
        "start": 1565.039,
        "duration": 4.64,
        "text": "step so if I want to look into what"
      },
      {
        "start": 1567.399,
        "duration": 5.561,
        "text": "happened after partitioning after"
      },
      {
        "start": 1569.679,
        "duration": 5.521,
        "text": "chunking after embedding I I can go into"
      },
      {
        "start": 1572.96,
        "duration": 4.719,
        "text": "this directory and see the results un"
      },
      {
        "start": 1575.2,
        "duration": 4.44,
        "text": "structure will save them even if you"
      },
      {
        "start": 1577.679,
        "duration": 5.281,
        "text": "don't specify the working directory it's"
      },
      {
        "start": 1579.64,
        "duration": 6.36,
        "text": "just going to be a path and will be"
      },
      {
        "start": 1582.96,
        "duration": 6.199,
        "text": "somewhat obscure and uh maybe not as"
      },
      {
        "start": 1586.0,
        "duration": 5.64,
        "text": "easy to find and so when you build a"
      },
      {
        "start": 1589.159,
        "duration": 5.921,
        "text": "pipeline from a source to the"
      },
      {
        "start": 1591.64,
        "duration": 6.84,
        "text": "destination you build it from a bunch of"
      },
      {
        "start": 1595.08,
        "duration": 6.079,
        "text": "configurations so think of it as"
      },
      {
        "start": 1598.48,
        "duration": 5.439,
        "text": "building a tower with Lego brakes it's"
      },
      {
        "start": 1601.159,
        "duration": 4.481,
        "text": "that easy so this is all it takes this"
      },
      {
        "start": 1603.919,
        "duration": 4.401,
        "text": "is the whole"
      },
      {
        "start": 1605.64,
        "duration": 6.56,
        "text": "Pipeline and let me explain what these"
      },
      {
        "start": 1608.32,
        "duration": 6.359,
        "text": "configs are so you specify pipeline do"
      },
      {
        "start": 1612.2,
        "duration": 5.04,
        "text": "from configs the first one"
      },
      {
        "start": 1614.679,
        "duration": 5.48,
        "text": "context uh and this is a this is called"
      },
      {
        "start": 1617.24,
        "duration": 6.159,
        "text": "a processor config it controls General"
      },
      {
        "start": 1620.159,
        "duration": 6.12,
        "text": "Behavior verbosity the working Direction"
      },
      {
        "start": 1623.399,
        "duration": 6.52,
        "text": "number of processes the general behavior"
      },
      {
        "start": 1626.279,
        "duration": 7.041,
        "text": "of the pipeline next you specify the"
      },
      {
        "start": 1629.919,
        "duration": 6.281,
        "text": "location of your documents in this case"
      },
      {
        "start": 1633.32,
        "duration": 4.599,
        "text": "it's a local directory so this is where"
      },
      {
        "start": 1636.2,
        "duration": 5.599,
        "text": "we're going to be indexing the files"
      },
      {
        "start": 1637.919,
        "duration": 6.24,
        "text": "from downloading the files from uh and"
      },
      {
        "start": 1641.799,
        "duration": 5.76,
        "text": "where we going to be connecting to you"
      },
      {
        "start": 1644.159,
        "duration": 6.561,
        "text": "can replace these three lines with a"
      },
      {
        "start": 1647.559,
        "duration": 6.041,
        "text": "different connect ctor for example S3"
      },
      {
        "start": 1650.72,
        "duration": 7.079,
        "text": "Azure blob storage you can connect to"
      },
      {
        "start": 1653.6,
        "duration": 7.0,
        "text": "SharePoint or Confluence uh and there"
      },
      {
        "start": 1657.799,
        "duration": 5.281,
        "text": "are 20 different sources that are"
      },
      {
        "start": 1660.6,
        "duration": 4.559,
        "text": "supported so if your source is different"
      },
      {
        "start": 1663.08,
        "duration": 4.319,
        "text": "if you're not doing this with local"
      },
      {
        "start": 1665.159,
        "duration": 4.321,
        "text": "documents that these are the connect uh"
      },
      {
        "start": 1667.399,
        "duration": 5.921,
        "text": "the configurations that you will need to"
      },
      {
        "start": 1669.48,
        "duration": 6.559,
        "text": "replace and for example here I no I"
      },
      {
        "start": 1673.32,
        "duration": 4.359,
        "text": "cannot switch to a different tab but you"
      },
      {
        "start": 1676.039,
        "duration": 5.64,
        "text": "can find documentation for all the"
      },
      {
        "start": 1677.679,
        "duration": 8.801,
        "text": "source conectors uh in unstructured docs"
      },
      {
        "start": 1681.679,
        "duration": 8.441,
        "text": "once the data is ingested next step is"
      },
      {
        "start": 1686.48,
        "duration": 4.919,
        "text": "partitioning the files into those"
      },
      {
        "start": 1690.12,
        "duration": 3.84,
        "text": "document"
      },
      {
        "start": 1691.399,
        "duration": 5.561,
        "text": "elements and this is what partitioner"
      },
      {
        "start": 1693.96,
        "duration": 5.52,
        "text": "config controls this is where I decide"
      },
      {
        "start": 1696.96,
        "duration": 4.839,
        "text": "that I want to use the API and I provide"
      },
      {
        "start": 1699.48,
        "duration": 4.96,
        "text": "my credentials we have different"
      },
      {
        "start": 1701.799,
        "duration": 5.081,
        "text": "strategies for partitioning the document"
      },
      {
        "start": 1704.44,
        "duration": 4.64,
        "text": "you have fast strategy and high"
      },
      {
        "start": 1706.88,
        "duration": 5.2,
        "text": "resolution strategy essentially if"
      },
      {
        "start": 1709.08,
        "duration": 7.0,
        "text": "you're processing markdown or HTML files"
      },
      {
        "start": 1712.08,
        "duration": 7.0,
        "text": "you can use fast to um use rule based"
      },
      {
        "start": 1716.08,
        "duration": 5.68,
        "text": "parsers to quickly get the results and"
      },
      {
        "start": 1719.08,
        "duration": 5.479,
        "text": "hes strategy is tailored for image-based"
      },
      {
        "start": 1721.76,
        "duration": 6.32,
        "text": "documents such as PDF power points"
      },
      {
        "start": 1724.559,
        "duration": 6.441,
        "text": "images and so on the cool thing is that"
      },
      {
        "start": 1728.08,
        "duration": 5.64,
        "text": "if your PDFs only contain text and"
      },
      {
        "start": 1731.0,
        "duration": 4.559,
        "text": "nothing else then fast strategy will"
      },
      {
        "start": 1733.72,
        "duration": 5.12,
        "text": "also work for them and it's going to be"
      },
      {
        "start": 1735.559,
        "duration": 7.0,
        "text": "very very fast so you may be able to use"
      },
      {
        "start": 1738.84,
        "duration": 7.0,
        "text": "it too um this block of parameters just"
      },
      {
        "start": 1742.559,
        "duration": 5.72,
        "text": "makes uh PDF processing a little bit"
      },
      {
        "start": 1745.84,
        "duration": 5.679,
        "text": "faster by setting concurrency levels and"
      },
      {
        "start": 1748.279,
        "duration": 6.681,
        "text": "things like that and that's it at that"
      },
      {
        "start": 1751.519,
        "duration": 6.52,
        "text": "step your documents are partitioned uh"
      },
      {
        "start": 1754.96,
        "duration": 6.439,
        "text": "split into document elements they have"
      },
      {
        "start": 1758.039,
        "duration": 5.201,
        "text": "metadata they are in Json format and"
      },
      {
        "start": 1761.399,
        "duration": 4.28,
        "text": "normally if you want to build a rag"
      },
      {
        "start": 1763.24,
        "duration": 4.88,
        "text": "application next you have to chunk"
      },
      {
        "start": 1765.679,
        "duration": 5.161,
        "text": "documents and you have to generate"
      },
      {
        "start": 1768.12,
        "duration": 5.32,
        "text": "settings for them so the next logical"
      },
      {
        "start": 1770.84,
        "duration": 4.8,
        "text": "step in the pipeline is uh the chunkin"
      },
      {
        "start": 1773.44,
        "duration": 3.4,
        "text": "step which is configured with the"
      },
      {
        "start": 1775.64,
        "duration": 3.759,
        "text": "chunker"
      },
      {
        "start": 1776.84,
        "duration": 5.959,
        "text": "config un structure offers several"
      },
      {
        "start": 1779.399,
        "duration": 7.801,
        "text": "different chunkin strategies um that you"
      },
      {
        "start": 1782.799,
        "duration": 7.201,
        "text": "can use to split your documents"
      },
      {
        "start": 1787.2,
        "duration": 5.04,
        "text": "into chunks that will fit into the"
      },
      {
        "start": 1790.0,
        "duration": 4.76,
        "text": "context window of your embedding models"
      },
      {
        "start": 1792.24,
        "duration": 4.799,
        "text": "and are small enough to contain precise"
      },
      {
        "start": 1794.76,
        "duration": 4.72,
        "text": "information and enough"
      },
      {
        "start": 1797.039,
        "duration": 4.041,
        "text": "context but you might be thinking that"
      },
      {
        "start": 1799.48,
        "duration": 4.72,
        "text": "wait a second we already split the"
      },
      {
        "start": 1801.08,
        "duration": 5.36,
        "text": "document into elements why do I need to"
      },
      {
        "start": 1804.2,
        "duration": 5.04,
        "text": "chunk them aren't they already chunked"
      },
      {
        "start": 1806.44,
        "duration": 4.839,
        "text": "and the answer to to this is they're not"
      },
      {
        "start": 1809.24,
        "duration": 3.88,
        "text": "really chunked because you may have a"
      },
      {
        "start": 1811.279,
        "duration": 4.64,
        "text": "very very long paragraph which is a"
      },
      {
        "start": 1813.12,
        "duration": 6.76,
        "text": "single document element which will not"
      },
      {
        "start": 1815.919,
        "duration": 6.401,
        "text": "fit your maximum limit so that element"
      },
      {
        "start": 1819.88,
        "duration": 5.48,
        "text": "will need to be split into smaller"
      },
      {
        "start": 1822.32,
        "duration": 5.32,
        "text": "elements and then you may have a whole"
      },
      {
        "start": 1825.36,
        "duration": 3.319,
        "text": "bunch of small elements for example a"
      },
      {
        "start": 1827.64,
        "duration": 5.44,
        "text": "list"
      },
      {
        "start": 1828.679,
        "duration": 7.441,
        "text": "will uh come out as a bunch of list item"
      },
      {
        "start": 1833.08,
        "duration": 5.839,
        "text": "elements so every list item might be"
      },
      {
        "start": 1836.12,
        "duration": 6.48,
        "text": "quite small and you may want to merge"
      },
      {
        "start": 1838.919,
        "duration": 7.161,
        "text": "those elements into a bigger chunk"
      },
      {
        "start": 1842.6,
        "duration": 4.48,
        "text": "so this is uh why you still need the"
      },
      {
        "start": 1846.08,
        "duration": 4.12,
        "text": "chunking"
      },
      {
        "start": 1847.08,
        "duration": 7.4,
        "text": "step and the chunking strategies"
      },
      {
        "start": 1850.2,
        "duration": 7.24,
        "text": "control how smaller elements are merged"
      },
      {
        "start": 1854.48,
        "duration": 5.159,
        "text": "so by title strategy make sure that you"
      },
      {
        "start": 1857.44,
        "duration": 4.92,
        "text": "don't merge content from different"
      },
      {
        "start": 1859.639,
        "duration": 4.201,
        "text": "sections this helps with the retrieval"
      },
      {
        "start": 1862.36,
        "duration": 5.08,
        "text": "performance down the line but there are"
      },
      {
        "start": 1863.84,
        "duration": 6.4,
        "text": "other strategies that take into account"
      },
      {
        "start": 1867.44,
        "duration": 6.119,
        "text": "other aspects like for example whether"
      },
      {
        "start": 1870.24,
        "duration": 6.159,
        "text": "two elements are similar or not whether"
      },
      {
        "start": 1873.559,
        "duration": 5.041,
        "text": "we can merge them into one chunk based"
      },
      {
        "start": 1876.399,
        "duration": 2.201,
        "text": "on"
      },
      {
        "start": 1879.799,
        "duration": 5.36,
        "text": "similarity after the documents are"
      },
      {
        "start": 1882.32,
        "duration": 6.199,
        "text": "chunked next step is to embed the"
      },
      {
        "start": 1885.159,
        "duration": 5.76,
        "text": "documents and unstructured doesn't uh"
      },
      {
        "start": 1888.519,
        "duration": 4.4,
        "text": "host embedding models but we integrate"
      },
      {
        "start": 1890.919,
        "duration": 4.801,
        "text": "with a number of embedding model"
      },
      {
        "start": 1892.919,
        "duration": 4.76,
        "text": "providers here I'm using hugging phase"
      },
      {
        "start": 1895.72,
        "duration": 7.12,
        "text": "but you can also use embedding models"
      },
      {
        "start": 1897.679,
        "duration": 6.041,
        "text": "from open AI uh verx AI octo Ai and a"
      },
      {
        "start": 1902.84,
        "duration": 3.799,
        "text": "few"
      },
      {
        "start": 1903.72,
        "duration": 5.52,
        "text": "others depending on your provider you"
      },
      {
        "start": 1906.639,
        "duration": 6.081,
        "text": "may need to provide your open sorry your"
      },
      {
        "start": 1909.24,
        "duration": 6.36,
        "text": "key like open AI key for example uh for"
      },
      {
        "start": 1912.72,
        "duration": 6.199,
        "text": "hugging phas we just need to give it the"
      },
      {
        "start": 1915.6,
        "duration": 5.72,
        "text": "name of the embedding model"
      },
      {
        "start": 1918.919,
        "duration": 4.161,
        "text": "and fantastic um sorry to interrupt you"
      },
      {
        "start": 1921.32,
        "duration": 4.599,
        "text": "but there's a question here from"
      },
      {
        "start": 1923.08,
        "duration": 5.599,
        "text": "automate Labs that um they say it's the"
      },
      {
        "start": 1925.919,
        "duration": 4.921,
        "text": "perfect time to ask um which is can I"
      },
      {
        "start": 1928.679,
        "duration": 3.801,
        "text": "define so they want to know can can they"
      },
      {
        "start": 1930.84,
        "duration": 3.92,
        "text": "Define some elements themselves like"
      },
      {
        "start": 1932.48,
        "duration": 3.88,
        "text": "some titles so the chunking will respect"
      },
      {
        "start": 1934.76,
        "duration": 5.12,
        "text": "their you know quote unquote options of"
      },
      {
        "start": 1936.36,
        "duration": 6.799,
        "text": "titles and chunking by titles for"
      },
      {
        "start": 1939.88,
        "duration": 5.919,
        "text": "example uh no so the chunking strategy"
      },
      {
        "start": 1943.159,
        "duration": 6.921,
        "text": "by title it's"
      },
      {
        "start": 1945.799,
        "duration": 8.681,
        "text": "um the title means just a section so we"
      },
      {
        "start": 1950.08,
        "duration": 7.68,
        "text": "we def we identify which document ele"
      },
      {
        "start": 1954.48,
        "duration": 5.799,
        "text": "elements are section titles and then we"
      },
      {
        "start": 1957.76,
        "duration": 3.96,
        "text": "respect that structure and hierarchy of"
      },
      {
        "start": 1960.279,
        "duration": 7.201,
        "text": "the"
      },
      {
        "start": 1961.72,
        "duration": 7.16,
        "text": "document uh you would have to um no I"
      },
      {
        "start": 1967.48,
        "duration": 4.76,
        "text": "don't think there's a straightforward"
      },
      {
        "start": 1968.88,
        "duration": 6.2,
        "text": "way to to modify"
      },
      {
        "start": 1972.24,
        "duration": 5.919,
        "text": "that perfect thank you sorry again sorry"
      },
      {
        "start": 1975.08,
        "duration": 6.439,
        "text": "to interrupt but they This was um the"
      },
      {
        "start": 1978.159,
        "duration": 6.161,
        "text": "timing thank you I'm happy to answer any"
      },
      {
        "start": 1981.519,
        "duration": 6.361,
        "text": "more questions if if uh if you"
      },
      {
        "start": 1984.32,
        "duration": 5.04,
        "text": "have if not I'll just continue yeah let"
      },
      {
        "start": 1987.88,
        "duration": 4.36,
        "text": "let's continue um because we also want"
      },
      {
        "start": 1989.36,
        "duration": 5.84,
        "text": "to um see the the destination of this um"
      },
      {
        "start": 1992.24,
        "duration": 5.84,
        "text": "from from yeah and that's the next step"
      },
      {
        "start": 1995.2,
        "duration": 5.16,
        "text": "so once the documents are embedded and"
      },
      {
        "start": 1998.08,
        "duration": 5.12,
        "text": "by the way chunkin step and embedding"
      },
      {
        "start": 2000.36,
        "duration": 4.799,
        "text": "step are optional so if you just want to"
      },
      {
        "start": 2003.2,
        "duration": 4.68,
        "text": "pre-process a whole bunch of PDFs and"
      },
      {
        "start": 2005.159,
        "duration": 5.041,
        "text": "then load them somewhere and sech"
      },
      {
        "start": 2007.88,
        "duration": 4.0,
        "text": "through them if rag is not your use case"
      },
      {
        "start": 2010.2,
        "duration": 4.28,
        "text": "you don't have to do chunking and"
      },
      {
        "start": 2011.88,
        "duration": 4.72,
        "text": "embedding but these are essential steps"
      },
      {
        "start": 2014.48,
        "duration": 4.64,
        "text": "for a rag pipeline"
      },
      {
        "start": 2016.6,
        "duration": 6.6,
        "text": "specifically and we can load the results"
      },
      {
        "start": 2019.12,
        "duration": 6.72,
        "text": "into Aster DB and for that we need these"
      },
      {
        "start": 2023.2,
        "duration": 5.04,
        "text": "three configs connection config where"
      },
      {
        "start": 2025.84,
        "duration": 6.439,
        "text": "you set your authentication"
      },
      {
        "start": 2028.24,
        "duration": 6.399,
        "text": "options upload Stager uh will check that"
      },
      {
        "start": 2032.279,
        "duration": 5.12,
        "text": "everything is okay with your data and"
      },
      {
        "start": 2034.639,
        "duration": 5.92,
        "text": "then ASB uploader config this is where"
      },
      {
        "start": 2037.399,
        "duration": 6.481,
        "text": "your specify the name space and the"
      },
      {
        "start": 2040.559,
        "duration": 4.321,
        "text": "collection uh where you want to upload"
      },
      {
        "start": 2043.88,
        "duration": 3.92,
        "text": "the"
      },
      {
        "start": 2044.88,
        "duration": 5.84,
        "text": "results and that's it so it may look"
      },
      {
        "start": 2047.8,
        "duration": 6.879,
        "text": "intimidating at first but this is all it"
      },
      {
        "start": 2050.72,
        "duration": 8.04,
        "text": "takes to build a full end to end ETL"
      },
      {
        "start": 2054.679,
        "duration": 8.121,
        "text": "pipeline from any of the 20 sources to"
      },
      {
        "start": 2058.76,
        "duration": 7.24,
        "text": "uh the destination of your choice with"
      },
      {
        "start": 2062.8,
        "duration": 7.92,
        "text": "uh partitioning and chunking and"
      },
      {
        "start": 2066.0,
        "duration": 7.2,
        "text": "embedding um all taken care of so um"
      },
      {
        "start": 2070.72,
        "duration": 4.84,
        "text": "takes a moment to get used to but when"
      },
      {
        "start": 2073.2,
        "duration": 3.879,
        "text": "you see it as a bunch of Lego breaks it"
      },
      {
        "start": 2075.56,
        "duration": 4.0,
        "text": "becomes much"
      },
      {
        "start": 2077.079,
        "duration": 5.6,
        "text": "easier fantastic thank you so much for"
      },
      {
        "start": 2079.56,
        "duration": 5.2,
        "text": "that presentation and it's you know it I"
      },
      {
        "start": 2082.679,
        "duration": 3.92,
        "text": "don't even know python um but that that"
      },
      {
        "start": 2084.76,
        "duration": 4.24,
        "text": "looks approachable to me which which is"
      },
      {
        "start": 2086.599,
        "duration": 5.08,
        "text": "just a testament of how intuitive um it"
      },
      {
        "start": 2089.0,
        "duration": 4.359,
        "text": "really is so thank you again for that um"
      },
      {
        "start": 2091.679,
        "duration": 3.561,
        "text": "there's a question by by James here in"
      },
      {
        "start": 2093.359,
        "duration": 3.56,
        "text": "the comments who's asking so the"
      },
      {
        "start": 2095.24,
        "duration": 3.96,
        "text": "embedding step is really just the"
      },
      {
        "start": 2096.919,
        "duration": 4.601,
        "text": "creation of the VOR and James the answer"
      },
      {
        "start": 2099.2,
        "duration": 4.32,
        "text": "is correct there's a specialized class"
      },
      {
        "start": 2101.52,
        "duration": 4.0,
        "text": "of machine learning models that um they"
      },
      {
        "start": 2103.52,
        "duration": 4.4,
        "text": "literally embed they embed natural"
      },
      {
        "start": 2105.52,
        "duration": 3.96,
        "text": "language in high-dimensional space and"
      },
      {
        "start": 2107.92,
        "duration": 3.679,
        "text": "so they just tend to group semantic"
      },
      {
        "start": 2109.48,
        "duration": 3.8,
        "text": "meaning words closer to each other in"
      },
      {
        "start": 2111.599,
        "duration": 4.401,
        "text": "that space than words that are further"
      },
      {
        "start": 2113.28,
        "duration": 4.52,
        "text": "away um but yes they just embed words in"
      },
      {
        "start": 2116.0,
        "duration": 3.359,
        "text": "space and the output of those is Vector"
      },
      {
        "start": 2117.8,
        "duration": 3.48,
        "text": "embeddings which is just a big list of"
      },
      {
        "start": 2119.359,
        "duration": 4.641,
        "text": "numbers um usually the models have a"
      },
      {
        "start": 2121.28,
        "duration": 4.799,
        "text": "deterministic length um so um thank you"
      },
      {
        "start": 2124.0,
        "duration": 4.28,
        "text": "so much for that Maria and and it's a"
      },
      {
        "start": 2126.079,
        "duration": 3.401,
        "text": "really one question I have and this is"
      },
      {
        "start": 2128.28,
        "duration": 4.319,
        "text": "not in the comments it's just me being"
      },
      {
        "start": 2129.48,
        "duration": 6.44,
        "text": "curious mainly because I'm um kind of a"
      },
      {
        "start": 2132.599,
        "duration": 5.121,
        "text": "novice at python is there an alternate"
      },
      {
        "start": 2135.92,
        "duration": 3.24,
        "text": "are there different language sdks for"
      },
      {
        "start": 2137.72,
        "duration": 4.08,
        "text": "unstructured meaning if I wanted to do"
      },
      {
        "start": 2139.16,
        "duration": 6.64,
        "text": "this in JavaScript could I or"
      },
      {
        "start": 2141.8,
        "duration": 8.36,
        "text": "yet yes you can you uh well not for"
      },
      {
        "start": 2145.8,
        "duration": 8.2,
        "text": "ingest ingest pipelines are uh only in"
      },
      {
        "start": 2150.16,
        "duration": 5.919,
        "text": "Python you can process files one by one"
      },
      {
        "start": 2154.0,
        "duration": 4.76,
        "text": "with a JavaScript SDK we have a"
      },
      {
        "start": 2156.079,
        "duration": 5.881,
        "text": "JavaScript SDK for that uh and you can"
      },
      {
        "start": 2158.76,
        "duration": 5.559,
        "text": "also send files one by one to uh the"
      },
      {
        "start": 2161.96,
        "duration": 3.8,
        "text": "endpoint directly there's uh you can"
      },
      {
        "start": 2164.319,
        "duration": 5.441,
        "text": "just send post"
      },
      {
        "start": 2165.76,
        "duration": 7.0,
        "text": "requests um I personally prefer the"
      },
      {
        "start": 2169.76,
        "duration": 5.8,
        "text": "injust pipeline because a I like Python"
      },
      {
        "start": 2172.76,
        "duration": 7.24,
        "text": "and B it's just the simplest way you"
      },
      {
        "start": 2175.56,
        "duration": 8.2,
        "text": "have this very easy way to build the"
      },
      {
        "start": 2180.0,
        "duration": 6.28,
        "text": "whole pipeline by just uh configuring"
      },
      {
        "start": 2183.76,
        "duration": 5.76,
        "text": "the steps essentially where do you start"
      },
      {
        "start": 2186.28,
        "duration": 6.88,
        "text": "what do you do and where do you land and"
      },
      {
        "start": 2189.52,
        "duration": 6.12,
        "text": "um it's just the the the most um"
      },
      {
        "start": 2193.16,
        "duration": 3.56,
        "text": "userfriendly way for me to to build"
      },
      {
        "start": 2195.64,
        "duration": 3.56,
        "text": "these"
      },
      {
        "start": 2196.72,
        "duration": 5.08,
        "text": "pipelines fantastic great thank you so"
      },
      {
        "start": 2199.2,
        "duration": 4.36,
        "text": "much and that everybody is how"
      },
      {
        "start": 2201.8,
        "duration": 4.6,
        "text": "practically you can use unstructured to"
      },
      {
        "start": 2203.56,
        "duration": 7.16,
        "text": "go from a bunch of PDFs or Excel sheets"
      },
      {
        "start": 2206.4,
        "duration": 6.6,
        "text": "or soon we were told movies um to"
      },
      {
        "start": 2210.72,
        "duration": 4.639,
        "text": "embeddings to then a vector database and"
      },
      {
        "start": 2213.0,
        "duration": 3.88,
        "text": "rag um and again as as Maria said if if"
      },
      {
        "start": 2215.359,
        "duration": 3.201,
        "text": "if rag is not your use case you could"
      },
      {
        "start": 2216.88,
        "duration": 3.6,
        "text": "just cut it off at the at the step where"
      },
      {
        "start": 2218.56,
        "duration": 4.759,
        "text": "you have a bunch of Json and process"
      },
      {
        "start": 2220.48,
        "duration": 5.639,
        "text": "that however you want um great so now"
      },
      {
        "start": 2223.319,
        "duration": 5.681,
        "text": "that the things uploaded into ASB let's"
      },
      {
        "start": 2226.119,
        "duration": 5.601,
        "text": "say with that step um I'd love to invoke"
      },
      {
        "start": 2229.0,
        "duration": 4.68,
        "text": "the Eric uh my my colleague and and pick"
      },
      {
        "start": 2231.72,
        "duration": 5.08,
        "text": "it up from there and and turn it into an"
      },
      {
        "start": 2233.68,
        "duration": 5.6,
        "text": "actual endtoend um rag application so we"
      },
      {
        "start": 2236.8,
        "duration": 4.84,
        "text": "extracted a bunch of data from PDFs we"
      },
      {
        "start": 2239.28,
        "duration": 5.039,
        "text": "followed this Pipeline and data is now"
      },
      {
        "start": 2241.64,
        "duration": 6.04,
        "text": "in astb um Eric how might we then use"
      },
      {
        "start": 2244.319,
        "duration": 6.681,
        "text": "this to finish our rag application yeah"
      },
      {
        "start": 2247.68,
        "duration": 5.439,
        "text": "yeah so um if if if it's okay with with"
      },
      {
        "start": 2251.0,
        "duration": 5.64,
        "text": "everyone what I'll do is I'll show maybe"
      },
      {
        "start": 2253.119,
        "duration": 6.48,
        "text": "two two kind of um I guess demo"
      },
      {
        "start": 2256.64,
        "duration": 4.28,
        "text": "ingest examples that that are kind of"
      },
      {
        "start": 2259.599,
        "duration": 3.161,
        "text": "you know once you have your data in"
      },
      {
        "start": 2260.92,
        "duration": 4.56,
        "text": "astrab you've gone through this pipeline"
      },
      {
        "start": 2262.76,
        "duration": 4.76,
        "text": "that Maria showed um building a rag"
      },
      {
        "start": 2265.48,
        "duration": 4.52,
        "text": "application off of that and one is a"
      },
      {
        "start": 2267.52,
        "duration": 5.2,
        "text": "code based solution and one is a I guess"
      },
      {
        "start": 2270.0,
        "duration": 7.88,
        "text": "what we'd call a low code based solution"
      },
      {
        "start": 2272.72,
        "duration": 5.16,
        "text": "so um so let me go ahead and share"
      },
      {
        "start": 2278.079,
        "duration": 5.841,
        "text": "could we um can I can I ask for can I"
      },
      {
        "start": 2282.2,
        "duration": 4.96,
        "text": "express a preference for which one we do"
      },
      {
        "start": 2283.92,
        "duration": 6.08,
        "text": "first you have a plan oh great um if we"
      },
      {
        "start": 2287.16,
        "duration": 4.28,
        "text": "could do the the low code first just"
      },
      {
        "start": 2290.0,
        "duration": 3.56,
        "text": "because we can start at the most"
      },
      {
        "start": 2291.44,
        "duration": 3.72,
        "text": "accessible to the most people assuming"
      },
      {
        "start": 2293.56,
        "duration": 3.32,
        "text": "vast majority of people don't write a"
      },
      {
        "start": 2295.16,
        "duration": 4.8,
        "text": "lot of code and then we can go to the"
      },
      {
        "start": 2296.88,
        "duration": 4.92,
        "text": "code one provided there's enough time"
      },
      {
        "start": 2299.96,
        "duration": 4.359,
        "text": "cool so I'm getting a message that says"
      },
      {
        "start": 2301.8,
        "duration": 7.24,
        "text": "the screen is actively being shared ask"
      },
      {
        "start": 2304.319,
        "duration": 7.8,
        "text": "the presenter to stop sharing um"
      },
      {
        "start": 2309.04,
        "duration": 6.92,
        "text": "is is there any sharing going"
      },
      {
        "start": 2312.119,
        "duration": 6.121,
        "text": "on there's no one sharing um at least"
      },
      {
        "start": 2315.96,
        "duration": 5.0,
        "text": "that I can see um but crowdcast maybe is"
      },
      {
        "start": 2318.24,
        "duration": 5.359,
        "text": "having some issues um no I I don't think"
      },
      {
        "start": 2320.96,
        "duration": 4.879,
        "text": "anyone's sharing right now okay then"
      },
      {
        "start": 2323.599,
        "duration": 4.161,
        "text": "maybe I'll hop back on briefly and see"
      },
      {
        "start": 2325.839,
        "duration": 4.081,
        "text": "if I can get that"
      },
      {
        "start": 2327.76,
        "duration": 4.319,
        "text": "working yeah that's that's okay we"
      },
      {
        "start": 2329.92,
        "duration": 3.52,
        "text": "appreciate that you guys want to talk"
      },
      {
        "start": 2332.079,
        "duration": 4.76,
        "text": "for"
      },
      {
        "start": 2333.44,
        "duration": 4.56,
        "text": "30 he just like left yes there's a"
      },
      {
        "start": 2336.839,
        "duration": 2.201,
        "text": "question in in the audience we can"
      },
      {
        "start": 2338.0,
        "duration": 3.72,
        "text": "actually this is the perfect time to"
      },
      {
        "start": 2339.04,
        "duration": 5.559,
        "text": "take some of those um there was a"
      },
      {
        "start": 2341.72,
        "duration": 5.0,
        "text": "question about um using a domain"
      },
      {
        "start": 2344.599,
        "duration": 4.121,
        "text": "specific is it better to use and this is"
      },
      {
        "start": 2346.72,
        "duration": 3.92,
        "text": "a question from eera I hope I'm not"
      },
      {
        "start": 2348.72,
        "duration": 3.68,
        "text": "rooting this name or mispronouncing it"
      },
      {
        "start": 2350.64,
        "duration": 4.32,
        "text": "is it better to use a domain specific AI"
      },
      {
        "start": 2352.4,
        "duration": 5.32,
        "text": "model as an embed embedded I assume they"
      },
      {
        "start": 2354.96,
        "duration": 4.159,
        "text": "mean embedding the model um I'm not sure"
      },
      {
        "start": 2357.72,
        "duration": 2.96,
        "text": "what I understand the question because I"
      },
      {
        "start": 2359.119,
        "duration": 3.161,
        "text": "don't know if I don't know what a domain"
      },
      {
        "start": 2360.68,
        "duration": 2.76,
        "text": "specific AI model is Maria do you have"
      },
      {
        "start": 2362.28,
        "duration": 4.799,
        "text": "an answer to"
      },
      {
        "start": 2363.44,
        "duration": 5.52,
        "text": "this yes I believe uh the question is so"
      },
      {
        "start": 2367.079,
        "duration": 3.801,
        "text": "when you have embedding metal embedding"
      },
      {
        "start": 2368.96,
        "duration": 5.04,
        "text": "models you"
      },
      {
        "start": 2370.88,
        "duration": 5.84,
        "text": "can go especially for open source models"
      },
      {
        "start": 2374.0,
        "duration": 5.04,
        "text": "for example you can go to uh there's a"
      },
      {
        "start": 2376.72,
        "duration": 4.639,
        "text": "litter board by hugging face that shows"
      },
      {
        "start": 2379.04,
        "duration": 5.079,
        "text": "the metrics on academic benchmarks how"
      },
      {
        "start": 2381.359,
        "duration": 6.201,
        "text": "they perform and you may there are a lot"
      },
      {
        "start": 2384.119,
        "duration": 6.281,
        "text": "of general purpose embedding models that"
      },
      {
        "start": 2387.56,
        "duration": 5.2,
        "text": "were trained on"
      },
      {
        "start": 2390.4,
        "duration": 4.719,
        "text": "um just text from the internet"
      },
      {
        "start": 2392.76,
        "duration": 5.2,
        "text": "essentially but let's say if you're"
      },
      {
        "start": 2395.119,
        "duration": 6.321,
        "text": "trying to use that kind of model"
      },
      {
        "start": 2397.96,
        "duration": 5.08,
        "text": "on legal documents there can be a lot of"
      },
      {
        "start": 2401.44,
        "duration": 3.32,
        "text": "very specific"
      },
      {
        "start": 2403.04,
        "duration": 5.079,
        "text": "terminology"
      },
      {
        "start": 2404.76,
        "duration": 5.72,
        "text": "abbreviations and it may not necessarily"
      },
      {
        "start": 2408.119,
        "duration": 5.761,
        "text": "do a generic model may not necessarily"
      },
      {
        "start": 2410.48,
        "duration": 5.56,
        "text": "do as well on those documents so ideally"
      },
      {
        "start": 2413.88,
        "duration": 5.16,
        "text": "if you have an embedding model that was"
      },
      {
        "start": 2416.04,
        "duration": 6.96,
        "text": "tailored or fine-tuned to a"
      },
      {
        "start": 2419.04,
        "duration": 6.319,
        "text": "domain it will perform better"
      },
      {
        "start": 2423.0,
        "duration": 4.48,
        "text": "yes fantastic thank you so much and look"
      },
      {
        "start": 2425.359,
        "duration": 4.681,
        "text": "we have Eric's screen share finally was"
      },
      {
        "start": 2427.48,
        "duration": 4.599,
        "text": "perfect way to to fill some some"
      },
      {
        "start": 2430.04,
        "duration": 4.799,
        "text": "downtime hey Eric um yeah let's go how"
      },
      {
        "start": 2432.079,
        "duration": 5.28,
        "text": "do we finish the application yeah so um"
      },
      {
        "start": 2434.839,
        "duration": 3.881,
        "text": "I I want to um emphasize by the way that"
      },
      {
        "start": 2437.359,
        "duration": 3.801,
        "text": "a lot of what you're about to see is"
      },
      {
        "start": 2438.72,
        "duration": 4.8,
        "text": "under very heavy development so this is"
      },
      {
        "start": 2441.16,
        "duration": 4.52,
        "text": "kind of like a preview of of a way you"
      },
      {
        "start": 2443.52,
        "duration": 4.04,
        "text": "can do it it already is working end to"
      },
      {
        "start": 2445.68,
        "duration": 4.24,
        "text": "end in a way that I think will be"
      },
      {
        "start": 2447.56,
        "duration": 5.48,
        "text": "impressive but um but there's still work"
      },
      {
        "start": 2449.92,
        "duration": 5.159,
        "text": "to do to make it like handle more cases"
      },
      {
        "start": 2453.04,
        "duration": 5.12,
        "text": "with that said um what you're looking at"
      },
      {
        "start": 2455.079,
        "duration": 6.76,
        "text": "here is what's called Lang Lang flow um"
      },
      {
        "start": 2458.16,
        "duration": 8.48,
        "text": "if you're unfamiliar with it think of"
      },
      {
        "start": 2461.839,
        "duration": 7.48,
        "text": "L AI pipeline a rag pipeline really a a"
      },
      {
        "start": 2466.64,
        "duration": 4.719,
        "text": "pretty significant um number of"
      },
      {
        "start": 2469.319,
        "duration": 5.28,
        "text": "components that they have available and"
      },
      {
        "start": 2471.359,
        "duration": 4.96,
        "text": "chaining the different outputs together"
      },
      {
        "start": 2474.599,
        "duration": 4.161,
        "text": "such that you're building that pipeline"
      },
      {
        "start": 2476.319,
        "duration": 4.28,
        "text": "almost in a graphical way that's like a"
      },
      {
        "start": 2478.76,
        "duration": 4.0,
        "text": "really highlevel way of saying what it"
      },
      {
        "start": 2480.599,
        "duration": 4.121,
        "text": "does um but I'll show you this little"
      },
      {
        "start": 2482.76,
        "duration": 4.12,
        "text": "pipeline this is actually like by the"
      },
      {
        "start": 2484.72,
        "duration": 5.24,
        "text": "standards of some of the ones that that"
      },
      {
        "start": 2486.88,
        "duration": 6.04,
        "text": "exist in Lang flow maybe on the simpler"
      },
      {
        "start": 2489.96,
        "duration": 7.28,
        "text": "side but I think it's how we can do this"
      },
      {
        "start": 2492.92,
        "duration": 6.76,
        "text": "end to end step that um that basically"
      },
      {
        "start": 2497.24,
        "duration": 5.079,
        "text": "uh is is what what what we're trying to"
      },
      {
        "start": 2499.68,
        "duration": 5.2,
        "text": "do to build this rag application so we"
      },
      {
        "start": 2502.319,
        "duration": 4.52,
        "text": "have an unstructured component um what"
      },
      {
        "start": 2504.88,
        "duration": 4.56,
        "text": "this component is going to do is it's"
      },
      {
        "start": 2506.839,
        "duration": 5.321,
        "text": "abstracting away that python code and"
      },
      {
        "start": 2509.44,
        "duration": 5.0,
        "text": "instead what it's asking for is a file"
      },
      {
        "start": 2512.16,
        "duration": 5.159,
        "text": "and an API key kind of like what Maria"
      },
      {
        "start": 2514.44,
        "duration": 5.12,
        "text": "was showing you um the components of the"
      },
      {
        "start": 2517.319,
        "duration": 5.681,
        "text": "pipeline are kind of hidden right now"
      },
      {
        "start": 2519.56,
        "duration": 5.96,
        "text": "like that's an advanced feature so it's"
      },
      {
        "start": 2523.0,
        "duration": 5.72,
        "text": "and obviously that can be exposed but"
      },
      {
        "start": 2525.52,
        "duration": 5.319,
        "text": "what happens is that file gets passed in"
      },
      {
        "start": 2528.72,
        "duration": 6.08,
        "text": "um a number of files get passed in and"
      },
      {
        "start": 2530.839,
        "duration": 6.881,
        "text": "this data then gets stored into astrab"
      },
      {
        "start": 2534.8,
        "duration": 5.279,
        "text": "in a vector store so basically we have"
      },
      {
        "start": 2537.72,
        "duration": 3.72,
        "text": "all these uh you know what what we're"
      },
      {
        "start": 2540.079,
        "duration": 3.681,
        "text": "looking at here is those different"
      },
      {
        "start": 2541.44,
        "duration": 4.96,
        "text": "narrative texts um you can view the"
      },
      {
        "start": 2543.76,
        "duration": 5.64,
        "text": "actual text of it um unstructured is"
      },
      {
        "start": 2546.4,
        "duration": 4.959,
        "text": "doing all the processing of this um and"
      },
      {
        "start": 2549.4,
        "duration": 4.719,
        "text": "then what we can do is we can start to"
      },
      {
        "start": 2551.359,
        "duration": 5.76,
        "text": "actually build a rag pipeline so this"
      },
      {
        "start": 2554.119,
        "duration": 5.521,
        "text": "one is taking the data that was just"
      },
      {
        "start": 2557.119,
        "duration": 4.24,
        "text": "outputed by unstructured and saying Hey"
      },
      {
        "start": 2559.64,
        "duration": 3.959,
        "text": "I want to do some kind of template"
      },
      {
        "start": 2561.359,
        "duration": 4.521,
        "text": "transformation on that data in this case"
      },
      {
        "start": 2563.599,
        "duration": 4.0,
        "text": "we're keeping it as simple as possible"
      },
      {
        "start": 2565.88,
        "duration": 4.04,
        "text": "all we're saying is we want to pull the"
      },
      {
        "start": 2567.599,
        "duration": 4.52,
        "text": "text field from that data but you could"
      },
      {
        "start": 2569.92,
        "duration": 3.76,
        "text": "imagine you can construct a template"
      },
      {
        "start": 2572.119,
        "duration": 4.44,
        "text": "that does all kinds of interesting"
      },
      {
        "start": 2573.68,
        "duration": 6.32,
        "text": "things like maybe it's tagged by some"
      },
      {
        "start": 2576.559,
        "duration": 6.0,
        "text": "parameter of the got extracted um which"
      },
      {
        "start": 2580.0,
        "duration": 4.72,
        "text": "by the way could be far more I guess"
      },
      {
        "start": 2582.559,
        "duration": 5.081,
        "text": "like indepth in this declaration of"
      },
      {
        "start": 2584.72,
        "duration": 4.0,
        "text": "independence example that I'm showing um"
      },
      {
        "start": 2587.64,
        "duration": 2.84,
        "text": "but right now we're just going to"
      },
      {
        "start": 2588.72,
        "duration": 3.72,
        "text": "extract the text so we're saying"
      },
      {
        "start": 2590.48,
        "duration": 3.879,
        "text": "unstructured Parts is the data stores it"
      },
      {
        "start": 2592.44,
        "duration": 3.8,
        "text": "into Astra and we're going to build a"
      },
      {
        "start": 2594.359,
        "duration": 4.601,
        "text": "template that's just the text that gets"
      },
      {
        "start": 2596.24,
        "duration": 4.52,
        "text": "extracted so you can see what we get out"
      },
      {
        "start": 2598.96,
        "duration": 3.44,
        "text": "of that is essentially here's the"
      },
      {
        "start": 2600.76,
        "duration": 3.799,
        "text": "context for some queries that you might"
      },
      {
        "start": 2602.4,
        "duration": 3.88,
        "text": "want to run doesn't have to just be the"
      },
      {
        "start": 2604.559,
        "duration": 4.241,
        "text": "full text of the document it can be a"
      },
      {
        "start": 2606.28,
        "duration": 4.88,
        "text": "lot more interesting than"
      },
      {
        "start": 2608.8,
        "duration": 4.84,
        "text": "that what we can do is we can start to"
      },
      {
        "start": 2611.16,
        "duration": 5.72,
        "text": "build the rag application so we're going"
      },
      {
        "start": 2613.64,
        "duration": 5.56,
        "text": "to pipe that text in to the"
      },
      {
        "start": 2616.88,
        "duration": 4.0,
        "text": "document that is going to underline The"
      },
      {
        "start": 2619.2,
        "duration": 4.6,
        "text": "Prompt template that we're going to"
      },
      {
        "start": 2620.88,
        "duration": 5.199,
        "text": "create so once again we have all kinds"
      },
      {
        "start": 2623.8,
        "duration": 4.12,
        "text": "of flexibility and what we build here"
      },
      {
        "start": 2626.079,
        "duration": 4.52,
        "text": "this is a really simple one we're just"
      },
      {
        "start": 2627.92,
        "duration": 5.159,
        "text": "instructing the llm answer the users's"
      },
      {
        "start": 2630.599,
        "duration": 4.641,
        "text": "question based on the document provided"
      },
      {
        "start": 2633.079,
        "duration": 4.561,
        "text": "here's the user's question give me your"
      },
      {
        "start": 2635.24,
        "duration": 5.119,
        "text": "answer again we can make that much more"
      },
      {
        "start": 2637.64,
        "duration": 5.679,
        "text": "interesting if we wanted to um where the"
      },
      {
        "start": 2640.359,
        "duration": 5.2,
        "text": "power really starts to come into play so"
      },
      {
        "start": 2643.319,
        "duration": 4.441,
        "text": "now um I can type the question and I"
      },
      {
        "start": 2645.559,
        "duration": 4.921,
        "text": "said just what is this document about at"
      },
      {
        "start": 2647.76,
        "duration": 5.2,
        "text": "a high level so just to give it some"
      },
      {
        "start": 2650.48,
        "duration": 5.56,
        "text": "creative freedom to answer a question"
      },
      {
        "start": 2652.96,
        "duration": 6.879,
        "text": "and now this prompt template what we're"
      },
      {
        "start": 2656.04,
        "duration": 5.76,
        "text": "do is we're GNA pipe that in to open AI"
      },
      {
        "start": 2659.839,
        "duration": 6.081,
        "text": "basically what we're doing is taking the"
      },
      {
        "start": 2661.8,
        "duration": 7.36,
        "text": "question using my open a a API key open"
      },
      {
        "start": 2665.92,
        "duration": 5.88,
        "text": "AI key to then generate the embeddings"
      },
      {
        "start": 2669.16,
        "duration": 4.8,
        "text": "and compare it to what unstructured"
      },
      {
        "start": 2671.8,
        "duration": 4.64,
        "text": "produced in order to help answer that"
      },
      {
        "start": 2673.96,
        "duration": 5.84,
        "text": "question and then the last step is we"
      },
      {
        "start": 2676.44,
        "duration": 5.76,
        "text": "get an output so um all those are"
      },
      {
        "start": 2679.8,
        "duration": 4.799,
        "text": "chained together let's see what happens"
      },
      {
        "start": 2682.2,
        "duration": 4.68,
        "text": "we get a response from the model and it"
      },
      {
        "start": 2684.599,
        "duration": 5.52,
        "text": "says this document is the Declaration of"
      },
      {
        "start": 2686.88,
        "duration": 5.88,
        "text": "Independence adopted in 1776 and you'll"
      },
      {
        "start": 2690.119,
        "duration": 4.561,
        "text": "notice it's very specific to say at a"
      },
      {
        "start": 2692.76,
        "duration": 5.319,
        "text": "high level here's some of the things"
      },
      {
        "start": 2694.68,
        "duration": 5.679,
        "text": "that it says so so the more"
      },
      {
        "start": 2698.079,
        "duration": 5.0,
        "text": "what we did here is we started with"
      },
      {
        "start": 2700.359,
        "duration": 5.401,
        "text": "literally just uploading a file and we"
      },
      {
        "start": 2703.079,
        "duration": 4.401,
        "text": "built a pipeline where you can in this"
      },
      {
        "start": 2705.76,
        "duration": 3.88,
        "text": "case it's a really simple question"
      },
      {
        "start": 2707.48,
        "duration": 4.24,
        "text": "answer prompt but you can imagine"
      },
      {
        "start": 2709.64,
        "duration": 4.52,
        "text": "there's so much we can do here to make"
      },
      {
        "start": 2711.72,
        "duration": 4.839,
        "text": "it more interesting if we wanted to and"
      },
      {
        "start": 2714.16,
        "duration": 5.76,
        "text": "that's all done graphically by chaining"
      },
      {
        "start": 2716.559,
        "duration": 5.28,
        "text": "together these inputs in Lang flow um I"
      },
      {
        "start": 2719.92,
        "duration": 4.84,
        "text": "didn't have to do any code I just had to"
      },
      {
        "start": 2721.839,
        "duration": 6.24,
        "text": "kind of gather my API keys and put them"
      },
      {
        "start": 2724.76,
        "duration": 5.44,
        "text": "in the right place right so um that's"
      },
      {
        "start": 2728.079,
        "duration": 4.601,
        "text": "what we think is like you know you can"
      },
      {
        "start": 2730.2,
        "duration": 4.44,
        "text": "build a code-based solution but you"
      },
      {
        "start": 2732.68,
        "duration": 5.12,
        "text": "don't have to if something like this is"
      },
      {
        "start": 2734.64,
        "duration": 5.32,
        "text": "more friendly to you to start building"
      },
      {
        "start": 2737.8,
        "duration": 4.2,
        "text": "your pipeline you basically have at your"
      },
      {
        "start": 2739.96,
        "duration": 5.32,
        "text": "fingertips a chat bot that we just"
      },
      {
        "start": 2742.0,
        "duration": 7.52,
        "text": "created from almost no code at all right"
      },
      {
        "start": 2745.28,
        "duration": 6.48,
        "text": "thanks to the power of both Astra and"
      },
      {
        "start": 2749.52,
        "duration": 5.079,
        "text": "unstructured that's a that's a great"
      },
      {
        "start": 2751.76,
        "duration": 5.44,
        "text": "point you just make um as as a result of"
      },
      {
        "start": 2754.599,
        "duration": 5.161,
        "text": "the power of Astra and unstructured in"
      },
      {
        "start": 2757.2,
        "duration": 4.8,
        "text": "this langlow diagram um what it looks"
      },
      {
        "start": 2759.76,
        "duration": 5.4,
        "text": "like we're doing is reading a text file"
      },
      {
        "start": 2762.0,
        "duration": 6.079,
        "text": "and then adding the entire text file to"
      },
      {
        "start": 2765.16,
        "duration": 4.959,
        "text": "The Prompt um in a true rag application"
      },
      {
        "start": 2768.079,
        "duration": 4.52,
        "text": "correct me if I'm wrong we would"
      },
      {
        "start": 2770.119,
        "duration": 5.881,
        "text": "retrieve chunks that are ranked based on"
      },
      {
        "start": 2772.599,
        "duration": 5.801,
        "text": "similarity to a user's query from a"
      },
      {
        "start": 2776.0,
        "duration": 4.44,
        "text": "vector store which we don't have here um"
      },
      {
        "start": 2778.4,
        "duration": 4.88,
        "text": "and then feed just the highest ranked"
      },
      {
        "start": 2780.44,
        "duration": 4.24,
        "text": "chunks aha there it is okay so there's"
      },
      {
        "start": 2783.28,
        "duration": 3.12,
        "text": "the AST okay this is what I was asking"
      },
      {
        "start": 2784.68,
        "duration": 3.6,
        "text": "where's the astb where's the vector"
      },
      {
        "start": 2786.4,
        "duration": 5.08,
        "text": "where's the similar he was the chunking"
      },
      {
        "start": 2788.28,
        "duration": 4.839,
        "text": "you you you're exactly right and um good"
      },
      {
        "start": 2791.48,
        "duration": 3.599,
        "text": "for calling me out on this because this"
      },
      {
        "start": 2793.119,
        "duration": 4.24,
        "text": "is a little bit for sake of just making"
      },
      {
        "start": 2795.079,
        "duration": 4.201,
        "text": "the the demo a little easier but the the"
      },
      {
        "start": 2797.359,
        "duration": 3.681,
        "text": "truth is you're right we wouldn't just"
      },
      {
        "start": 2799.28,
        "duration": 4.72,
        "text": "want to pull all the text we would have"
      },
      {
        "start": 2801.04,
        "duration": 5.44,
        "text": "a big database so instead we would pipe"
      },
      {
        "start": 2804.0,
        "duration": 5.8,
        "text": "this to Astra and then do some the"
      },
      {
        "start": 2806.48,
        "duration": 5.599,
        "text": "filter right here and do a retrieve"
      },
      {
        "start": 2809.8,
        "duration": 4.68,
        "text": "basically this Retriever with search"
      },
      {
        "start": 2812.079,
        "duration": 5.28,
        "text": "results that then get piped down to this"
      },
      {
        "start": 2814.48,
        "duration": 8.32,
        "text": "so we filter basically the inputs to the"
      },
      {
        "start": 2817.359,
        "duration": 7.801,
        "text": "most relevant um rows from the astb"
      },
      {
        "start": 2822.8,
        "duration": 4.559,
        "text": "table fantastic okay and that's that's"
      },
      {
        "start": 2825.16,
        "duration": 3.88,
        "text": "how so with unstructured we've uploaded"
      },
      {
        "start": 2827.359,
        "duration": 4.2,
        "text": "the chunks of let's say the Declaration"
      },
      {
        "start": 2829.04,
        "duration": 4.72,
        "text": "of Independence um maybe chunked by"
      },
      {
        "start": 2831.559,
        "duration": 4.56,
        "text": "every thousand characters or so and we"
      },
      {
        "start": 2833.76,
        "duration": 5.48,
        "text": "then ingested them into Astra and we"
      },
      {
        "start": 2836.119,
        "duration": 5.521,
        "text": "could search for something like um"
      },
      {
        "start": 2839.24,
        "duration": 4.92,
        "text": "Liberty and so when when a user types"
      },
      {
        "start": 2841.64,
        "duration": 4.56,
        "text": "this word that's converted into a vector"
      },
      {
        "start": 2844.16,
        "duration": 4.56,
        "text": "and it's compared for what vectors in"
      },
      {
        "start": 2846.2,
        "duration": 5.0,
        "text": "the database what chunks are closest to"
      },
      {
        "start": 2848.72,
        "duration": 4.68,
        "text": "this word and we get maybe the top five"
      },
      {
        "start": 2851.2,
        "duration": 5.84,
        "text": "we put them in a prompt and say here um"
      },
      {
        "start": 2853.4,
        "duration": 6.0,
        "text": "Mr or Mrs or non-binary llm um here's"
      },
      {
        "start": 2857.04,
        "duration": 4.96,
        "text": "the context answer the question is is"
      },
      {
        "start": 2859.4,
        "duration": 5.84,
        "text": "that representative of a true rag flow"
      },
      {
        "start": 2862.0,
        "duration": 5.24,
        "text": "that's absolutely right and um while I I"
      },
      {
        "start": 2865.24,
        "duration": 3.0,
        "text": "probably can't run it end to end I can"
      },
      {
        "start": 2867.24,
        "duration": 4.079,
        "text": "show you"
      },
      {
        "start": 2868.24,
        "duration": 6.68,
        "text": "the for the way that you interact with"
      },
      {
        "start": 2871.319,
        "duration": 5.081,
        "text": "langlow to do what what Tes just said is"
      },
      {
        "start": 2874.92,
        "duration": 4.04,
        "text": "essentially you say okay well I'm"
      },
      {
        "start": 2876.4,
        "duration": 4.84,
        "text": "outputting data here I I know what the"
      },
      {
        "start": 2878.96,
        "duration": 5.2,
        "text": "format of that data is I know what I can"
      },
      {
        "start": 2881.24,
        "duration": 6.319,
        "text": "connect it to to"
      },
      {
        "start": 2884.16,
        "duration": 6.12,
        "text": "build drag and drop a connection and say"
      },
      {
        "start": 2887.559,
        "duration": 5.04,
        "text": "okay instead of par going directly to"
      },
      {
        "start": 2890.28,
        "duration": 4.279,
        "text": "the parse text from unstructured We're"
      },
      {
        "start": 2892.599,
        "duration": 5.081,
        "text": "first going to ingest it and then do"
      },
      {
        "start": 2894.559,
        "duration": 5.0,
        "text": "some search query like liberty before we"
      },
      {
        "start": 2897.68,
        "duration": 4.08,
        "text": "pass those results to the next stage of"
      },
      {
        "start": 2899.559,
        "duration": 2.201,
        "text": "the"
      },
      {
        "start": 2902.599,
        "duration": 5.72,
        "text": "pipeline fantastic great um and I hope"
      },
      {
        "start": 2906.119,
        "duration": 4.881,
        "text": "for for the there was the question above"
      },
      {
        "start": 2908.319,
        "duration": 3.881,
        "text": "that this is exactly the one um we"
      },
      {
        "start": 2911.0,
        "duration": 3.599,
        "text": "answered I'm going to see if I can find"
      },
      {
        "start": 2912.2,
        "duration": 5.08,
        "text": "it here in the comments um because it"
      },
      {
        "start": 2914.599,
        "duration": 5.201,
        "text": "was so perfect um and I'm also going to"
      },
      {
        "start": 2917.28,
        "duration": 3.92,
        "text": "highlight it here um where they asked"
      },
      {
        "start": 2919.8,
        "duration": 2.64,
        "text": "practically what this would look like"
      },
      {
        "start": 2921.2,
        "duration": 3.52,
        "text": "and we just did that right we we"
      },
      {
        "start": 2922.44,
        "duration": 4.52,
        "text": "captured practically how you would"
      },
      {
        "start": 2924.72,
        "duration": 5.359,
        "text": "ingest a document with unstructured and"
      },
      {
        "start": 2926.96,
        "duration": 5.72,
        "text": "then expose it to an llm with Lang flow"
      },
      {
        "start": 2930.079,
        "duration": 5.561,
        "text": "um we have about 10 minutes left um Eric"
      },
      {
        "start": 2932.68,
        "duration": 4.879,
        "text": "you did mention you had two examples um"
      },
      {
        "start": 2935.64,
        "duration": 4.479,
        "text": "can we take a look at the other one"
      },
      {
        "start": 2937.559,
        "duration": 6.961,
        "text": "yeah the the more code focused version"
      },
      {
        "start": 2940.119,
        "duration": 6.0,
        "text": "yep correct um I'm gonna again it'll"
      },
      {
        "start": 2944.52,
        "duration": 3.2,
        "text": "just take 10 seconds because for"
      },
      {
        "start": 2946.119,
        "duration": 4.281,
        "text": "whatever reason it doesn't let me"
      },
      {
        "start": 2947.72,
        "duration": 7.2,
        "text": "reshare after someone's been sharing"
      },
      {
        "start": 2950.4,
        "duration": 6.719,
        "text": "including myself so I'll be right no"
      },
      {
        "start": 2954.92,
        "duration": 4.84,
        "text": "problem um while let's let's do this"
      },
      {
        "start": 2957.119,
        "duration": 5.801,
        "text": "dance again Maria um while Eric comes"
      },
      {
        "start": 2959.76,
        "duration": 4.64,
        "text": "back there was a question um from David"
      },
      {
        "start": 2962.92,
        "duration": 3.199,
        "text": "Edwards which I think is more for"
      },
      {
        "start": 2964.4,
        "duration": 3.0,
        "text": "langlow which maybe I could answer but I"
      },
      {
        "start": 2966.119,
        "duration": 3.361,
        "text": "I genuinely don't know he's asking can"
      },
      {
        "start": 2967.4,
        "duration": 4.64,
        "text": "you connect two documents to The Prompt"
      },
      {
        "start": 2969.48,
        "duration": 5.599,
        "text": "for example RFI response and evaluation"
      },
      {
        "start": 2972.04,
        "duration": 4.6,
        "text": "criteria now unstructured would give us"
      },
      {
        "start": 2975.079,
        "duration": 4.561,
        "text": "would take all the documents and just"
      },
      {
        "start": 2976.64,
        "duration": 6.12,
        "text": "put them in Astra so Astra here would be"
      },
      {
        "start": 2979.64,
        "duration": 6.4,
        "text": "responsible for retrieving any number of"
      },
      {
        "start": 2982.76,
        "duration": 6.76,
        "text": "related documents to the prom exactly so"
      },
      {
        "start": 2986.04,
        "duration": 6.039,
        "text": "let's say you uh you can have thousands"
      },
      {
        "start": 2989.52,
        "duration": 5.24,
        "text": "or millions of documents and then you"
      },
      {
        "start": 2992.079,
        "duration": 5.601,
        "text": "can have an a pro pre-processing"
      },
      {
        "start": 2994.76,
        "duration": 6.2,
        "text": "pipeline that ingests all of that"
      },
      {
        "start": 2997.68,
        "duration": 6.399,
        "text": "uh transforms them stores them and then"
      },
      {
        "start": 3000.96,
        "duration": 6.2,
        "text": "when you retrieve uh in Astra and you"
      },
      {
        "start": 3004.079,
        "duration": 5.161,
        "text": "use similarity search you can absolutely"
      },
      {
        "start": 3007.16,
        "duration": 3.959,
        "text": "retrieve chunks from different documents"
      },
      {
        "start": 3009.24,
        "duration": 3.319,
        "text": "if they're relevant to the query if"
      },
      {
        "start": 3011.119,
        "duration": 3.96,
        "text": "they're similar to the query I should"
      },
      {
        "start": 3012.559,
        "duration": 6.76,
        "text": "say because similarity search finds a"
      },
      {
        "start": 3015.079,
        "duration": 7.441,
        "text": "similar document um and yeah you can"
      },
      {
        "start": 3019.319,
        "duration": 6.441,
        "text": "have chunks from different documents"
      },
      {
        "start": 3022.52,
        "duration": 4.88,
        "text": "added as context to your llm in the"
      },
      {
        "start": 3025.76,
        "duration": 3.88,
        "text": "prompt"
      },
      {
        "start": 3027.4,
        "duration": 4.159,
        "text": "perfect thank you so much and Sean Shan"
      },
      {
        "start": 3029.64,
        "duration": 3.88,
        "text": "Duffy says um Eric had to leave and come"
      },
      {
        "start": 3031.559,
        "duration": 5.121,
        "text": "back because it's the Clark Kent to"
      },
      {
        "start": 3033.52,
        "duration": 5.0,
        "text": "Superman shift that's funny um yeah"
      },
      {
        "start": 3036.68,
        "duration": 5.04,
        "text": "excellent Superman will you show us the"
      },
      {
        "start": 3038.52,
        "duration": 6.92,
        "text": "the second um code heavy variant yes um"
      },
      {
        "start": 3041.72,
        "duration": 7.52,
        "text": "is my screen visible right now it is"
      },
      {
        "start": 3045.44,
        "duration": 6.52,
        "text": "awesome fantastic so um this notebook I"
      },
      {
        "start": 3049.24,
        "duration": 5.44,
        "text": "think will be shared um by the way and a"
      },
      {
        "start": 3051.96,
        "duration": 4.68,
        "text": "lot of it's really what Maria covered so"
      },
      {
        "start": 3054.68,
        "duration": 3.439,
        "text": "I'm going to kind of skip ahead here but"
      },
      {
        "start": 3056.64,
        "duration": 4.28,
        "text": "what I'm doing basically is setting up"
      },
      {
        "start": 3058.119,
        "duration": 5.44,
        "text": "in Google collaba U the unstructured"
      },
      {
        "start": 3060.92,
        "duration": 4.879,
        "text": "dependencies and I'm parsing uh this"
      },
      {
        "start": 3063.559,
        "duration": 4.481,
        "text": "paper which some of you may be familiar"
      },
      {
        "start": 3065.799,
        "duration": 4.121,
        "text": "with if you you know read about llms"
      },
      {
        "start": 3068.04,
        "duration": 4.36,
        "text": "attention is all you need it was kind of"
      },
      {
        "start": 3069.92,
        "duration": 5.24,
        "text": "one of the more like it's a pretty"
      },
      {
        "start": 3072.4,
        "duration": 6.28,
        "text": "famous paper talking about some of of"
      },
      {
        "start": 3075.16,
        "duration": 7.159,
        "text": "this stuff and what's happening is we're"
      },
      {
        "start": 3078.68,
        "duration": 5.679,
        "text": "parsing unstructured and um I'm I'm not"
      },
      {
        "start": 3082.319,
        "duration": 4.48,
        "text": "yet building the full pipeline this is"
      },
      {
        "start": 3084.359,
        "duration": 4.561,
        "text": "just an example of parsing it um but"
      },
      {
        "start": 3086.799,
        "duration": 4.28,
        "text": "here we get into kind of what Maria was"
      },
      {
        "start": 3088.92,
        "duration": 4.56,
        "text": "showing which is I build that full end"
      },
      {
        "start": 3091.079,
        "duration": 5.04,
        "text": "end pipeline which is starting with a"
      },
      {
        "start": 3093.48,
        "duration": 4.76,
        "text": "partition you know chunking and bedding"
      },
      {
        "start": 3096.119,
        "duration": 5.401,
        "text": "and then storing into Astra just like"
      },
      {
        "start": 3098.24,
        "duration": 5.2,
        "text": "she showed um that runs that stores into"
      },
      {
        "start": 3101.52,
        "duration": 4.16,
        "text": "Astra and just to give you like a little"
      },
      {
        "start": 3103.44,
        "duration": 5.32,
        "text": "preview of what you get out of that um"
      },
      {
        "start": 3105.68,
        "duration": 5.28,
        "text": "you can see little pieces of that paper"
      },
      {
        "start": 3108.76,
        "duration": 5.319,
        "text": "according to the chunking strategy get"
      },
      {
        "start": 3110.96,
        "duration": 7.52,
        "text": "stored as content and of course every"
      },
      {
        "start": 3114.079,
        "duration": 6.961,
        "text": "row of this table has an Associated V"
      },
      {
        "start": 3118.48,
        "duration": 6.879,
        "text": "I think I use open AI in fettings here"
      },
      {
        "start": 3121.04,
        "duration": 6.88,
        "text": "with my key so um so here's the part I"
      },
      {
        "start": 3125.359,
        "duration": 5.121,
        "text": "want to show you um given that we have"
      },
      {
        "start": 3127.92,
        "duration": 6.32,
        "text": "that so what I'm actually doing this is"
      },
      {
        "start": 3130.48,
        "duration": 5.8,
        "text": "totally outside of the realm of um"
      },
      {
        "start": 3134.24,
        "duration": 4.24,
        "text": "unstructured unstructured basically"
      },
      {
        "start": 3136.28,
        "duration": 4.44,
        "text": "orchestrated the step of getting this"
      },
      {
        "start": 3138.48,
        "duration": 4.2,
        "text": "table set up the way we want it which is"
      },
      {
        "start": 3140.72,
        "duration": 3.879,
        "text": "really the most important part and now"
      },
      {
        "start": 3142.68,
        "duration": 4.84,
        "text": "we're trying to build a rag application"
      },
      {
        "start": 3144.599,
        "duration": 4.921,
        "text": "and this is a super super simple example"
      },
      {
        "start": 3147.52,
        "duration": 4.599,
        "text": "of course but it shows you how easy it"
      },
      {
        "start": 3149.52,
        "duration": 6.36,
        "text": "is so um all I'm doing is I'm using"
      },
      {
        "start": 3152.119,
        "duration": 7.161,
        "text": "langing chain um and I'm importing two"
      },
      {
        "start": 3155.88,
        "duration": 5.84,
        "text": "uh basically two packages um uh classes"
      },
      {
        "start": 3159.28,
        "duration": 4.44,
        "text": "from those packages so the astb vector"
      },
      {
        "start": 3161.72,
        "duration": 5.119,
        "text": "store where we just basically tell it"
      },
      {
        "start": 3163.72,
        "duration": 5.399,
        "text": "what embeddings are we using open AI um"
      },
      {
        "start": 3166.839,
        "duration": 4.121,
        "text": "this is the name of my collection and"
      },
      {
        "start": 3169.119,
        "duration": 5.161,
        "text": "then the parameters that are required"
      },
      {
        "start": 3170.96,
        "duration": 6.8,
        "text": "for authentication with Astra so really"
      },
      {
        "start": 3174.28,
        "duration": 6.44,
        "text": "simple line to connect to seeing right"
      },
      {
        "start": 3177.76,
        "duration": 6.4,
        "text": "here it's just you know this right here"
      },
      {
        "start": 3180.72,
        "duration": 5.879,
        "text": "um and then all I have page is with one"
      },
      {
        "start": 3184.16,
        "duration": 4.6,
        "text": "simple call in Python and there by the"
      },
      {
        "start": 3186.599,
        "duration": 5.2,
        "text": "way there's plenty more you can do other"
      },
      {
        "start": 3188.76,
        "duration": 4.92,
        "text": "than just a similarity search with score"
      },
      {
        "start": 3191.799,
        "duration": 4.76,
        "text": "um but what I'm doing is I'm searching"
      },
      {
        "start": 3193.68,
        "duration": 5.159,
        "text": "for the word German translation because"
      },
      {
        "start": 3196.559,
        "duration": 4.28,
        "text": "there's some paragraph that talks about"
      },
      {
        "start": 3198.839,
        "duration": 4.52,
        "text": "um a test that they use to basically"
      },
      {
        "start": 3200.839,
        "duration": 4.401,
        "text": "test English to German translation and"
      },
      {
        "start": 3203.359,
        "duration": 3.841,
        "text": "I'm saying just return one result for"
      },
      {
        "start": 3205.24,
        "duration": 4.319,
        "text": "this particular example"
      },
      {
        "start": 3207.2,
        "duration": 3.8,
        "text": "um it shows you the similarity score"
      },
      {
        "start": 3209.559,
        "duration": 3.641,
        "text": "you'll notice that this kind of goes"
      },
      {
        "start": 3211.0,
        "duration": 5.319,
        "text": "back a little bit to what Maria was"
      },
      {
        "start": 3213.2,
        "duration": 6.639,
        "text": "talking about which is um just just"
      },
      {
        "start": 3216.319,
        "duration": 6.52,
        "text": "because we're splitting documents in I"
      },
      {
        "start": 3219.839,
        "duration": 5.161,
        "text": "should say partitioning documents into"
      },
      {
        "start": 3222.839,
        "duration": 4.24,
        "text": "particular chunks uh you still have a"
      },
      {
        "start": 3225.0,
        "duration": 4.52,
        "text": "chunking strategy after to make sure it"
      },
      {
        "start": 3227.079,
        "duration": 4.48,
        "text": "stays within your context window for the"
      },
      {
        "start": 3229.52,
        "duration": 4.4,
        "text": "llm that you're using so this is"
      },
      {
        "start": 3231.559,
        "duration": 5.04,
        "text": "actually like a portion of a paragraph"
      },
      {
        "start": 3233.92,
        "duration": 5.04,
        "text": "that's why it starts out a mid sentence"
      },
      {
        "start": 3236.599,
        "duration": 4.801,
        "text": "but what it found was the most similar"
      },
      {
        "start": 3238.96,
        "duration": 7.92,
        "text": "result to that query it says more"
      },
      {
        "start": 3241.4,
        "duration": 7.399,
        "text": "paralyzable and then achieves 28.4 Blu"
      },
      {
        "start": 3246.88,
        "duration": 4.64,
        "text": "on the English to German translation"
      },
      {
        "start": 3248.799,
        "duration": 6.76,
        "text": "test improving over the existing best"
      },
      {
        "start": 3251.52,
        "duration": 7.0,
        "text": "result by over two um that should all be"
      },
      {
        "start": 3255.559,
        "duration": 5.641,
        "text": "um with yeah so that actually came right"
      },
      {
        "start": 3258.52,
        "duration": 4.68,
        "text": "from the abstract here so that's a"
      },
      {
        "start": 3261.2,
        "duration": 4.399,
        "text": "portion that got pulled from that"
      },
      {
        "start": 3263.2,
        "duration": 3.76,
        "text": "document that's a row in astb with a"
      },
      {
        "start": 3265.599,
        "duration": 3.041,
        "text": "particular vector"
      },
      {
        "start": 3266.96,
        "duration": 4.119,
        "text": "and the only thing I needed to do to"
      },
      {
        "start": 3268.64,
        "duration": 5.36,
        "text": "start building um you know a really"
      },
      {
        "start": 3271.079,
        "duration": 4.641,
        "text": "simple in this case application um is"
      },
      {
        "start": 3274.0,
        "duration": 4.4,
        "text": "connect to that Vector store that"
      },
      {
        "start": 3275.72,
        "duration": 6.28,
        "text": "unstructured created for us and stored"
      },
      {
        "start": 3278.4,
        "duration": 5.919,
        "text": "the data for us and then just print some"
      },
      {
        "start": 3282.0,
        "duration": 4.28,
        "text": "results that come direct from Lang chain"
      },
      {
        "start": 3284.319,
        "duration": 3.961,
        "text": "you can use Lang chain you can use l"
      },
      {
        "start": 3286.28,
        "duration": 4.799,
        "text": "index you can use all kinds of other"
      },
      {
        "start": 3288.28,
        "duration": 5.2,
        "text": "even custom Solutions if you so choose"
      },
      {
        "start": 3291.079,
        "duration": 4.401,
        "text": "um as long as you already have you know"
      },
      {
        "start": 3293.48,
        "duration": 3.72,
        "text": "and this the data is already there"
      },
      {
        "start": 3295.48,
        "duration": 6.0,
        "text": "that's why it's so simple just one"
      },
      {
        "start": 3297.2,
        "duration": 4.28,
        "text": "little block of code to start building"
      },
      {
        "start": 3308.44,
        "duration": 6.639,
        "text": "some excellent um that that's so we've"
      },
      {
        "start": 3313.28,
        "duration": 3.72,
        "text": "what we've done is we've taken the data"
      },
      {
        "start": 3315.079,
        "duration": 3.801,
        "text": "from un structured uploaded into Astra"
      },
      {
        "start": 3317.0,
        "duration": 4.68,
        "text": "and we've we've looked at how you can"
      },
      {
        "start": 3318.88,
        "duration": 6.4,
        "text": "finish the application um both with a"
      },
      {
        "start": 3321.68,
        "duration": 6.24,
        "text": "visual loo tool and with um an actual"
      },
      {
        "start": 3325.28,
        "duration": 4.839,
        "text": "extension of The Notebook um I I want to"
      },
      {
        "start": 3327.92,
        "duration": 4.84,
        "text": "um pause here and ask the both of you if"
      },
      {
        "start": 3330.119,
        "duration": 5.0,
        "text": "if you have anything um extra to share"
      },
      {
        "start": 3332.76,
        "duration": 4.48,
        "text": "any calls to action anything to plug any"
      },
      {
        "start": 3335.119,
        "duration": 3.96,
        "text": "links any socials um and then and then"
      },
      {
        "start": 3337.24,
        "duration": 4.0,
        "text": "we'll wrap up as well this is a call to"
      },
      {
        "start": 3339.079,
        "duration": 3.601,
        "text": "action for the the people here um in the"
      },
      {
        "start": 3341.24,
        "duration": 3.92,
        "text": "comments there are some questions but"
      },
      {
        "start": 3342.68,
        "duration": 4.879,
        "text": "we're almost at time so if you want them"
      },
      {
        "start": 3345.16,
        "duration": 3.6,
        "text": "to have somewhere to to find you and ask"
      },
      {
        "start": 3347.559,
        "duration": 3.24,
        "text": "questions whether it's a Discord"
      },
      {
        "start": 3348.76,
        "duration": 4.96,
        "text": "Community or slack or maybe X formerly"
      },
      {
        "start": 3350.799,
        "duration": 6.0,
        "text": "known as Twitter now is the"
      },
      {
        "start": 3353.72,
        "duration": 6.44,
        "text": "time yeah I just want to encourage"
      },
      {
        "start": 3356.799,
        "duration": 4.681,
        "text": "people to try the collab notebooks and"
      },
      {
        "start": 3360.16,
        "duration": 2.56,
        "text": "we're going to share them as well"
      },
      {
        "start": 3361.48,
        "duration": 2.68,
        "text": "probably in the description for the"
      },
      {
        "start": 3362.72,
        "duration": 4.2,
        "text": "videos and in the"
      },
      {
        "start": 3364.16,
        "duration": 6.639,
        "text": "recordings and if you go to unstructured"
      },
      {
        "start": 3366.92,
        "duration": 6.24,
        "text": "documentation uh we have an example code"
      },
      {
        "start": 3370.799,
        "duration": 4.201,
        "text": "tab there and we have a whole bunch of"
      },
      {
        "start": 3373.16,
        "duration": 5.879,
        "text": "different notebooks that you can try to"
      },
      {
        "start": 3375.0,
        "duration": 5.64,
        "text": "build rag which is very basic and a"
      },
      {
        "start": 3379.039,
        "duration": 3.681,
        "text": "little bit more complicated with"
      },
      {
        "start": 3380.64,
        "duration": 4.76,
        "text": "different sources different types of"
      },
      {
        "start": 3382.72,
        "duration": 4.96,
        "text": "documents so um I think the best way to"
      },
      {
        "start": 3385.4,
        "duration": 5.52,
        "text": "learn anything thing is to start"
      },
      {
        "start": 3387.68,
        "duration": 6.679,
        "text": "building something and I just hope that"
      },
      {
        "start": 3390.92,
        "duration": 6.52,
        "text": "you have time and and desire to learn"
      },
      {
        "start": 3394.359,
        "duration": 5.361,
        "text": "and build R rag applications and I hope"
      },
      {
        "start": 3397.44,
        "duration": 7.399,
        "text": "that our notebooks can be"
      },
      {
        "start": 3399.72,
        "duration": 7.119,
        "text": "helpful fantastic Eric yeah um I was"
      },
      {
        "start": 3404.839,
        "duration": 4.441,
        "text": "just going to make a quick men um that"
      },
      {
        "start": 3406.839,
        "duration": 4.28,
        "text": "that that that that's a spot on and I"
      },
      {
        "start": 3409.28,
        "duration": 3.36,
        "text": "wanted to say too that from the"
      },
      {
        "start": 3411.119,
        "duration": 4.561,
        "text": "perspective of someone who's working at"
      },
      {
        "start": 3412.64,
        "duration": 5.719,
        "text": "data stacks on Astra um I want to say"
      },
      {
        "start": 3415.68,
        "duration": 5.399,
        "text": "how why un structured like to me is so"
      },
      {
        "start": 3418.359,
        "duration": 4.641,
        "text": "invaluable and so amazing and um before"
      },
      {
        "start": 3421.079,
        "duration": 4.321,
        "text": "I worked at data Stacks I was a"
      },
      {
        "start": 3423.0,
        "duration": 4.24,
        "text": "statistical consultant and I I can tell"
      },
      {
        "start": 3425.4,
        "duration": 5.159,
        "text": "you that having worked with clients big"
      },
      {
        "start": 3427.24,
        "duration": 5.04,
        "text": "and small it's constant that they say oh"
      },
      {
        "start": 3430.559,
        "duration": 4.24,
        "text": "I have some data oh what's that data"
      },
      {
        "start": 3432.28,
        "duration": 5.799,
        "text": "look like handwritten notes or it's you"
      },
      {
        "start": 3434.799,
        "duration": 5.32,
        "text": "know scraps it's nothing structured and"
      },
      {
        "start": 3438.079,
        "duration": 3.441,
        "text": "um of course everything that we do in"
      },
      {
        "start": 3440.119,
        "duration": 3.72,
        "text": "Astra and everything we do with these"
      },
      {
        "start": 3441.52,
        "duration": 4.4,
        "text": "rag applications assumes we have these"
      },
      {
        "start": 3443.839,
        "duration": 5.081,
        "text": "nice tables where we have text that"
      },
      {
        "start": 3445.92,
        "duration": 5.8,
        "text": "that's been vectorized and chunked Etc"
      },
      {
        "start": 3448.92,
        "duration": 4.919,
        "text": "and so having that link between data"
      },
      {
        "start": 3451.72,
        "duration": 5.399,
        "text": "that exists in the real world and data"
      },
      {
        "start": 3453.839,
        "duration": 6.441,
        "text": "that we need to build the rag"
      },
      {
        "start": 3457.119,
        "duration": 5.161,
        "text": "applications is not only like important"
      },
      {
        "start": 3460.28,
        "duration": 4.519,
        "text": "but like fundamental and that's why I"
      },
      {
        "start": 3462.28,
        "duration": 5.88,
        "text": "think this is so"
      },
      {
        "start": 3464.799,
        "duration": 5.201,
        "text": "cool absolutely um well listen I know"
      },
      {
        "start": 3468.16,
        "duration": 4.28,
        "text": "it's um it's it's been a quite a"
      },
      {
        "start": 3470.0,
        "duration": 5.44,
        "text": "discussion we we covered a lot um and"
      },
      {
        "start": 3472.44,
        "duration": 4.56,
        "text": "it's it's no small feat to show up live"
      },
      {
        "start": 3475.44,
        "duration": 3.76,
        "text": "and share your and just walk through"
      },
      {
        "start": 3477.0,
        "duration": 3.88,
        "text": "your content so um on behalf of everyone"
      },
      {
        "start": 3479.2,
        "duration": 3.159,
        "text": "in the audience um and myself thank you"
      },
      {
        "start": 3480.88,
        "duration": 3.04,
        "text": "so much for coming here and and sharing"
      },
      {
        "start": 3482.359,
        "duration": 3.44,
        "text": "all of this with everybody thank you for"
      },
      {
        "start": 3483.92,
        "duration": 3.96,
        "text": "those in the comments this discussion in"
      },
      {
        "start": 3485.799,
        "duration": 4.0,
        "text": "the comments was so valuable I've never"
      },
      {
        "start": 3487.88,
        "duration": 3.56,
        "text": "actually never had a live stream um"
      },
      {
        "start": 3489.799,
        "duration": 4.32,
        "text": "where everybody has been so engaged with"
      },
      {
        "start": 3491.44,
        "duration": 6.599,
        "text": "so many questions thank you Kevin um and"
      },
      {
        "start": 3494.119,
        "duration": 6.321,
        "text": "Maria and Ilia and Mars and um all Sean"
      },
      {
        "start": 3498.039,
        "duration": 3.961,
        "text": "Spyros David all the people who posed"
      },
      {
        "start": 3500.44,
        "duration": 4.72,
        "text": "such great questions thank you thank you"
      },
      {
        "start": 3502.0,
        "duration": 3.16,
        "text": "thank you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-11T21:38:34.522970+00:00"
}