{
  "video_id": "p4W04Az-G0s",
  "title": "DS210.25 Spark for Data Loading | Operations with Apache Cassandra",
  "description": "#DataStaxAcademy #DS210\nDS210.25 SPARK FOR DATA LOADING\nSpark provides convenient functionality for loading large external datasets into Apache Cassandra tables in parallel. Learn more about using Spark for data loading in this unit.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-12T01:25:00Z",
  "thumbnail": "https://i.ytimg.com/vi/p4W04Az-G0s/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "datastax",
    "tutorial",
    "apache_cassandra"
  ],
  "url": "https://www.youtube.com/watch?v=p4W04Az-G0s",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's talk about spark for data loading kind of a funny concept what are you talking about surprisingly it works really well an xml file json file whatever and using that facility to insert data quickly into a cassandra cluster but with the added bonus of being able to do a little transform along the way so since it's spark load the data do something with it dump it into cassandra that is a cool way of loading a lot of data especially whenever you're trying to manipulate that data as well let's say that you have some fairly dirty data it's not really cleaned up you need to get some things done maybe normalize it okay fine you can use spark does a great job so in this case here's a little spark job now if you're not into scala this will probably make you freak out a little bit but trust me this is what it's doing so what we have is just this very simple setup where we take a csv file with users in it we read that file in break it down into an array and then dump that array directly into cassandra using the save to cassandra command that is super simple now if you were to execute this as a spark job then it would read it and parallelize this task across the cluster it is really a good way to do this in a performant way so if you'll notice there's nothing really fancy going on here this is just take a file manipulate it into an array and dump it into cassandra this example is a little more complicated we're actually counting how much of that data got into the system a little more now if there's any manipulations need to be done this is the time to do it let's say we're making it all uppercase or validating that the date is correct that sort of thing that could be done inside of the spark job so all of the data once it's created can then be put directly into a cassandra table just like every other spark job that you're running inside of a datastax enterprise cluster super simple right so this gives you a quick overview of how to load data into a cassandra cluster using spark",
    "segments": [
      {
        "start": 1.43,
        "duration": 5.33,
        "text": "[Music]"
      },
      {
        "start": 7.04,
        "duration": 4.08,
        "text": "let's talk about spark for data loading"
      },
      {
        "start": 9.36,
        "duration": 3.76,
        "text": "kind of a funny concept what are you"
      },
      {
        "start": 11.12,
        "duration": 4.479,
        "text": "talking about surprisingly"
      },
      {
        "start": 13.12,
        "duration": 4.079,
        "text": "it works really well an xml file json"
      },
      {
        "start": 15.599,
        "duration": 4.641,
        "text": "file whatever"
      },
      {
        "start": 17.199,
        "duration": 5.761,
        "text": "and using that facility to insert"
      },
      {
        "start": 20.24,
        "duration": 4.72,
        "text": "data quickly into a cassandra cluster"
      },
      {
        "start": 22.96,
        "duration": 4.0,
        "text": "but with the added bonus of being able"
      },
      {
        "start": 24.96,
        "duration": 4.319,
        "text": "to do a little transform along the way"
      },
      {
        "start": 26.96,
        "duration": 3.12,
        "text": "so since it's spark load the data do"
      },
      {
        "start": 29.279,
        "duration": 3.44,
        "text": "something with it"
      },
      {
        "start": 30.08,
        "duration": 4.4,
        "text": "dump it into cassandra that is a cool"
      },
      {
        "start": 32.719,
        "duration": 3.041,
        "text": "way of loading a lot of data especially"
      },
      {
        "start": 34.48,
        "duration": 2.32,
        "text": "whenever you're trying to manipulate"
      },
      {
        "start": 35.76,
        "duration": 2.799,
        "text": "that data as well"
      },
      {
        "start": 36.8,
        "duration": 3.439,
        "text": "let's say that you have some fairly"
      },
      {
        "start": 38.559,
        "duration": 3.201,
        "text": "dirty data it's not really cleaned up"
      },
      {
        "start": 40.239,
        "duration": 2.721,
        "text": "you need to get some things done maybe"
      },
      {
        "start": 41.76,
        "duration": 3.36,
        "text": "normalize it"
      },
      {
        "start": 42.96,
        "duration": 3.119,
        "text": "okay fine you can use spark does a great"
      },
      {
        "start": 45.12,
        "duration": 3.04,
        "text": "job so"
      },
      {
        "start": 46.079,
        "duration": 3.761,
        "text": "in this case here's a little spark job"
      },
      {
        "start": 48.16,
        "duration": 3.44,
        "text": "now if you're not into scala this will"
      },
      {
        "start": 49.84,
        "duration": 4.239,
        "text": "probably make you freak out a little bit"
      },
      {
        "start": 51.6,
        "duration": 4.479,
        "text": "but trust me this is what it's doing so"
      },
      {
        "start": 54.079,
        "duration": 3.601,
        "text": "what we have is just this very simple"
      },
      {
        "start": 56.079,
        "duration": 4.32,
        "text": "setup where we take a csv"
      },
      {
        "start": 57.68,
        "duration": 3.679,
        "text": "file with users in it we read that file"
      },
      {
        "start": 60.399,
        "duration": 2.881,
        "text": "in"
      },
      {
        "start": 61.359,
        "duration": 4.241,
        "text": "break it down into an array and then"
      },
      {
        "start": 63.28,
        "duration": 4.48,
        "text": "dump that array directly into cassandra"
      },
      {
        "start": 65.6,
        "duration": 4.64,
        "text": "using the save to cassandra command"
      },
      {
        "start": 67.76,
        "duration": 3.76,
        "text": "that is super simple now if you were to"
      },
      {
        "start": 70.24,
        "duration": 4.08,
        "text": "execute this"
      },
      {
        "start": 71.52,
        "duration": 4.16,
        "text": "as a spark job then it would read it and"
      },
      {
        "start": 74.32,
        "duration": 3.92,
        "text": "parallelize this task"
      },
      {
        "start": 75.68,
        "duration": 4.56,
        "text": "across the cluster it is really a good"
      },
      {
        "start": 78.24,
        "duration": 3.36,
        "text": "way to do this in a performant way"
      },
      {
        "start": 80.24,
        "duration": 3.68,
        "text": "so if you'll notice there's nothing"
      },
      {
        "start": 81.6,
        "duration": 4.96,
        "text": "really fancy going on here this is just"
      },
      {
        "start": 83.92,
        "duration": 4.48,
        "text": "take a file manipulate it into an array"
      },
      {
        "start": 86.56,
        "duration": 3.04,
        "text": "and dump it into cassandra"
      },
      {
        "start": 88.4,
        "duration": 2.88,
        "text": "this example is a little more"
      },
      {
        "start": 89.6,
        "duration": 3.92,
        "text": "complicated we're actually counting how"
      },
      {
        "start": 91.28,
        "duration": 3.92,
        "text": "much of that data got into the system"
      },
      {
        "start": 93.52,
        "duration": 3.44,
        "text": "a little more now if there's any"
      },
      {
        "start": 95.2,
        "duration": 3.599,
        "text": "manipulations need to be done"
      },
      {
        "start": 96.96,
        "duration": 3.119,
        "text": "this is the time to do it let's say"
      },
      {
        "start": 98.799,
        "duration": 3.841,
        "text": "we're making it all"
      },
      {
        "start": 100.079,
        "duration": 3.121,
        "text": "uppercase or validating that the date is"
      },
      {
        "start": 102.64,
        "duration": 2.4,
        "text": "correct"
      },
      {
        "start": 103.2,
        "duration": 3.279,
        "text": "that sort of thing that could be done"
      },
      {
        "start": 105.04,
        "duration": 3.68,
        "text": "inside of the spark job"
      },
      {
        "start": 106.479,
        "duration": 3.92,
        "text": "so all of the data once it's created can"
      },
      {
        "start": 108.72,
        "duration": 2.24,
        "text": "then be put directly into a cassandra"
      },
      {
        "start": 110.399,
        "duration": 2.08,
        "text": "table"
      },
      {
        "start": 110.96,
        "duration": 2.96,
        "text": "just like every other spark job that"
      },
      {
        "start": 112.479,
        "duration": 2.561,
        "text": "you're running inside of a datastax"
      },
      {
        "start": 113.92,
        "duration": 3.44,
        "text": "enterprise cluster"
      },
      {
        "start": 115.04,
        "duration": 4.48,
        "text": "super simple right so this gives you a"
      },
      {
        "start": 117.36,
        "duration": 9.52,
        "text": "quick overview of how to load data"
      },
      {
        "start": 119.52,
        "duration": 7.36,
        "text": "into a cassandra cluster using spark"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T01:20:21.728875+00:00"
}