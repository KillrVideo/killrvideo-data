{
  "video_id": "hyR_zfBDBjY",
  "title": "Example of the Week: Ingest Avro from Apache Kafka to DataStax Enterprise",
  "description": "This example shows how to ingest Avro records from Kafka to a table in DataStax Enterprise using the DataStax Apache Kafka Connector.\nCheck out DataStax Examples for more code samples - https://www.datastax.com/examples\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastaxdevs?sub_confirmation=1 \nTwitter: https://twitter.com/datastaxdevs\nTwitch: https://www.twitch.tv/datastaxdevs\n\nABOUT DATASTAX DEVELOPERS\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache Cassandra©.  Go to datastax.com/dev to access our free online courses, tutorials, and more.",
  "published_at": "2020-07-09T16:38:20Z",
  "thumbnail": "https://i.ytimg.com/vi/hyR_zfBDBjY/hqdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "datastax",
    "apache_cassandra",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=hyR_zfBDBjY",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Applause] [Music] hi everyone this is Rebecca data stacks I'm back again this week with another example the example I'm going to walk you through today it's going to demonstrate how to ingest Avril records from Katka into data stacks enterprise and we're going to be doing this using the data sacks patchy cafe connector but we're also going to be taking the easy route with this example by using docker and docker compose to set up our environment quickly so we don't have to spend tons of time setting up zookeeper and capture brokers capulet connects so let's just have a look at the files included with this project we have a docker file that will build the image of catholic connect with the data stacks captive connector installed now we also have a docker file that builds an image of a or a producer we have a docker composed yamo file compose is a tool for defining and running multi container docker applications and with compose you use a llamó file to configure your application services so then what the single commands you can basically create and start all the services from just this configuration we also have the configuration or the connector itself and then we have the katka every producer to write records to captain and this uses the average serialized cat director key and record value so once we've built our docker images and use the docker compose to get our containers running there's going to be six docker containers being zookeeper capital broker have to connect confluence schema registry data stacks Enterprise and then the producer these are all gonna be using the same docker network so once we write all these records to Kafka from the Avro CAPA producer the data sex Capra conductor will be started and all those records will be streamed into dude stacks enterprise from katka and those records be written into the table so in terms of prerequisites there isn't a whole lot we need to do since we're using docker makes it pretty easy so you're going to need docker and docker compose I'm actually using docker desktop for a Mac which actually covers both of these so I'm good to go before we get started just in case you don't know what Avro is Avro is a row based storage format and it's widely used as a serialization platform so Avro stores the data definition or schema in JSON format which makes it really easy to read and interpret by any program and the data itself is stored in binary format which makes it really compact and efficient so let's get started here first I'm just going to clone down this repo just have a look inside there notice we have the couple of docker files been mentioned earlier the configuration or the connector producer it's like a schema up to draw a file in the docker compose dot yellow file so we've already looked at the directory event there now let's build the katka connector image from the docker file some of this takes a little while so I'm just going to speed up the video through the longer parts of this okay looks like it's done also let's build the average a producer image from its docker file wow that was a lot okay so finally we're going to use docker compose to get all our containers up and running just just have a look at the docker compose dot yeah mol file for a second okay so you can see the different containers that we're going to be starting up here we have our zookeeper caprica broker the confluence schema registry also katka connect with the configuration and data stacks enterprise and finally the kefka producer alright so let's just run that docker compose up command let's see that it's starting up zookeeper there and Kafka the confluence schema registry and it is Sachs enterprise you okay so it looks like we're ready to go looks like we have all the containers that we needed so let's just have a look at what we do next so it looks like we're going to be setting up the flow of the data from Kafka to DSE so first we're going to create our cap the topic if you're familiar with Kafka you'll know what that is so on our cap co-broker container and we're just going to start a bath session and from there we are going to create this Avro stream topic so looks like up next from our DSC container we're going to be starting CTL SH just exit that container and start cql SH in our DSC container okay so it says start the CQ l shell and then we need to copy and paste the contents at schema a dot out into CQ l sh so let's just open that file take a look so I'm just going to grab the contents here okay so there is our schema okay so we have our schema in place so now we're going to run our captrick producer so soap in a bad session on our producer container and we'll use the maven to run compile and run the producer oh by the way this is supposed to write a thousand records to Caprica I believe yes write a thousand records to capture using the Java producer it's still going on it looks like that's finished so the next step is we need to start the data sex capture connector and we're going to use the Catholic Connect REST API to do that so let's just grab this command scroll command leave that container okay so we just started the connector so hopefully our rows have been written to our data stacks Enterprise instance so we're just going to open up CQ LSH on that container and have a look so we're just going to grab a sample from the avro UDT table just ten rows just to see if this worked okay looks like this was the success and see all our data in there this one too taken a lot more time to set up if we had just done this all from scratch so you can see how useful docker can really be for learning purposes anyway I hope you enjoyed walking through this example with me today if you have any questions or feedback please let us know I'll see you next time [Music]",
    "segments": [
      {
        "start": 0.67,
        "duration": 3.44,
        "text": "[Applause]"
      },
      {
        "start": 5.11,
        "duration": 3.08,
        "text": "[Music]"
      },
      {
        "start": 12.23,
        "duration": 5.44,
        "text": "hi everyone this is Rebecca data stacks"
      },
      {
        "start": 15.599,
        "duration": 4.35,
        "text": "I'm back again this week with another"
      },
      {
        "start": 17.67,
        "duration": 4.26,
        "text": "example the example I'm going to walk"
      },
      {
        "start": 19.949,
        "duration": 4.771,
        "text": "you through today it's going to"
      },
      {
        "start": 21.93,
        "duration": 4.8,
        "text": "demonstrate how to ingest Avril records"
      },
      {
        "start": 24.72,
        "duration": 4.41,
        "text": "from Katka into data stacks"
      },
      {
        "start": 26.73,
        "duration": 3.93,
        "text": "enterprise and we're going to be doing"
      },
      {
        "start": 29.13,
        "duration": 4.29,
        "text": "this using the data sacks"
      },
      {
        "start": 30.66,
        "duration": 4.829,
        "text": "patchy cafe connector but we're also"
      },
      {
        "start": 33.42,
        "duration": 4.5,
        "text": "going to be taking the easy route with"
      },
      {
        "start": 35.489,
        "duration": 4.23,
        "text": "this example by using docker and docker"
      },
      {
        "start": 37.92,
        "duration": 3.99,
        "text": "compose to set up our environment"
      },
      {
        "start": 39.719,
        "duration": 5.671,
        "text": "quickly so we don't have to spend tons"
      },
      {
        "start": 41.91,
        "duration": 7.079,
        "text": "of time setting up zookeeper and capture"
      },
      {
        "start": 45.39,
        "duration": 5.849,
        "text": "brokers capulet connects so let's just"
      },
      {
        "start": 48.989,
        "duration": 4.5,
        "text": "have a look at the files included with"
      },
      {
        "start": 51.239,
        "duration": 4.41,
        "text": "this project we have a docker file that"
      },
      {
        "start": 53.489,
        "duration": 4.861,
        "text": "will build the image of catholic connect"
      },
      {
        "start": 55.649,
        "duration": 5.881,
        "text": "with the data stacks captive connector"
      },
      {
        "start": 58.35,
        "duration": 7.309,
        "text": "installed now we also have a docker file"
      },
      {
        "start": 61.53,
        "duration": 6.99,
        "text": "that builds an image of a or a producer"
      },
      {
        "start": 65.659,
        "duration": 6.881,
        "text": "we have a docker composed"
      },
      {
        "start": 68.52,
        "duration": 5.959,
        "text": "yamo file compose is a tool for defining"
      },
      {
        "start": 72.54,
        "duration": 4.59,
        "text": "and running multi container docker"
      },
      {
        "start": 74.479,
        "duration": 4.771,
        "text": "applications and with compose you use a"
      },
      {
        "start": 77.13,
        "duration": 5.55,
        "text": "llamó file to configure your application"
      },
      {
        "start": 79.25,
        "duration": 5.74,
        "text": "services so then what the single"
      },
      {
        "start": 82.68,
        "duration": 4.619,
        "text": "commands you can basically create and"
      },
      {
        "start": 84.99,
        "duration": 4.739,
        "text": "start all the services from just this"
      },
      {
        "start": 87.299,
        "duration": 4.32,
        "text": "configuration we also have the"
      },
      {
        "start": 89.729,
        "duration": 5.011,
        "text": "configuration or the connector itself"
      },
      {
        "start": 91.619,
        "duration": 5.551,
        "text": "and then we have the katka every"
      },
      {
        "start": 94.74,
        "duration": 5.22,
        "text": "producer to write records to captain and"
      },
      {
        "start": 97.17,
        "duration": 5.46,
        "text": "this uses the average serialized cat"
      },
      {
        "start": 99.96,
        "duration": 5.07,
        "text": "director key and record value so once"
      },
      {
        "start": 102.63,
        "duration": 6.39,
        "text": "we've built our docker images and use"
      },
      {
        "start": 105.03,
        "duration": 5.549,
        "text": "the docker compose to get our containers"
      },
      {
        "start": 109.02,
        "duration": 5.849,
        "text": "running there's going to be six docker"
      },
      {
        "start": 110.579,
        "duration": 7.411,
        "text": "containers being zookeeper capital"
      },
      {
        "start": 114.869,
        "duration": 5.731,
        "text": "broker have to connect confluence schema"
      },
      {
        "start": 117.99,
        "duration": 5.879,
        "text": "registry data stacks Enterprise and then"
      },
      {
        "start": 120.6,
        "duration": 5.85,
        "text": "the producer these are all gonna be"
      },
      {
        "start": 123.869,
        "duration": 4.86,
        "text": "using the same docker network so once we"
      },
      {
        "start": 126.45,
        "duration": 4.9,
        "text": "write all these records to Kafka from"
      },
      {
        "start": 128.729,
        "duration": 4.541,
        "text": "the Avro CAPA producer the data"
      },
      {
        "start": 131.35,
        "duration": 4.32,
        "text": "sex Capra conductor will be started and"
      },
      {
        "start": 133.27,
        "duration": 5.28,
        "text": "all those records will be streamed into"
      },
      {
        "start": 135.67,
        "duration": 5.42,
        "text": "dude stacks enterprise from katka and"
      },
      {
        "start": 138.55,
        "duration": 5.79,
        "text": "those records be written into the table"
      },
      {
        "start": 141.09,
        "duration": 6.49,
        "text": "so in terms of prerequisites there isn't"
      },
      {
        "start": 144.34,
        "duration": 6.66,
        "text": "a whole lot we need to do since we're"
      },
      {
        "start": 147.58,
        "duration": 4.95,
        "text": "using docker makes it pretty easy so"
      },
      {
        "start": 151.0,
        "duration": 5.79,
        "text": "you're going to need docker and docker"
      },
      {
        "start": 152.53,
        "duration": 8.04,
        "text": "compose I'm actually using docker"
      },
      {
        "start": 156.79,
        "duration": 8.34,
        "text": "desktop for a Mac which actually covers"
      },
      {
        "start": 160.57,
        "duration": 6.87,
        "text": "both of these so I'm good to go before"
      },
      {
        "start": 165.13,
        "duration": 5.22,
        "text": "we get started just in case you don't"
      },
      {
        "start": 167.44,
        "duration": 5.58,
        "text": "know what Avro is Avro is a row based"
      },
      {
        "start": 170.35,
        "duration": 6.33,
        "text": "storage format and it's widely used as a"
      },
      {
        "start": 173.02,
        "duration": 6.81,
        "text": "serialization platform so Avro stores"
      },
      {
        "start": 176.68,
        "duration": 5.31,
        "text": "the data definition or schema in JSON"
      },
      {
        "start": 179.83,
        "duration": 4.47,
        "text": "format which makes it really easy to"
      },
      {
        "start": 181.99,
        "duration": 4.62,
        "text": "read and interpret by any program and"
      },
      {
        "start": 184.3,
        "duration": 4.89,
        "text": "the data itself is stored in binary"
      },
      {
        "start": 186.61,
        "duration": 5.34,
        "text": "format which makes it really compact and"
      },
      {
        "start": 189.19,
        "duration": 5.1,
        "text": "efficient so let's get started here"
      },
      {
        "start": 191.95,
        "duration": 4.79,
        "text": "first I'm just going to clone down this"
      },
      {
        "start": 194.29,
        "duration": 2.45,
        "text": "repo"
      },
      {
        "start": 199.91,
        "duration": 4.89,
        "text": "just have a look inside there"
      },
      {
        "start": 213.29,
        "duration": 5.32,
        "text": "notice we have the couple of docker"
      },
      {
        "start": 215.82,
        "duration": 8.21,
        "text": "files been mentioned earlier the"
      },
      {
        "start": 218.61,
        "duration": 5.42,
        "text": "configuration or the connector producer"
      },
      {
        "start": 224.15,
        "duration": 7.66,
        "text": "it's like a schema up to draw a file in"
      },
      {
        "start": 227.37,
        "duration": 6.0,
        "text": "the docker compose dot yellow file so"
      },
      {
        "start": 231.81,
        "duration": 4.95,
        "text": "we've already looked at the directory"
      },
      {
        "start": 233.37,
        "duration": 7.28,
        "text": "event there now let's build the katka"
      },
      {
        "start": 236.76,
        "duration": 3.89,
        "text": "connector image from the docker file"
      },
      {
        "start": 248.21,
        "duration": 4.6,
        "text": "some of this takes a little while so I'm"
      },
      {
        "start": 250.86,
        "duration": 6.59,
        "text": "just going to speed up the video through"
      },
      {
        "start": 252.81,
        "duration": 4.64,
        "text": "the longer parts of this"
      },
      {
        "start": 264.27,
        "duration": 10.17,
        "text": "okay looks like it's done also let's"
      },
      {
        "start": 268.53,
        "duration": 8.81,
        "text": "build the average a producer image from"
      },
      {
        "start": 274.44,
        "duration": 2.9,
        "text": "its docker file"
      },
      {
        "start": 289.17,
        "duration": 7.06,
        "text": "wow that was a lot okay so finally we're"
      },
      {
        "start": 294.34,
        "duration": 5.52,
        "text": "going to use docker compose to get all"
      },
      {
        "start": 296.23,
        "duration": 6.87,
        "text": "our containers up and running just just"
      },
      {
        "start": 299.86,
        "duration": 9.75,
        "text": "have a look at the docker compose dot"
      },
      {
        "start": 303.1,
        "duration": 8.1,
        "text": "yeah mol file for a second okay so you"
      },
      {
        "start": 309.61,
        "duration": 4.17,
        "text": "can see the different containers that"
      },
      {
        "start": 311.2,
        "duration": 5.96,
        "text": "we're going to be starting up here we"
      },
      {
        "start": 313.78,
        "duration": 7.8,
        "text": "have our zookeeper caprica broker the"
      },
      {
        "start": 317.16,
        "duration": 11.08,
        "text": "confluence schema registry also katka"
      },
      {
        "start": 321.58,
        "duration": 9.69,
        "text": "connect with the configuration and data"
      },
      {
        "start": 328.24,
        "duration": 9.12,
        "text": "stacks enterprise and finally the kefka"
      },
      {
        "start": 331.27,
        "duration": 15.12,
        "text": "producer alright so let's just run that"
      },
      {
        "start": 337.36,
        "duration": 22.01,
        "text": "docker compose up command let's see that"
      },
      {
        "start": 346.39,
        "duration": 22.82,
        "text": "it's starting up zookeeper there and"
      },
      {
        "start": 359.37,
        "duration": 9.84,
        "text": "Kafka the confluence schema registry"
      },
      {
        "start": 374.139,
        "duration": 3.981,
        "text": "and it is Sachs enterprise"
      },
      {
        "start": 383.08,
        "duration": 2.059,
        "text": "you"
      },
      {
        "start": 407.06,
        "duration": 5.68,
        "text": "okay so it looks like we're ready to go"
      },
      {
        "start": 410.73,
        "duration": 4.83,
        "text": "looks like we have all the containers"
      },
      {
        "start": 412.74,
        "duration": 5.7,
        "text": "that we needed so let's just have a look"
      },
      {
        "start": 415.56,
        "duration": 5.76,
        "text": "at what we do next so it looks like"
      },
      {
        "start": 418.44,
        "duration": 6.72,
        "text": "we're going to be setting up the flow of"
      },
      {
        "start": 421.32,
        "duration": 5.46,
        "text": "the data from Kafka to DSE so first"
      },
      {
        "start": 425.16,
        "duration": 3.6,
        "text": "we're going to create our cap the topic"
      },
      {
        "start": 426.78,
        "duration": 4.56,
        "text": "if you're familiar with Kafka you'll"
      },
      {
        "start": 428.76,
        "duration": 4.5,
        "text": "know what that is so on our cap"
      },
      {
        "start": 431.34,
        "duration": 5.48,
        "text": "co-broker container and we're just going"
      },
      {
        "start": 433.26,
        "duration": 3.56,
        "text": "to start a bath session"
      },
      {
        "start": 440.65,
        "duration": 14.079,
        "text": "and from there we are going to create"
      },
      {
        "start": 444.439,
        "duration": 15.181,
        "text": "this Avro stream topic so looks like up"
      },
      {
        "start": 454.729,
        "duration": 12.301,
        "text": "next from our DSC container we're going"
      },
      {
        "start": 459.62,
        "duration": 13.409,
        "text": "to be starting CTL SH just exit that"
      },
      {
        "start": 467.03,
        "duration": 13.83,
        "text": "container and start cql SH in our DSC"
      },
      {
        "start": 473.029,
        "duration": 9.781,
        "text": "container okay so it says start the CQ l"
      },
      {
        "start": 480.86,
        "duration": 6.54,
        "text": "shell and then we need to copy and paste"
      },
      {
        "start": 482.81,
        "duration": 11.039,
        "text": "the contents at schema a dot out into CQ"
      },
      {
        "start": 487.4,
        "duration": 14.669,
        "text": "l sh so let's just open that file take a"
      },
      {
        "start": 493.849,
        "duration": 22.531,
        "text": "look so I'm just going to grab the"
      },
      {
        "start": 502.069,
        "duration": 20.671,
        "text": "contents here okay so there is our"
      },
      {
        "start": 516.38,
        "duration": 9.24,
        "text": "schema okay so we have our schema in"
      },
      {
        "start": 522.74,
        "duration": 8.88,
        "text": "place so now we're going to run our"
      },
      {
        "start": 525.62,
        "duration": 13.409,
        "text": "captrick producer so soap in a bad"
      },
      {
        "start": 531.62,
        "duration": 11.399,
        "text": "session on our producer container and"
      },
      {
        "start": 539.029,
        "duration": 6.74,
        "text": "we'll use the maven to run compile and"
      },
      {
        "start": 543.019,
        "duration": 2.75,
        "text": "run the producer"
      },
      {
        "start": 550.37,
        "duration": 29.99,
        "text": "oh by the way this is supposed to write"
      },
      {
        "start": 566.449,
        "duration": 17.161,
        "text": "a thousand records to Caprica I believe"
      },
      {
        "start": 580.36,
        "duration": 10.51,
        "text": "yes write a thousand records to capture"
      },
      {
        "start": 583.61,
        "duration": 9.44,
        "text": "using the Java producer it's still going"
      },
      {
        "start": 590.87,
        "duration": 2.18,
        "text": "on"
      },
      {
        "start": 607.42,
        "duration": 8.589,
        "text": "it looks like that's finished so the"
      },
      {
        "start": 613.22,
        "duration": 4.739,
        "text": "next step is we need to start the data"
      },
      {
        "start": 616.009,
        "duration": 4.89,
        "text": "sex capture connector and we're going to"
      },
      {
        "start": 617.959,
        "duration": 6.331,
        "text": "use the Catholic Connect REST API to do"
      },
      {
        "start": 620.899,
        "duration": 22.831,
        "text": "that so let's just grab this command"
      },
      {
        "start": 624.29,
        "duration": 26.219,
        "text": "scroll command leave that container okay"
      },
      {
        "start": 643.73,
        "duration": 9.089,
        "text": "so we just started the connector so"
      },
      {
        "start": 650.509,
        "duration": 6.301,
        "text": "hopefully our rows have been written to"
      },
      {
        "start": 652.819,
        "duration": 6.241,
        "text": "our data stacks Enterprise instance so"
      },
      {
        "start": 656.81,
        "duration": 10.019,
        "text": "we're just going to open up CQ LSH on"
      },
      {
        "start": 659.06,
        "duration": 10.23,
        "text": "that container and have a look so we're"
      },
      {
        "start": 666.829,
        "duration": 7.62,
        "text": "just going to grab a sample from the"
      },
      {
        "start": 669.29,
        "duration": 7.789,
        "text": "avro UDT table just ten rows just to see"
      },
      {
        "start": 674.449,
        "duration": 2.63,
        "text": "if this worked"
      },
      {
        "start": 679.98,
        "duration": 8.769,
        "text": "okay looks like this was the success and"
      },
      {
        "start": 683.68,
        "duration": 7.5,
        "text": "see all our data in there this one too"
      },
      {
        "start": 688.749,
        "duration": 4.53,
        "text": "taken a lot more time to set up if we"
      },
      {
        "start": 691.18,
        "duration": 5.099,
        "text": "had just done this all from scratch so"
      },
      {
        "start": 693.279,
        "duration": 6.42,
        "text": "you can see how useful docker can really"
      },
      {
        "start": 696.279,
        "duration": 5.61,
        "text": "be for learning purposes anyway I hope"
      },
      {
        "start": 699.699,
        "duration": 3.24,
        "text": "you enjoyed walking through this example"
      },
      {
        "start": 701.889,
        "duration": 3.331,
        "text": "with me today"
      },
      {
        "start": 702.939,
        "duration": 6.301,
        "text": "if you have any questions or feedback"
      },
      {
        "start": 705.22,
        "duration": 6.469,
        "text": "please let us know I'll see you next"
      },
      {
        "start": 709.24,
        "duration": 2.449,
        "text": "time"
      },
      {
        "start": 719.44,
        "duration": 3.089,
        "text": "[Music]"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T02:23:55.788490+00:00"
}