{
  "video_id": "yFpn6bd_D-I",
  "title": "Automating Data Operations for Apache Cassandra with Apache Airflow",
  "description": "Joint workshop with Rahul Singh - Anant Corp.\n\nPlease find below links that could be useful to you during the session (expand)üëá\n\nüí¨ DISCORD - chat, questions\nhttps://dtsx.io/discord\n\n‚ùìMentimeter  - quiz, games\nTBD\n\nüìö GITHUB  - materials\nhttps://github.com/Anant/example-cassandra-etl-with-airflow-and-spark\n\nüõ¢Ô∏è ASTRA  - database\nhttps://astra.dev/yt-12-07\n\nüìÖ UPCOMING EVENTS\nhttps://www.datastax.com/workshops\n\nüöÄ üöÄ ENJOY !! üöÄ üöÄ",
  "published_at": "2022-12-07T18:52:31Z",
  "thumbnail": "https://i.ytimg.com/vi/yFpn6bd_D-I/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "workshop",
    "cassandra",
    "database",
    "apache_cassandra",
    "astra",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=yFpn6bd_D-I",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "I found someone new now I have more room no more formats to change [Music] living in the future [Music] [Applause] [Music] foreign [Music] good morning good afternoon good evening wherever you are I can't believe it's almost end of 2022 but we have a very special guest today we have Rahul Singh from Anand and um you know basically he's going to be doing most of the workshop today um we usually don't have a lot of um kind of uh working with big data um kind of workshops uh but uh you know today is going to be a little bit different uh but before we dive into the workshop I just want to make sure everybody can hear me well you can hear two voices uh Rahul do you just want to say something so that you know we have only two voices and no more than two voices because sometimes the echo is very irritating you know so sure yeah hey thanks for having me Rags I appreciate it um yeah this is great I mean Cassandra is Big Data so uh you know it's at some point you gotta have some talks about big data so I'm I'm glad to be adding to your uh yeah that's great you know so what I meant is like I have a little bit more of the Big Data ecosystem that's kind of what I meant didn't uh and and we're definitely gonna see a lot more today having said that this is as many of you know um you know it's going to be a Hands-On Workshop uh and uh let me go to the next slide okay my name is uh raghavan shunivas and hopefully you're gonna be able to see me um and I work as a developer Advocate at um data Stacks I come from the Java background in the kubernetes background um uh are you guys seeing an update of the slides um Rahul uh do you want to confirm if you the slides are getting up no it's still on the first one it's pretty strange why this is happening and uh oh okay all right so now we should see that okay um so I um I'm a big fan of distributor systems I really love to teach and communicate um but one thing I'm really passionate about is the inner loop which is really about kind of the productivity gains you can you can realize um and uh you know I uh I use quarkus um you know other things but primarily I love to teach and communicate uh I also a mechanical engineer you know my undergrad was in mechanical engineering uh I like these workshops because not only you know do you all learn you know the attendees you know thanks for taking the time again uh but I also learn a lot as well um so with that said let's let's introduce the rest of the team which is behind the workshop um a lot of lot of effort goes in we have workshops typically every Wednesday around the same time uh we have a few more workshops till the end of the end of the year uh end of the month end of the year and then we're gonna start off with the boot camp in January 2023 which is typically you know how we roll um you can live stream this on YouTube um typically we used to have a twitch as well but but today we don't have a twitch um so you know you basically look at YouTube and and feel free to chat on YouTube uh you know either Rahul or myself or any of our teammates are going to be uh on the chat and glad to uh answer your questions uh for more detailed questions however we suggest you use discard uh which is you know for more long-lived kind of uh discussions and for more detailed discussions and not only uh our developer Advocate team but the entire data Stacks is going to be there to kind of help you out okay um what is a data sex devs Workshop without minty right you know so so some of you have been here before you know what minty is and I'm going to talk about that in a second but with that said um we also have a number of resources you know I say we are living in the Golden Age of developers we have so many resources available for us right and most of it is pretty much free right um so you know you can go to GitHub github.com datastags devs you know it's probably the right way uh uh the probably where to get started because there are a whole bunch of repositories in there um you know today we are using a different GitHub repository uh but but you know we will we will talk about that in a second um we use gitpart and most of our workshops um the nice thing about gitpart is you don't have to install anything uh and I know that Rahul was instrumental in getting us started on gitpart you know in our initial Journey so thank you there uh because you know I'm a big fan of gitpart as well and I'm sure if you have never used gitpart you will appreciate this as well we're going to use astrodb uh get that out of the way initially and then Rahul doesn't to jump into the meat of the workshop so so a very brief introduction about Cassandra Astra and so on and then we're going to get into the meat of the workshop with that said let's do a mentee um and uh give me a second so that I can switch my lights oh did I do something wrong there question six of six no that's not a good idea right laughs so you already have a hint but let me see if I can all right so let me go ahead and present so basically what you're going to do is you're going to go to www.minti.com and use the code 3518069 um ideally you know if you can do it on your mobile phone that's that's the best okay um I will also put in the code here but I could also do this so you can use this QR code as well but you know minty.com 35 18069 probably the easiest way right okay let's get started let's see if people are joining I don't think there are people joining yet uh I only see a few people turning um let's get some more people going I don't know we have quite a few um viewers um so let's give it a little bit of time thank you for those who have joined already um and and remember to remember that we usually have a quiz at the end of the workshop uh so you know great idea to kind of get used to it because when it comes to the quiz you not only have to get the right answer but you also have to be fast a lot of times I've tried to be um you know on the quiz and you know I'm just not fast enough right um so so um getting used to it right now will help you win some swag later uh and and today we have a really cool looking swag that I'll I'll show you um and thanks for uh I don't you know for doing this thanks to anante Rahul for doing this as well okay so with that the first question we ask and and I really like to see the dots pop up everywhere right where in the world you're from you don't need to be precise um is that Canada that's the first Dot looks like Italy yeah it looks like yeah in Africa uh one that's one in India yeah yeah is cool to see these dots right uh let's see a few more dots um nice nice and again you know um that'll be great a few more from Canada wow okay across the world yeah actually all right that's nice welcome welcome from wherever you are um in the world um any of you rooting for your favorite uh team to win the World Cup soccer tournament uh you know just feel free to put it in uh in chat uh you know if your team is still alive if your country's team is still alive unfortunately India never qualified you know that's where I come from uh and the USA uh you know just got eliminated what's your favorite team Rahul yes last week so you know yeah oh it's not it's not great from a cricket perspective either okay so if we do ask you some questions what is your experience level with Apache Cassandra um this is so that we can kind of tune our presentation or tune our emphasis um there are there is one experienced developer or administrator a new node SQL Guru uh and that must be Rahul or I'm not doing a mentee I don't want to break the game [Laughter] all right we have no SQL gurus who watch our travel all right um all right give it give it a couple of minutes um okay Mulan and again if you want to join this you know if you're late it's three five one eight zero six nine and like I said before we're gonna have a quiz where it really matters that you know you do it on time um as fast as you can but also answer correctly okay question two what is your experience with the patchy airflow is this the first time you heard of it are you an experienced developer or administrator again watch out okay as we thought you know most most everybody is kind of a beginner on Apache airflow okay but we have a few um and and maybe you have some tips for the experienced developer or administrator as well right uh it's always great to have these questions because it kind of gives us an idea of who the audience is uh um and and usually you know we we have an assumption but sometimes um you know we are thrown off and and we try to correct as we go along because we want to make it as applicable to the audience as possible fine final question is this your first data Stacks devs Workshop yes yes two yeses three S's we do get a lot of repeat data Stacks devs attendees uh but last count of how many workshops I attended all right um fifth or more Workshop okay that is cool so the majority um you know I are new um I forgot to put on the quiz music I will add that right now if I can um all right um so the majority of them are are new to uh the workshop which is kind of what we expected any any comments rather on on how this kind of looks um anything that surprises you sure yeah no it's uh yeah no it's good to have new people uh joining the you know the community um you know Cassandra is one of those things that people don't uh who are new to it first of all don't understand how awesome it is but when they see a workshop like this especially showing how easy it is to get started with Astra without having to have your own cluster I think it's great to have new people involved so yeah welcome glad to have you guys okay um so keep this going um either the mentee because we're going to have a quiz uh later okay but now we will move back to the presentation uh so give me a second before I can get there all right we're done with the mentee so moving on all right so today's challenge is we're gonna do some um ETL although Apache airflow is not an ETL tool right and you and Raul will talk about this um but but basically um we will talk about kind of automating operations um because whenever you talk about ETL kind of it conjures up this thing about manual you know kind of having to do all that uh and and whenever you do something manually uh you know it tends to be error prone so um you know automating it is it's going to be a lot better and let me stop the music so that you know it's not irritating to people [Music] all right um so um you know we've even built all this on gitpart as you can see here uh and you know we'll have a spark Master uh and so on which which we'll walk through um during this session and and like I said before it's all Hands-On you're going to be doing this um after a very brief introduction to Cassandra we're going to go through uh um the rest of the agenda and uh Rahul do you want to talk a little bit about this yeah sure sure so one of the things that I like to do is explain to people why something is important so we're going to just spend a little bit of time on open data platform why airflow has become such a popular tool right in the open data platform world and then we'll cover a little bit about airflow and Spark but the meat of the presentation really is about um running ETL processes in spark but coordinating it with airflow and also I'll walk through some real world example I'm not going to have the code as a Hands-On walkthrough but to do more advanced operations than for example cleaning information uh in Cassandra which is that's a common need and you'll see how useful it is to to leverage airflow and Spark as an ecosystem so with that said a very short introduction of nosql um it's basically a catchy hashtag that um you know people came up with uh and it kind of stuck uh it's really about everything that is not SQL okay SQL was uh very popular but the problem with sequel is that um you know you cannot keep scaling vertically for um forever right you know you you kind of kill over at some point and you're gonna fall off right uh the nosql no SQL is really about horizontal scaling um it's really about kind of leveraging uh commodity hardware and being able to distribute your data but let the interest I mean let the software or the platform take care of it and that's why you know this term nosql has become popular um and and if you think about this uh June 11 2009 I mean this is about the time that cloud also kind of started catching up right um but but the nice thing about nosql and the cloud is that there are a lot of similarities uh and it's a nice Synergy between nosql and the cloud um and and you know you can take advantage of both when you kind of combine them because typically if you think about relational versus nosql relationalists like I said you know is about scaling vertically to a point and then you know you basically hit the limits um with nosql it's really about horizontal scaling easy to scale um and and really um you know one of one of the downsides that you may have heard about people talking about nosql is that you know there is no concept of asset transactions right but even that is changing in today's work um you might have heard you might have seen some presentations in Cassandra day where we talked about asset transactions and uh we have a Cassandra Summit coming up in March where we're going to have more details about this um but you know again we will talk about that later again uh but but the point again I'm trying to make here is that um some of these lines are actually blurring a little bit and and some of the relational databases you can also do some scaling but but typically no SQL was really born in the cloud even before the cloud was there kind of you know if you want to think about it that way okay you mean have heard about this it's called the cap theorem or also referred to as the Brewers conjecture and essentially if you want to think about distributed system characteristics there is consistency availability and partition tolerance um you can only have two of these in a failure scenario when there is no failure everything hunky-dory you can have all three no problem right but when you you know actually come into a failure scenario which is typical in a distributed system you have to pick two of the three or you have to sacrifice one of the three okay um typically nobody none of the nosql systems really sacrifice partition tolerance because what happens in a partition tolerant or something that does not have partition tolerances um you know anybody can notice the inconsistencies there right um on the other hand if you sacrifice consistency there is still a concept of eventual consistency where I'm not on top of the leaderboard for 15 seconds big deal right but maybe it is a big deal right and that is why we are adding acid transactions as well um so you can tune your consistency levels and so on but if you make it you know um acid consistency then you may not be available for a certain amount of time so in other words you always have to sacrifice one of the two okay and the ecosystem kind of distributes itself right into either a AP system or a CP system like I said you know nobody really gives up on partition tolerance um so you know you'll see a number of these different uh databases or nosql databases kind of aligning into AP or CP but again you know there is ways in which you can say I want um you know different levels of consistency and depending on the different levels of consistency like you know whether it's eventual consistency or very strong consistency your system may not be available and so on so so um again the point is that you know it's for Big Data it's the the nice thing about uh about Cassandra is that it basically scales linearly and you know um Rahul and I were at AWS re invent uh we spoke to so many customers about Patrick Sandra and one of the things they like include the people like you know who came from Apple Netflix price line and so on is the fact that it linearly scales many of the nosql systems you know they go to a point and then the kind of level off not so in the case of uh Cassandra okay but I know that you guys are waiting to get your hands on so let's start with the the Hands-On okay so to do that let me bring up my so this is the um GitHub link if you if you just put a pound GitHub and I'll I'll try to put it myself you should get the link on chat okay if everything is working okay let's see it should work hmm okay so there you go it was a little bit late okay and all that I do is just follow along on this um you know GitHub link okay so it's github.com ananth which is the company an example Cassandra ETL with the airflow and Spark so what we're going to do is we're going to open this in gitpart the moment you open in gitpart okay so open link I'm gonna open link a new window but I've already opened this so and what the heck let me open a new one okay and and basically it will ask you for the GitHub uh ID or you can use I think you can use gitlab and others as well but but you know GitHub is probably the easiest okay and what it'll do is it'll bring bring up this git pod link okay and I opened it in new window but you can do it in a new tab you know whichever um works for you and you'll see here that I have the Repository uh and and if I do a uh git remote right minus V it will give you the link as well okay so basically it's linked here and if I change any of these um it's it's kind of you can use it like you get um terminal if you will so if you if you look at this this is really visual studio um on the cloud uh you have a file explorer window uh and you can edit any of these so if you you know if you click on this it shows you how to you can create tables and so on um and you can edit this okay so so if I do something like that or setup.sh and you know so basically this is the editor and then you can run commands here okay so if I do an Astra DB list it's probably going to fail because I haven't installed Astra yet but I'm going to install it in a second okay all right so let me go back here okay and continue I'm gonna create a database name called workshops okay and the airflow demo and again you can all that you need to do is click on this link I've already clear created all this so I've already created a security token uh I'm going to install the Astro CLI now okay so I'm going to go here and I'm going to install the Astro CLI okay and then everything should be hunky-dory all that you need to do is grab the command from the GitHub and and just add it here okay so let me go to two small heads here and get rid of the quiz music okay so this might be better when I'm doing a demo okay so it worked and now what I'm gonna do is make sure that Astra is in my path right um so I'm going to Source this and let me bring this up so that you guys can see it okay and it asked me to enter an astro token okay so so when you go to Astra you can create a token and the way you create it is you know it's pretty straightforward um let me show it here but all the steps are in there okay if you go to astron.datastacks.com and um just sign in okay and if you go to your organization organization settings okay token management okay and select the role database administrator it could be a little bit um more fine-grained but but you know we just picked that um the database administrator and just make sure you use this you save this client ID client secret and token somewhere um you know you can just say download the token details and it'll be there okay or you can copy paste it you know you can do whatever you want okay um because once you navigate away from this page you're not going to get this anymore okay I've already cut and pasted this and let me see if I can find that um I've kind of sneaked it in so let me see if I can find it and I'm going to put the token in there okay so just give me a second to find out all right and I'm going to put the token in here okay and hopefully it's all going to work so I'm gonna get rid of this I don't really need that okay I put the token in there and I'm good to go and I'm going to clear this okay so now I should if I list the um if I run the command Astro DB list I should be able to see it okay so you can see um you know there are five um you know different databases one of the things like key things I forgot to tell you was that uh you get a 25 dollars per month free credit so you don't have to put your credit card all that you need to do is put in your email that's it or you can use your GitHub link as well I use my email um and then you get 25 which is actually pretty good to run some fairly significant production workloads believe it or not okay um so what I did to summarize was kind of talked about um nosql talked about Cassandra as the linear scalable nosql database right and then what we did was we um we you know brought up gitpart just by just clicking on that link right and then you we created a token in Astra but keep the client ID and secrets as well because we're going to use it and Rahul is going to talk about that um and once you're done with this notice there are two windows the terminal window there are two Terminals and will be kind of uh going between one and one and the other the all commands window and the Run air flow window you know so so you know we're all is going to talk about all that with that said let me go back to the presentation um okay let me see where it was Okay so with that said Rahul it's all yours let's see some some thumbs up on our um you know in our chat to yeah to see if you know any of you are able to create the database and if you're not able to create the database please reach out and chat and we'll be glad to help you uh one thing again I forgot to mention was you need to use a uh link and and maybe this is relevant let me show that really quickly let me go back again to the astral link okay you have to use a free tier um not all of them are free tiers okay so so if you want to create a new database okay um you have to pick the North America and a region that is free okay some of these are not free uh the best thing to do is just use U.S east not you know like not Virginia okay uh like if you if you use um oh I'm sorry Google Cloud okay and use Monk's Corner this is what you want to use okay and you'll be good to go um some some of these are might be locked for you you know for me it's all open because you know I'm special right uh but but you know some of you some of these might might show us lot so what I would do is you know just pick Google Cloud North America and Monks Corner um to be able to create your database okay I think I've talked enough and I will let Rahul drive it so let me make sure that your screen is showing up okay just give me a second and uh I will thank you I appreciate that the walkthrough um and and you know uh give me one second I'm gonna get my Skype sharing sorry Siri is always listening in on me you bet uh you should be able to see my full screen now um all right give me a second and all right you're good uh excellent uh well thanks again uh ranks and data stacks for for having uh us uh just a little bit about um you know what our company does we help platform owners just think big so we help them with their platform design and essentially companies that have a global customer base and data stacks and Astra are great Partners um to help us give people what they want um and the way we do that is you know we have a Playbook which we're going to preview a little bit about about uh and in our framework we use tools like Cassandra spark Kafka airflow is part of our framework and um and ultimately once you build a platform you know we have an approach for how to manage it and even if you're using a SAS system like Astra you're gonna have other systems that and it becomes complicated so you need a little bit of a guidance so that's what we do we help people with their big platforms um we've helped quite a number of customers over the years a lot of brands that you probably use uh um you know we work with them and it's just like Cassandra I was explaining to Cassandra at a holiday party yesterday and I said yeah you know pretty much every app on your iPhone I mean it's powered by Cassandra in the background or Netflix Spotify you know Walmart Priceline everything you imagined as a as a global company the used technology like Cassandra and that's the type of work we work uh those are the clients we work with a quick preview uh of like where this Playbook and framework fits in for every challenge uh we have a tool so from when we go from a business challenge to how to make it into a platform our Playbook is about how do you design the platform how do you integrate real-time data with the rest of your systems uh once you know what your design is going to be what your platform is going to be is it going to be AWS is it going to be Azure uh ultimately then it's what specific components need to be used and you know if you're in this uh big data or data analytics ecosystem it's hard to figure out what tools to use and so that's where our framework comes in we tell people here are the different categories of tools here's what you can you know use in this category um and we're going to focus mainly on the framework today you know specifically airflow and Spark and how it works with Cassandra and uh you know how we do that we help people with Professional Services and we help them coordinate their managed Services uh we don't want to reinvent the wheel there's companies like data Stacks that have Astra we recommend people use something as a service so that you don't have to basically do busy work when you when you have a company like Astra providing a database as a service that town that would have gone into managing the SRE managing the devops you can actually focus on other things like data engineering and data Ops uh in fact there was a talk by somebody uh at AWS re invent last where they had migrated a bunch of databases to the cloud and they were saying well what do we do now we're dbas what do we do now well the new job for the DBA is data engineering data Ops infrastructures Code and and that's really where uh you know your Innovation should be going is a new feature it's not like running servers take care of your customers right you know yes exactly take care of your customers um so uh you know I give a big picture of you uh because that's just the way I think I always want to know why I'm doing something so when we work on data analytics platforms you know why do we do that well because business platforms have lots and lots of systems across many different areas that connect people process information and systems and we have this vision of what we say is Enterprise Consciousness which is real-time data across users and processes and systems um and and Technologies like Cassandra spark Kafka Pulsar they help us get there but data analytics are the heart of any business platform and every company has some system uh the bigger companies need technologies like Cassandra uh the modern open data platform is a relatively New Concept it used to be you know proprietary Technologies like SQL Server Oracle and you've kind of like you did everything around it and then there was Hadoop and Hadoop had 50 different tools to do what you needed to do uh and so now uh what we see is a a movement towards an open data platform which uses open core or open source tools and you kind of bring them together uh to do what you need instead of getting it from one vendor you're kind of piecing it together um so our Playbook is about helping people figure out what to use right um and and Cassandra is at the heart of that uh we love Cassandra because it has this idea of a data fabric built in and you can run it on different clouds you can run it on different uh systems like containers and VMS on premise uh Etc and what I mean by a data fabric and and Cassandra really uh you know alongside databases like couchbase they made it a possible to have cross data center replication and that's huge and we're going to talk a little bit about how you know when you're running a spark job to do ETL on a different data center you're not going to be impacting your transactional traffic right so so if you're using Aster you don't have to worry about all this stuff but uh because Astra just scales you know horizontally as you start to use more and more of it um but the idea of Commander as a data fabric came about because Cassandra scales infinitely horizontally you can put more and more workloads on it and and everybody can have access to that information in real time whether it's an analytical process a transactional process a machine learning process uh you name it um and you know there are native offerings like you know Cosmos but they're like prohibitively expensive and that's what we see people will use Cosmos and they'll say this is this is too expensive for us so then they'll maybe go to Astra or maybe they'll do K Sandra right because well quite honestly those companies are doing something for everybody and and each customer that uses Cassandra they have some special need for it um and uh you know Technologies like Aster make it easy to do those special needs uh in different scenarios a data platform is not just a database right it's it's actually a Confluence of lots of different things there's a stream involved nearly in a mature data platform there's a scheduler and that's what airflow is it's a scheduler um there are internal apis there are external apis that you may be bringing data into uh or sending data out to um but it's not always just like you know what Rags was talking about you have not only relational or not only SQL systems alongside with SQL and relational systems alongside with data lake houses or data warehouses and that's really where inside the core of the data platform um we need technologies like airflow to help us move or at least to coordinate the pipelines of information right and we want something that's repeatable automated uh not error prone uh so you start to understand that you know airflow is is a part of a much bigger picture it's not just about Cassandra and Spark it could be used to get data out of Cassandra put it into a a data with that you're using like snow like snowflake for example or it could be taking data out of a data lake house like uh S3 Delta Lake Etc and put it into Cassandra right so there's a lot of different use cases well I just want you to understand that we're going to talk about like one little part of it but it can do a lot more um we focus on distributed real-time components right um distributed real time means things that scale infinitely things that work with other things that scale infinitely uh Cassandra data sacks Astra they work really really well with spark but you know spark can be run by Google for you right Google dataproc is like a Google version of spark or AWS EMR um there are other Technologies like Presto that work well so in an open data platform our focus is in distributed real-time components there's a lot of tools out that are out there that are open source that we don't really care much about it's just well everybody else does that right but how do you choose from that landscape right like there are so many different Technologies out there for data and AI right this is just data and AI there's even more Technologies in general from a software perspective like CRM right those are like millions of options but in data and AI there's a lot of options a lot of Open Source options uh even the Linux foundation's data and AI landscape there's so many different tools out there right so you're not going to need all of them because quite frankly a lot of them do the same thing so how do you choose these tools um well we do a lot of research and we ask ourselves what is the industry using we don't want to reinvent the wheel and when we do research on open tools and open platforms we notice these patterns over and over and over again we see Technologies like spark everywhere we see Technologies like airflow everywhere and I'm not saying just Technologies like spark we see spark everywhere we see airflow everywhere and they work well with other tools like omensin is an open source data catalog that can store data in of the whole data ecosystem in your company inside cassand and januscraft or DSE graph but guess what it uses to collect all the metadata it uses airflow it uses airflow to basically populate all that data so if if another open source project as mature as amundsen is using airflow um that's really the signal that I look for is who's using these tools um the other signal for airflow is there was an airflow conference and I think there have been a few in the last few years and who are the people speaking about airflow you got Uber you got Apple these companies are using airflow and these are also the same companies that use Cassandra right so they need this technology because they don't want to reinvent the wheel um and ultimately what we do is we put all these things together and we categorize them and we look at the open source strategy we look at the the Google strategy we look at the Amazon strategy uh that's because not every company is just going to use all open source tools Hook Line and Sinker they're going to maybe use a lot of things on their cloud of choice and then they're going to need something different um the the benefit of airflow is that it is becoming a standard okay so there is a company called astronomer like datastacks supports Cassandra astronomer supports airflow but there's a managed version of airflow from Amazon there's a managed version of airflow from Google also so technology uh specifically if it starts to be managed by the clouds that's another good signal that okay there's a critical mass enough that people need it that they're going to make it a commodity right so you don't necessarily have to run your own airflow instance um because these clouds will help you do it for you we're going to focus uh in our in our in a cult Playbook there's a design component there's an evaluation which comes framework tools and then there's finally like how do you execute uh we're going to focus on data Ops right this is kind of where airflow really fits uh is how do you orchestrate and operate these data operations um and then in terms of operation right there's the how do we set it up how do we monitor it well in in terms of operation airflow can also be used for certain administrative functions and then finally when you put it all together when you put all the Lego blocks together uh we come up with a reference architecture where we say okay well if you're doing real time if you're doing real-time data and real-time data out real-time analytics real-time machine learning this is kind of the technology stack to use here are all the components when you put them together this is what it looks like um now in the data modernization aspect of platforms there are a lot of tools we're not going to get into today okay um but there's a lot of tools for ETL there's a lot of tools from a reverse ETL and what I mean by ETL it's not from your CSV file into Cassandra I'm talking about data coming from Salesforce getting it into Casino data coming from Cassandra and sending it back to Salesforce right that's reverse ETL there's a lot of Open Source tools out there that can help you do that um but on on this left uh you know column here um I have amundsen an airflow uh highlighted because amundsen is an open source data catalog so is data HUD uh but I'm just going to use this airflow and uh you know in a mature data platform you eventually have a data catalog and maybe that can be another talk how to set up Amazon on Astra but today we're going to focus on inflow um and the last thing about our Playbook which is really important is the reason orchestration automation continues integration and delivery the reason it's important in the setup column of our approach is that people across the board who are in development um operations architecture it allows them to be reliant on that process it allows people to easily get trained on that process you don't have to be an expert if something's automated you can just kind of start using it and you can you learn more and more of it um we came up with this schema because I tell even though it's really great I feel like only the largest companies in the world can Implement ITIL if you don't know what that is you can Google it it's called it information um autism Library infrastructure Library um it's a way it's a patterns and practices for how to manage technology at scale well you know when we were working with our clients a lot of the the folks who are in the startup world are in growth mode they don't quite get ITIL they're not quite their company that's been around for 20 years right so they need some guidance on how do we do this so that you know when when employee five leaves and employee number 50 comes that they're able to kind of quickly get involved so documentation is a very big big part of our approach but the best part about automation especially with a tool like airflow is the documentation can be in the automation the way something runs and how well it runs can be integrated in there right so when you put it all together we talk about a bunch of stuff uh airflow plus Spark is one combination but spark has different languages and we can get into that so you can do a spark program in Python and Scala and Java and R even C sharp if you want in Conklin if you want all of that together around uh put uh sorry combined into a airflow dag it makes a data operation system for you for Cassandra and it's good enough uh for a lot of the things that you do right now with the Cron job you can replace it with airflow and you can do a lot more in fact you know we'll show some automated systems that we've designed to allow companies to to make it self-service so like I need this data deleted from production well you want to have some approvals in place but it allows people to do this in a semi-automated fashion so you can put more intelligence and organization around your airflow dags if you want so uh or you can do the simple things do you want to spell it out yep yes yes exactly and I'll show a picture of what a dag is so um a dag is a shorthand for it's an acronym for directed acyclic graph yep and and dag basically uh the way I describe it is it's directed meaning it goes one way it's it and you can um directly meaning there's a direction to it right it's not and then acyclic means it does not infinitely keep going uh and then it's a graph meaning it's a it's a collection of things that go in One Direction so why is that important well we use dags in airflow we also use dags in spark it's the way to farm out complex workflows and tasks and execute them at scale that's the only way to do it is to plan out the work so a dag is kind of a plan of work a plan of tasks uh and when you see the visual you'll understand what I'm talking about and when we get into the Hands-On you'll see that as well you'll see what the dag looks like so what is here uh it's it's a scheduler it's used for scheduling and automating workflows and tasks so I want to make sure that you understand that there's a difference between automating ETL and doing ETL so that's why we have a separation in our code there's some code that is doing the ETL in Python but the python code we have in airflow it coordinates that task it it takes care it takes care of basically sequencing the work in a logical way right um You can use airflow to do a lot of different things you can use it to to manage a pipeline of of ETL tasks or things that are dependent so for example three data sets come in from external sources into S3 you're collecting it via one type of task right then those three data sets get combined into another data set now you can do that yourself you can have front jobs that run for those three earlier and then you can have another one that runs like every six hours but with airflow there's intelligence behind it it won't run the job that's dependent um unless and until all of the uh previous tasks are done and you can go all out with that you can do simple workflows like we're going to do or you can do complex ones uh what I like about uh uh and you'll see that GUI is that you can choose for tasks to be run one time uh at a particular date in the future with a certain business logic like every Sunday except on a holiday or stuff like that or you can do a simple crime just to cron to run schedule tasks on your cluster to do certain things like ETL or export uh and and you know Archive of information you can do that same syntax with with um uh with airflow or you can say things like every day right where you can say every month at this time um and all of this is managed in this GUI so that if there are errors if there are things that are taking longer you can see it all in one place and we're going to explore the UI uh when we get started but this is kind of like the top level of airflow remember it is not an ETL tool okay it is a tool that helps you coordinate ETL processes and it can do a lot more architecture uh is very simple um there is a database uh that it stores information in uh technically you can use you know any database but postgres is the is a recommendation um we have a web server that people can use uh to see what the jobs are how long they're running the web server also has an API so you can actually automate the automations in airflow if you wanted to and there's a scheduler that runs in the background and there are different executors we're going to use an Executor that just does one computer but if you use one of the other executors like the kubernetes executor or the salary executor it is possible to have many many many workers on many many many computers doing this for you so airflow itself is scalable it's gonna be working against Big Data against things like spark which is already a distributed system but airflow itself can scale and you can have hundreds of jobs running on airflow across many many computers without issue and how does it do that um what is the devops of airflow well your code which is a python file if it lands in the right directory airflow schedule will pick it up and it will just show you hey I've got a new dag what do you want to do with it you want to unpause it uh and it comes with a bunch of like examples um so you know we'll see the example ones and we'll use hours for the Hands-On here's a quick snapshot of adapt so for example uh you know you want to you know analyze uh some information to you you have an ingest process you analyze it to make sure everything's good you check the integrity and only after the Integrity has been checked you can then run a fork of tasks to then save the data and report it and at the same time you can say if there are errors found right if and only if there are errors filed go ahead and describe what Integrity issues there were and email that out and the fact is that you can have so many Forks in there you can get it really complicated um the benefit of having it as a dag is you see these green lines around this like boxes here if a particular task in the whole dag is slow or it's pending it'll have a different color or if it failed it would be red so in one quick dashboard you can see your whole workflow whether everything is green or if there's some yellow or you know and then there's actually about 12 or 15 different states like pending skipped active um which you know we have a pretty simple Dax and we're not going to see everything but uh you get to see different views of your dad this is just one view uh there are other views that are that give you more intelligence I'm gonna just turn off my one man it's really responsible even if you don't want to talk to it the rise of the Machines it starts with Apple Siri you know seriously exactly it works to help everyone um yes exactly gpd3 will just like completely do the talk for me uh here is the main dashboard when you log into airflow where you see all of the uh the dags whether they're active or paused uh you can like unpause it by clicking this toggle um these are basically showing you the previous executions if it was good if it was bad you can force a run you can refresh that the file um it's pretty uh intuitive uh they've added more and more and more Tools in here but I find it that once you get introduced to the UI you you're pretty much good to go you can just continue exploring and learn what's wrong for Apache airflow um foreign line tools to get it started there are command line tools to execute certain jobs it also has an API so it's UI command line and API so you can automate the automation uh through any of those with the command line or the API hey man we're we're either building robots or we're maintaining them right like that's the future um and and I could talk about Apache spark uh and go super deep into it but I'm gonna leave that for another session because spark is a it's a it's more than a Swiss Army night it's like the Swiss Army like it can do so many things for you at a very high level um but basically Spar um is a catch-all for when you need to do big data processing it's kind of the first thing you go to uh especially if you want to get data into Cassandra or get data out of the center very fast uh like lots and lots of information uh it can also do streaming we're not going to use the streaming function today um but Apache spark is a batch system that is the next Generation after Hadoop Hadoop had mapreduce and yarn uh spark builds upon that and makes it easier to do big data so these days even if you're using the whole Hadoop ecosystem you're likely using spark on Hadoop in our case we're just gonna have a local spark uh spark as I mentioned also has a dag it has a directed acyclic graph for the tasks it's running uh which are more fine-grained right like if you have a map operation that needs to run on um you know 20 different partitions across 20 different servers uh internally spark will make a dag and it'll keep track of that work and it'll execute it most people don't get into that level of detail but I just wanted to clarify since airflow has a dag and Spark is a dag and I wanted to make sure that you guys knew the difference um like airflow there are lots of different uh connections to outside systems so airflow can connect to different databases and different systems to do operations spark can basically connect to everything out there there's always a like spark connector for Apache Cassandra allows you to connect your Cassandra from spark but there are connectors for Kafka there's connectors for Hive or or data Lakes built on parquet you name it there's a connector for it lots of libraries there's other tools inside spark which again we're not going to get to but it has a graph processing Library it's not a graph database it's a graphic processing Library it has a main learning library it has full-fledged data frames to do operations in memory and then either you know send them back to the source system or to send them to new systems um basically you can anything with big data you can do it um basically um you know Marcus has a question um and he was like are these diagrams in GitHub and uh yeah I know that we're gonna add the slides we were just waiting for it to be completely up to date and and you know actually Rahul is looking for some feedback if you have any feedback and there's just put in there he'll update the slides and put it back in the same repository correct yep yep exactly and and um the a lot of the stuff that I use I don't necessarily recreate there's our public resources so um like for example the architecture diagram that I was showing you that's straight out of the airflow documentation um and I'll actually show a few more but yeah this deck will be put onto um again you'll have access to it and the data Stacks team will make sure you get the right links okay thanks um yeah sorry I have the chat here but I'm so focused on the talk that I'm not getting it thank you for bringing that up all right I appreciate it um so what I'm gonna do is to to scan over when I say it's a bonus round um what I'm doing here is explaining a advanced use of of airflow in in a way that I think a level can get their um their mind around because there's a challenge that I see all the time with with people that are using Cassandra uh which is they need to do data hygiene which is beyond TTL and when you have billions of Records across terabytes of data in Cassandra you have to basically uh iterate and if you don't have the right indexes even then even if you have the right indexes uh you have to basically do a spark job to uh to do these Advanced things like deletion or manipulation of information right let's say you found an error in your logic hopefully you don't but let's say you found an error in your logic you need to update data across billions of rows uh spark would be the way you do it so in Big Data there's lots of big data options right you have your um cold data stores and you have your hot data stores and when I put star uh on these items these all use cql so what I'm showing you you can use with any cql compliant system but the other thing is that this approach with using ETL sorry using spark with airflow to clean data or to do some hygiene you can actually take it and apply it to any of your cold data stores too like a data Lake right this is the same general practice um so it goes what to show you goes beyond just Cassandra and cleaning data and Cassandra it's it's actually any system um when you are um you know doing uh Big Data generally you try to clean your data before it goes into your database right you remove duplicates you you you you know you you get rid of like extra information you fix structural issues all this stuff you do it as part of your data engineer right your data rattling um but you may still even if you do everything properly you may still have to do something after the fact after it's inside your your big data system like Cassandra it could be that you have revisions of information and you you have a revision eviction policy that is it is not simply oh let's put a TL on it right or you need to move a customer's information from one cluster to another cluster right it's a multi-tenant system um you need to enforce gdpr the right to be forgotten I want to delete all my data from your systems right and so you can't just at least not now you can't just go to Cassandra cql sh and say delete from this table where this is equal to this and it'll magically do it right it doesn't do that um or you know update this thing to this value and you have some like complex sub query to to basically find out where to update that information so so the things that we were used to in the relational world we just can't take it for granted in non-relational and in SQL you would say hey delete right here's my where clause and then in your where Clause you have another sub query and everything would just work right you just do it even if it took 12 hours I mean you have you have those things right yeah it's a 12-hour job you're right yeah at least you can have a query which does that yeah but but yeah um you know you need to be a little um a little bit more mindful in in you know no SQL where performance matters right yeah yeah but performance batters and it's it's actually not as easy if you you mean this this particular uh you know type of delete operation correct it just won't work it's syntactically it won't work in cql right um now in spark SQL if you're connected to a data set uh in spark some data sets they support this type of deletion okay like for example in data breaks in Apache spark inside databricks they support this type of Syntax for Delta Lake because you know data breaks made Delta Lake and they made it right but you can't rely on this type of syntax even inside spark SQL which is the ability to do SQL with joins and Wares and all that you can do that against Cassandra if you're using spark right that's another whole another talk but um you can't rely on this delete operation so what do you do then what do you want to do deletes on tons and tons of information how would you do about it well you can make a function right um and this is a Scala function which gives it a particular where Clause it's reusable and I'm going to bank on later on because if you have a function that predictably deletes data from a particular table that's one type of automation but what if you want to give the ability for other people to do this on a recurring basis or to do this that's where something like airflow comes in right so here you know what you want to delete right where do you want to delete it and you want to delete it you can kind of do the same thing with with Scala I thought um right now there's not Maybe I'm Wrong uh data Stacks might have put it out but there's not a way to do kind of scalable deletes in Cassandra in this same way you can do it like here uh with spark so what we do is we basically say select the data from this table go through each partition in a parallelized fashion go ahead and delete it using cql and this is scalable because spark will go gather all the data that you want to filter from get it for you and then it will run these delete operations using cql and token aware so you can do millions of deletes like Cassandra cluster with spark and it will be scalable you can also use Scala and Spark to dedupe information this is a fairly complicated thing my colleague Obi found something on step back overflow that did it and we operationalized it into this function um and technically you can use it for any type of deduping but this is like for deduping documents in in casera this is a common use case where you have duplicate partitions duplicate you know versions and you want to get rid of anything uh that is not necessary this is the type of thing that again it's very hard in SQL um and and this code is is open source so you know we'll make sure it's already linked um so you don't have to go reinvent the wheel right so cleaning data deleting it deduping data these are common scenarios where we have to do it over and over and over again and you know we're not going to go tell a data analyst like okay go log into the spark cluster here's to the kingdom do your damage right what we're going to do is control that situation in combining baked uh logic like this with airflow allows you to control that situation and to give people self-serviceability um here is a complex dat which allows us to move hundreds of tables across hundreds of key spaces uh at scale right and the way it does it is it actually is um keeping the metadata for which tables and Rahul might have lost uh your audio okay let me see if I can bring back Rahul please bear with me let me check what's going on I know he was having some issues with us uh network provider but uh see give me a sec in the meantime if you're having problems let me know we use Skype for an infrastructure so maybe that's what's going on here so in the meantime if any of you are having trouble with the setting up the database in gitpart you know please let me know um while I try to reach out to Rahul and see if his if he can get him back okay he said he just got disconnected and he's gonna go to the backup line I'm glad he could do that if you have any questions you know feel free to put it in the chat while we wait for Rahul to come back hey you're back that's it that's a wireless broadband connection I think you're live already so uh if you want to start sharing your presentation then yeah or otherwise I can bring it up I got it yeah thank you I appreciate that noise um sorry about that folks uh I've been having issues with our primary internet provider and I'm not gonna gossip about which ones are good but it's a different conversation um but anyways what I wanted to finish this thought is that look you can use airflow for settings you can use or complex things it's just how you put it all together um and you know when we do the Hands-On we're doing a very simple operation but if you can just imagine and think about the complex processes that you have in your organization to get data in get data out um any recurring processes that you do on Big Data that's where uh we can we can automate it so uh you all should have this uh repository uh available to you um this is what Rags was using to get you started and uh you know I actually you know followed along with exactly what Rags did and got my Astra um CLI set up um so I already have my CLI but um go ahead and bring it up um and what we're gonna do is we're going to walk through the airflow bit there we go and uh just so folks that are you know new to gitpod um get pod is is all based on container based on uh in the background in kubernetes so when you're not using your git pod it just kind of like suspends it but as you can see that was like five seconds and it's back right um um yeah this is my this is one of my tokens uh that you know I basically dragged in here and since I'm gonna you know destroy my database I don't really care but but uh one of the things to to note is that if you have like a file in your desktop let's say you downloaded the CSV or the or the Json token file from um here like I'll show you right so here's my Astro UI if I downloaded this file and I put it on my desktop in fact I already have this file so I'm going to overwrite it I can actually um you don't see it because it's off screen but I can actually just drag that specific file from my desktop uh onto my UI in my git pod and it'll be there right so um anyways I I went ahead and um configured my my Astra CLI earlier um so if you say Astra oh hold on one second yes bring up a new bash yeah yeah that's exactly um well you know what I'll go ahead and reset it re reinstall it because it looks like it didn't um you know keep my change I didn't commit anything so that's why I think I think the nice part about you know if you're if you're falling behind in the workshop it just go back to you know square a like what uh you know Rahul was doing here you know he's just gonna install Astro again and no big deal it should hopefully work I know as well so if you're falling behind don't worry about it yeah exactly so yeah exactly so um I'm in Step 1C uh which is installed all the Astro CLI um you know I've already uh database setup uh so it is I'll install everything and uh the reason we need to close this is that uh it goes into the pack right so like um and so here's here's one of my terminals which is what I'm going to use to install stuff and then I'm going to run my airflow commands here uh you already have a split screen um but I've got Astra in here right so I'm going to do is say and so I sourced and if you want to see what that file is it basically has uh you know where to um you know pick up the CLI right and where the home directory is going to be so um I've got Astro now in my command right so I'm going to do Astra setup and the aster token uh there's one set of tokens for just you know when I make the database and then there's an organizational level token which I have here um and it's the one that has Astra in it Astra CS and I copied it and I just right click right and it's gonna put that there and I've got Astra set up and so if I do Astra DB list it's got my two databases um I have this this old database that I use for other demonstrations and I'm using the same exact database name that is in the demo so I'm I'm doing exactly what's in there uh this is the new Workshops the database that I made and I made a key space also called uh um airflow demo now if um you know you didn't have the same exact key space that's okay but um this command that I'm going to paste which is also in the documentation it's going to create uh in the uh the database at this particular key space if it doesn't exist right but it already does and for some of you know that quite a few of you um have attended data Stacks there's workshop and chances are you may have already created a workshops database in which case you know you really don't need to do anything other than you know kind of grab the token right and uh and then you can you know you can create the workspace because even if you try to create the workshop you know it's not going to create it uh if it's already there okay so so all of that instructions are in there perfect um so because we're connecting to a cloud database uh we have to have a secure connect bundle uh and I'm just you know pasting the command to download the secure connect bundle for this particular database um and here we go and this is going to allow us to basically talk to Astra securely from our from our gitbot um so what are we doing with our demonstration right we're going to be getting data into a Cassandra table where we're gonna take it out of that casino we'll put into another Cassandra table uh using spark um and uh we need we need a table structure right we need some tables in there a key space does not have any data it's just a container for tables so um we're going to be creating two uh tables one is previous employees by job title and then there's another one day's work by previous employees by job title and this is a you know Common scenario we have to do some Roll-Ups on some data on a batch process maybe you're doing it in real time but you want to reconcile this information uh and so these two tables we're going to go ahead and create them you can do it in the GUI uh but with Astra dbcli you can just say hey in this table uh sorry in this database go ahead and run this file and if you're used to cql sh yes if you've created a key space with another name like you know Nadia flow demo um you just have to change you know the setup to use that particular key space not a big deal exactly yeah exactly exactly um so we've got uh uh a database we've got a key space and we have two tables in there um and you can verify um you know uh those tables by you know going into cql Stage on the GUI and we'll see it in a second but um now we're going to trust the fact that we have uh these these tables in there um and the next thing that we're going to do is we're going to set up airflow this is a sample script uh there's many different ways to run uh airflow okay um this is just running it on one computer one container so all the components are here okay um and it uses not postgres but it uses cql light because it's a demonstration this is a perfect concept type system right so you don't have to run postgres but it's recommended if you go into production you do a real database basically um and uh you know maybe somebody will commit code to run airflow on Astra or Cassandra but right now in production you need a postgres database and the recommendation is if you're on the cloud don't run postgres by yourself right just use like a managed database like on digitalocean we use manage postgres or our AWS there's RDS so what's happening here uh we're setting some specific versions for airflow um because if you don't pop in it that'll download the latest one and we don't have control over which libraries got downloaded right and then once it's installed it's a Python program right so we're just installing airflow using pip we have python on our system and the DB init will in our case make a a schema inside sqlite and then we need a user we need a first user an admin user to be able to log in the user system is pretty robust you have roles you have permissions um to give people the ability to just weed what's going on to see what's going on to um you know uh to to start and stop jobs for a certain uh you know set of jobs um but we're gonna have a super user We're not gonna get into the you know role-based access control but it has a pretty robust system um and once the database is create a database is created once we have a user in there we run the web server the web server is what gives us the API ability uh as well as the ability to uh you know call apis uh and UI simultaneously you can have it um and most people they forget this they forget that after you run the web server you still need the scheduler so the scheduler is what is kind of like watching out for changes in your files it's the one that's saying oh it's time to go run this job right and it's it's the whole manager for all the stuff that airflow is doing for you uh but you need all these components you need a database right uh you need a web server and you need a scheduler um and um it's actually pretty simple I mean when you remember the diagram um earlier with the airflow system runs like this between the web server and the scheduler this is a simple setup you can have a much more complex setup later on if you want to so I'm going to go ahead and run oops I'll do that I'm going to go ahead and run the setup program and just mind that it's going to download install and it's going to ask you for a password this is a throwaway system I'm not that concerned so you know um I'm just gonna have like admin and one two three four five or whatever but um one of the biggest security vulnerabilities for airflow was that people were not securing even after they created a user you know a secure user they weren't protecting this endpoint okay and when we were recommending airflow at a client they're like this software is insecure I'm like no the software is fine you have to protect the endpoint with a firewall you can't just put it out there uh and that's the thing with any you know open source tool I remember when people were like oh uh which I don't like that database but like is insecure or Cassandra is insecure because people can just log in using Cassandra Cassandra I'm like yeah that's true but anybody that knows what they're doing they wouldn't just leave Cassandra Cassandra as a default user um and you know because you're using Astra you have some security built in already right people can't access that database unless they have a secure connect bundle and the token and everything okay so a simple password that I'm not going to forget and that's gonna ask me again and you will have to make note of that uh so yeah you know keep it simple uh just for because it's throw away but uh I completely agree with you Rahul you know there's always this balance right between getting started then and running a production system it's two two different beats yeah ephemeral right uh Astra is gonna store your data which you can come back to um but you know Astro databases are serverless it can get suspended uh gitpod is ephemeral it starts up when you need it when you're not using it it gets suspended in reality you're gonna have a database that is always running and always has data in and out so it's not going to get suspended um you know there are some takeaways in terms of like uh we'll get to that in terms of how I would recommend you use airflow but this is just a learning experience and you can run it you know you're doing this Hands-On the goal is for you to understand what interflow is and how to use it so airflow is at this point running okay and um I'm just going to keep this here I'm not going to turn it off but we'll notice over here that certain ports are exposed right and port 8080 is exposed and and Port 8793 is exposed um so if you click on this it actually shows you the airflow interface so I'm just going to log in and I'm not going to do anything right now but it gives us the warnings right don't use SQL Lite in production it tells you oh he doesn't like my password it's too simple but that's Chrome telling me that uh do not use sequential executors a sequential executor just runs the next thing in in the system uh and then it'll do it but as I mentioned their salary executor that can do in parallel there's kubernetes executor that will just create containers for each job and do it for you and under the hood it's using the kubernetes you know job container type basically right um here are example dags okay and uh you don't even have to look at the code uh in your uh Visual Studio in your git pod you can actually just click on this right here and you go to the graph View and that this is a simple uh thing but you can start to see uh you know at any given uh job or in this and I call it a job or dag is the same um you have different views uh the grid basically shows you your previous executions and how long it took and we'll show that in a second the graph shows you the current running uh dag in this case there's only one task um you know what is scheduled what is deferred failed Etc the calendar shows you previous executions so if you have a job or a dag running for years um you can actually see the history what was the success here how many were successful how many failed over the course of a year um task duration and okay when you see all this you know what I'm gonna wait I'm gonna wait until we're running some real jobs but um I continue uh to go to the spark setup uh one thing that that's interesting uh you want if you're curious to see what this code is without even looking at Visual Studio in gitpod you can see the code right here right it tells you which version of your code is running so because um the the schedule every five minutes it's watching out for files okay and so if you notice that your job is not working as intended you can just go in here and say what what version of my code is driving all right okay so we're gonna we're gonna put this aside we're gonna come around question um Again unchecked by shashikant who's asking is airflow only for data related ETL jobs automation or any other workflows could be created what are other examples and ion thanks for performing it is not just for ETL think of it as a workflow manager yes exactly thank you Ian I appreciate that um so actually I'll bring this back screen back okay um it's not just for ETL uh anything that is a long-running process that is dependent on other long running processes or you have subsequent long running processes and you are coordinating it across a distributed system right many many things like I want data to come in from S3 into Cassandra I want data to go from Cassandra into snowflake DB I want some spark job to enhance this information so you can do the whole uh kind of range of import export processing you can run machine learning tasks in sequence so anytime you were doing sequence of tasks with data you can you can use it and and this is a relatively new uh feature which um uh I have not done a presentation on but in this case what it's showing is you can have jobs triggered by S3 events so in this case two different S3 data sets when they get updated it triggers this dag and then that DAC produces this S3 uh um you know object then that uh gets used subsequently this is one kind of dag where data getting updated we'll we'll run this and then still update this automatically right you don't have to do this on a schedule um same thing here you have two data sets here you have another uh file here and um the outputs are all getting consumed eventually to have two different outputs so this is an example of it could it could not just be ETL it could be I want to take this information and I want to enhance it I want to run some machine learning process to classify information I don't want to save the data here um yes so hopefully that answers your question uh here's an example of a workflow and actually we have example code where um you have a uh a airflow DAC that will create a Google dataproc cluster which is a spark cluster okay and it will create an aster database and it will go and do some data process with Astra on the spark cluster and then it can destroy the spark cluster so airflow can actually be used to coordinate some of the infrastructure that you need from an ephemeral basis right when you have a long-running job that needs hundreds of computers you don't want those hundreds of computers running in your spark cluster you can use airflow to say make me a temporary cluster run the job and then go ahead and delete it okay um thank you I appreciate the question um so we've got airflow running we've validated that um we also need to run spark so I'm going to make a another shella here for Simplicity and um so this is not the way you would um you know run spark in production um but it's you know your your container your desktop this is perfectly fine um and uh you know once we run the spark Master it actually runs a web server that gives us information about where to connect our slave processes and it's actually if you look here um it's it's going to be Port 8081 so if you click on this this is spark it's running so what we're going to do is we're going to grab this this is our local address for our for our spark master and we're going to make note of this and we're going to use it in a few places okay uh and I'm just going to drop it in my readme for uh actually you know what I'm going to make a new file called scratch I'm going to just drop this in here and um the next thing I'm going to do is I'm going to start a a slave process that is going to connect master um and by the way if you're a user of data Stacks Enterprise um you don't have to do any of this spark just can come up on the cluster you enable it and it takes care of all this and technically speaking you can use airflow with data sex Enterprise right because it has spark and Ask Cassandra in it um in fact we've done a lot of that so so you have here in your master I'm going to refresh this should be coming up soon happen here no you didn't know yeah um let's do a refresh in a second okay there you go so we have a worker um you know in a real environment you will have 6 10 200 workers basically that have their own amount of memory their amount amount of cores um and it's pretty impressive I mean I don't know how I got this but I've got 64 64 gigs in my container so I can do a lot of damage which doesn't seems amazing the things you can do in guitar right you know it's like and yeah this is a lot I mean I know you can run you can run Cassandra on this container if you wanted to yeah so we've yeah so we've got uh air flow right we got that running uh we've got our spark cluster running and so the next thing is we want to run a process that is uh gonna run a spark job for us um and before I I'll get that started I'm just going to go over the code right so we have here a dad and I'm going to go over what it's doing and that and we're also have also have some code that is being coordinated by the data so dag is a does it take that directed basically right and this code right here is it's a task right I have a task that's using this operator bash operator you can use a spark operator you can use a Google Cloud operator to do specific things but this is a batch operator that's just running a spark command I could run the same exact command on my computer I could put the same exact command in my uh cronjour but I'm using a bash operator to do that for me and in my bachelor operator I'm saying run this uh you know uh uh version of the connector with my spark instance use this properties file and here's my extract and load um which we'll look at in a second here's another task and I could have many many tasks defined like this but the way it knows which one to run for second is this this syntax right here right so this job is dependent on this job and using the syntax you can create very complex uh Dax so uh I'll show you one uh quickly uh that's a little bit more complex it's one of the tutorial ones and I'm going to click on code lots of things like e extract transform and load right and it's got all these other uh you know kind of like helpers in here to tell it what the uh the documentation is going to look like but here's here's the dag the load task is dependent on the transform tag the transform tag is depend on the extract tag okay and so I'm going to go back to um my spark so let's take a look at extract and loads extract and load is a standard spark Python program there's nothing special about this you can run this on any spark instance uh connected to Cassandra without an issue okay and this other one etl.py again standard job okay um so in our case in our Dag load and write is happening first which is this one right extract and load and then ETL which is ETL that Pi is happening next so what's happening here we're doing an extract and load and we're taking a CSV file which is here okay take a look at the data a little big lots of data in here and we're taking it and we're selecting the data from that CSV data frame and we're putting it into this data frame and we're saving it to our Cassandra or instance or Aster instance so how does it know where to to put all this well there's a properties file which has this information and we need to update this to the right one okay so we're gonna go back here and all of these all of these instructions anyway so including updating the properties cons file yeah um just make sure you have you know exactly details from Astra and the master um that's really what you need here that's basically what you need and you know you don't have to change this because when we down we named it exactly this right um and we have our username and we have our password and so this information is going to come from that um the generated information right the generated token so we've got our client ID and client secret so I'm going to just go ahead and grab this good thing I dropped that file in there right uh you wouldn't leave it in there right don't leave passwords in Source control yeah and and even if you have it temporarily you know remember not to check them in exactly um in fact I should put that in the git ignore first leave out any generated tokens and so it it goes uh essentially um I think it's okay to do this in here I feel like um everything else is set up for you so you don't have to you know mess with it but you can kind of it's self-explanatory there's documentation on the spark connector on GitHub uh data Stacks has released that um and then you know the rest of the stuff again it's it's all spark stuff but bear with me this is not a spark talk it's about airflow I don't want to get too uh too much into details and so we take the data from the file put it into the key space name that we have as a configuration property and we put into this table so once it's inside previous employees by by job title the next job will be run for us which is etl.py so what etl.py is doing is saying okay well let's go ahead and connect to the e-space and the database and we use this tool to make it easy for us to run spark SQL so in spark SQL you have to connect it to account catalog again if you're using data Stacks Enterprise you don't have to do any of this you just go into spark and you can use it um but what we're doing is we're doing an SQL query that you could not do in cql right we're running in at like a legitimate SQL query with some uh you know absolute value uh with a date difference between the first day and the last day and it's calculating this for us right you couldn't do that I guess you could but you wouldn't want to do it in cql sh with user-defined functions um and we see we take that data and then we save it to this other table right that's it that's all the whole thing is doing get data from CSV dump it into one table do some calculation and then put into another table right and you would think well you know what I don't I don't need anything I could just do it as part of a spark job but then what if this needs to happen on a cyclical basis every six hours a Cron job um is not distributed it runs only on that one computer well if you have a high availability airflow setup with postgres with or salary uh with many many workers that job will get executed no matter what it will make sure it gets done right so we've got our property set up we know what our dags are um but there's one missing piece in order for um airflow to know about your your job um you have to copy this file into the right folder and this is uh you know I used to be like why do I have to copy this file why doesn't this look for it well this is what allows you to edit files play with them uh make your changes and then when you're ready you copy it and so from a devops perspective you can set up a process where in GitHub or gitlab right you can have a branch that if you do a pull request or a merge request into that Branch they can automatically do what I'm about to do for you and that's the way we do it right a merge request gets approved it gets copied into the right folder and all of a sudden the users can start using that so where does this need to go uh there's a folder that we're going to make which is uh airflow is already configured to look for it but we don't have the folder and we're going to just say it's um oops we don't have it in there so I'm just going to make sure okay and I'm going to copy this spark dag which I showed you earlier to there so I'm just going to say copy I'm not going to move it uh okay you know what I'll move it yeah I'll do with instructions say that should be okay as well yeah okay I moved it and what we're going to do is we're going to be waiting on uh this you we don't want to wait five minutes so what you can do is you can just say refresh and then a few seconds um we should see our dag so how do you know what to look for oh you have named it very specifically we have named it as example Cassandra ETL right and I can just look forward to Sandra because I think yeah so I'm just going to go ahead and do refresh a few seconds and it it takes a few uh seconds sometimes because this is not the high availability uh version of it um but it does come up um come on give it a second and while we're waiting for it to come up um we do need to um set up a connection inside uh airflow um so back here when we're looking at the DAC okay there is a value uh we were using for um the connection okay and a connection is basically something that is reused as a variable but it's a special type of variable it's a it's a variable that is used for operators that have data in there okay but it could be a system or a service as well um so in my let's see here we go um anyways it's it's a it's a variable in our case which is like our spark master of default um and we're basically going to set it in airflow so we can reuse that data later on okay um so admin section has variables configurations and connections amongst other things we're going to go to connections and right now it just has yarn assuming that it's going to run on a Hadoop cluster but we're going to go ahead and edit this and we're just going to update the host to our product master okay and you know in the background what it's done for us it's kind of set out an environment variables so whenever we're running spark from bash it'll always send it to that particular uh spark master and if you notice there's many different types of connections web hdfs redshift um you know Livy uh Cassandra right we're not using Cassandra directly we're using spark but you can you can connect it to Cassandra cluster um lots of different existing types of connectors in here um variables are things that you would use over and over again that are not connections um and then configurations are specifically inside airflow.cfg so in this case for security reasons we're not exposing it but if you did you would be able to see the contents of this airflow CFG okay remember this connections all right let's see if our Cassandra job uh got shown here make sure our uh scheduler yep okay it's still running and it is uh always kind of like looking in the background to see if there's stuff in here we go I told you uh so we can filter it and if I find the exact thing it'll bring it up here so we have now the code that we uh were looking at earlier you know instance right so we've got an ETL job this is one View and we have the graph it's not a complicated job it's just one and then the next thing um and if I want to double check to make sure like is it the right code yeah it looks exactly like the code that I was looking at earlier right um so what's the interval so in this case we did not put an interval because we want to run this ad hoc but we can look at other examples to say well how could uh you know what does that job look like that I want to run daily or what I want to run every minute or something like that right so these are Chron syntaxes and if I look in here that is basically being set at your schedule right it's one way to set it um or or when is the next scheduled instance so lots to dig deep into the uh the configuration of a DAC but I'm just going to go ahead and run my DAC and cross my fingers that is so I'm pretty sure it's gonna work rags and Stefano testing the heck out of it if you want what I can do is you can go to Astra and make sure nothing you know that no table is created right and then you know once it runs you can show that you know the tables got created up to you yeah yeah no exactly so uh you know I'm gonna run a um to see a run a command um in my uh Master shell right um and I'm gonna just say select Star right all right there's nothing in there yeah so we know there's nothing there the table is there though right good and um um you can enable and run the the job from here or you can enable and run the job from here if you if you find it in your big list so you can enable it so let's unpause it and unpausing it won't do anything for us because it's not a an a schedule to do this in any particular interval so I'm going to go ahead and trigger it and I can trigger it as is where I can trigger it with configuration meaning I can override certain settings if I wanted it to to run slightly differently like a different table or different key space and I haven't written the uh the dag in a way to take external configuration or external variables like that but you can do that um it's running right uh in here and I can go ahead and click this directly and it's running the dag and this is the job that it's running if I wanted to see review all right so again does a you know a question about one question about sparked Cassandra integration typically in Cassandra we design tables meant for specific query right you know which makes absolute sense uh because you design based on use cases but here we are making any type of SQL query uh SQL query uh what kind of load does it put on Cassandra great great question um and so the way that spark works with Cassandra with the spark connector is that it is token aware generally um and it will go and it'll say go to the whole thing with my token ranges grab the data and if there's not a predicate meaning if there's not an index in there um it will pull all the data and it will do the filter on the spark side that's that's the way Big Data works if you don't have a a way to filter it on the on the system itself um so if you have a star schema you can do joins in spark and there's no load but when you're doing something like in our case we're doing a full scan and a full you know processing and a full save it's getting all the data but it breaks apart the work and it doesn't do a cql command the way you think about it right it's not doing a select star on one computer it's doing a select star but it's doing it by token ranges based on the loan of that machine and it grabs it brings it together parallelizes the work dust up in memory and then it goes ahead and saves it back um great great question um so unfortunately but fortunately for us we hit an error but what happened I want to know what happened so let's take a look at our audit log and what it's saying is um okay it started running up here and there was an issue I think probably here so let's take a look at what what happened it said an upstream command failed right and I'm gonna go ahead back to the graph View and I'm going to say what happened here right let's click on this and I can go to the log for that particular specific task and it said da um please install it okay the example executor requires blah blah blah okay um bye I thought we had taken care of it but that's fine I'll go ahead and then and um and run it and and this is basically it's related nothing it has nothing to do with what we're doing but it's just a airflow saying hey there's an example code here that needs this operator and you don't have it so we're just going to satisfy that okay and it'll take care of it [Music] um could it be related to putting the double quotes you know I've never tried I mean I didn't try it with the double quotes uh I don't think so so he would have told us if this is specifically related for that yeah um so what we're gonna do is um go back to our dad trigger it again and see what happens and uh this view this grid view uh is one of the newer views what it's doing is showing us hey this one thing failed right and it took 26 seconds and this other one didn't run because this Upstream one failed so it's like a sequential view of the tasks and it looks like it's working it didn't fail yet you know hopefully it's fine um let's take a look let's take a look and and this is the part where you know airflow starts to really give us the value of it's managing this work for us um let's see uh enter location okay so right girl you may be let's go back and change this to there we go and we've saved our file we haven't made any changes to our spark deck right so we should be fine and then go back and let's hit back here and I can just go ahead and run right I don't even have to go to the trigger I can just say run um oh that only works with this so fine we'll we'll have to run the whole thing because it is basically saying I can run that particular task for you right now but in this case because that's only a feature in the door so let's see I got third run running it's running yeah and that CSV has a lot of stuff in there so it's not a you know it's not a small file fingers crossed yeah good so this means that actually it's doing stuff um and you know while that's running we can look at task duration right so you know how long did this thing take over time you get to see ups and downs for a particular uh you know job um and then you can see for the last 100 runs or the last 365 run uh it also shows you if you have configured it how many retries did it take for a particular task you can configure that you can say this particular task go up to five times go up to ten times and that's just a part of the the function that you wrap your command in okay um Landing times how long did it take overall right in this case uh took about 63 seconds which means that something is finished right so it the first step finished and then the second step has started right taking some time remember it's a it's it's not a lot of information but it's it's big enough for you to understand the value of spark you know and so this whole thing was successful so if I go to the graph view I can see these are green good green is good um and I'm not just talking about American dollars uh um and if I look for um actually I want to see it in the in the in the uh the master view here let's just scroll down to um there you go so here you go so it shows us a history on the dashboard of hey there were some failures right and there's zero running how many successes there were and how many are queued up and that's what these mean queued success running and failed and then these are basically telling you what is the number of statuses so there's been two successes uh new zero deferred so I could continue doing this here um I could jump directly to one of these interfaces so this this master view is actually a nice shortcut to see what you need and get to exactly what you need to run so if there was an error I could just say you know uh go to the graph View click on it go to the log right and that's it that's airflow for you to be able to organize your tasks and execute them let's verify first of all that did this do what we wanted it to do um so in my other department right back yep we moved the data to Cassandra and we then processed it from Cassandra into another casino table this is one command uh this is the data that we inputted from the CSV file and then the data that we processed into the uh days worked by previous employees by job title right so put me back to straight Count's question right these two tables were designed in the way that Cassandra table should be designed and Spark is doing the hard work of materializing this view so basically you need to keep the partition key in mind when you're designing the tables correct kind of it uh with Cassandra exactly exactly so this is the the demonstration um you know we have a lot of examples on uh on our GitHub in fact I'll show you the I'll go back to the presentation so that I can walk you through um you know what are the examples there um and this is for you guys to play with right it's not it's it's all open source and and again you know if you want to do it yourself um the instructions are there um if you've already set up gitpart it'll be around for a little bit if you want to be sure you can just pin it and you can try it out you know anytime uh unfortunately we're running out of time um so we'll have to keep going yes exactly and here are just you go to the the github.com and you just look for airflow there's like seven different repositories about doing different things like even one with Cassandra on Presto one with DVT the one I mentioned with Google dataproc and Astra uh one that uses uh amundsen uh with Cassandra so lots of other examples for you to take a look at um pick things you don't have to run your own Spark okay just just think about it if you if you're running complex spark jobs on hundreds and hundreds of jobs managing your spark cluster is tough so um think about using a managed one um not all spark code is created equal don't let anybody just say here's my spark job can you put it as an automation um if you have a compiled spark job it allows you to have control over uh the quality of that spark job on the airflow side um you don't have to run it by yourself if you don't want to uh you can run it on so Google uh composer managed workspace manage workflows for uh Apache airflow from Amazon and then astronomer makes it easy to use on any cloud um not all dags just work so sometimes you really have to think about different Stacks different applications different customers um in terms of inside the organization sometimes you may have to have different airflow instances for security reason um the same dag May to see is it running okay right and we may have to put some retries in there it's not a set it and forget it but once you figure this out it is a set it and forget it uh key takeaways don't reinvent the wheel use the passion spark like literally that's the best tool for Big Data operations on Cassandra um you can do other stuff but Apache spark is a very common way of doing it use a scheduler so Apache airflow with python and python spark is a very quick and iterative process to do these things right you notice I can make a coach range copy it and then run it and then once it's baked in I can maybe think about a compile job as well um thank you and I'm going to pass it back to right all right um so I know that we are going to do minty um and we are almost there uh and what we're gonna do here is um you know usually we have a form where you can submit your uh um you know Windows supplementary but but what we're going to do here is you're gonna mail it to um rahu it's s-i-n-g-h correct uh yeah you hold that sing it on that us okay and and and and you can put it in chat as well just to you know make sure um that it's there um and it's really a cool looking shirt there no architect as you can see okay uh all right so let me grab the screen all right and let me make sure that you are okay that's good all right so um you know we are just about to jump to mentee you just give me a second so that I'll be ready to go all right and okay Chris time you guys ready um please feel free to join again if you haven't three five one eight zero six nine okay and I'm gonna put this three five one eight zero six nine oh I don't have the yeah it's still showing my miles what did I do okay there you go so it's three five one eight zero six nine okay and let me start the quiz music just to have a little bit of fun if it's too loud let me know I will try to okay you guys ready again three five one eight zero six nine and let me see what I am okay okay all right we have a few people joining question number one of six and rahula did I lose you are you still around uh looks like we lost Ronald again um give it a second and I'll make sure that he's gonna be joining I'll in the meantime you know you people can join in looks like gravel has some connection issues again Riley or maybe a mute that's what I see here let me make sure all is there otherwise we can just proceed if it okay question number one you guys ready again you know for those of you who are joining there are quite a few who are there uh all right so very first question answer fast but most importantly answer it correctly the best that we use today pretty straightforward question is it Hadoop is it Astra is it good part is it none remember it's a database as a service that we use today what did we use today we did not use anything or did we use git power did we use Astra or had you which one of these is a DB as a service I'm sure we used all of them right all right nobody got it wrong which is cool but let's see how fast um people answer it uh Rahul is back yeah perfect okay Joker is the fastest nice all right cool but it could be anybody's game Simba Shashi marks Ajay Ricardo shaudia anybody okay moving on next question two of six answer fast but make sure you get it right which of the following is not a characteristic of Astra it's Apache Cassandra in the cloud it's Cloud agnostic is it single tenant and there is no free tier remember this is a little bit backwards question so let's see how how people are gonna answer this perfect I think I made it too easy this is not good nice nice yeah this is not good all right let's see who got it first again you know you've got to be fast right and I think Nick was the fastest nice yeah good job Nick yeah I think it's our name um you know you know it's funny uh at Cassandra day we were doing this and uh I was playing against Mick the PMC of Apache Cassandra and we were just like that was that was that was fun Nick is fast all right now now we go to more you know tough questions what is Apache airflow is it a workflow tool to manage any ETL operation and data pipeline or is it the infrastructure as code to is it really a big data framework or is it stream processing let's see all right time's up and man we can't seem to stump our audience here I was hoping at least you would get some wrong right you know our audience is really getting everything here it's cool it's basically a workflow it means that we did a good job but but we failed in setting the questions to be a little bit tough right you know we'll see we'll see how it progresses but but we'll maybe there is a change to the leaderboard that's might be right maybe not Nick was still the fastest wow it's cool and Joker is still in contention uh so we'll see how it goes okay [Music] Let's see we have three more questions four or five and six and so fast but most importantly get it right um is it an ATL tool this is kind of I think it's a tough question in my opinion even though answers are very straightforward right is it really an ATL too maybe not could be should be uh oh it's confusing I was talking to rags earlier it's like can Cassandra be a q should it be a cube exactly yeah so I I'll let you know you you talk about this yeah so um I'm I'm glad that you know uh some people you know were thinking that yeah I could it is a TTL tool or it could be the thing is that airflow's purpose is not to do the ETL your code is doing the ETF your code is doing the extraction your code is doing the the transformation what it's doing for you is it's wrapping it it's like a Cron job can you use it yeah exactly just scheduling it exactly perfect yeah okay at least we managed to stump a few people let's see if there's big changes oh it looks like it Joker it re-establishes the lead it was the fastest but it could be anybody's game including Santa I'm looking for Santa in December right so let's see all right guys five or six we have two more what is not a component of airflow you saw a bunch of these um it was a database schema manager worker scheduler which is again not a component of airflow two one time's up oh this guy totally distributed huh maybe not yeah maybe it's the same salmon who got it right right so the schema manager is not a component of airflow and I don't think we ever talked about that right so um yeah um anything you want to add there or yeah it you can you can consider part of your data operations that at some point you have to make schema changes right and it is possible to to do a schema change and do a migration with airflow right but airflow is not doing that for you it's not managing the the tables and columns and stuff for you um and it would take a lot of work to kind of rig it to do that so it's just definitely not part of the whole uh airflow feature set last question for all the marbles let's see how the ooh is it shaurya is going to be in the lead maybe just a jingle Joker it looks like um but it's still anybody's game uh Nick who's the fastest could still jump in there last question question six of six all right you know the drill how can you reuse the same database system without hardcoding the values this seems like a tough one right is it using data set is it using configuration use it using setting is it using connection I don't know if well you talked about this right I think you did you said pay attention yeah I did yeah and the answer is it's connection actually right and eight people got it wrong yes wow right um so I I guess could you do it with the configuration yeah so in a configuration and airflow is an airflow airflow configuration meaning what it what it takes to run uh air flow so it's definitely not that now there are data sets uh which we saw which are like you know how do you connect data sets to jobs um but but tends to be that that is a object that is that can be then fed by a connection um in a setting um again the setting is very generic it's really just top level airflow setting uh which you can say setting and configuration is synonymous but the the right approach for reusing the same database or or connection string is in connections and in fact in the database world we we say connections training right and that's what the connection is for connecting to an existing system that you're going to reuse over and over again um and having it in a place if you can think about it you have a developing environment you have a production environment um you can have the same connection token in airflow and then have uh your users practice in the dev environment against the dev connections and then when their code goes to the higher environment it would be using a um production token yeah production perfect so I know we saved the hardest for the last right but everybody is eagerly waiting for who's the winner let's see do do all right nice wow Simba came in French congratulations what I want good job so what I would like you uh the top three to do um are you gonna give out three t-shirts or uh yeah yeah so three of them that is shaurya Joker and Ajay when uh the cool looking architect shirts um and I will let you know in a moment how you can do that but but please take a screenshot of this um you know you uh we don't want you um giving a screenshot somewhere else but but take it on your device okay um take it on your device where it tells you you're the winner okay so don't take a screenshot of this I'm sorry I was wrong okay so take a screenshot on your device and save it for a moment and then we will let you know how to get your swag okay that's it a couple things and mentee um we'd like to get your feedback you know um how would you like to um improve our next Workshop you know for instance we need to be on time but but there was a lot of material to cover um and you know it's not like we taken too long but but hopefully you know it's been enjoyable so far please put in what you'd like us to improve okay or do that we thank you for the mentee all right so now moving on to the presentation um what you will do is uh take a screenshot of your midi screen right congratulations again to the first second and third place um and send it to uh and I'll put it in here I'll put it in chat I think it should be there if you just do a swag you should be able to find it let's see if it works thanks shaurya have some extra data Stacks uh glass cups uh sorry the steel cups I'll send those along too we have a lot of extra ones from before so cool is it us does that work you know maybe I got it but okay okay yep all right yeah oh definitely okay and if you want you want that again you know just put in um exclamation swag and you'll you'll be able to see again for more detailed discussions you know jump on discard um and and there are lots of people answering questions uh I don't know if you are in Discord um you know but uh I'll get on there yeah yeah you can watch out for anything else um we do have a lot of workshops Wednesday workshops I know that uh Eon I think you know mentioned that he's going to be back uh but you know um others please free to join as well um we have a casabra summit and it's coming up on March 13th 14th I know I've been working with rahu to get him to do some submissions I don't know if you already did Raul otherwise treat this as you know again you know nagging you here okay uh but uh cfp closes uh in in about uh three or four days um so don't be left out uh put in put in a um a submission uh and and um you know we're gonna build on the momentum that we've had with the community and um you know this is going to be happening March 13th 14th at San Jose uh there's gonna be a training day followed by a number of different um you know sessions not just by data Stacks but pretty much by the entire community and with that said I want to thank you all very much thank you and we are off thank you thanks for having us I appreciate it and as always don't forget to click that subscribe button and ring that Bell to get notifications for all of our future upcoming imagine a being gifted with powers from the goddess of Cassandra those Powers until she can multiply it will move with Limitless speed and unmask hidden knowledge with those Powers she was able to fully understand the connectedness of the world what she saw was a world in need of understanding from that day forward she sought to bestow her powers on all who came into contact with her empowering them to achieve wondrous feats",
    "segments": [
      {
        "start": 47.239,
        "duration": 10.371,
        "text": "I found someone new now I have more room"
      },
      {
        "start": 54.0,
        "duration": 13.939,
        "text": "no more formats to change"
      },
      {
        "start": 57.61,
        "duration": 14.41,
        "text": "[Music]"
      },
      {
        "start": 67.939,
        "duration": 16.861,
        "text": "living in the future"
      },
      {
        "start": 72.02,
        "duration": 13.82,
        "text": "[Music]"
      },
      {
        "start": 84.8,
        "duration": 11.2,
        "text": "[Applause]"
      },
      {
        "start": 85.84,
        "duration": 11.38,
        "text": "[Music]"
      },
      {
        "start": 96.0,
        "duration": 25.01,
        "text": "foreign"
      },
      {
        "start": 97.22,
        "duration": 23.79,
        "text": "[Music]"
      },
      {
        "start": 123.439,
        "duration": 5.621,
        "text": "good morning good afternoon good evening"
      },
      {
        "start": 126.479,
        "duration": 5.881,
        "text": "wherever you are I can't believe it's"
      },
      {
        "start": 129.06,
        "duration": 6.84,
        "text": "almost end of 2022 but we have a very"
      },
      {
        "start": 132.36,
        "duration": 6.9,
        "text": "special guest today we have"
      },
      {
        "start": 135.9,
        "duration": 6.24,
        "text": "Rahul Singh from Anand and um you know"
      },
      {
        "start": 139.26,
        "duration": 4.86,
        "text": "basically he's going to be doing most of"
      },
      {
        "start": 142.14,
        "duration": 4.62,
        "text": "the workshop today"
      },
      {
        "start": 144.12,
        "duration": 5.82,
        "text": "um we usually don't have a lot of um"
      },
      {
        "start": 146.76,
        "duration": 6.3,
        "text": "kind of uh working with big data um kind"
      },
      {
        "start": 149.94,
        "duration": 5.58,
        "text": "of workshops uh but uh you know today is"
      },
      {
        "start": 153.06,
        "duration": 5.28,
        "text": "going to be a little bit different uh"
      },
      {
        "start": 155.52,
        "duration": 5.1,
        "text": "but before we dive into the workshop I"
      },
      {
        "start": 158.34,
        "duration": 6.259,
        "text": "just want to make sure everybody can"
      },
      {
        "start": 160.62,
        "duration": 6.119,
        "text": "hear me well you can hear two voices uh"
      },
      {
        "start": 164.599,
        "duration": 4.541,
        "text": "Rahul do you just want to say something"
      },
      {
        "start": 166.739,
        "duration": 4.741,
        "text": "so that you know we have only two voices"
      },
      {
        "start": 169.14,
        "duration": 4.08,
        "text": "and no more than two voices because"
      },
      {
        "start": 171.48,
        "duration": 3.78,
        "text": "sometimes the echo is very irritating"
      },
      {
        "start": 173.22,
        "duration": 4.799,
        "text": "you know so"
      },
      {
        "start": 175.26,
        "duration": 3.9,
        "text": "sure yeah hey thanks for having me Rags"
      },
      {
        "start": 178.019,
        "duration": 4.021,
        "text": "I appreciate it"
      },
      {
        "start": 179.16,
        "duration": 6.96,
        "text": "um yeah this is great I mean Cassandra"
      },
      {
        "start": 182.04,
        "duration": 5.82,
        "text": "is Big Data so uh you know it's at some"
      },
      {
        "start": 186.12,
        "duration": 3.78,
        "text": "point you gotta have some talks about"
      },
      {
        "start": 187.86,
        "duration": 6.06,
        "text": "big data so I'm I'm glad to be adding to"
      },
      {
        "start": 189.9,
        "duration": 6.78,
        "text": "your uh yeah that's great you know so"
      },
      {
        "start": 193.92,
        "duration": 5.52,
        "text": "what I meant is like I have a little bit"
      },
      {
        "start": 196.68,
        "duration": 4.86,
        "text": "more of the Big Data ecosystem that's"
      },
      {
        "start": 199.44,
        "duration": 3.719,
        "text": "kind of what I meant didn't uh and and"
      },
      {
        "start": 201.54,
        "duration": 2.94,
        "text": "we're definitely gonna see a lot more"
      },
      {
        "start": 203.159,
        "duration": 4.141,
        "text": "today"
      },
      {
        "start": 204.48,
        "duration": 4.8,
        "text": "having said that this is as many of you"
      },
      {
        "start": 207.3,
        "duration": 5.84,
        "text": "know um you know it's going to be a"
      },
      {
        "start": 209.28,
        "duration": 6.78,
        "text": "Hands-On Workshop uh and uh"
      },
      {
        "start": 213.14,
        "duration": 6.06,
        "text": "let me"
      },
      {
        "start": 216.06,
        "duration": 3.14,
        "text": "go to the next slide"
      },
      {
        "start": 219.9,
        "duration": 7.979,
        "text": "okay my name is uh raghavan shunivas and"
      },
      {
        "start": 224.94,
        "duration": 4.68,
        "text": "hopefully you're gonna be able to see me"
      },
      {
        "start": 227.879,
        "duration": 4.621,
        "text": "um and I work"
      },
      {
        "start": 229.62,
        "duration": 3.72,
        "text": "as a developer Advocate at"
      },
      {
        "start": 232.5,
        "duration": 4.319,
        "text": "um"
      },
      {
        "start": 233.34,
        "duration": 5.759,
        "text": "data Stacks I come from the Java"
      },
      {
        "start": 236.819,
        "duration": 4.021,
        "text": "background in the kubernetes background"
      },
      {
        "start": 239.099,
        "duration": 4.86,
        "text": "um"
      },
      {
        "start": 240.84,
        "duration": 6.14,
        "text": "uh are you guys seeing an update of the"
      },
      {
        "start": 243.959,
        "duration": 3.021,
        "text": "slides um"
      },
      {
        "start": 247.62,
        "duration": 5.839,
        "text": "Rahul uh do you want to confirm if you"
      },
      {
        "start": 250.379,
        "duration": 3.08,
        "text": "the slides are getting up"
      },
      {
        "start": 254.4,
        "duration": 6.059,
        "text": "no it's still on the first one it's"
      },
      {
        "start": 257.1,
        "duration": 5.7,
        "text": "pretty strange why this is happening"
      },
      {
        "start": 260.459,
        "duration": 5.101,
        "text": "and uh"
      },
      {
        "start": 262.8,
        "duration": 5.459,
        "text": "oh okay all right"
      },
      {
        "start": 265.56,
        "duration": 4.44,
        "text": "so now we should see that"
      },
      {
        "start": 268.259,
        "duration": 4.621,
        "text": "okay"
      },
      {
        "start": 270.0,
        "duration": 4.5,
        "text": "um so I um I'm a big fan of distributor"
      },
      {
        "start": 272.88,
        "duration": 2.759,
        "text": "systems I really love to teach and"
      },
      {
        "start": 274.5,
        "duration": 3.3,
        "text": "communicate"
      },
      {
        "start": 275.639,
        "duration": 4.201,
        "text": "um but one thing I'm really passionate"
      },
      {
        "start": 277.8,
        "duration": 5.22,
        "text": "about is the inner loop which is really"
      },
      {
        "start": 279.84,
        "duration": 5.52,
        "text": "about kind of the productivity gains you"
      },
      {
        "start": 283.02,
        "duration": 7.5,
        "text": "can you can realize"
      },
      {
        "start": 285.36,
        "duration": 7.619,
        "text": "um and uh you know I uh I use quarkus"
      },
      {
        "start": 290.52,
        "duration": 5.34,
        "text": "um you know other things but primarily I"
      },
      {
        "start": 292.979,
        "duration": 5.28,
        "text": "love to teach and communicate uh I also"
      },
      {
        "start": 295.86,
        "duration": 4.02,
        "text": "a mechanical engineer you know my"
      },
      {
        "start": 298.259,
        "duration": 5.701,
        "text": "undergrad was in mechanical engineering"
      },
      {
        "start": 299.88,
        "duration": 7.08,
        "text": "uh I like these workshops because not"
      },
      {
        "start": 303.96,
        "duration": 4.38,
        "text": "only you know do you all learn you know"
      },
      {
        "start": 306.96,
        "duration": 4.14,
        "text": "the attendees you know thanks for taking"
      },
      {
        "start": 308.34,
        "duration": 4.139,
        "text": "the time again uh but I also learn a lot"
      },
      {
        "start": 311.1,
        "duration": 3.24,
        "text": "as well"
      },
      {
        "start": 312.479,
        "duration": 4.44,
        "text": "um so with that said"
      },
      {
        "start": 314.34,
        "duration": 6.74,
        "text": "let's"
      },
      {
        "start": 316.919,
        "duration": 4.161,
        "text": "let's introduce the rest of the team"
      },
      {
        "start": 321.12,
        "duration": 2.76,
        "text": "which is behind the workshop"
      },
      {
        "start": 323.1,
        "duration": 3.3,
        "text": "um"
      },
      {
        "start": 323.88,
        "duration": 3.84,
        "text": "a lot of lot of effort goes in we have"
      },
      {
        "start": 326.4,
        "duration": 3.78,
        "text": "workshops typically every Wednesday"
      },
      {
        "start": 327.72,
        "duration": 4.62,
        "text": "around the same time uh we have a few"
      },
      {
        "start": 330.18,
        "duration": 4.079,
        "text": "more workshops till the end of the end"
      },
      {
        "start": 332.34,
        "duration": 4.919,
        "text": "of the year uh end of the month end of"
      },
      {
        "start": 334.259,
        "duration": 7.561,
        "text": "the year and then we're gonna start off"
      },
      {
        "start": 337.259,
        "duration": 7.741,
        "text": "with the boot camp in January 2023 which"
      },
      {
        "start": 341.82,
        "duration": 7.439,
        "text": "is typically you know how we roll"
      },
      {
        "start": 345.0,
        "duration": 6.539,
        "text": "um you can live stream this on YouTube"
      },
      {
        "start": 349.259,
        "duration": 4.261,
        "text": "um typically we used to have a twitch as"
      },
      {
        "start": 351.539,
        "duration": 3.241,
        "text": "well but but today we don't have a"
      },
      {
        "start": 353.52,
        "duration": 3.239,
        "text": "twitch"
      },
      {
        "start": 354.78,
        "duration": 4.979,
        "text": "um so you know you"
      },
      {
        "start": 356.759,
        "duration": 5.761,
        "text": "basically look at YouTube and and feel"
      },
      {
        "start": 359.759,
        "duration": 5.701,
        "text": "free to chat on YouTube uh you know"
      },
      {
        "start": 362.52,
        "duration": 7.14,
        "text": "either Rahul or myself or any of our"
      },
      {
        "start": 365.46,
        "duration": 7.38,
        "text": "teammates are going to be uh on the chat"
      },
      {
        "start": 369.66,
        "duration": 5.16,
        "text": "and glad to uh answer your questions uh"
      },
      {
        "start": 372.84,
        "duration": 5.639,
        "text": "for more detailed questions however we"
      },
      {
        "start": 374.82,
        "duration": 7.02,
        "text": "suggest you use discard uh which is you"
      },
      {
        "start": 378.479,
        "duration": 5.461,
        "text": "know for more long-lived kind of uh"
      },
      {
        "start": 381.84,
        "duration": 4.56,
        "text": "discussions and for more detailed"
      },
      {
        "start": 383.94,
        "duration": 4.8,
        "text": "discussions and not only uh our"
      },
      {
        "start": 386.4,
        "duration": 4.2,
        "text": "developer Advocate team but the entire"
      },
      {
        "start": 388.74,
        "duration": 5.0,
        "text": "data Stacks is going to be there to kind"
      },
      {
        "start": 390.6,
        "duration": 3.14,
        "text": "of help you out okay"
      },
      {
        "start": 393.78,
        "duration": 3.419,
        "text": "um"
      },
      {
        "start": 394.44,
        "duration": 4.979,
        "text": "what is a data sex devs Workshop without"
      },
      {
        "start": 397.199,
        "duration": 3.84,
        "text": "minty right you know so so some of you"
      },
      {
        "start": 399.419,
        "duration": 3.181,
        "text": "have been here before you know what"
      },
      {
        "start": 401.039,
        "duration": 3.0,
        "text": "minty is and I'm going to talk about"
      },
      {
        "start": 402.6,
        "duration": 3.18,
        "text": "that in a second"
      },
      {
        "start": 404.039,
        "duration": 5.701,
        "text": "but with that said"
      },
      {
        "start": 405.78,
        "duration": 5.699,
        "text": "um we also have a number of resources"
      },
      {
        "start": 409.74,
        "duration": 4.86,
        "text": "you know I say we are living in the"
      },
      {
        "start": 411.479,
        "duration": 5.401,
        "text": "Golden Age of developers we have so many"
      },
      {
        "start": 414.6,
        "duration": 4.98,
        "text": "resources available for us right and"
      },
      {
        "start": 416.88,
        "duration": 4.52,
        "text": "most of it is pretty much free right"
      },
      {
        "start": 419.58,
        "duration": 4.92,
        "text": "um so you know you can go to GitHub"
      },
      {
        "start": 421.4,
        "duration": 5.079,
        "text": "github.com datastags devs you know it's"
      },
      {
        "start": 424.5,
        "duration": 3.78,
        "text": "probably the right way uh uh the"
      },
      {
        "start": 426.479,
        "duration": 3.901,
        "text": "probably where to get started because"
      },
      {
        "start": 428.28,
        "duration": 4.38,
        "text": "there are a whole bunch of repositories"
      },
      {
        "start": 430.38,
        "duration": 5.159,
        "text": "in there um you know today we are using"
      },
      {
        "start": 432.66,
        "duration": 4.74,
        "text": "a different GitHub repository uh but but"
      },
      {
        "start": 435.539,
        "duration": 3.241,
        "text": "you know we will we will talk about that"
      },
      {
        "start": 437.4,
        "duration": 3.18,
        "text": "in a second"
      },
      {
        "start": 438.78,
        "duration": 3.359,
        "text": "um we use gitpart and most of our"
      },
      {
        "start": 440.58,
        "duration": 3.239,
        "text": "workshops"
      },
      {
        "start": 442.139,
        "duration": 3.84,
        "text": "um the nice thing about gitpart is you"
      },
      {
        "start": 443.819,
        "duration": 3.72,
        "text": "don't have to install anything uh and I"
      },
      {
        "start": 445.979,
        "duration": 4.141,
        "text": "know that Rahul was instrumental in"
      },
      {
        "start": 447.539,
        "duration": 4.741,
        "text": "getting us started on gitpart you know"
      },
      {
        "start": 450.12,
        "duration": 3.96,
        "text": "in our initial Journey so thank you"
      },
      {
        "start": 452.28,
        "duration": 3.78,
        "text": "there uh because you know I'm a big fan"
      },
      {
        "start": 454.08,
        "duration": 3.66,
        "text": "of gitpart as well and I'm sure if you"
      },
      {
        "start": 456.06,
        "duration": 3.72,
        "text": "have never used gitpart you will"
      },
      {
        "start": 457.74,
        "duration": 4.38,
        "text": "appreciate this as well we're going to"
      },
      {
        "start": 459.78,
        "duration": 4.68,
        "text": "use astrodb uh get that out of the way"
      },
      {
        "start": 462.12,
        "duration": 5.519,
        "text": "initially and then Rahul doesn't to jump"
      },
      {
        "start": 464.46,
        "duration": 5.84,
        "text": "into the meat of the workshop so so a"
      },
      {
        "start": 467.639,
        "duration": 6.12,
        "text": "very brief introduction about Cassandra"
      },
      {
        "start": 470.3,
        "duration": 5.98,
        "text": "Astra and so on and then we're going to"
      },
      {
        "start": 473.759,
        "duration": 5.22,
        "text": "get into the meat of the workshop"
      },
      {
        "start": 476.28,
        "duration": 4.74,
        "text": "with that said let's do a mentee"
      },
      {
        "start": 478.979,
        "duration": 5.701,
        "text": "um and uh"
      },
      {
        "start": 481.02,
        "duration": 5.88,
        "text": "give me a second so that I can switch my"
      },
      {
        "start": 484.68,
        "duration": 5.579,
        "text": "lights"
      },
      {
        "start": 486.9,
        "duration": 5.22,
        "text": "oh did I do something wrong there"
      },
      {
        "start": 490.259,
        "duration": 3.66,
        "text": "question six of six no that's not a good"
      },
      {
        "start": 492.12,
        "duration": 4.44,
        "text": "idea right"
      },
      {
        "start": 493.919,
        "duration": 4.861,
        "text": "laughs"
      },
      {
        "start": 496.56,
        "duration": 3.78,
        "text": "so you already have a hint"
      },
      {
        "start": 498.78,
        "duration": 5.16,
        "text": "but"
      },
      {
        "start": 500.34,
        "duration": 6.259,
        "text": "let me see if I can all right so let me"
      },
      {
        "start": 503.94,
        "duration": 2.659,
        "text": "go ahead and present"
      },
      {
        "start": 508.5,
        "duration": 2.959,
        "text": "so basically what you're going to do is"
      },
      {
        "start": 509.94,
        "duration": 6.779,
        "text": "you're going to go to"
      },
      {
        "start": 511.459,
        "duration": 5.26,
        "text": "www.minti.com and use the code 3518069"
      },
      {
        "start": 517.08,
        "duration": 4.199,
        "text": "um ideally you know if you can do it on"
      },
      {
        "start": 518.76,
        "duration": 4.56,
        "text": "your mobile phone that's that's the best"
      },
      {
        "start": 521.279,
        "duration": 3.0,
        "text": "okay"
      },
      {
        "start": 523.32,
        "duration": 4.079,
        "text": "um"
      },
      {
        "start": 524.279,
        "duration": 4.981,
        "text": "I will also put in the code here but I"
      },
      {
        "start": 527.399,
        "duration": 5.281,
        "text": "could also do this so you can use this"
      },
      {
        "start": 529.26,
        "duration": 8.82,
        "text": "QR code as well but you know minty.com"
      },
      {
        "start": 532.68,
        "duration": 9.0,
        "text": "35 18069 probably the easiest way right"
      },
      {
        "start": 538.08,
        "duration": 5.58,
        "text": "okay let's get started"
      },
      {
        "start": 541.68,
        "duration": 3.96,
        "text": "let's see if people are joining I don't"
      },
      {
        "start": 543.66,
        "duration": 4.739,
        "text": "think there are people joining yet uh I"
      },
      {
        "start": 545.64,
        "duration": 4.68,
        "text": "only see a few people turning um let's"
      },
      {
        "start": 548.399,
        "duration": 4.141,
        "text": "get some more people going"
      },
      {
        "start": 550.32,
        "duration": 4.32,
        "text": "I don't know we have quite a few"
      },
      {
        "start": 552.54,
        "duration": 3.78,
        "text": "um viewers"
      },
      {
        "start": 554.64,
        "duration": 4.02,
        "text": "um so"
      },
      {
        "start": 556.32,
        "duration": 4.38,
        "text": "let's give it a little bit of time thank"
      },
      {
        "start": 558.66,
        "duration": 5.34,
        "text": "you for those who have joined already"
      },
      {
        "start": 560.7,
        "duration": 5.88,
        "text": "um and and remember to remember that we"
      },
      {
        "start": 564.0,
        "duration": 6.06,
        "text": "usually have a quiz at the end of the"
      },
      {
        "start": 566.58,
        "duration": 5.939,
        "text": "workshop uh so you know great idea to"
      },
      {
        "start": 570.06,
        "duration": 4.8,
        "text": "kind of get used to it because when it"
      },
      {
        "start": 572.519,
        "duration": 3.721,
        "text": "comes to the quiz you not only have to"
      },
      {
        "start": 574.86,
        "duration": 2.36,
        "text": "get the right answer but you also have"
      },
      {
        "start": 576.24,
        "duration": 3.779,
        "text": "to be fast"
      },
      {
        "start": 577.22,
        "duration": 5.619,
        "text": "a lot of times I've tried to be"
      },
      {
        "start": 580.019,
        "duration": 5.101,
        "text": "um you know on the quiz and you know I'm"
      },
      {
        "start": 582.839,
        "duration": 3.541,
        "text": "just not fast enough right"
      },
      {
        "start": 585.12,
        "duration": 3.12,
        "text": "um so so"
      },
      {
        "start": 586.38,
        "duration": 4.98,
        "text": "um getting used to it right now will"
      },
      {
        "start": 588.24,
        "duration": 5.52,
        "text": "help you win some swag later uh and and"
      },
      {
        "start": 591.36,
        "duration": 4.74,
        "text": "today we have a really cool looking swag"
      },
      {
        "start": 593.76,
        "duration": 6.48,
        "text": "that I'll I'll show you um and thanks"
      },
      {
        "start": 596.1,
        "duration": 6.96,
        "text": "for uh I don't you know for doing this"
      },
      {
        "start": 600.24,
        "duration": 5.7,
        "text": "thanks to anante Rahul for doing this as"
      },
      {
        "start": 603.06,
        "duration": 5.58,
        "text": "well okay so with that"
      },
      {
        "start": 605.94,
        "duration": 4.26,
        "text": "the first question we ask and and I"
      },
      {
        "start": 608.64,
        "duration": 3.84,
        "text": "really like to see the dots pop up"
      },
      {
        "start": 610.2,
        "duration": 6.3,
        "text": "everywhere right where in the world"
      },
      {
        "start": 612.48,
        "duration": 6.9,
        "text": "you're from you don't need to be precise"
      },
      {
        "start": 616.5,
        "duration": 5.22,
        "text": "um is that Canada that's the first Dot"
      },
      {
        "start": 619.38,
        "duration": 5.3,
        "text": "looks like Italy yeah"
      },
      {
        "start": 621.72,
        "duration": 7.5,
        "text": "it looks like yeah"
      },
      {
        "start": 624.68,
        "duration": 6.099,
        "text": "in Africa uh one that's one in India"
      },
      {
        "start": 629.22,
        "duration": 4.2,
        "text": "yeah yeah"
      },
      {
        "start": 630.779,
        "duration": 4.981,
        "text": "is cool to see these dots right uh let's"
      },
      {
        "start": 633.42,
        "duration": 3.96,
        "text": "see a few more dots"
      },
      {
        "start": 635.76,
        "duration": 3.42,
        "text": "um nice nice"
      },
      {
        "start": 637.38,
        "duration": 3.36,
        "text": "and again you know"
      },
      {
        "start": 639.18,
        "duration": 5.48,
        "text": "um that'll be great"
      },
      {
        "start": 640.74,
        "duration": 3.92,
        "text": "a few more from Canada wow okay"
      },
      {
        "start": 646.04,
        "duration": 5.26,
        "text": "across the world yeah actually"
      },
      {
        "start": 649.5,
        "duration": 3.839,
        "text": "all right"
      },
      {
        "start": 651.3,
        "duration": 5.039,
        "text": "that's nice welcome welcome from"
      },
      {
        "start": 653.339,
        "duration": 4.921,
        "text": "wherever you are um in the world"
      },
      {
        "start": 656.339,
        "duration": 5.281,
        "text": "um any of you"
      },
      {
        "start": 658.26,
        "duration": 6.66,
        "text": "rooting for your favorite uh team to win"
      },
      {
        "start": 661.62,
        "duration": 6.06,
        "text": "the World Cup soccer tournament uh you"
      },
      {
        "start": 664.92,
        "duration": 5.4,
        "text": "know just feel free to put it in uh"
      },
      {
        "start": 667.68,
        "duration": 4.56,
        "text": "in chat uh you know if your team is"
      },
      {
        "start": 670.32,
        "duration": 4.139,
        "text": "still alive if your country's team is"
      },
      {
        "start": 672.24,
        "duration": 3.719,
        "text": "still alive unfortunately India never"
      },
      {
        "start": 674.459,
        "duration": 4.981,
        "text": "qualified you know that's where I come"
      },
      {
        "start": 675.959,
        "duration": 4.5,
        "text": "from uh and the USA uh you know just got"
      },
      {
        "start": 679.44,
        "duration": 3.72,
        "text": "eliminated"
      },
      {
        "start": 680.459,
        "duration": 5.301,
        "text": "what's your favorite team Rahul"
      },
      {
        "start": 683.16,
        "duration": 2.6,
        "text": "yes"
      },
      {
        "start": 689.48,
        "duration": 6.34,
        "text": "last week so you know yeah"
      },
      {
        "start": 693.3,
        "duration": 4.8,
        "text": "oh it's not it's not great from a"
      },
      {
        "start": 695.82,
        "duration": 3.78,
        "text": "cricket perspective either okay so if we"
      },
      {
        "start": 698.1,
        "duration": 4.14,
        "text": "do ask you some questions what is your"
      },
      {
        "start": 699.6,
        "duration": 5.7,
        "text": "experience level with Apache Cassandra"
      },
      {
        "start": 702.24,
        "duration": 6.779,
        "text": "um this is so that we can kind of tune"
      },
      {
        "start": 705.3,
        "duration": 5.94,
        "text": "our presentation or tune our emphasis"
      },
      {
        "start": 709.019,
        "duration": 4.32,
        "text": "um there are there is one experienced"
      },
      {
        "start": 711.24,
        "duration": 7.279,
        "text": "developer or administrator a new node"
      },
      {
        "start": 713.339,
        "duration": 5.18,
        "text": "SQL Guru uh and that must be Rahul or"
      },
      {
        "start": 718.64,
        "duration": 3.07,
        "text": "I'm not doing a mentee I don't want to"
      },
      {
        "start": 721.2,
        "duration": 2.16,
        "text": "break the game"
      },
      {
        "start": 721.71,
        "duration": 4.11,
        "text": "[Laughter]"
      },
      {
        "start": 723.36,
        "duration": 4.74,
        "text": "all right we have no SQL gurus who watch"
      },
      {
        "start": 725.82,
        "duration": 3.9,
        "text": "our travel all right"
      },
      {
        "start": 728.1,
        "duration": 2.7,
        "text": "um all right give it give it a couple of"
      },
      {
        "start": 729.72,
        "duration": 1.799,
        "text": "minutes"
      },
      {
        "start": 730.8,
        "duration": 2.219,
        "text": "um"
      },
      {
        "start": 731.519,
        "duration": 3.421,
        "text": "okay"
      },
      {
        "start": 733.019,
        "duration": 4.681,
        "text": "Mulan"
      },
      {
        "start": 734.94,
        "duration": 4.86,
        "text": "and again if you want to join this you"
      },
      {
        "start": 737.7,
        "duration": 4.5,
        "text": "know if you're late it's three five one"
      },
      {
        "start": 739.8,
        "duration": 4.8,
        "text": "eight zero six nine and like I said"
      },
      {
        "start": 742.2,
        "duration": 4.139,
        "text": "before we're gonna have a quiz where it"
      },
      {
        "start": 744.6,
        "duration": 2.76,
        "text": "really matters that you know you do it"
      },
      {
        "start": 746.339,
        "duration": 3.961,
        "text": "on time"
      },
      {
        "start": 747.36,
        "duration": 5.7,
        "text": "um as fast as you can but also answer"
      },
      {
        "start": 750.3,
        "duration": 4.74,
        "text": "correctly okay question two"
      },
      {
        "start": 753.06,
        "duration": 3.66,
        "text": "what is your experience with the patchy"
      },
      {
        "start": 755.04,
        "duration": 3.979,
        "text": "airflow is this the first time you heard"
      },
      {
        "start": 756.72,
        "duration": 2.299,
        "text": "of it"
      },
      {
        "start": 760.26,
        "duration": 4.74,
        "text": "are you an experienced developer or"
      },
      {
        "start": 762.0,
        "duration": 5.94,
        "text": "administrator again watch out"
      },
      {
        "start": 765.0,
        "duration": 5.22,
        "text": "okay as we thought you know most most"
      },
      {
        "start": 767.94,
        "duration": 3.899,
        "text": "everybody is kind of a beginner on"
      },
      {
        "start": 770.22,
        "duration": 3.6,
        "text": "Apache airflow"
      },
      {
        "start": 771.839,
        "duration": 4.261,
        "text": "okay but we have a few"
      },
      {
        "start": 773.82,
        "duration": 3.42,
        "text": "um and and maybe you have some tips for"
      },
      {
        "start": 776.1,
        "duration": 4.08,
        "text": "the experienced developer or"
      },
      {
        "start": 777.24,
        "duration": 4.8,
        "text": "administrator as well right"
      },
      {
        "start": 780.18,
        "duration": 3.24,
        "text": "uh it's always great to have these"
      },
      {
        "start": 782.04,
        "duration": 4.2,
        "text": "questions because it kind of gives us an"
      },
      {
        "start": 783.42,
        "duration": 6.06,
        "text": "idea of who the audience is uh"
      },
      {
        "start": 786.24,
        "duration": 4.62,
        "text": "um and and usually you know we we have"
      },
      {
        "start": 789.48,
        "duration": 3.359,
        "text": "an assumption but"
      },
      {
        "start": 790.86,
        "duration": 3.84,
        "text": "sometimes"
      },
      {
        "start": 792.839,
        "duration": 3.901,
        "text": "um you know we are thrown off and and we"
      },
      {
        "start": 794.7,
        "duration": 4.079,
        "text": "try to correct as we go along because we"
      },
      {
        "start": 796.74,
        "duration": 3.659,
        "text": "want to make it as applicable to the"
      },
      {
        "start": 798.779,
        "duration": 3.961,
        "text": "audience as possible"
      },
      {
        "start": 800.399,
        "duration": 5.341,
        "text": "fine final question is this your first"
      },
      {
        "start": 802.74,
        "duration": 5.839,
        "text": "data Stacks devs Workshop"
      },
      {
        "start": 805.74,
        "duration": 2.839,
        "text": "yes yes"
      },
      {
        "start": 808.68,
        "duration": 6.48,
        "text": "two yeses three S's"
      },
      {
        "start": 811.92,
        "duration": 6.539,
        "text": "we do get a lot of repeat data Stacks"
      },
      {
        "start": 815.16,
        "duration": 6.359,
        "text": "devs attendees uh but last count of how"
      },
      {
        "start": 818.459,
        "duration": 3.661,
        "text": "many workshops I attended all right"
      },
      {
        "start": 821.519,
        "duration": 4.38,
        "text": "um"
      },
      {
        "start": 822.12,
        "duration": 5.82,
        "text": "fifth or more Workshop okay that is cool"
      },
      {
        "start": 825.899,
        "duration": 5.281,
        "text": "so the majority"
      },
      {
        "start": 827.94,
        "duration": 6.0,
        "text": "um you know I are new"
      },
      {
        "start": 831.18,
        "duration": 7.159,
        "text": "um I forgot to put on the quiz music I"
      },
      {
        "start": 833.94,
        "duration": 4.399,
        "text": "will add that right now if I can"
      },
      {
        "start": 838.74,
        "duration": 4.219,
        "text": "um"
      },
      {
        "start": 840.24,
        "duration": 2.719,
        "text": "all right"
      },
      {
        "start": 844.98,
        "duration": 5.94,
        "text": "um so the majority of them are are new"
      },
      {
        "start": 848.579,
        "duration": 4.44,
        "text": "to uh the workshop which is kind of what"
      },
      {
        "start": 850.92,
        "duration": 4.44,
        "text": "we expected any any comments rather on"
      },
      {
        "start": 853.019,
        "duration": 5.041,
        "text": "on how this kind of looks"
      },
      {
        "start": 855.36,
        "duration": 6.18,
        "text": "um anything that surprises you"
      },
      {
        "start": 858.06,
        "duration": 6.0,
        "text": "sure yeah no it's uh yeah no it's good"
      },
      {
        "start": 861.54,
        "duration": 3.96,
        "text": "to have new people uh joining the you"
      },
      {
        "start": 864.06,
        "duration": 4.68,
        "text": "know the community"
      },
      {
        "start": 865.5,
        "duration": 6.06,
        "text": "um you know Cassandra is one of those"
      },
      {
        "start": 868.74,
        "duration": 4.92,
        "text": "things that people don't uh who are new"
      },
      {
        "start": 871.56,
        "duration": 4.38,
        "text": "to it first of all don't understand how"
      },
      {
        "start": 873.66,
        "duration": 4.739,
        "text": "awesome it is but when they see a"
      },
      {
        "start": 875.94,
        "duration": 4.5,
        "text": "workshop like this especially showing"
      },
      {
        "start": 878.399,
        "duration": 3.601,
        "text": "how easy it is to get started with Astra"
      },
      {
        "start": 880.44,
        "duration": 3.36,
        "text": "without having to have your own cluster"
      },
      {
        "start": 882.0,
        "duration": 3.959,
        "text": "I think it's great to have new people"
      },
      {
        "start": 883.8,
        "duration": 3.36,
        "text": "involved so yeah welcome glad to have"
      },
      {
        "start": 885.959,
        "duration": 2.281,
        "text": "you guys okay"
      },
      {
        "start": 887.16,
        "duration": 3.419,
        "text": "um"
      },
      {
        "start": 888.24,
        "duration": 4.32,
        "text": "so keep this going"
      },
      {
        "start": 890.579,
        "duration": 5.88,
        "text": "um either the mentee because we're going"
      },
      {
        "start": 892.56,
        "duration": 6.42,
        "text": "to have a quiz uh later okay but now we"
      },
      {
        "start": 896.459,
        "duration": 6.861,
        "text": "will move back to the presentation uh so"
      },
      {
        "start": 898.98,
        "duration": 4.34,
        "text": "give me a second before I can get there"
      },
      {
        "start": 904.079,
        "duration": 6.741,
        "text": "all right we're done with the mentee so"
      },
      {
        "start": 907.98,
        "duration": 2.84,
        "text": "moving on"
      },
      {
        "start": 912.0,
        "duration": 4.5,
        "text": "all right"
      },
      {
        "start": 913.86,
        "duration": 3.599,
        "text": "so today's challenge is we're gonna do"
      },
      {
        "start": 916.5,
        "duration": 2.22,
        "text": "some"
      },
      {
        "start": 917.459,
        "duration": 3.841,
        "text": "um"
      },
      {
        "start": 918.72,
        "duration": 5.1,
        "text": "ETL although Apache airflow is not an"
      },
      {
        "start": 921.3,
        "duration": 4.02,
        "text": "ETL tool right and you and Raul will"
      },
      {
        "start": 923.82,
        "duration": 3.0,
        "text": "talk about this"
      },
      {
        "start": 925.32,
        "duration": 3.18,
        "text": "um but but basically"
      },
      {
        "start": 926.82,
        "duration": 3.6,
        "text": "um we will talk about kind of automating"
      },
      {
        "start": 928.5,
        "duration": 4.44,
        "text": "operations"
      },
      {
        "start": 930.42,
        "duration": 5.82,
        "text": "um because whenever you talk about ETL"
      },
      {
        "start": 932.94,
        "duration": 5.519,
        "text": "kind of it conjures up this thing about"
      },
      {
        "start": 936.24,
        "duration": 4.32,
        "text": "manual you know kind of having to do all"
      },
      {
        "start": 938.459,
        "duration": 4.32,
        "text": "that uh and and whenever you do"
      },
      {
        "start": 940.56,
        "duration": 4.2,
        "text": "something manually uh you know it tends"
      },
      {
        "start": 942.779,
        "duration": 4.161,
        "text": "to be error prone so um you know"
      },
      {
        "start": 944.76,
        "duration": 5.939,
        "text": "automating it is it's going to be a lot"
      },
      {
        "start": 946.94,
        "duration": 7.74,
        "text": "better and let me stop the music so that"
      },
      {
        "start": 950.699,
        "duration": 3.981,
        "text": "you know it's not irritating to people"
      },
      {
        "start": 955.15,
        "duration": 3.049,
        "text": "[Music]"
      },
      {
        "start": 956.459,
        "duration": 3.361,
        "text": "all right"
      },
      {
        "start": 958.199,
        "duration": 4.08,
        "text": "um so"
      },
      {
        "start": 959.82,
        "duration": 5.28,
        "text": "um you know we've even built all this on"
      },
      {
        "start": 962.279,
        "duration": 5.281,
        "text": "gitpart as you can see here uh and you"
      },
      {
        "start": 965.1,
        "duration": 4.38,
        "text": "know we'll have a spark Master uh and so"
      },
      {
        "start": 967.56,
        "duration": 5.339,
        "text": "on which which we'll walk through"
      },
      {
        "start": 969.48,
        "duration": 5.219,
        "text": "um during this session and and like I"
      },
      {
        "start": 972.899,
        "duration": 3.601,
        "text": "said before it's all Hands-On you're"
      },
      {
        "start": 974.699,
        "duration": 3.721,
        "text": "going to be doing this"
      },
      {
        "start": 976.5,
        "duration": 4.68,
        "text": "um after a very brief introduction to"
      },
      {
        "start": 978.42,
        "duration": 5.4,
        "text": "Cassandra we're going to go through uh"
      },
      {
        "start": 981.18,
        "duration": 3.779,
        "text": "um the rest of the agenda and uh Rahul"
      },
      {
        "start": 983.82,
        "duration": 3.48,
        "text": "do you want to talk a little bit about"
      },
      {
        "start": 984.959,
        "duration": 5.581,
        "text": "this yeah"
      },
      {
        "start": 987.3,
        "duration": 5.76,
        "text": "sure sure so one of the things that I"
      },
      {
        "start": 990.54,
        "duration": 4.2,
        "text": "like to do is explain to people why"
      },
      {
        "start": 993.06,
        "duration": 4.079,
        "text": "something is important so we're going to"
      },
      {
        "start": 994.74,
        "duration": 4.86,
        "text": "just spend a little bit of time on open"
      },
      {
        "start": 997.139,
        "duration": 4.32,
        "text": "data platform why airflow has become"
      },
      {
        "start": 999.6,
        "duration": 4.5,
        "text": "such a popular tool right in the open"
      },
      {
        "start": 1001.459,
        "duration": 4.38,
        "text": "data platform world and then we'll cover"
      },
      {
        "start": 1004.1,
        "duration": 3.599,
        "text": "a little bit about airflow and Spark but"
      },
      {
        "start": 1005.839,
        "duration": 3.3,
        "text": "the meat of the presentation really is"
      },
      {
        "start": 1007.699,
        "duration": 4.32,
        "text": "about"
      },
      {
        "start": 1009.139,
        "duration": 5.82,
        "text": "um running ETL processes in spark but"
      },
      {
        "start": 1012.019,
        "duration": 5.641,
        "text": "coordinating it with airflow and also"
      },
      {
        "start": 1014.959,
        "duration": 4.861,
        "text": "I'll walk through some real world"
      },
      {
        "start": 1017.66,
        "duration": 4.859,
        "text": "example I'm not going to have the code"
      },
      {
        "start": 1019.82,
        "duration": 4.499,
        "text": "as a Hands-On walkthrough but to do more"
      },
      {
        "start": 1022.519,
        "duration": 4.021,
        "text": "advanced operations than for example"
      },
      {
        "start": 1024.319,
        "duration": 4.681,
        "text": "cleaning information uh in Cassandra"
      },
      {
        "start": 1026.54,
        "duration": 4.32,
        "text": "which is that's a common need and you'll"
      },
      {
        "start": 1029.0,
        "duration": 5.88,
        "text": "see how useful it is to to leverage"
      },
      {
        "start": 1030.86,
        "duration": 6.12,
        "text": "airflow and Spark as an ecosystem"
      },
      {
        "start": 1034.88,
        "duration": 4.86,
        "text": "so with that said a very short"
      },
      {
        "start": 1036.98,
        "duration": 6.06,
        "text": "introduction of nosql um it's basically"
      },
      {
        "start": 1039.74,
        "duration": 6.0,
        "text": "a catchy hashtag that"
      },
      {
        "start": 1043.04,
        "duration": 4.74,
        "text": "um you know people came up with uh and"
      },
      {
        "start": 1045.74,
        "duration": 6.66,
        "text": "it kind of stuck uh it's really about"
      },
      {
        "start": 1047.78,
        "duration": 7.98,
        "text": "everything that is not SQL okay SQL was"
      },
      {
        "start": 1052.4,
        "duration": 6.06,
        "text": "uh very popular but the problem with"
      },
      {
        "start": 1055.76,
        "duration": 5.7,
        "text": "sequel is that um you know you cannot"
      },
      {
        "start": 1058.46,
        "duration": 5.76,
        "text": "keep scaling vertically for"
      },
      {
        "start": 1061.46,
        "duration": 4.56,
        "text": "um forever right you know you you kind"
      },
      {
        "start": 1064.22,
        "duration": 5.339,
        "text": "of kill over at some point and you're"
      },
      {
        "start": 1066.02,
        "duration": 5.88,
        "text": "gonna fall off right uh the nosql no SQL"
      },
      {
        "start": 1069.559,
        "duration": 4.62,
        "text": "is really about horizontal scaling"
      },
      {
        "start": 1071.9,
        "duration": 5.7,
        "text": "um it's really about kind of leveraging"
      },
      {
        "start": 1074.179,
        "duration": 7.021,
        "text": "uh commodity hardware and being able to"
      },
      {
        "start": 1077.6,
        "duration": 5.54,
        "text": "distribute your data but let the"
      },
      {
        "start": 1081.2,
        "duration": 4.44,
        "text": "interest I mean let the"
      },
      {
        "start": 1083.14,
        "duration": 4.779,
        "text": "software or the platform take care of it"
      },
      {
        "start": 1085.64,
        "duration": 4.56,
        "text": "and that's why you know this term nosql"
      },
      {
        "start": 1087.919,
        "duration": 5.041,
        "text": "has become popular"
      },
      {
        "start": 1090.2,
        "duration": 6.66,
        "text": "um and and if you think about this uh"
      },
      {
        "start": 1092.96,
        "duration": 6.54,
        "text": "June 11 2009 I mean this is about the"
      },
      {
        "start": 1096.86,
        "duration": 4.199,
        "text": "time that cloud also kind of started"
      },
      {
        "start": 1099.5,
        "duration": 3.539,
        "text": "catching up right"
      },
      {
        "start": 1101.059,
        "duration": 4.86,
        "text": "um but but the nice thing about nosql"
      },
      {
        "start": 1103.039,
        "duration": 5.64,
        "text": "and the cloud is that there are a lot of"
      },
      {
        "start": 1105.919,
        "duration": 5.041,
        "text": "similarities uh and it's a nice Synergy"
      },
      {
        "start": 1108.679,
        "duration": 4.021,
        "text": "between nosql and the cloud"
      },
      {
        "start": 1110.96,
        "duration": 3.66,
        "text": "um and and you know you can take"
      },
      {
        "start": 1112.7,
        "duration": 3.599,
        "text": "advantage of both when you kind of"
      },
      {
        "start": 1114.62,
        "duration": 3.799,
        "text": "combine them because typically if you"
      },
      {
        "start": 1116.299,
        "duration": 4.26,
        "text": "think about relational versus nosql"
      },
      {
        "start": 1118.419,
        "duration": 4.841,
        "text": "relationalists like I said you know is"
      },
      {
        "start": 1120.559,
        "duration": 3.721,
        "text": "about scaling vertically to a point and"
      },
      {
        "start": 1123.26,
        "duration": 3.48,
        "text": "then"
      },
      {
        "start": 1124.28,
        "duration": 4.32,
        "text": "you know you basically hit the limits"
      },
      {
        "start": 1126.74,
        "duration": 5.819,
        "text": "um with nosql it's really about"
      },
      {
        "start": 1128.6,
        "duration": 6.72,
        "text": "horizontal scaling easy to scale"
      },
      {
        "start": 1132.559,
        "duration": 4.561,
        "text": "um and and really"
      },
      {
        "start": 1135.32,
        "duration": 3.359,
        "text": "um you know one of one of the downsides"
      },
      {
        "start": 1137.12,
        "duration": 3.66,
        "text": "that you may have heard about people"
      },
      {
        "start": 1138.679,
        "duration": 4.261,
        "text": "talking about nosql is that you know"
      },
      {
        "start": 1140.78,
        "duration": 4.5,
        "text": "there is no concept of asset"
      },
      {
        "start": 1142.94,
        "duration": 4.38,
        "text": "transactions right but even that is"
      },
      {
        "start": 1145.28,
        "duration": 3.6,
        "text": "changing in today's work"
      },
      {
        "start": 1147.32,
        "duration": 3.96,
        "text": "um you might have heard you might have"
      },
      {
        "start": 1148.88,
        "duration": 4.56,
        "text": "seen some presentations in Cassandra day"
      },
      {
        "start": 1151.28,
        "duration": 4.38,
        "text": "where we talked about asset transactions"
      },
      {
        "start": 1153.44,
        "duration": 4.32,
        "text": "and uh we have a Cassandra Summit coming"
      },
      {
        "start": 1155.66,
        "duration": 4.08,
        "text": "up in March where we're going to have"
      },
      {
        "start": 1157.76,
        "duration": 4.26,
        "text": "more details about this"
      },
      {
        "start": 1159.74,
        "duration": 4.98,
        "text": "um but you know again we will talk about"
      },
      {
        "start": 1162.02,
        "duration": 5.46,
        "text": "that later again uh but but the point"
      },
      {
        "start": 1164.72,
        "duration": 4.5,
        "text": "again I'm trying to make here is that"
      },
      {
        "start": 1167.48,
        "duration": 3.72,
        "text": "um some of these lines are actually"
      },
      {
        "start": 1169.22,
        "duration": 4.199,
        "text": "blurring a little bit and and some of"
      },
      {
        "start": 1171.2,
        "duration": 5.479,
        "text": "the relational databases you can also do"
      },
      {
        "start": 1173.419,
        "duration": 5.88,
        "text": "some scaling but but typically no SQL"
      },
      {
        "start": 1176.679,
        "duration": 5.021,
        "text": "was really born in the cloud even before"
      },
      {
        "start": 1179.299,
        "duration": 4.5,
        "text": "the cloud was there kind of you know if"
      },
      {
        "start": 1181.7,
        "duration": 3.42,
        "text": "you want to think about it that way okay"
      },
      {
        "start": 1183.799,
        "duration": 3.721,
        "text": "you mean"
      },
      {
        "start": 1185.12,
        "duration": 4.439,
        "text": "have heard about this it's called the"
      },
      {
        "start": 1187.52,
        "duration": 4.5,
        "text": "cap theorem or also referred to as the"
      },
      {
        "start": 1189.559,
        "duration": 3.721,
        "text": "Brewers conjecture and essentially if"
      },
      {
        "start": 1192.02,
        "duration": 3.06,
        "text": "you want to think about distributed"
      },
      {
        "start": 1193.28,
        "duration": 3.54,
        "text": "system characteristics there is"
      },
      {
        "start": 1195.08,
        "duration": 3.42,
        "text": "consistency availability and partition"
      },
      {
        "start": 1196.82,
        "duration": 4.56,
        "text": "tolerance"
      },
      {
        "start": 1198.5,
        "duration": 6.179,
        "text": "um you can only have"
      },
      {
        "start": 1201.38,
        "duration": 5.4,
        "text": "two of these in a failure scenario when"
      },
      {
        "start": 1204.679,
        "duration": 4.681,
        "text": "there is no failure everything"
      },
      {
        "start": 1206.78,
        "duration": 5.58,
        "text": "hunky-dory you can have all three no"
      },
      {
        "start": 1209.36,
        "duration": 5.16,
        "text": "problem right but when you you know"
      },
      {
        "start": 1212.36,
        "duration": 4.559,
        "text": "actually come into a failure scenario"
      },
      {
        "start": 1214.52,
        "duration": 6.539,
        "text": "which is typical in a distributed system"
      },
      {
        "start": 1216.919,
        "duration": 7.561,
        "text": "you have to pick two of the three or you"
      },
      {
        "start": 1221.059,
        "duration": 6.12,
        "text": "have to sacrifice one of the three okay"
      },
      {
        "start": 1224.48,
        "duration": 5.16,
        "text": "um typically nobody none of the nosql"
      },
      {
        "start": 1227.179,
        "duration": 4.021,
        "text": "systems really sacrifice partition"
      },
      {
        "start": 1229.64,
        "duration": 4.32,
        "text": "tolerance because what happens in a"
      },
      {
        "start": 1231.2,
        "duration": 5.4,
        "text": "partition tolerant or something that"
      },
      {
        "start": 1233.96,
        "duration": 4.5,
        "text": "does not have partition tolerances"
      },
      {
        "start": 1236.6,
        "duration": 4.14,
        "text": "um you know anybody can notice the"
      },
      {
        "start": 1238.46,
        "duration": 4.74,
        "text": "inconsistencies there right"
      },
      {
        "start": 1240.74,
        "duration": 4.799,
        "text": "um on the other hand if you sacrifice"
      },
      {
        "start": 1243.2,
        "duration": 4.62,
        "text": "consistency there is still a concept of"
      },
      {
        "start": 1245.539,
        "duration": 4.621,
        "text": "eventual consistency where I'm not on"
      },
      {
        "start": 1247.82,
        "duration": 5.94,
        "text": "top of the leaderboard for 15 seconds"
      },
      {
        "start": 1250.16,
        "duration": 5.519,
        "text": "big deal right but maybe it is a big"
      },
      {
        "start": 1253.76,
        "duration": 4.32,
        "text": "deal right and that is why we are adding"
      },
      {
        "start": 1255.679,
        "duration": 3.841,
        "text": "acid transactions as well"
      },
      {
        "start": 1258.08,
        "duration": 3.66,
        "text": "um so you can tune your consistency"
      },
      {
        "start": 1259.52,
        "duration": 5.76,
        "text": "levels and so on but"
      },
      {
        "start": 1261.74,
        "duration": 6.0,
        "text": "if you make it you know um acid"
      },
      {
        "start": 1265.28,
        "duration": 4.08,
        "text": "consistency then you may not be"
      },
      {
        "start": 1267.74,
        "duration": 3.72,
        "text": "available for a certain amount of time"
      },
      {
        "start": 1269.36,
        "duration": 6.36,
        "text": "so in other words you always have to"
      },
      {
        "start": 1271.46,
        "duration": 7.5,
        "text": "sacrifice one of the two okay and"
      },
      {
        "start": 1275.72,
        "duration": 6.78,
        "text": "the ecosystem kind of distributes itself"
      },
      {
        "start": 1278.96,
        "duration": 6.06,
        "text": "right into either a AP system or a CP"
      },
      {
        "start": 1282.5,
        "duration": 5.64,
        "text": "system like I said you know nobody"
      },
      {
        "start": 1285.02,
        "duration": 4.8,
        "text": "really gives up on partition tolerance"
      },
      {
        "start": 1288.14,
        "duration": 4.62,
        "text": "um so you know you'll see a number of"
      },
      {
        "start": 1289.82,
        "duration": 6.12,
        "text": "these different uh databases or nosql"
      },
      {
        "start": 1292.76,
        "duration": 6.0,
        "text": "databases kind of aligning into AP or CP"
      },
      {
        "start": 1295.94,
        "duration": 5.52,
        "text": "but again you know there is ways in"
      },
      {
        "start": 1298.76,
        "duration": 4.26,
        "text": "which you can say I want"
      },
      {
        "start": 1301.46,
        "duration": 3.54,
        "text": "um you know different levels of"
      },
      {
        "start": 1303.02,
        "duration": 3.84,
        "text": "consistency and depending on the"
      },
      {
        "start": 1305.0,
        "duration": 3.419,
        "text": "different levels of consistency like you"
      },
      {
        "start": 1306.86,
        "duration": 4.74,
        "text": "know whether it's eventual consistency"
      },
      {
        "start": 1308.419,
        "duration": 6.061,
        "text": "or very strong consistency your system"
      },
      {
        "start": 1311.6,
        "duration": 5.459,
        "text": "may not be available and so on so so"
      },
      {
        "start": 1314.48,
        "duration": 6.24,
        "text": "um again the point is that you know it's"
      },
      {
        "start": 1317.059,
        "duration": 6.901,
        "text": "for Big Data it's the the nice thing"
      },
      {
        "start": 1320.72,
        "duration": 7.26,
        "text": "about uh about Cassandra is that it"
      },
      {
        "start": 1323.96,
        "duration": 7.199,
        "text": "basically scales linearly and you know"
      },
      {
        "start": 1327.98,
        "duration": 4.8,
        "text": "um Rahul and I were at AWS re invent uh"
      },
      {
        "start": 1331.159,
        "duration": 3.241,
        "text": "we spoke to so many customers about"
      },
      {
        "start": 1332.78,
        "duration": 4.68,
        "text": "Patrick Sandra and one of the things"
      },
      {
        "start": 1334.4,
        "duration": 5.88,
        "text": "they like include the people like you"
      },
      {
        "start": 1337.46,
        "duration": 4.92,
        "text": "know who came from Apple Netflix price"
      },
      {
        "start": 1340.28,
        "duration": 4.08,
        "text": "line and so on is the fact that it"
      },
      {
        "start": 1342.38,
        "duration": 4.56,
        "text": "linearly scales many of the nosql"
      },
      {
        "start": 1344.36,
        "duration": 6.42,
        "text": "systems you know they go to a point and"
      },
      {
        "start": 1346.94,
        "duration": 7.44,
        "text": "then the kind of level off not so in the"
      },
      {
        "start": 1350.78,
        "duration": 5.04,
        "text": "case of uh Cassandra okay but I know"
      },
      {
        "start": 1354.38,
        "duration": 4.86,
        "text": "that you guys are waiting to get your"
      },
      {
        "start": 1355.82,
        "duration": 5.219,
        "text": "hands on so let's start with the the"
      },
      {
        "start": 1359.24,
        "duration": 6.86,
        "text": "Hands-On okay"
      },
      {
        "start": 1361.039,
        "duration": 5.061,
        "text": "so to do that let me bring up my"
      },
      {
        "start": 1367.4,
        "duration": 5.34,
        "text": "so this is the um GitHub link if you if"
      },
      {
        "start": 1371.059,
        "duration": 3.301,
        "text": "you just put a pound GitHub and I'll"
      },
      {
        "start": 1372.74,
        "duration": 5.9,
        "text": "I'll try to put it myself"
      },
      {
        "start": 1374.36,
        "duration": 4.28,
        "text": "you should get the link on chat"
      },
      {
        "start": 1379.76,
        "duration": 4.039,
        "text": "okay if everything is working okay"
      },
      {
        "start": 1384.799,
        "duration": 3.681,
        "text": "let's see it should work"
      },
      {
        "start": 1389.9,
        "duration": 2.54,
        "text": "hmm"
      },
      {
        "start": 1393.38,
        "duration": 3.179,
        "text": "okay so there you go it was a little bit"
      },
      {
        "start": 1395.48,
        "duration": 3.9,
        "text": "late okay"
      },
      {
        "start": 1396.559,
        "duration": 4.561,
        "text": "and all that I do is just follow along"
      },
      {
        "start": 1399.38,
        "duration": 4.26,
        "text": "on this"
      },
      {
        "start": 1401.12,
        "duration": 3.86,
        "text": "um you know GitHub link okay so it's"
      },
      {
        "start": 1403.64,
        "duration": 4.26,
        "text": "github.com"
      },
      {
        "start": 1404.98,
        "duration": 5.92,
        "text": "ananth which is the company an example"
      },
      {
        "start": 1407.9,
        "duration": 4.5,
        "text": "Cassandra ETL with the airflow and Spark"
      },
      {
        "start": 1410.9,
        "duration": 3.42,
        "text": "so what we're going to do is we're going"
      },
      {
        "start": 1412.4,
        "duration": 4.2,
        "text": "to open this in gitpart the moment you"
      },
      {
        "start": 1414.32,
        "duration": 5.219,
        "text": "open in gitpart okay"
      },
      {
        "start": 1416.6,
        "duration": 6.3,
        "text": "so open link I'm gonna open link a new"
      },
      {
        "start": 1419.539,
        "duration": 7.281,
        "text": "window but I've already opened this so"
      },
      {
        "start": 1422.9,
        "duration": 3.92,
        "text": "and what the heck let me open a new one"
      },
      {
        "start": 1433.039,
        "duration": 4.02,
        "text": "okay and and basically it will ask you"
      },
      {
        "start": 1435.44,
        "duration": 4.739,
        "text": "for the GitHub"
      },
      {
        "start": 1437.059,
        "duration": 5.221,
        "text": "uh ID or you can use"
      },
      {
        "start": 1440.179,
        "duration": 3.601,
        "text": "I think you can use gitlab and others as"
      },
      {
        "start": 1442.28,
        "duration": 3.36,
        "text": "well but but you know GitHub is probably"
      },
      {
        "start": 1443.78,
        "duration": 3.84,
        "text": "the easiest okay"
      },
      {
        "start": 1445.64,
        "duration": 6.32,
        "text": "and what it'll do is it'll bring bring"
      },
      {
        "start": 1447.62,
        "duration": 4.34,
        "text": "up this git pod link okay"
      },
      {
        "start": 1453.14,
        "duration": 4.26,
        "text": "and I opened it in new window but you"
      },
      {
        "start": 1455.48,
        "duration": 3.84,
        "text": "can do it in a new tab you know"
      },
      {
        "start": 1457.4,
        "duration": 4.139,
        "text": "whichever"
      },
      {
        "start": 1459.32,
        "duration": 3.96,
        "text": "um works for you and you'll see here"
      },
      {
        "start": 1461.539,
        "duration": 5.541,
        "text": "that I have the Repository"
      },
      {
        "start": 1463.28,
        "duration": 6.84,
        "text": "uh and and if I do a"
      },
      {
        "start": 1467.08,
        "duration": 6.4,
        "text": "uh git remote right"
      },
      {
        "start": 1470.12,
        "duration": 5.36,
        "text": "minus V it will give you the link as"
      },
      {
        "start": 1473.48,
        "duration": 5.28,
        "text": "well okay so basically it's linked here"
      },
      {
        "start": 1475.48,
        "duration": 5.079,
        "text": "and if I change any of these"
      },
      {
        "start": 1478.76,
        "duration": 3.18,
        "text": "um it's it's kind of you can use it like"
      },
      {
        "start": 1480.559,
        "duration": 4.081,
        "text": "you get"
      },
      {
        "start": 1481.94,
        "duration": 4.2,
        "text": "um terminal if you will so if you if you"
      },
      {
        "start": 1484.64,
        "duration": 2.76,
        "text": "look at this this is really visual"
      },
      {
        "start": 1486.14,
        "duration": 3.419,
        "text": "studio"
      },
      {
        "start": 1487.4,
        "duration": 4.92,
        "text": "um on the cloud uh you have a file"
      },
      {
        "start": 1489.559,
        "duration": 4.921,
        "text": "explorer window uh and you can edit any"
      },
      {
        "start": 1492.32,
        "duration": 4.02,
        "text": "of these so if you you know if you click"
      },
      {
        "start": 1494.48,
        "duration": 3.9,
        "text": "on this it shows you how to you can"
      },
      {
        "start": 1496.34,
        "duration": 6.199,
        "text": "create tables and so on"
      },
      {
        "start": 1498.38,
        "duration": 4.159,
        "text": "um and you can edit this okay so"
      },
      {
        "start": 1503.24,
        "duration": 4.919,
        "text": "so if I do something like that or"
      },
      {
        "start": 1505.58,
        "duration": 4.26,
        "text": "setup.sh and you know so basically this"
      },
      {
        "start": 1508.159,
        "duration": 4.801,
        "text": "is the editor and then you can run"
      },
      {
        "start": 1509.84,
        "duration": 4.98,
        "text": "commands here okay so if I do an Astra"
      },
      {
        "start": 1512.96,
        "duration": 3.3,
        "text": "DB list it's probably going to fail"
      },
      {
        "start": 1514.82,
        "duration": 3.719,
        "text": "because I haven't installed Astra yet"
      },
      {
        "start": 1516.26,
        "duration": 3.84,
        "text": "but I'm going to install it in a second"
      },
      {
        "start": 1518.539,
        "duration": 4.801,
        "text": "okay"
      },
      {
        "start": 1520.1,
        "duration": 6.66,
        "text": "all right so let me go back here okay"
      },
      {
        "start": 1523.34,
        "duration": 6.12,
        "text": "and continue I'm gonna create a database"
      },
      {
        "start": 1526.76,
        "duration": 5.34,
        "text": "name called workshops okay and the"
      },
      {
        "start": 1529.46,
        "duration": 4.62,
        "text": "airflow demo and again you can all that"
      },
      {
        "start": 1532.1,
        "duration": 5.22,
        "text": "you need to do is click on this link"
      },
      {
        "start": 1534.08,
        "duration": 6.36,
        "text": "I've already clear created all this"
      },
      {
        "start": 1537.32,
        "duration": 6.06,
        "text": "so I've already created a security token"
      },
      {
        "start": 1540.44,
        "duration": 4.02,
        "text": "uh I'm going to install the Astro CLI"
      },
      {
        "start": 1543.38,
        "duration": 3.0,
        "text": "now okay"
      },
      {
        "start": 1544.46,
        "duration": 4.26,
        "text": "so I'm going to go here and I'm going to"
      },
      {
        "start": 1546.38,
        "duration": 3.659,
        "text": "install the Astro CLI"
      },
      {
        "start": 1548.72,
        "duration": 3.12,
        "text": "okay and then everything should be"
      },
      {
        "start": 1550.039,
        "duration": 3.961,
        "text": "hunky-dory"
      },
      {
        "start": 1551.84,
        "duration": 5.219,
        "text": "all that you need to do is"
      },
      {
        "start": 1554.0,
        "duration": 5.039,
        "text": "grab the command from the GitHub and and"
      },
      {
        "start": 1557.059,
        "duration": 5.281,
        "text": "just add it here okay"
      },
      {
        "start": 1559.039,
        "duration": 6.74,
        "text": "so let me go to two small heads here and"
      },
      {
        "start": 1562.34,
        "duration": 3.439,
        "text": "get rid of the quiz music"
      },
      {
        "start": 1567.74,
        "duration": 2.88,
        "text": "okay"
      },
      {
        "start": 1568.88,
        "duration": 3.179,
        "text": "so this might be better when I'm doing a"
      },
      {
        "start": 1570.62,
        "duration": 4.74,
        "text": "demo okay"
      },
      {
        "start": 1572.059,
        "duration": 5.941,
        "text": "so it worked and now what I'm gonna do"
      },
      {
        "start": 1575.36,
        "duration": 4.08,
        "text": "is make sure that Astra is in my path"
      },
      {
        "start": 1578.0,
        "duration": 3.36,
        "text": "right"
      },
      {
        "start": 1579.44,
        "duration": 4.76,
        "text": "um so I'm going to Source this"
      },
      {
        "start": 1581.36,
        "duration": 2.84,
        "text": "and"
      },
      {
        "start": 1585.679,
        "duration": 4.521,
        "text": "let me bring this up so that you guys"
      },
      {
        "start": 1587.72,
        "duration": 2.48,
        "text": "can see it"
      },
      {
        "start": 1590.48,
        "duration": 5.52,
        "text": "okay and it asked me to enter an astro"
      },
      {
        "start": 1593.12,
        "duration": 5.939,
        "text": "token okay so so when you go to Astra"
      },
      {
        "start": 1596.0,
        "duration": 5.159,
        "text": "you can create a token and the way you"
      },
      {
        "start": 1599.059,
        "duration": 4.081,
        "text": "create it is you know it's pretty"
      },
      {
        "start": 1601.159,
        "duration": 4.201,
        "text": "straightforward"
      },
      {
        "start": 1603.14,
        "duration": 4.98,
        "text": "um let me show it here"
      },
      {
        "start": 1605.36,
        "duration": 3.679,
        "text": "but all the steps are in there okay if"
      },
      {
        "start": 1608.12,
        "duration": 3.84,
        "text": "you go to"
      },
      {
        "start": 1609.039,
        "duration": 5.581,
        "text": "astron.datastacks.com and"
      },
      {
        "start": 1611.96,
        "duration": 2.66,
        "text": "um just sign in"
      },
      {
        "start": 1619.7,
        "duration": 5.64,
        "text": "okay and if you go to your organization"
      },
      {
        "start": 1622.84,
        "duration": 4.48,
        "text": "organization settings"
      },
      {
        "start": 1625.34,
        "duration": 3.78,
        "text": "okay token management"
      },
      {
        "start": 1627.32,
        "duration": 4.859,
        "text": "okay and select the role database"
      },
      {
        "start": 1629.12,
        "duration": 4.32,
        "text": "administrator it could be a little bit"
      },
      {
        "start": 1632.179,
        "duration": 3.48,
        "text": "um"
      },
      {
        "start": 1633.44,
        "duration": 4.08,
        "text": "more fine-grained but but you know we"
      },
      {
        "start": 1635.659,
        "duration": 4.801,
        "text": "just picked that um"
      },
      {
        "start": 1637.52,
        "duration": 5.94,
        "text": "the database administrator and just make"
      },
      {
        "start": 1640.46,
        "duration": 5.88,
        "text": "sure you use this you save this client"
      },
      {
        "start": 1643.46,
        "duration": 4.199,
        "text": "ID client secret and token somewhere"
      },
      {
        "start": 1646.34,
        "duration": 2.76,
        "text": "um you know you can just say download"
      },
      {
        "start": 1647.659,
        "duration": 4.081,
        "text": "the token details and it'll be there"
      },
      {
        "start": 1649.1,
        "duration": 4.86,
        "text": "okay or you can copy paste it you know"
      },
      {
        "start": 1651.74,
        "duration": 4.439,
        "text": "you can do whatever you want okay"
      },
      {
        "start": 1653.96,
        "duration": 3.66,
        "text": "um because once you navigate away from"
      },
      {
        "start": 1656.179,
        "duration": 4.141,
        "text": "this page you're not going to get this"
      },
      {
        "start": 1657.62,
        "duration": 6.0,
        "text": "anymore okay I've already cut and pasted"
      },
      {
        "start": 1660.32,
        "duration": 5.52,
        "text": "this and let me see if I can find that"
      },
      {
        "start": 1663.62,
        "duration": 5.34,
        "text": "um I've kind of sneaked it in"
      },
      {
        "start": 1665.84,
        "duration": 4.92,
        "text": "so let me see if I can find it"
      },
      {
        "start": 1668.96,
        "duration": 3.3,
        "text": "and I'm going to put the token in there"
      },
      {
        "start": 1670.76,
        "duration": 3.68,
        "text": "okay so just give me a second to find"
      },
      {
        "start": 1672.26,
        "duration": 2.18,
        "text": "out"
      },
      {
        "start": 1678.2,
        "duration": 5.28,
        "text": "all right"
      },
      {
        "start": 1680.6,
        "duration": 5.16,
        "text": "and I'm going to put the token in here"
      },
      {
        "start": 1683.48,
        "duration": 4.26,
        "text": "okay and hopefully it's all going to"
      },
      {
        "start": 1685.76,
        "duration": 3.96,
        "text": "work so I'm gonna get rid of this I"
      },
      {
        "start": 1687.74,
        "duration": 4.679,
        "text": "don't really need that okay"
      },
      {
        "start": 1689.72,
        "duration": 4.439,
        "text": "I put the token in there"
      },
      {
        "start": 1692.419,
        "duration": 2.701,
        "text": "and I'm good to go and I'm going to"
      },
      {
        "start": 1694.159,
        "duration": 2.101,
        "text": "clear this"
      },
      {
        "start": 1695.12,
        "duration": 5.039,
        "text": "okay"
      },
      {
        "start": 1696.26,
        "duration": 6.899,
        "text": "so now I should if I list the um if I"
      },
      {
        "start": 1700.159,
        "duration": 5.941,
        "text": "run the command Astro DB list"
      },
      {
        "start": 1703.159,
        "duration": 5.221,
        "text": "I should be able to see it"
      },
      {
        "start": 1706.1,
        "duration": 4.079,
        "text": "okay so you can see"
      },
      {
        "start": 1708.38,
        "duration": 3.72,
        "text": "um you know there are"
      },
      {
        "start": 1710.179,
        "duration": 4.561,
        "text": "five"
      },
      {
        "start": 1712.1,
        "duration": 4.079,
        "text": "um you know different databases one of"
      },
      {
        "start": 1714.74,
        "duration": 3.9,
        "text": "the things like key things I forgot to"
      },
      {
        "start": 1716.179,
        "duration": 5.401,
        "text": "tell you was that uh you get a 25"
      },
      {
        "start": 1718.64,
        "duration": 4.62,
        "text": "dollars per month free credit so you"
      },
      {
        "start": 1721.58,
        "duration": 3.0,
        "text": "don't have to put your credit card all"
      },
      {
        "start": 1723.26,
        "duration": 3.06,
        "text": "that you need to do is put in your email"
      },
      {
        "start": 1724.58,
        "duration": 4.68,
        "text": "that's it or you can use your GitHub"
      },
      {
        "start": 1726.32,
        "duration": 5.719,
        "text": "link as well I use my email"
      },
      {
        "start": 1729.26,
        "duration": 5.279,
        "text": "um and then you get 25 which is actually"
      },
      {
        "start": 1732.039,
        "duration": 4.12,
        "text": "pretty good to run some fairly"
      },
      {
        "start": 1734.539,
        "duration": 4.52,
        "text": "significant production workloads believe"
      },
      {
        "start": 1736.159,
        "duration": 7.62,
        "text": "it or not okay um"
      },
      {
        "start": 1739.059,
        "duration": 5.921,
        "text": "so what I did to summarize was kind of"
      },
      {
        "start": 1743.779,
        "duration": 3.601,
        "text": "talked about"
      },
      {
        "start": 1744.98,
        "duration": 5.34,
        "text": "um nosql talked about Cassandra as the"
      },
      {
        "start": 1747.38,
        "duration": 5.399,
        "text": "linear scalable nosql database right and"
      },
      {
        "start": 1750.32,
        "duration": 5.64,
        "text": "then what we did was we"
      },
      {
        "start": 1752.779,
        "duration": 5.64,
        "text": "um we you know brought up gitpart just"
      },
      {
        "start": 1755.96,
        "duration": 5.939,
        "text": "by just clicking on that link right and"
      },
      {
        "start": 1758.419,
        "duration": 5.941,
        "text": "then you we created a token in Astra but"
      },
      {
        "start": 1761.899,
        "duration": 4.441,
        "text": "keep the client ID and secrets as well"
      },
      {
        "start": 1764.36,
        "duration": 3.66,
        "text": "because we're going to use it and Rahul"
      },
      {
        "start": 1766.34,
        "duration": 2.219,
        "text": "is going to talk about that"
      },
      {
        "start": 1768.02,
        "duration": 2.7,
        "text": "um"
      },
      {
        "start": 1768.559,
        "duration": 3.301,
        "text": "and once you're done with this notice"
      },
      {
        "start": 1770.72,
        "duration": 3.3,
        "text": "there are two windows the terminal"
      },
      {
        "start": 1771.86,
        "duration": 5.34,
        "text": "window there are two Terminals and will"
      },
      {
        "start": 1774.02,
        "duration": 4.92,
        "text": "be kind of uh going between one and one"
      },
      {
        "start": 1777.2,
        "duration": 3.719,
        "text": "and the other the all commands window"
      },
      {
        "start": 1778.94,
        "duration": 4.08,
        "text": "and the Run air flow window you know so"
      },
      {
        "start": 1780.919,
        "duration": 4.98,
        "text": "so you know we're all is going to talk"
      },
      {
        "start": 1783.02,
        "duration": 5.96,
        "text": "about all that with that said let me go"
      },
      {
        "start": 1785.899,
        "duration": 3.081,
        "text": "back to the presentation"
      },
      {
        "start": 1789.2,
        "duration": 3.68,
        "text": "um okay let me see where it was"
      },
      {
        "start": 1794.299,
        "duration": 3.441,
        "text": "Okay so"
      },
      {
        "start": 1797.84,
        "duration": 5.52,
        "text": "with that said Rahul it's all yours"
      },
      {
        "start": 1800.299,
        "duration": 6.541,
        "text": "let's see some some thumbs up on our um"
      },
      {
        "start": 1803.36,
        "duration": 4.86,
        "text": "you know in our chat to yeah to see if"
      },
      {
        "start": 1806.84,
        "duration": 3.3,
        "text": "you know any of you are able to create"
      },
      {
        "start": 1808.22,
        "duration": 3.66,
        "text": "the database and if you're not able to"
      },
      {
        "start": 1810.14,
        "duration": 4.74,
        "text": "create the database please reach out and"
      },
      {
        "start": 1811.88,
        "duration": 6.12,
        "text": "chat and we'll be glad to help you uh"
      },
      {
        "start": 1814.88,
        "duration": 6.299,
        "text": "one thing again I forgot to mention was"
      },
      {
        "start": 1818.0,
        "duration": 6.12,
        "text": "you need to use a uh link and and maybe"
      },
      {
        "start": 1821.179,
        "duration": 4.081,
        "text": "this is relevant let me show that really"
      },
      {
        "start": 1824.12,
        "duration": 5.059,
        "text": "quickly"
      },
      {
        "start": 1825.26,
        "duration": 3.919,
        "text": "let me go back again to the astral link"
      },
      {
        "start": 1830.12,
        "duration": 5.039,
        "text": "okay you have to use a free tier"
      },
      {
        "start": 1833.0,
        "duration": 4.2,
        "text": "um not all of them are free tiers okay"
      },
      {
        "start": 1835.159,
        "duration": 3.0,
        "text": "so so if you want to create a new"
      },
      {
        "start": 1837.2,
        "duration": 2.099,
        "text": "database"
      },
      {
        "start": 1838.159,
        "duration": 3.421,
        "text": "okay"
      },
      {
        "start": 1839.299,
        "duration": 6.12,
        "text": "um you have to pick the North America"
      },
      {
        "start": 1841.58,
        "duration": 6.479,
        "text": "and a region that is free okay some of"
      },
      {
        "start": 1845.419,
        "duration": 5.101,
        "text": "these are not free uh the best thing to"
      },
      {
        "start": 1848.059,
        "duration": 5.701,
        "text": "do is just use U.S east not you know"
      },
      {
        "start": 1850.52,
        "duration": 4.8,
        "text": "like not Virginia okay uh like if you if"
      },
      {
        "start": 1853.76,
        "duration": 3.48,
        "text": "you use"
      },
      {
        "start": 1855.32,
        "duration": 4.2,
        "text": "um oh I'm sorry Google Cloud"
      },
      {
        "start": 1857.24,
        "duration": 4.26,
        "text": "okay and use Monk's Corner this is what"
      },
      {
        "start": 1859.52,
        "duration": 4.74,
        "text": "you want to use okay and you'll be good"
      },
      {
        "start": 1861.5,
        "duration": 4.32,
        "text": "to go um some some of these are might be"
      },
      {
        "start": 1864.26,
        "duration": 3.84,
        "text": "locked for you you know for me it's all"
      },
      {
        "start": 1865.82,
        "duration": 4.68,
        "text": "open because you know I'm special right"
      },
      {
        "start": 1868.1,
        "duration": 4.86,
        "text": "uh but but you know some of you some of"
      },
      {
        "start": 1870.5,
        "duration": 5.159,
        "text": "these might might show us lot so what I"
      },
      {
        "start": 1872.96,
        "duration": 5.16,
        "text": "would do is you know just pick Google"
      },
      {
        "start": 1875.659,
        "duration": 4.5,
        "text": "Cloud North America and Monks Corner"
      },
      {
        "start": 1878.12,
        "duration": 3.419,
        "text": "um to be able to create your database"
      },
      {
        "start": 1880.159,
        "duration": 3.781,
        "text": "okay"
      },
      {
        "start": 1881.539,
        "duration": 3.38,
        "text": "I think I've talked enough and I will"
      },
      {
        "start": 1883.94,
        "duration": 4.619,
        "text": "let"
      },
      {
        "start": 1884.919,
        "duration": 6.521,
        "text": "Rahul drive it so let me"
      },
      {
        "start": 1888.559,
        "duration": 6.0,
        "text": "make sure that your screen is showing up"
      },
      {
        "start": 1891.44,
        "duration": 6.32,
        "text": "okay just give me a second"
      },
      {
        "start": 1894.559,
        "duration": 3.201,
        "text": "and uh"
      },
      {
        "start": 1898.82,
        "duration": 4.68,
        "text": "I will"
      },
      {
        "start": 1901.46,
        "duration": 3.36,
        "text": "thank you I appreciate that the"
      },
      {
        "start": 1903.5,
        "duration": 3.24,
        "text": "walkthrough"
      },
      {
        "start": 1904.82,
        "duration": 4.26,
        "text": "um and and you know uh give me one"
      },
      {
        "start": 1906.74,
        "duration": 4.98,
        "text": "second I'm gonna get my"
      },
      {
        "start": 1909.08,
        "duration": 4.979,
        "text": "Skype sharing"
      },
      {
        "start": 1911.72,
        "duration": 5.059,
        "text": "sorry Siri is always listening in on me"
      },
      {
        "start": 1914.059,
        "duration": 2.72,
        "text": "you bet"
      },
      {
        "start": 1917.0,
        "duration": 3.899,
        "text": "uh you should be able to see my full"
      },
      {
        "start": 1919.34,
        "duration": 3.3,
        "text": "screen now"
      },
      {
        "start": 1920.899,
        "duration": 4.581,
        "text": "um all right give me a second"
      },
      {
        "start": 1922.64,
        "duration": 2.84,
        "text": "and"
      },
      {
        "start": 1928.94,
        "duration": 4.02,
        "text": "all right you're good"
      },
      {
        "start": 1930.98,
        "duration": 4.74,
        "text": "uh"
      },
      {
        "start": 1932.96,
        "duration": 6.48,
        "text": "excellent uh well thanks again uh ranks"
      },
      {
        "start": 1935.72,
        "duration": 5.339,
        "text": "and data stacks for for having uh us uh"
      },
      {
        "start": 1939.44,
        "duration": 3.479,
        "text": "just a little bit about"
      },
      {
        "start": 1941.059,
        "duration": 4.86,
        "text": "um you know what our company does we"
      },
      {
        "start": 1942.919,
        "duration": 4.86,
        "text": "help platform owners just think big so"
      },
      {
        "start": 1945.919,
        "duration": 4.081,
        "text": "we help them with their platform design"
      },
      {
        "start": 1947.779,
        "duration": 4.62,
        "text": "and essentially companies that have a"
      },
      {
        "start": 1950.0,
        "duration": 4.86,
        "text": "global customer base and data stacks and"
      },
      {
        "start": 1952.399,
        "duration": 5.341,
        "text": "Astra are great Partners"
      },
      {
        "start": 1954.86,
        "duration": 4.919,
        "text": "um to help us give people what they want"
      },
      {
        "start": 1957.74,
        "duration": 3.6,
        "text": "um and the way we do that is you know we"
      },
      {
        "start": 1959.779,
        "duration": 4.14,
        "text": "have a Playbook which we're going to"
      },
      {
        "start": 1961.34,
        "duration": 4.14,
        "text": "preview a little bit about about uh and"
      },
      {
        "start": 1963.919,
        "duration": 5.401,
        "text": "in our framework we use tools like"
      },
      {
        "start": 1965.48,
        "duration": 6.059,
        "text": "Cassandra spark Kafka airflow is part of"
      },
      {
        "start": 1969.32,
        "duration": 3.54,
        "text": "our framework and"
      },
      {
        "start": 1971.539,
        "duration": 2.701,
        "text": "um and ultimately once you build a"
      },
      {
        "start": 1972.86,
        "duration": 3.539,
        "text": "platform you know we have an approach"
      },
      {
        "start": 1974.24,
        "duration": 6.0,
        "text": "for how to manage it and even if you're"
      },
      {
        "start": 1976.399,
        "duration": 5.821,
        "text": "using a SAS system like Astra you're"
      },
      {
        "start": 1980.24,
        "duration": 3.299,
        "text": "gonna have other systems that and it"
      },
      {
        "start": 1982.22,
        "duration": 2.339,
        "text": "becomes complicated so you need a little"
      },
      {
        "start": 1983.539,
        "duration": 3.36,
        "text": "bit of a guidance so that's what we do"
      },
      {
        "start": 1984.559,
        "duration": 3.661,
        "text": "we help people with their big platforms"
      },
      {
        "start": 1986.899,
        "duration": 4.441,
        "text": "um we've helped quite a number of"
      },
      {
        "start": 1988.22,
        "duration": 5.4,
        "text": "customers over the years a lot of brands"
      },
      {
        "start": 1991.34,
        "duration": 4.5,
        "text": "that you probably use uh"
      },
      {
        "start": 1993.62,
        "duration": 3.72,
        "text": "um you know we work with them and it's"
      },
      {
        "start": 1995.84,
        "duration": 2.9,
        "text": "just like Cassandra I was explaining to"
      },
      {
        "start": 1997.34,
        "duration": 4.079,
        "text": "Cassandra at a holiday party yesterday"
      },
      {
        "start": 1998.74,
        "duration": 4.659,
        "text": "and I said yeah you know pretty much"
      },
      {
        "start": 2001.419,
        "duration": 3.961,
        "text": "every app on your iPhone I mean it's"
      },
      {
        "start": 2003.399,
        "duration": 4.681,
        "text": "powered by Cassandra in the background"
      },
      {
        "start": 2005.38,
        "duration": 5.1,
        "text": "or Netflix Spotify you know Walmart"
      },
      {
        "start": 2008.08,
        "duration": 4.079,
        "text": "Priceline everything you imagined as a"
      },
      {
        "start": 2010.48,
        "duration": 3.419,
        "text": "as a global company the used technology"
      },
      {
        "start": 2012.159,
        "duration": 3.721,
        "text": "like Cassandra and that's the type of"
      },
      {
        "start": 2013.899,
        "duration": 3.481,
        "text": "work we work uh those are the clients we"
      },
      {
        "start": 2015.88,
        "duration": 4.74,
        "text": "work with"
      },
      {
        "start": 2017.38,
        "duration": 5.7,
        "text": "a quick preview uh of like where this"
      },
      {
        "start": 2020.62,
        "duration": 5.159,
        "text": "Playbook and framework fits in for every"
      },
      {
        "start": 2023.08,
        "duration": 4.199,
        "text": "challenge uh we have a tool so from when"
      },
      {
        "start": 2025.779,
        "duration": 3.24,
        "text": "we go from a business challenge to how"
      },
      {
        "start": 2027.279,
        "duration": 3.181,
        "text": "to make it into a platform our Playbook"
      },
      {
        "start": 2029.019,
        "duration": 3.841,
        "text": "is about how do you design the platform"
      },
      {
        "start": 2030.46,
        "duration": 4.74,
        "text": "how do you integrate real-time data with"
      },
      {
        "start": 2032.86,
        "duration": 3.6,
        "text": "the rest of your systems uh once you"
      },
      {
        "start": 2035.2,
        "duration": 2.699,
        "text": "know what your design is going to be"
      },
      {
        "start": 2036.46,
        "duration": 3.18,
        "text": "what your platform is going to be is it"
      },
      {
        "start": 2037.899,
        "duration": 3.9,
        "text": "going to be AWS is it going to be Azure"
      },
      {
        "start": 2039.64,
        "duration": 4.919,
        "text": "uh ultimately then it's what specific"
      },
      {
        "start": 2041.799,
        "duration": 5.76,
        "text": "components need to be used and you know"
      },
      {
        "start": 2044.559,
        "duration": 6.001,
        "text": "if you're in this uh big data or data"
      },
      {
        "start": 2047.559,
        "duration": 5.161,
        "text": "analytics ecosystem it's hard to figure"
      },
      {
        "start": 2050.56,
        "duration": 3.599,
        "text": "out what tools to use and so that's"
      },
      {
        "start": 2052.72,
        "duration": 3.48,
        "text": "where our framework comes in we tell"
      },
      {
        "start": 2054.159,
        "duration": 3.661,
        "text": "people here are the different categories"
      },
      {
        "start": 2056.2,
        "duration": 3.84,
        "text": "of tools here's what you can you know"
      },
      {
        "start": 2057.82,
        "duration": 4.019,
        "text": "use in this category"
      },
      {
        "start": 2060.04,
        "duration": 3.119,
        "text": "um and we're going to focus mainly on"
      },
      {
        "start": 2061.839,
        "duration": 3.181,
        "text": "the framework today you know"
      },
      {
        "start": 2063.159,
        "duration": 3.661,
        "text": "specifically airflow and Spark and how"
      },
      {
        "start": 2065.02,
        "duration": 4.26,
        "text": "it works with Cassandra"
      },
      {
        "start": 2066.82,
        "duration": 4.019,
        "text": "and uh you know how we do that we help"
      },
      {
        "start": 2069.28,
        "duration": 3.42,
        "text": "people with Professional Services and we"
      },
      {
        "start": 2070.839,
        "duration": 3.721,
        "text": "help them coordinate their managed"
      },
      {
        "start": 2072.7,
        "duration": 3.84,
        "text": "Services uh we don't want to reinvent"
      },
      {
        "start": 2074.56,
        "duration": 3.599,
        "text": "the wheel there's companies like data"
      },
      {
        "start": 2076.54,
        "duration": 3.9,
        "text": "Stacks that have Astra we recommend"
      },
      {
        "start": 2078.159,
        "duration": 4.5,
        "text": "people use something as a service so"
      },
      {
        "start": 2080.44,
        "duration": 3.419,
        "text": "that you don't have to basically do busy"
      },
      {
        "start": 2082.659,
        "duration": 3.301,
        "text": "work when you when you have a company"
      },
      {
        "start": 2083.859,
        "duration": 4.381,
        "text": "like Astra providing a database as a"
      },
      {
        "start": 2085.96,
        "duration": 4.439,
        "text": "service that town"
      },
      {
        "start": 2088.24,
        "duration": 5.159,
        "text": "that would have gone into managing the"
      },
      {
        "start": 2090.399,
        "duration": 4.621,
        "text": "SRE managing the devops you can actually"
      },
      {
        "start": 2093.399,
        "duration": 3.841,
        "text": "focus on other things like data"
      },
      {
        "start": 2095.02,
        "duration": 5.46,
        "text": "engineering and data Ops uh in fact"
      },
      {
        "start": 2097.24,
        "duration": 6.0,
        "text": "there was a talk by somebody uh at AWS"
      },
      {
        "start": 2100.48,
        "duration": 5.34,
        "text": "re invent last where they had migrated a"
      },
      {
        "start": 2103.24,
        "duration": 3.96,
        "text": "bunch of databases to the cloud and they"
      },
      {
        "start": 2105.82,
        "duration": 3.36,
        "text": "were saying well what do we do now we're"
      },
      {
        "start": 2107.2,
        "duration": 5.7,
        "text": "dbas what do we do now well the new job"
      },
      {
        "start": 2109.18,
        "duration": 5.7,
        "text": "for the DBA is data engineering data Ops"
      },
      {
        "start": 2112.9,
        "duration": 4.08,
        "text": "infrastructures Code and and that's"
      },
      {
        "start": 2114.88,
        "duration": 3.959,
        "text": "really where uh you know your Innovation"
      },
      {
        "start": 2116.98,
        "duration": 4.619,
        "text": "should be going is a new feature it's"
      },
      {
        "start": 2118.839,
        "duration": 6.061,
        "text": "not like running servers take care of"
      },
      {
        "start": 2121.599,
        "duration": 5.941,
        "text": "your customers right you know"
      },
      {
        "start": 2124.9,
        "duration": 4.02,
        "text": "yes exactly take care of your customers"
      },
      {
        "start": 2127.54,
        "duration": 4.2,
        "text": "um"
      },
      {
        "start": 2128.92,
        "duration": 4.5,
        "text": "so uh you know I give a big picture of"
      },
      {
        "start": 2131.74,
        "duration": 3.06,
        "text": "you uh because that's just the way I"
      },
      {
        "start": 2133.42,
        "duration": 4.14,
        "text": "think I always want to know why I'm"
      },
      {
        "start": 2134.8,
        "duration": 4.86,
        "text": "doing something so when we work on data"
      },
      {
        "start": 2137.56,
        "duration": 4.2,
        "text": "analytics platforms you know why do we"
      },
      {
        "start": 2139.66,
        "duration": 5.04,
        "text": "do that well because business platforms"
      },
      {
        "start": 2141.76,
        "duration": 5.579,
        "text": "have lots and lots of systems across"
      },
      {
        "start": 2144.7,
        "duration": 5.58,
        "text": "many different areas that connect people"
      },
      {
        "start": 2147.339,
        "duration": 4.621,
        "text": "process information and systems and we"
      },
      {
        "start": 2150.28,
        "duration": 2.88,
        "text": "have this vision of what we say is"
      },
      {
        "start": 2151.96,
        "duration": 3.96,
        "text": "Enterprise Consciousness which is"
      },
      {
        "start": 2153.16,
        "duration": 4.919,
        "text": "real-time data across users and"
      },
      {
        "start": 2155.92,
        "duration": 4.56,
        "text": "processes and systems"
      },
      {
        "start": 2158.079,
        "duration": 5.52,
        "text": "um and and Technologies like Cassandra"
      },
      {
        "start": 2160.48,
        "duration": 6.119,
        "text": "spark Kafka Pulsar they help us get"
      },
      {
        "start": 2163.599,
        "duration": 5.581,
        "text": "there but data analytics are the heart"
      },
      {
        "start": 2166.599,
        "duration": 6.301,
        "text": "of any business platform and every"
      },
      {
        "start": 2169.18,
        "duration": 4.86,
        "text": "company has some system uh the bigger"
      },
      {
        "start": 2172.9,
        "duration": 2.699,
        "text": "companies need technologies like"
      },
      {
        "start": 2174.04,
        "duration": 5.039,
        "text": "Cassandra"
      },
      {
        "start": 2175.599,
        "duration": 6.301,
        "text": "uh the modern open data platform is a"
      },
      {
        "start": 2179.079,
        "duration": 5.401,
        "text": "relatively New Concept it used to be you"
      },
      {
        "start": 2181.9,
        "duration": 4.56,
        "text": "know proprietary Technologies like"
      },
      {
        "start": 2184.48,
        "duration": 4.08,
        "text": "SQL Server Oracle and you've kind of"
      },
      {
        "start": 2186.46,
        "duration": 4.68,
        "text": "like you did everything around it and"
      },
      {
        "start": 2188.56,
        "duration": 4.92,
        "text": "then there was Hadoop and Hadoop had 50"
      },
      {
        "start": 2191.14,
        "duration": 6.18,
        "text": "different tools to do what you needed to"
      },
      {
        "start": 2193.48,
        "duration": 6.78,
        "text": "do uh and so now uh what we see is a a"
      },
      {
        "start": 2197.32,
        "duration": 6.0,
        "text": "movement towards an open data platform"
      },
      {
        "start": 2200.26,
        "duration": 4.2,
        "text": "which uses open core or open source"
      },
      {
        "start": 2203.32,
        "duration": 3.66,
        "text": "tools and you kind of bring them"
      },
      {
        "start": 2204.46,
        "duration": 3.84,
        "text": "together uh to do what you need instead"
      },
      {
        "start": 2206.98,
        "duration": 3.359,
        "text": "of getting it from one vendor you're"
      },
      {
        "start": 2208.3,
        "duration": 3.36,
        "text": "kind of piecing it together"
      },
      {
        "start": 2210.339,
        "duration": 4.081,
        "text": "um so our Playbook is about helping"
      },
      {
        "start": 2211.66,
        "duration": 5.459,
        "text": "people figure out what to use right"
      },
      {
        "start": 2214.42,
        "duration": 5.46,
        "text": "um and and Cassandra is at the heart of"
      },
      {
        "start": 2217.119,
        "duration": 5.401,
        "text": "that uh we love Cassandra because it has"
      },
      {
        "start": 2219.88,
        "duration": 4.14,
        "text": "this idea of a data fabric built in and"
      },
      {
        "start": 2222.52,
        "duration": 3.54,
        "text": "you can run it on different clouds you"
      },
      {
        "start": 2224.02,
        "duration": 6.36,
        "text": "can run it on different uh systems like"
      },
      {
        "start": 2226.06,
        "duration": 6.24,
        "text": "containers and VMS on premise uh Etc and"
      },
      {
        "start": 2230.38,
        "duration": 4.32,
        "text": "what I mean by a data fabric"
      },
      {
        "start": 2232.3,
        "duration": 4.38,
        "text": "and and Cassandra really uh you know"
      },
      {
        "start": 2234.7,
        "duration": 4.44,
        "text": "alongside databases like couchbase they"
      },
      {
        "start": 2236.68,
        "duration": 4.98,
        "text": "made it a possible to have cross data"
      },
      {
        "start": 2239.14,
        "duration": 3.42,
        "text": "center replication and that's huge and"
      },
      {
        "start": 2241.66,
        "duration": 2.82,
        "text": "we're going to talk a little bit about"
      },
      {
        "start": 2242.56,
        "duration": 4.5,
        "text": "how you know when you're running a spark"
      },
      {
        "start": 2244.48,
        "duration": 4.139,
        "text": "job to do ETL on a different data center"
      },
      {
        "start": 2247.06,
        "duration": 3.9,
        "text": "you're not going to be impacting your"
      },
      {
        "start": 2248.619,
        "duration": 3.48,
        "text": "transactional traffic right so so if"
      },
      {
        "start": 2250.96,
        "duration": 3.0,
        "text": "you're using Aster you don't have to"
      },
      {
        "start": 2252.099,
        "duration": 3.721,
        "text": "worry about all this stuff but uh"
      },
      {
        "start": 2253.96,
        "duration": 3.78,
        "text": "because Astra just scales you know"
      },
      {
        "start": 2255.82,
        "duration": 3.42,
        "text": "horizontally as you start to use more"
      },
      {
        "start": 2257.74,
        "duration": 3.42,
        "text": "and more of it"
      },
      {
        "start": 2259.24,
        "duration": 4.26,
        "text": "um but the idea of Commander as a data"
      },
      {
        "start": 2261.16,
        "duration": 4.86,
        "text": "fabric came about because Cassandra"
      },
      {
        "start": 2263.5,
        "duration": 4.92,
        "text": "scales infinitely horizontally you can"
      },
      {
        "start": 2266.02,
        "duration": 4.319,
        "text": "put more and more workloads on it and"
      },
      {
        "start": 2268.42,
        "duration": 3.48,
        "text": "and everybody can have access to that"
      },
      {
        "start": 2270.339,
        "duration": 3.181,
        "text": "information in real time whether it's an"
      },
      {
        "start": 2271.9,
        "duration": 3.78,
        "text": "analytical process a transactional"
      },
      {
        "start": 2273.52,
        "duration": 3.36,
        "text": "process a machine learning process uh"
      },
      {
        "start": 2275.68,
        "duration": 4.08,
        "text": "you name it"
      },
      {
        "start": 2276.88,
        "duration": 5.04,
        "text": "um and you know there are native"
      },
      {
        "start": 2279.76,
        "duration": 4.8,
        "text": "offerings like you know Cosmos but"
      },
      {
        "start": 2281.92,
        "duration": 4.56,
        "text": "they're like prohibitively expensive and"
      },
      {
        "start": 2284.56,
        "duration": 3.779,
        "text": "that's what we see people will use"
      },
      {
        "start": 2286.48,
        "duration": 3.78,
        "text": "Cosmos and they'll say this is this is"
      },
      {
        "start": 2288.339,
        "duration": 3.901,
        "text": "too expensive for us so then they'll"
      },
      {
        "start": 2290.26,
        "duration": 4.98,
        "text": "maybe go to Astra or maybe they'll do K"
      },
      {
        "start": 2292.24,
        "duration": 5.28,
        "text": "Sandra right because"
      },
      {
        "start": 2295.24,
        "duration": 4.68,
        "text": "well quite honestly those companies are"
      },
      {
        "start": 2297.52,
        "duration": 5.04,
        "text": "doing something for everybody and and"
      },
      {
        "start": 2299.92,
        "duration": 5.04,
        "text": "each customer that uses Cassandra they"
      },
      {
        "start": 2302.56,
        "duration": 4.26,
        "text": "have some special need for it"
      },
      {
        "start": 2304.96,
        "duration": 3.54,
        "text": "um and uh you know Technologies like"
      },
      {
        "start": 2306.82,
        "duration": 5.34,
        "text": "Aster make it easy to do those special"
      },
      {
        "start": 2308.5,
        "duration": 6.48,
        "text": "needs uh in different scenarios"
      },
      {
        "start": 2312.16,
        "duration": 5.82,
        "text": "a data platform is not just a database"
      },
      {
        "start": 2314.98,
        "duration": 5.46,
        "text": "right it's it's actually a Confluence of"
      },
      {
        "start": 2317.98,
        "duration": 4.26,
        "text": "lots of different things there's a"
      },
      {
        "start": 2320.44,
        "duration": 3.179,
        "text": "stream involved nearly in a mature data"
      },
      {
        "start": 2322.24,
        "duration": 4.2,
        "text": "platform there's a scheduler and that's"
      },
      {
        "start": 2323.619,
        "duration": 4.561,
        "text": "what airflow is it's a scheduler"
      },
      {
        "start": 2326.44,
        "duration": 3.419,
        "text": "um there are internal apis there are"
      },
      {
        "start": 2328.18,
        "duration": 4.98,
        "text": "external apis that you may be bringing"
      },
      {
        "start": 2329.859,
        "duration": 5.461,
        "text": "data into uh or sending data out to"
      },
      {
        "start": 2333.16,
        "duration": 4.02,
        "text": "um but it's not always just like you"
      },
      {
        "start": 2335.32,
        "duration": 5.279,
        "text": "know what Rags was talking about you"
      },
      {
        "start": 2337.18,
        "duration": 6.54,
        "text": "have not only relational or not only SQL"
      },
      {
        "start": 2340.599,
        "duration": 5.701,
        "text": "systems alongside with SQL and"
      },
      {
        "start": 2343.72,
        "duration": 5.1,
        "text": "relational systems alongside with data"
      },
      {
        "start": 2346.3,
        "duration": 5.819,
        "text": "lake houses or data warehouses and"
      },
      {
        "start": 2348.82,
        "duration": 5.16,
        "text": "that's really where inside the core of"
      },
      {
        "start": 2352.119,
        "duration": 4.381,
        "text": "the data platform"
      },
      {
        "start": 2353.98,
        "duration": 5.4,
        "text": "um we need technologies like airflow to"
      },
      {
        "start": 2356.5,
        "duration": 5.579,
        "text": "help us move or at least to coordinate"
      },
      {
        "start": 2359.38,
        "duration": 4.32,
        "text": "the pipelines of information right and"
      },
      {
        "start": 2362.079,
        "duration": 5.401,
        "text": "we want something that's repeatable"
      },
      {
        "start": 2363.7,
        "duration": 5.1,
        "text": "automated uh not error prone uh so you"
      },
      {
        "start": 2367.48,
        "duration": 3.84,
        "text": "start to understand that you know"
      },
      {
        "start": 2368.8,
        "duration": 4.559,
        "text": "airflow is is a part of a much bigger"
      },
      {
        "start": 2371.32,
        "duration": 4.38,
        "text": "picture it's not just about Cassandra"
      },
      {
        "start": 2373.359,
        "duration": 5.521,
        "text": "and Spark it could be used to get data"
      },
      {
        "start": 2375.7,
        "duration": 5.639,
        "text": "out of Cassandra put it into a a data"
      },
      {
        "start": 2378.88,
        "duration": 4.32,
        "text": "with that you're using like snow like"
      },
      {
        "start": 2381.339,
        "duration": 4.381,
        "text": "snowflake for example or it could be"
      },
      {
        "start": 2383.2,
        "duration": 6.12,
        "text": "taking data out of a data lake house"
      },
      {
        "start": 2385.72,
        "duration": 5.399,
        "text": "like uh S3 Delta Lake Etc and put it"
      },
      {
        "start": 2389.32,
        "duration": 3.6,
        "text": "into Cassandra right so there's a lot of"
      },
      {
        "start": 2391.119,
        "duration": 2.941,
        "text": "different use cases well I just want you"
      },
      {
        "start": 2392.92,
        "duration": 3.24,
        "text": "to understand that we're going to talk"
      },
      {
        "start": 2394.06,
        "duration": 4.26,
        "text": "about like one little part of it but it"
      },
      {
        "start": 2396.16,
        "duration": 4.32,
        "text": "can do a lot more"
      },
      {
        "start": 2398.32,
        "duration": 4.259,
        "text": "um we focus on distributed real-time"
      },
      {
        "start": 2400.48,
        "duration": 4.26,
        "text": "components right"
      },
      {
        "start": 2402.579,
        "duration": 4.02,
        "text": "um distributed real time means things"
      },
      {
        "start": 2404.74,
        "duration": 3.54,
        "text": "that scale infinitely things that work"
      },
      {
        "start": 2406.599,
        "duration": 5.341,
        "text": "with other things that scale infinitely"
      },
      {
        "start": 2408.28,
        "duration": 5.88,
        "text": "uh Cassandra data sacks Astra they work"
      },
      {
        "start": 2411.94,
        "duration": 4.2,
        "text": "really really well with spark but you"
      },
      {
        "start": 2414.16,
        "duration": 3.66,
        "text": "know spark can be run by Google for you"
      },
      {
        "start": 2416.14,
        "duration": 5.16,
        "text": "right Google dataproc is like a Google"
      },
      {
        "start": 2417.82,
        "duration": 4.74,
        "text": "version of spark or AWS EMR"
      },
      {
        "start": 2421.3,
        "duration": 3.72,
        "text": "um there are other Technologies like"
      },
      {
        "start": 2422.56,
        "duration": 5.16,
        "text": "Presto that work well so in an open data"
      },
      {
        "start": 2425.02,
        "duration": 4.26,
        "text": "platform our focus is in distributed"
      },
      {
        "start": 2427.72,
        "duration": 3.72,
        "text": "real-time components there's a lot of"
      },
      {
        "start": 2429.28,
        "duration": 3.78,
        "text": "tools out that are out there that are"
      },
      {
        "start": 2431.44,
        "duration": 3.78,
        "text": "open source that we don't really care"
      },
      {
        "start": 2433.06,
        "duration": 3.66,
        "text": "much about it's just well everybody else"
      },
      {
        "start": 2435.22,
        "duration": 2.76,
        "text": "does that right"
      },
      {
        "start": 2436.72,
        "duration": 2.399,
        "text": "but how do you choose from that"
      },
      {
        "start": 2437.98,
        "duration": 4.02,
        "text": "landscape"
      },
      {
        "start": 2439.119,
        "duration": 5.521,
        "text": "right like there are so many different"
      },
      {
        "start": 2442.0,
        "duration": 5.099,
        "text": "Technologies out there for data and AI"
      },
      {
        "start": 2444.64,
        "duration": 5.219,
        "text": "right this is just data and AI there's"
      },
      {
        "start": 2447.099,
        "duration": 5.101,
        "text": "even more Technologies in general from a"
      },
      {
        "start": 2449.859,
        "duration": 4.561,
        "text": "software perspective like CRM right"
      },
      {
        "start": 2452.2,
        "duration": 3.899,
        "text": "those are like millions of options but"
      },
      {
        "start": 2454.42,
        "duration": 4.32,
        "text": "in data and AI there's a lot of options"
      },
      {
        "start": 2456.099,
        "duration": 4.801,
        "text": "a lot of Open Source options uh even the"
      },
      {
        "start": 2458.74,
        "duration": 3.3,
        "text": "Linux foundation's data and AI landscape"
      },
      {
        "start": 2460.9,
        "duration": 2.64,
        "text": "there's so many different tools out"
      },
      {
        "start": 2462.04,
        "duration": 4.26,
        "text": "there right so you're not going to need"
      },
      {
        "start": 2463.54,
        "duration": 4.319,
        "text": "all of them because quite frankly a lot"
      },
      {
        "start": 2466.3,
        "duration": 3.24,
        "text": "of them do the same thing so how do you"
      },
      {
        "start": 2467.859,
        "duration": 4.021,
        "text": "choose these tools"
      },
      {
        "start": 2469.54,
        "duration": 4.74,
        "text": "um well we do a lot of research and we"
      },
      {
        "start": 2471.88,
        "duration": 4.199,
        "text": "ask ourselves what is the industry using"
      },
      {
        "start": 2474.28,
        "duration": 3.78,
        "text": "we don't want to reinvent the wheel and"
      },
      {
        "start": 2476.079,
        "duration": 4.201,
        "text": "when we do research on open tools and"
      },
      {
        "start": 2478.06,
        "duration": 5.7,
        "text": "open platforms we notice these patterns"
      },
      {
        "start": 2480.28,
        "duration": 6.42,
        "text": "over and over and over again we see"
      },
      {
        "start": 2483.76,
        "duration": 6.12,
        "text": "Technologies like spark everywhere we"
      },
      {
        "start": 2486.7,
        "duration": 4.74,
        "text": "see Technologies like airflow everywhere"
      },
      {
        "start": 2489.88,
        "duration": 3.66,
        "text": "and I'm not saying just Technologies"
      },
      {
        "start": 2491.44,
        "duration": 4.56,
        "text": "like spark we see spark everywhere we"
      },
      {
        "start": 2493.54,
        "duration": 4.98,
        "text": "see airflow everywhere and they work"
      },
      {
        "start": 2496.0,
        "duration": 5.46,
        "text": "well with other tools like omensin is an"
      },
      {
        "start": 2498.52,
        "duration": 5.7,
        "text": "open source data catalog that can store"
      },
      {
        "start": 2501.46,
        "duration": 4.2,
        "text": "data in of the whole data ecosystem in"
      },
      {
        "start": 2504.22,
        "duration": 4.32,
        "text": "your company inside cassand and"
      },
      {
        "start": 2505.66,
        "duration": 5.22,
        "text": "januscraft or DSE graph but guess what"
      },
      {
        "start": 2508.54,
        "duration": 4.799,
        "text": "it uses to collect all the metadata it"
      },
      {
        "start": 2510.88,
        "duration": 4.68,
        "text": "uses airflow it uses airflow to"
      },
      {
        "start": 2513.339,
        "duration": 5.101,
        "text": "basically populate all that data so if"
      },
      {
        "start": 2515.56,
        "duration": 5.88,
        "text": "if another open source project as mature"
      },
      {
        "start": 2518.44,
        "duration": 4.679,
        "text": "as amundsen is using airflow"
      },
      {
        "start": 2521.44,
        "duration": 4.2,
        "text": "um that's really the signal that I look"
      },
      {
        "start": 2523.119,
        "duration": 5.22,
        "text": "for is who's using these tools"
      },
      {
        "start": 2525.64,
        "duration": 4.14,
        "text": "um the other signal for airflow is there"
      },
      {
        "start": 2528.339,
        "duration": 2.76,
        "text": "was an airflow conference and I think"
      },
      {
        "start": 2529.78,
        "duration": 3.6,
        "text": "there have been a few in the last few"
      },
      {
        "start": 2531.099,
        "duration": 4.921,
        "text": "years and who are the people speaking"
      },
      {
        "start": 2533.38,
        "duration": 6.0,
        "text": "about airflow you got Uber you got Apple"
      },
      {
        "start": 2536.02,
        "duration": 4.44,
        "text": "these companies are using airflow and"
      },
      {
        "start": 2539.38,
        "duration": 2.88,
        "text": "these are also the same companies that"
      },
      {
        "start": 2540.46,
        "duration": 3.06,
        "text": "use Cassandra right so they need this"
      },
      {
        "start": 2542.26,
        "duration": 2.76,
        "text": "technology because they don't want to"
      },
      {
        "start": 2543.52,
        "duration": 4.02,
        "text": "reinvent the wheel"
      },
      {
        "start": 2545.02,
        "duration": 3.78,
        "text": "um and ultimately what we do is we put"
      },
      {
        "start": 2547.54,
        "duration": 3.9,
        "text": "all these things together and we"
      },
      {
        "start": 2548.8,
        "duration": 5.279,
        "text": "categorize them and we look at the open"
      },
      {
        "start": 2551.44,
        "duration": 4.5,
        "text": "source strategy we look at the the"
      },
      {
        "start": 2554.079,
        "duration": 4.5,
        "text": "Google strategy we look at the Amazon"
      },
      {
        "start": 2555.94,
        "duration": 4.86,
        "text": "strategy uh that's because not every"
      },
      {
        "start": 2558.579,
        "duration": 4.26,
        "text": "company is just going to use all open"
      },
      {
        "start": 2560.8,
        "duration": 3.9,
        "text": "source tools Hook Line and Sinker"
      },
      {
        "start": 2562.839,
        "duration": 4.081,
        "text": "they're going to maybe use a lot of"
      },
      {
        "start": 2564.7,
        "duration": 3.06,
        "text": "things on their cloud of choice and then"
      },
      {
        "start": 2566.92,
        "duration": 2.52,
        "text": "they're going to need something"
      },
      {
        "start": 2567.76,
        "duration": 5.28,
        "text": "different"
      },
      {
        "start": 2569.44,
        "duration": 6.54,
        "text": "um the the benefit of airflow is that it"
      },
      {
        "start": 2573.04,
        "duration": 4.68,
        "text": "is becoming a standard okay so there is"
      },
      {
        "start": 2575.98,
        "duration": 4.74,
        "text": "a company called astronomer like"
      },
      {
        "start": 2577.72,
        "duration": 5.82,
        "text": "datastacks supports Cassandra astronomer"
      },
      {
        "start": 2580.72,
        "duration": 5.16,
        "text": "supports airflow but there's a managed"
      },
      {
        "start": 2583.54,
        "duration": 3.9,
        "text": "version of airflow from Amazon there's a"
      },
      {
        "start": 2585.88,
        "duration": 7.08,
        "text": "managed version of airflow from Google"
      },
      {
        "start": 2587.44,
        "duration": 8.04,
        "text": "also so technology uh specifically if it"
      },
      {
        "start": 2592.96,
        "duration": 4.379,
        "text": "starts to be managed by the clouds"
      },
      {
        "start": 2595.48,
        "duration": 4.859,
        "text": "that's another good signal that okay"
      },
      {
        "start": 2597.339,
        "duration": 4.441,
        "text": "there's a critical mass enough that"
      },
      {
        "start": 2600.339,
        "duration": 3.601,
        "text": "people need it that they're going to"
      },
      {
        "start": 2601.78,
        "duration": 3.9,
        "text": "make it a commodity right so you don't"
      },
      {
        "start": 2603.94,
        "duration": 3.36,
        "text": "necessarily have to run your own airflow"
      },
      {
        "start": 2605.68,
        "duration": 3.84,
        "text": "instance"
      },
      {
        "start": 2607.3,
        "duration": 4.38,
        "text": "um because these clouds will help you do"
      },
      {
        "start": 2609.52,
        "duration": 4.68,
        "text": "it for you"
      },
      {
        "start": 2611.68,
        "duration": 3.78,
        "text": "we're going to focus uh in our in our in"
      },
      {
        "start": 2614.2,
        "duration": 2.879,
        "text": "a cult Playbook there's a design"
      },
      {
        "start": 2615.46,
        "duration": 3.119,
        "text": "component there's an evaluation which"
      },
      {
        "start": 2617.079,
        "duration": 3.841,
        "text": "comes framework tools and then there's"
      },
      {
        "start": 2618.579,
        "duration": 4.02,
        "text": "finally like how do you execute uh we're"
      },
      {
        "start": 2620.92,
        "duration": 4.439,
        "text": "going to focus on data Ops right this is"
      },
      {
        "start": 2622.599,
        "duration": 5.641,
        "text": "kind of where airflow really fits uh is"
      },
      {
        "start": 2625.359,
        "duration": 4.681,
        "text": "how do you orchestrate and operate these"
      },
      {
        "start": 2628.24,
        "duration": 4.26,
        "text": "data operations"
      },
      {
        "start": 2630.04,
        "duration": 4.68,
        "text": "um and then in terms of operation right"
      },
      {
        "start": 2632.5,
        "duration": 4.02,
        "text": "there's the how do we set it up how do"
      },
      {
        "start": 2634.72,
        "duration": 4.56,
        "text": "we monitor it well in in terms of"
      },
      {
        "start": 2636.52,
        "duration": 6.559,
        "text": "operation airflow can also be used for"
      },
      {
        "start": 2639.28,
        "duration": 3.799,
        "text": "certain administrative functions"
      },
      {
        "start": 2643.599,
        "duration": 3.421,
        "text": "and then finally when you put it all"
      },
      {
        "start": 2645.579,
        "duration": 4.321,
        "text": "together when you put all the Lego"
      },
      {
        "start": 2647.02,
        "duration": 4.92,
        "text": "blocks together uh we come up with a"
      },
      {
        "start": 2649.9,
        "duration": 4.32,
        "text": "reference architecture where we say okay"
      },
      {
        "start": 2651.94,
        "duration": 4.74,
        "text": "well if you're doing real time if you're"
      },
      {
        "start": 2654.22,
        "duration": 4.139,
        "text": "doing real-time data and real-time data"
      },
      {
        "start": 2656.68,
        "duration": 3.48,
        "text": "out real-time analytics real-time"
      },
      {
        "start": 2658.359,
        "duration": 3.781,
        "text": "machine learning this is kind of the"
      },
      {
        "start": 2660.16,
        "duration": 3.06,
        "text": "technology stack to use here are all the"
      },
      {
        "start": 2662.14,
        "duration": 3.06,
        "text": "components when you put them together"
      },
      {
        "start": 2663.22,
        "duration": 3.24,
        "text": "this is what it looks like"
      },
      {
        "start": 2665.2,
        "duration": 4.44,
        "text": "um"
      },
      {
        "start": 2666.46,
        "duration": 6.18,
        "text": "now in the data modernization aspect of"
      },
      {
        "start": 2669.64,
        "duration": 5.459,
        "text": "platforms there are a lot of tools we're"
      },
      {
        "start": 2672.64,
        "duration": 4.8,
        "text": "not going to get into today okay"
      },
      {
        "start": 2675.099,
        "duration": 4.201,
        "text": "um but there's a lot of tools for ETL"
      },
      {
        "start": 2677.44,
        "duration": 4.32,
        "text": "there's a lot of tools from a reverse"
      },
      {
        "start": 2679.3,
        "duration": 4.319,
        "text": "ETL and what I mean by ETL it's not from"
      },
      {
        "start": 2681.76,
        "duration": 4.68,
        "text": "your CSV file into Cassandra I'm talking"
      },
      {
        "start": 2683.619,
        "duration": 5.401,
        "text": "about data coming from Salesforce"
      },
      {
        "start": 2686.44,
        "duration": 4.62,
        "text": "getting it into Casino data coming from"
      },
      {
        "start": 2689.02,
        "duration": 3.72,
        "text": "Cassandra and sending it back to"
      },
      {
        "start": 2691.06,
        "duration": 3.84,
        "text": "Salesforce right that's reverse ETL"
      },
      {
        "start": 2692.74,
        "duration": 4.56,
        "text": "there's a lot of Open Source tools out"
      },
      {
        "start": 2694.9,
        "duration": 5.459,
        "text": "there that can help you do that"
      },
      {
        "start": 2697.3,
        "duration": 4.68,
        "text": "um but on on this left uh you know"
      },
      {
        "start": 2700.359,
        "duration": 3.48,
        "text": "column here"
      },
      {
        "start": 2701.98,
        "duration": 4.5,
        "text": "um I have amundsen"
      },
      {
        "start": 2703.839,
        "duration": 4.681,
        "text": "an airflow uh highlighted because"
      },
      {
        "start": 2706.48,
        "duration": 4.98,
        "text": "amundsen is an open source data catalog"
      },
      {
        "start": 2708.52,
        "duration": 5.7,
        "text": "so is data HUD uh but I'm just going to"
      },
      {
        "start": 2711.46,
        "duration": 4.56,
        "text": "use this airflow and uh you know in a"
      },
      {
        "start": 2714.22,
        "duration": 3.66,
        "text": "mature data platform you eventually have"
      },
      {
        "start": 2716.02,
        "duration": 3.72,
        "text": "a data catalog and maybe that can be"
      },
      {
        "start": 2717.88,
        "duration": 3.66,
        "text": "another talk how to set up Amazon on"
      },
      {
        "start": 2719.74,
        "duration": 4.28,
        "text": "Astra but today we're going to focus on"
      },
      {
        "start": 2721.54,
        "duration": 2.48,
        "text": "inflow"
      },
      {
        "start": 2724.54,
        "duration": 2.579,
        "text": "um"
      },
      {
        "start": 2725.44,
        "duration": 4.34,
        "text": "and the last thing about our Playbook"
      },
      {
        "start": 2727.119,
        "duration": 5.161,
        "text": "which is really important is the reason"
      },
      {
        "start": 2729.78,
        "duration": 4.539,
        "text": "orchestration automation continues"
      },
      {
        "start": 2732.28,
        "duration": 4.559,
        "text": "integration and delivery the reason it's"
      },
      {
        "start": 2734.319,
        "duration": 4.981,
        "text": "important in the setup column of our"
      },
      {
        "start": 2736.839,
        "duration": 5.76,
        "text": "approach is that people across the board"
      },
      {
        "start": 2739.3,
        "duration": 6.24,
        "text": "who are in development"
      },
      {
        "start": 2742.599,
        "duration": 6.661,
        "text": "um operations architecture it allows"
      },
      {
        "start": 2745.54,
        "duration": 6.18,
        "text": "them to be reliant on that process it"
      },
      {
        "start": 2749.26,
        "duration": 3.96,
        "text": "allows people to easily get trained on"
      },
      {
        "start": 2751.72,
        "duration": 3.3,
        "text": "that process you don't have to be an"
      },
      {
        "start": 2753.22,
        "duration": 3.24,
        "text": "expert if something's automated you can"
      },
      {
        "start": 2755.02,
        "duration": 3.54,
        "text": "just kind of start using it and you can"
      },
      {
        "start": 2756.46,
        "duration": 4.8,
        "text": "you learn more and more of it"
      },
      {
        "start": 2758.56,
        "duration": 4.92,
        "text": "um we came up with this schema because I"
      },
      {
        "start": 2761.26,
        "duration": 4.2,
        "text": "tell even though it's really great I"
      },
      {
        "start": 2763.48,
        "duration": 3.48,
        "text": "feel like only the largest companies in"
      },
      {
        "start": 2765.46,
        "duration": 2.58,
        "text": "the world can Implement ITIL if you"
      },
      {
        "start": 2766.96,
        "duration": 3.659,
        "text": "don't know what that is you can Google"
      },
      {
        "start": 2768.04,
        "duration": 6.0,
        "text": "it it's called it information"
      },
      {
        "start": 2770.619,
        "duration": 5.041,
        "text": "um autism Library infrastructure Library"
      },
      {
        "start": 2774.04,
        "duration": 3.36,
        "text": "um it's a way it's a patterns and"
      },
      {
        "start": 2775.66,
        "duration": 3.659,
        "text": "practices for how to manage technology"
      },
      {
        "start": 2777.4,
        "duration": 3.719,
        "text": "at scale well you know when we were"
      },
      {
        "start": 2779.319,
        "duration": 3.421,
        "text": "working with our clients a lot of the"
      },
      {
        "start": 2781.119,
        "duration": 4.441,
        "text": "the folks who are in the startup world"
      },
      {
        "start": 2782.74,
        "duration": 4.92,
        "text": "are in growth mode they don't quite get"
      },
      {
        "start": 2785.56,
        "duration": 4.259,
        "text": "ITIL they're not quite their company"
      },
      {
        "start": 2787.66,
        "duration": 3.959,
        "text": "that's been around for 20 years right so"
      },
      {
        "start": 2789.819,
        "duration": 3.841,
        "text": "they need some guidance on how do we do"
      },
      {
        "start": 2791.619,
        "duration": 4.801,
        "text": "this so that you know when when employee"
      },
      {
        "start": 2793.66,
        "duration": 4.679,
        "text": "five leaves and employee number 50 comes"
      },
      {
        "start": 2796.42,
        "duration": 3.96,
        "text": "that they're able to kind of quickly get"
      },
      {
        "start": 2798.339,
        "duration": 4.861,
        "text": "involved so documentation is a very big"
      },
      {
        "start": 2800.38,
        "duration": 4.699,
        "text": "big part of our approach"
      },
      {
        "start": 2803.2,
        "duration": 4.2,
        "text": "but the best part about automation"
      },
      {
        "start": 2805.079,
        "duration": 3.821,
        "text": "especially with a tool like airflow is"
      },
      {
        "start": 2807.4,
        "duration": 2.34,
        "text": "the documentation can be in the"
      },
      {
        "start": 2808.9,
        "duration": 3.959,
        "text": "automation"
      },
      {
        "start": 2809.74,
        "duration": 7.46,
        "text": "the way something runs and how well it"
      },
      {
        "start": 2812.859,
        "duration": 4.341,
        "text": "runs can be integrated in there right"
      },
      {
        "start": 2817.66,
        "duration": 4.439,
        "text": "so when you put it all together we talk"
      },
      {
        "start": 2819.28,
        "duration": 3.9,
        "text": "about a bunch of stuff uh airflow plus"
      },
      {
        "start": 2822.099,
        "duration": 3.181,
        "text": "Spark"
      },
      {
        "start": 2823.18,
        "duration": 3.36,
        "text": "is one combination but spark has"
      },
      {
        "start": 2825.28,
        "duration": 3.839,
        "text": "different languages and we can get into"
      },
      {
        "start": 2826.54,
        "duration": 5.88,
        "text": "that so you can do a spark program in"
      },
      {
        "start": 2829.119,
        "duration": 5.601,
        "text": "Python and Scala and Java and R even C"
      },
      {
        "start": 2832.42,
        "duration": 5.22,
        "text": "sharp if you want in Conklin if you want"
      },
      {
        "start": 2834.72,
        "duration": 7.06,
        "text": "all of that together"
      },
      {
        "start": 2837.64,
        "duration": 7.439,
        "text": "around uh put uh sorry combined into a"
      },
      {
        "start": 2841.78,
        "duration": 5.579,
        "text": "airflow dag it makes a data operation"
      },
      {
        "start": 2845.079,
        "duration": 4.321,
        "text": "system for you for Cassandra and it's"
      },
      {
        "start": 2847.359,
        "duration": 3.601,
        "text": "good enough uh for a lot of the things"
      },
      {
        "start": 2849.4,
        "duration": 4.08,
        "text": "that you do right now with the Cron job"
      },
      {
        "start": 2850.96,
        "duration": 4.08,
        "text": "you can replace it with airflow and you"
      },
      {
        "start": 2853.48,
        "duration": 3.3,
        "text": "can do a lot more in fact you know we'll"
      },
      {
        "start": 2855.04,
        "duration": 4.02,
        "text": "show some automated systems that we've"
      },
      {
        "start": 2856.78,
        "duration": 4.92,
        "text": "designed to allow"
      },
      {
        "start": 2859.06,
        "duration": 4.74,
        "text": "companies to to make it self-service so"
      },
      {
        "start": 2861.7,
        "duration": 3.84,
        "text": "like I need this data deleted from"
      },
      {
        "start": 2863.8,
        "duration": 3.84,
        "text": "production well you want to have some"
      },
      {
        "start": 2865.54,
        "duration": 4.14,
        "text": "approvals in place but it allows people"
      },
      {
        "start": 2867.64,
        "duration": 4.199,
        "text": "to do this in a semi-automated fashion"
      },
      {
        "start": 2869.68,
        "duration": 4.5,
        "text": "so you can put more intelligence and"
      },
      {
        "start": 2871.839,
        "duration": 4.381,
        "text": "organization around your airflow dags if"
      },
      {
        "start": 2874.18,
        "duration": 4.159,
        "text": "you want so uh or you can do the simple"
      },
      {
        "start": 2876.22,
        "duration": 6.379,
        "text": "things"
      },
      {
        "start": 2878.339,
        "duration": 4.26,
        "text": "do you want to spell it out"
      },
      {
        "start": 2883.9,
        "duration": 3.6,
        "text": "yep"
      },
      {
        "start": 2885.04,
        "duration": 4.2,
        "text": "yes yes exactly and I'll show a picture"
      },
      {
        "start": 2887.5,
        "duration": 2.88,
        "text": "of what a dag is so"
      },
      {
        "start": 2889.24,
        "duration": 5.099,
        "text": "um"
      },
      {
        "start": 2890.38,
        "duration": 6.719,
        "text": "a dag is a shorthand for it's an acronym"
      },
      {
        "start": 2894.339,
        "duration": 6.961,
        "text": "for directed acyclic graph"
      },
      {
        "start": 2897.099,
        "duration": 6.421,
        "text": "yep and and dag basically uh the way I"
      },
      {
        "start": 2901.3,
        "duration": 5.34,
        "text": "describe it is it's directed meaning it"
      },
      {
        "start": 2903.52,
        "duration": 4.559,
        "text": "goes one way it's it and you can"
      },
      {
        "start": 2906.64,
        "duration": 4.02,
        "text": "um directly meaning there's a direction"
      },
      {
        "start": 2908.079,
        "duration": 4.861,
        "text": "to it right it's not and then acyclic"
      },
      {
        "start": 2910.66,
        "duration": 4.74,
        "text": "means it does not infinitely keep going"
      },
      {
        "start": 2912.94,
        "duration": 4.379,
        "text": "uh and then it's a graph meaning it's a"
      },
      {
        "start": 2915.4,
        "duration": 4.439,
        "text": "it's a collection of things that go in"
      },
      {
        "start": 2917.319,
        "duration": 5.641,
        "text": "One Direction so why is that important"
      },
      {
        "start": 2919.839,
        "duration": 6.861,
        "text": "well we use dags in airflow we also use"
      },
      {
        "start": 2922.96,
        "duration": 7.2,
        "text": "dags in spark it's the way to farm out"
      },
      {
        "start": 2926.7,
        "duration": 5.139,
        "text": "complex workflows and tasks and execute"
      },
      {
        "start": 2930.16,
        "duration": 3.72,
        "text": "them at scale that's the only way to do"
      },
      {
        "start": 2931.839,
        "duration": 4.561,
        "text": "it is to plan out the work so a dag is"
      },
      {
        "start": 2933.88,
        "duration": 4.439,
        "text": "kind of a plan of work a plan of tasks"
      },
      {
        "start": 2936.4,
        "duration": 3.0,
        "text": "uh and when you see the visual you'll"
      },
      {
        "start": 2938.319,
        "duration": 3.961,
        "text": "understand what I'm talking about and"
      },
      {
        "start": 2939.4,
        "duration": 4.5,
        "text": "when we get into the Hands-On you'll see"
      },
      {
        "start": 2942.28,
        "duration": 3.66,
        "text": "that as well you'll see what the dag"
      },
      {
        "start": 2943.9,
        "duration": 5.58,
        "text": "looks like"
      },
      {
        "start": 2945.94,
        "duration": 5.94,
        "text": "so what is here uh it's it's a scheduler"
      },
      {
        "start": 2949.48,
        "duration": 4.379,
        "text": "it's used for scheduling and automating"
      },
      {
        "start": 2951.88,
        "duration": 3.36,
        "text": "workflows and tasks so I want to make"
      },
      {
        "start": 2953.859,
        "duration": 3.781,
        "text": "sure that you understand that there's a"
      },
      {
        "start": 2955.24,
        "duration": 3.3,
        "text": "difference between automating"
      },
      {
        "start": 2957.64,
        "duration": 3.84,
        "text": "ETL"
      },
      {
        "start": 2958.54,
        "duration": 5.64,
        "text": "and doing ETL so that's why we have a"
      },
      {
        "start": 2961.48,
        "duration": 5.639,
        "text": "separation in our code there's some code"
      },
      {
        "start": 2964.18,
        "duration": 5.159,
        "text": "that is doing the ETL in Python"
      },
      {
        "start": 2967.119,
        "duration": 5.401,
        "text": "but the python code we have in airflow"
      },
      {
        "start": 2969.339,
        "duration": 4.621,
        "text": "it coordinates that task it it takes"
      },
      {
        "start": 2972.52,
        "duration": 5.76,
        "text": "care"
      },
      {
        "start": 2973.96,
        "duration": 7.32,
        "text": "it takes care of basically sequencing"
      },
      {
        "start": 2978.28,
        "duration": 6.299,
        "text": "the work in a logical way right"
      },
      {
        "start": 2981.28,
        "duration": 5.4,
        "text": "um You can use airflow to do a lot of"
      },
      {
        "start": 2984.579,
        "duration": 5.221,
        "text": "different things you can use it to to"
      },
      {
        "start": 2986.68,
        "duration": 4.7,
        "text": "manage a pipeline of of ETL tasks or"
      },
      {
        "start": 2989.8,
        "duration": 4.92,
        "text": "things that are dependent so for example"
      },
      {
        "start": 2991.38,
        "duration": 7.239,
        "text": "three data sets come in from external"
      },
      {
        "start": 2994.72,
        "duration": 6.18,
        "text": "sources into S3"
      },
      {
        "start": 2998.619,
        "duration": 4.921,
        "text": "you're collecting it via one type of"
      },
      {
        "start": 3000.9,
        "duration": 5.76,
        "text": "task right then those three data sets"
      },
      {
        "start": 3003.54,
        "duration": 5.1,
        "text": "get combined into another data set now"
      },
      {
        "start": 3006.66,
        "duration": 3.72,
        "text": "you can do that yourself you can have"
      },
      {
        "start": 3008.64,
        "duration": 3.179,
        "text": "front jobs that run for those three"
      },
      {
        "start": 3010.38,
        "duration": 4.08,
        "text": "earlier and then you can have another"
      },
      {
        "start": 3011.819,
        "duration": 5.341,
        "text": "one that runs like every six hours but"
      },
      {
        "start": 3014.46,
        "duration": 7.44,
        "text": "with airflow there's intelligence behind"
      },
      {
        "start": 3017.16,
        "duration": 8.1,
        "text": "it it won't run the job that's dependent"
      },
      {
        "start": 3021.9,
        "duration": 5.82,
        "text": "um unless and until all of the uh"
      },
      {
        "start": 3025.26,
        "duration": 4.02,
        "text": "previous tasks are done and you can go"
      },
      {
        "start": 3027.72,
        "duration": 3.48,
        "text": "all out with that you can do simple"
      },
      {
        "start": 3029.28,
        "duration": 4.02,
        "text": "workflows like we're going to do or you"
      },
      {
        "start": 3031.2,
        "duration": 5.159,
        "text": "can do complex ones"
      },
      {
        "start": 3033.3,
        "duration": 6.6,
        "text": "uh what I like about uh uh and you'll"
      },
      {
        "start": 3036.359,
        "duration": 6.48,
        "text": "see that GUI is that you can choose for"
      },
      {
        "start": 3039.9,
        "duration": 5.1,
        "text": "tasks to be run one time uh at a"
      },
      {
        "start": 3042.839,
        "duration": 4.441,
        "text": "particular date in the future with a"
      },
      {
        "start": 3045.0,
        "duration": 4.38,
        "text": "certain business logic like every Sunday"
      },
      {
        "start": 3047.28,
        "duration": 4.079,
        "text": "except on a holiday or stuff like that"
      },
      {
        "start": 3049.38,
        "duration": 4.56,
        "text": "or you can do a simple crime just to"
      },
      {
        "start": 3051.359,
        "duration": 5.041,
        "text": "cron to run schedule tasks on your"
      },
      {
        "start": 3053.94,
        "duration": 5.7,
        "text": "cluster to do certain things like ETL or"
      },
      {
        "start": 3056.4,
        "duration": 5.88,
        "text": "export uh and and you know Archive of"
      },
      {
        "start": 3059.64,
        "duration": 3.84,
        "text": "information you can do that same syntax"
      },
      {
        "start": 3062.28,
        "duration": 1.92,
        "text": "with with"
      },
      {
        "start": 3063.48,
        "duration": 2.82,
        "text": "um"
      },
      {
        "start": 3064.2,
        "duration": 4.02,
        "text": "uh with airflow or you can say things"
      },
      {
        "start": 3066.3,
        "duration": 3.6,
        "text": "like every day"
      },
      {
        "start": 3068.22,
        "duration": 3.06,
        "text": "right where you can say every month at"
      },
      {
        "start": 3069.9,
        "duration": 5.699,
        "text": "this time"
      },
      {
        "start": 3071.28,
        "duration": 7.2,
        "text": "um and all of this is managed in this"
      },
      {
        "start": 3075.599,
        "duration": 4.98,
        "text": "GUI so that if there are errors if there"
      },
      {
        "start": 3078.48,
        "duration": 3.359,
        "text": "are things that are taking longer you"
      },
      {
        "start": 3080.579,
        "duration": 3.48,
        "text": "can see it all in one place and we're"
      },
      {
        "start": 3081.839,
        "duration": 4.561,
        "text": "going to explore the UI uh when we get"
      },
      {
        "start": 3084.059,
        "duration": 5.641,
        "text": "started but this is kind of like the top"
      },
      {
        "start": 3086.4,
        "duration": 4.679,
        "text": "level of airflow remember it is not an"
      },
      {
        "start": 3089.7,
        "duration": 3.72,
        "text": "ETL tool"
      },
      {
        "start": 3091.079,
        "duration": 4.98,
        "text": "okay it is a tool that helps you"
      },
      {
        "start": 3093.42,
        "duration": 5.0,
        "text": "coordinate ETL processes and it can do a"
      },
      {
        "start": 3096.059,
        "duration": 6.06,
        "text": "lot more"
      },
      {
        "start": 3098.42,
        "duration": 6.88,
        "text": "architecture uh is very simple"
      },
      {
        "start": 3102.119,
        "duration": 5.881,
        "text": "um there is a database uh that it stores"
      },
      {
        "start": 3105.3,
        "duration": 4.5,
        "text": "information in uh technically you can"
      },
      {
        "start": 3108.0,
        "duration": 4.92,
        "text": "use you know any database but postgres"
      },
      {
        "start": 3109.8,
        "duration": 6.84,
        "text": "is the is a recommendation"
      },
      {
        "start": 3112.92,
        "duration": 6.96,
        "text": "um we have a web server that people can"
      },
      {
        "start": 3116.64,
        "duration": 5.699,
        "text": "use uh to see what the jobs are how long"
      },
      {
        "start": 3119.88,
        "duration": 5.179,
        "text": "they're running the web server also has"
      },
      {
        "start": 3122.339,
        "duration": 6.361,
        "text": "an API so you can actually"
      },
      {
        "start": 3125.059,
        "duration": 5.401,
        "text": "automate the automations in airflow if"
      },
      {
        "start": 3128.7,
        "duration": 5.58,
        "text": "you wanted to"
      },
      {
        "start": 3130.46,
        "duration": 4.96,
        "text": "and there's a scheduler that runs in the"
      },
      {
        "start": 3134.28,
        "duration": 3.42,
        "text": "background and there are different"
      },
      {
        "start": 3135.42,
        "duration": 4.8,
        "text": "executors we're going to use an Executor"
      },
      {
        "start": 3137.7,
        "duration": 4.44,
        "text": "that just does one computer but if you"
      },
      {
        "start": 3140.22,
        "duration": 4.26,
        "text": "use one of the other executors like the"
      },
      {
        "start": 3142.14,
        "duration": 5.64,
        "text": "kubernetes executor or the salary"
      },
      {
        "start": 3144.48,
        "duration": 5.339,
        "text": "executor it is possible to have many"
      },
      {
        "start": 3147.78,
        "duration": 5.46,
        "text": "many many workers on many many many"
      },
      {
        "start": 3149.819,
        "duration": 6.78,
        "text": "computers doing this for you so airflow"
      },
      {
        "start": 3153.24,
        "duration": 5.4,
        "text": "itself is scalable it's gonna be working"
      },
      {
        "start": 3156.599,
        "duration": 3.601,
        "text": "against Big Data against things like"
      },
      {
        "start": 3158.64,
        "duration": 4.02,
        "text": "spark which is already a distributed"
      },
      {
        "start": 3160.2,
        "duration": 4.379,
        "text": "system but airflow itself can scale and"
      },
      {
        "start": 3162.66,
        "duration": 3.84,
        "text": "you can have hundreds of jobs running on"
      },
      {
        "start": 3164.579,
        "duration": 3.961,
        "text": "airflow across many many computers"
      },
      {
        "start": 3166.5,
        "duration": 4.5,
        "text": "without issue"
      },
      {
        "start": 3168.54,
        "duration": 4.799,
        "text": "and how does it do that"
      },
      {
        "start": 3171.0,
        "duration": 5.579,
        "text": "um what is the devops of airflow well"
      },
      {
        "start": 3173.339,
        "duration": 5.881,
        "text": "your code which is a python file if it"
      },
      {
        "start": 3176.579,
        "duration": 4.02,
        "text": "lands in the right directory airflow"
      },
      {
        "start": 3179.22,
        "duration": 2.82,
        "text": "schedule will pick it up and it will"
      },
      {
        "start": 3180.599,
        "duration": 2.341,
        "text": "just show you hey I've got a new dag"
      },
      {
        "start": 3182.04,
        "duration": 2.34,
        "text": "what do you want to do with it you want"
      },
      {
        "start": 3182.94,
        "duration": 3.24,
        "text": "to unpause it"
      },
      {
        "start": 3184.38,
        "duration": 3.479,
        "text": "uh and it comes with a bunch of like"
      },
      {
        "start": 3186.18,
        "duration": 4.379,
        "text": "examples"
      },
      {
        "start": 3187.859,
        "duration": 4.5,
        "text": "um so you know we'll see the example"
      },
      {
        "start": 3190.559,
        "duration": 4.081,
        "text": "ones and we'll use hours for the"
      },
      {
        "start": 3192.359,
        "duration": 6.48,
        "text": "Hands-On"
      },
      {
        "start": 3194.64,
        "duration": 6.0,
        "text": "here's a quick snapshot of adapt so for"
      },
      {
        "start": 3198.839,
        "duration": 4.141,
        "text": "example uh you know you want to you know"
      },
      {
        "start": 3200.64,
        "duration": 4.86,
        "text": "analyze uh some information to you you"
      },
      {
        "start": 3202.98,
        "duration": 3.48,
        "text": "have an ingest process you analyze it to"
      },
      {
        "start": 3205.5,
        "duration": 2.94,
        "text": "make sure everything's good you check"
      },
      {
        "start": 3206.46,
        "duration": 4.26,
        "text": "the integrity and only after the"
      },
      {
        "start": 3208.44,
        "duration": 5.7,
        "text": "Integrity has been checked you can then"
      },
      {
        "start": 3210.72,
        "duration": 5.22,
        "text": "run a fork of tasks to then save the"
      },
      {
        "start": 3214.14,
        "duration": 4.919,
        "text": "data and report it and at the same time"
      },
      {
        "start": 3215.94,
        "duration": 5.58,
        "text": "you can say if there are errors found"
      },
      {
        "start": 3219.059,
        "duration": 4.081,
        "text": "right if and only if there are errors"
      },
      {
        "start": 3221.52,
        "duration": 3.48,
        "text": "filed go ahead and describe what"
      },
      {
        "start": 3223.14,
        "duration": 3.0,
        "text": "Integrity issues there were and email"
      },
      {
        "start": 3225.0,
        "duration": 3.599,
        "text": "that out"
      },
      {
        "start": 3226.14,
        "duration": 4.38,
        "text": "and the fact is that you can have so"
      },
      {
        "start": 3228.599,
        "duration": 3.48,
        "text": "many Forks in there you can get it"
      },
      {
        "start": 3230.52,
        "duration": 3.599,
        "text": "really complicated"
      },
      {
        "start": 3232.079,
        "duration": 4.381,
        "text": "um the benefit of having it as a dag is"
      },
      {
        "start": 3234.119,
        "duration": 3.96,
        "text": "you see these green lines around this"
      },
      {
        "start": 3236.46,
        "duration": 4.68,
        "text": "like boxes here"
      },
      {
        "start": 3238.079,
        "duration": 5.641,
        "text": "if a particular task in the whole dag is"
      },
      {
        "start": 3241.14,
        "duration": 4.439,
        "text": "slow or it's pending it'll have a"
      },
      {
        "start": 3243.72,
        "duration": 4.92,
        "text": "different color or if it failed it would"
      },
      {
        "start": 3245.579,
        "duration": 5.221,
        "text": "be red so in one quick dashboard you can"
      },
      {
        "start": 3248.64,
        "duration": 4.32,
        "text": "see your whole workflow whether"
      },
      {
        "start": 3250.8,
        "duration": 3.9,
        "text": "everything is green or if there's some"
      },
      {
        "start": 3252.96,
        "duration": 3.78,
        "text": "yellow or you know and then there's"
      },
      {
        "start": 3254.7,
        "duration": 5.52,
        "text": "actually about 12 or 15 different states"
      },
      {
        "start": 3256.74,
        "duration": 4.92,
        "text": "like pending skipped active"
      },
      {
        "start": 3260.22,
        "duration": 2.94,
        "text": "um which you know we have a pretty"
      },
      {
        "start": 3261.66,
        "duration": 3.899,
        "text": "simple Dax and we're not going to see"
      },
      {
        "start": 3263.16,
        "duration": 5.34,
        "text": "everything but uh you get to see"
      },
      {
        "start": 3265.559,
        "duration": 5.161,
        "text": "different views of your dad this is just"
      },
      {
        "start": 3268.5,
        "duration": 4.92,
        "text": "one view uh there are other views that"
      },
      {
        "start": 3270.72,
        "duration": 5.339,
        "text": "are that give you more intelligence"
      },
      {
        "start": 3273.42,
        "duration": 5.82,
        "text": "I'm gonna just turn off my one man it's"
      },
      {
        "start": 3276.059,
        "duration": 6.081,
        "text": "really responsible even if you don't"
      },
      {
        "start": 3279.24,
        "duration": 2.9,
        "text": "want to talk to it"
      },
      {
        "start": 3284.04,
        "duration": 4.819,
        "text": "the rise of the Machines it starts with"
      },
      {
        "start": 3286.38,
        "duration": 6.92,
        "text": "Apple Siri you know seriously"
      },
      {
        "start": 3288.859,
        "duration": 4.441,
        "text": "exactly it works to help everyone"
      },
      {
        "start": 3294.9,
        "duration": 4.8,
        "text": "um"
      },
      {
        "start": 3296.76,
        "duration": 5.579,
        "text": "yes exactly gpd3 will just like"
      },
      {
        "start": 3299.7,
        "duration": 5.22,
        "text": "completely do the talk for me uh here is"
      },
      {
        "start": 3302.339,
        "duration": 6.181,
        "text": "the main dashboard when you log into"
      },
      {
        "start": 3304.92,
        "duration": 6.06,
        "text": "airflow where you see all of the uh the"
      },
      {
        "start": 3308.52,
        "duration": 4.319,
        "text": "dags whether they're active or paused uh"
      },
      {
        "start": 3310.98,
        "duration": 3.119,
        "text": "you can like unpause it by clicking this"
      },
      {
        "start": 3312.839,
        "duration": 3.841,
        "text": "toggle"
      },
      {
        "start": 3314.099,
        "duration": 4.98,
        "text": "um these are basically showing you the"
      },
      {
        "start": 3316.68,
        "duration": 5.04,
        "text": "previous executions if it was good if it"
      },
      {
        "start": 3319.079,
        "duration": 5.101,
        "text": "was bad you can force a run you can"
      },
      {
        "start": 3321.72,
        "duration": 4.68,
        "text": "refresh that the file"
      },
      {
        "start": 3324.18,
        "duration": 3.78,
        "text": "um it's pretty uh intuitive uh they've"
      },
      {
        "start": 3326.4,
        "duration": 3.9,
        "text": "added more and more and more Tools in"
      },
      {
        "start": 3327.96,
        "duration": 4.74,
        "text": "here but I find it that once you get"
      },
      {
        "start": 3330.3,
        "duration": 4.019,
        "text": "introduced to the UI you you're pretty"
      },
      {
        "start": 3332.7,
        "duration": 4.74,
        "text": "much good to go you can just continue"
      },
      {
        "start": 3334.319,
        "duration": 6.621,
        "text": "exploring and learn what's wrong for"
      },
      {
        "start": 3337.44,
        "duration": 3.5,
        "text": "Apache airflow um"
      },
      {
        "start": 3343.74,
        "duration": 2.24,
        "text": "foreign"
      },
      {
        "start": 3347.059,
        "duration": 5.681,
        "text": "line tools to get it started there are"
      },
      {
        "start": 3350.7,
        "duration": 2.94,
        "text": "command line tools to execute certain"
      },
      {
        "start": 3352.74,
        "duration": 4.319,
        "text": "jobs"
      },
      {
        "start": 3353.64,
        "duration": 5.52,
        "text": "it also has an API so it's UI command"
      },
      {
        "start": 3357.059,
        "duration": 5.101,
        "text": "line and API so you can automate the"
      },
      {
        "start": 3359.16,
        "duration": 6.32,
        "text": "automation uh through any of those with"
      },
      {
        "start": 3362.16,
        "duration": 3.32,
        "text": "the command line or the API"
      },
      {
        "start": 3366.18,
        "duration": 4.139,
        "text": "hey man we're we're either building"
      },
      {
        "start": 3367.98,
        "duration": 5.42,
        "text": "robots or we're maintaining them right"
      },
      {
        "start": 3370.319,
        "duration": 3.081,
        "text": "like that's the future"
      },
      {
        "start": 3373.44,
        "duration": 3.3,
        "text": "um"
      },
      {
        "start": 3374.04,
        "duration": 5.819,
        "text": "and and I could talk about Apache spark"
      },
      {
        "start": 3376.74,
        "duration": 4.8,
        "text": "uh and go super deep into it but I'm"
      },
      {
        "start": 3379.859,
        "duration": 5.041,
        "text": "gonna leave that for another session"
      },
      {
        "start": 3381.54,
        "duration": 4.98,
        "text": "because spark is a it's a it's more than"
      },
      {
        "start": 3384.9,
        "duration": 3.84,
        "text": "a Swiss Army night it's like the Swiss"
      },
      {
        "start": 3386.52,
        "duration": 4.86,
        "text": "Army like it can do so many things for"
      },
      {
        "start": 3388.74,
        "duration": 4.319,
        "text": "you at a very high level"
      },
      {
        "start": 3391.38,
        "duration": 4.92,
        "text": "um but"
      },
      {
        "start": 3393.059,
        "duration": 4.861,
        "text": "basically Spar um is a catch-all for"
      },
      {
        "start": 3396.3,
        "duration": 4.38,
        "text": "when you need to do big data processing"
      },
      {
        "start": 3397.92,
        "duration": 4.86,
        "text": "it's kind of the first thing you go to"
      },
      {
        "start": 3400.68,
        "duration": 3.78,
        "text": "uh especially if you want to get data"
      },
      {
        "start": 3402.78,
        "duration": 4.62,
        "text": "into Cassandra or get data out of the"
      },
      {
        "start": 3404.46,
        "duration": 4.74,
        "text": "center very fast uh like lots and lots"
      },
      {
        "start": 3407.4,
        "duration": 3.6,
        "text": "of information uh it can also do"
      },
      {
        "start": 3409.2,
        "duration": 3.84,
        "text": "streaming we're not going to use the"
      },
      {
        "start": 3411.0,
        "duration": 5.04,
        "text": "streaming function today"
      },
      {
        "start": 3413.04,
        "duration": 6.66,
        "text": "um but Apache spark is a batch system"
      },
      {
        "start": 3416.04,
        "duration": 7.74,
        "text": "that is the next Generation after Hadoop"
      },
      {
        "start": 3419.7,
        "duration": 6.18,
        "text": "Hadoop had mapreduce and yarn uh spark"
      },
      {
        "start": 3423.78,
        "duration": 4.5,
        "text": "builds upon that and makes it easier to"
      },
      {
        "start": 3425.88,
        "duration": 5.459,
        "text": "do big data so these days even if you're"
      },
      {
        "start": 3428.28,
        "duration": 6.24,
        "text": "using the whole Hadoop ecosystem you're"
      },
      {
        "start": 3431.339,
        "duration": 5.28,
        "text": "likely using spark on Hadoop in our case"
      },
      {
        "start": 3434.52,
        "duration": 4.559,
        "text": "we're just gonna have a local spark uh"
      },
      {
        "start": 3436.619,
        "duration": 5.281,
        "text": "spark as I mentioned also has a dag"
      },
      {
        "start": 3439.079,
        "duration": 4.98,
        "text": "it has a directed acyclic graph for the"
      },
      {
        "start": 3441.9,
        "duration": 4.199,
        "text": "tasks it's running uh which are more"
      },
      {
        "start": 3444.059,
        "duration": 4.621,
        "text": "fine-grained right like if you have a"
      },
      {
        "start": 3446.099,
        "duration": 4.5,
        "text": "map operation that needs to run on"
      },
      {
        "start": 3448.68,
        "duration": 4.2,
        "text": "um you know 20 different partitions"
      },
      {
        "start": 3450.599,
        "duration": 4.861,
        "text": "across 20 different servers uh"
      },
      {
        "start": 3452.88,
        "duration": 4.02,
        "text": "internally spark will make a dag and"
      },
      {
        "start": 3455.46,
        "duration": 3.48,
        "text": "it'll keep track of that work and it'll"
      },
      {
        "start": 3456.9,
        "duration": 3.959,
        "text": "execute it most people don't get into"
      },
      {
        "start": 3458.94,
        "duration": 4.919,
        "text": "that level of detail but I just wanted"
      },
      {
        "start": 3460.859,
        "duration": 4.5,
        "text": "to clarify since airflow has a dag and"
      },
      {
        "start": 3463.859,
        "duration": 3.361,
        "text": "Spark is a dag and I wanted to make sure"
      },
      {
        "start": 3465.359,
        "duration": 4.381,
        "text": "that you guys knew the difference"
      },
      {
        "start": 3467.22,
        "duration": 4.56,
        "text": "um like airflow there are lots of"
      },
      {
        "start": 3469.74,
        "duration": 4.319,
        "text": "different uh connections to outside"
      },
      {
        "start": 3471.78,
        "duration": 3.48,
        "text": "systems so airflow can connect to"
      },
      {
        "start": 3474.059,
        "duration": 3.06,
        "text": "different databases and different"
      },
      {
        "start": 3475.26,
        "duration": 3.72,
        "text": "systems to do operations"
      },
      {
        "start": 3477.119,
        "duration": 3.361,
        "text": "spark can basically connect to"
      },
      {
        "start": 3478.98,
        "duration": 2.94,
        "text": "everything out there there's always a"
      },
      {
        "start": 3480.48,
        "duration": 3.359,
        "text": "like spark connector for Apache"
      },
      {
        "start": 3481.92,
        "duration": 3.6,
        "text": "Cassandra allows you to connect your"
      },
      {
        "start": 3483.839,
        "duration": 3.48,
        "text": "Cassandra from spark but there are"
      },
      {
        "start": 3485.52,
        "duration": 5.039,
        "text": "connectors for Kafka there's connectors"
      },
      {
        "start": 3487.319,
        "duration": 5.581,
        "text": "for Hive or or data Lakes built on"
      },
      {
        "start": 3490.559,
        "duration": 5.341,
        "text": "parquet you name it there's a connector"
      },
      {
        "start": 3492.9,
        "duration": 4.919,
        "text": "for it lots of libraries there's other"
      },
      {
        "start": 3495.9,
        "duration": 4.14,
        "text": "tools inside spark which again we're not"
      },
      {
        "start": 3497.819,
        "duration": 3.721,
        "text": "going to get to but it has a graph"
      },
      {
        "start": 3500.04,
        "duration": 2.94,
        "text": "processing Library it's not a graph"
      },
      {
        "start": 3501.54,
        "duration": 3.42,
        "text": "database it's a graphic processing"
      },
      {
        "start": 3502.98,
        "duration": 5.22,
        "text": "Library it has a main learning library"
      },
      {
        "start": 3504.96,
        "duration": 5.7,
        "text": "it has full-fledged data frames to do"
      },
      {
        "start": 3508.2,
        "duration": 3.899,
        "text": "operations in memory and then either you"
      },
      {
        "start": 3510.66,
        "duration": 3.72,
        "text": "know send them back to the source system"
      },
      {
        "start": 3512.099,
        "duration": 3.841,
        "text": "or to send them to new systems"
      },
      {
        "start": 3514.38,
        "duration": 4.8,
        "text": "um basically you can anything with big"
      },
      {
        "start": 3515.94,
        "duration": 4.74,
        "text": "data you can do it"
      },
      {
        "start": 3519.18,
        "duration": 3.48,
        "text": "um basically"
      },
      {
        "start": 3520.68,
        "duration": 5.22,
        "text": "um you know Marcus has a question"
      },
      {
        "start": 3522.66,
        "duration": 5.22,
        "text": "um and he was like are these diagrams in"
      },
      {
        "start": 3525.9,
        "duration": 3.419,
        "text": "GitHub and uh yeah I know that we're"
      },
      {
        "start": 3527.88,
        "duration": 4.32,
        "text": "gonna add the slides we were just"
      },
      {
        "start": 3529.319,
        "duration": 5.641,
        "text": "waiting for it to be completely up to"
      },
      {
        "start": 3532.2,
        "duration": 4.2,
        "text": "date and and you know actually Rahul is"
      },
      {
        "start": 3534.96,
        "duration": 2.879,
        "text": "looking for some feedback if you have"
      },
      {
        "start": 3536.4,
        "duration": 3.3,
        "text": "any feedback and there's just put in"
      },
      {
        "start": 3537.839,
        "duration": 6.26,
        "text": "there he'll update the slides and put it"
      },
      {
        "start": 3539.7,
        "duration": 4.399,
        "text": "back in the same repository correct yep"
      },
      {
        "start": 3545.52,
        "duration": 6.24,
        "text": "yep exactly and and um the a lot of the"
      },
      {
        "start": 3549.299,
        "duration": 5.101,
        "text": "stuff that I use I don't necessarily"
      },
      {
        "start": 3551.76,
        "duration": 4.2,
        "text": "recreate there's our public resources so"
      },
      {
        "start": 3554.4,
        "duration": 2.939,
        "text": "um like for example the architecture"
      },
      {
        "start": 3555.96,
        "duration": 3.359,
        "text": "diagram that I was showing you that's"
      },
      {
        "start": 3557.339,
        "duration": 4.081,
        "text": "straight out of the airflow"
      },
      {
        "start": 3559.319,
        "duration": 4.26,
        "text": "documentation"
      },
      {
        "start": 3561.42,
        "duration": 5.28,
        "text": "um and I'll actually show a few more but"
      },
      {
        "start": 3563.579,
        "duration": 5.28,
        "text": "yeah this deck will be put onto"
      },
      {
        "start": 3566.7,
        "duration": 3.899,
        "text": "um again you'll have access to it and"
      },
      {
        "start": 3568.859,
        "duration": 3.421,
        "text": "the data Stacks team will make sure you"
      },
      {
        "start": 3570.599,
        "duration": 3.0,
        "text": "get the right links"
      },
      {
        "start": 3572.28,
        "duration": 2.94,
        "text": "okay"
      },
      {
        "start": 3573.599,
        "duration": 4.081,
        "text": "thanks"
      },
      {
        "start": 3575.22,
        "duration": 4.32,
        "text": "um yeah sorry I have the chat here but"
      },
      {
        "start": 3577.68,
        "duration": 3.179,
        "text": "I'm so focused on the talk that I'm not"
      },
      {
        "start": 3579.54,
        "duration": 4.519,
        "text": "getting it thank you for bringing that"
      },
      {
        "start": 3580.859,
        "duration": 3.2,
        "text": "up all right I appreciate it"
      },
      {
        "start": 3585.72,
        "duration": 7.02,
        "text": "um so what I'm gonna do is to to scan"
      },
      {
        "start": 3589.5,
        "duration": 6.14,
        "text": "over when I say it's a bonus round"
      },
      {
        "start": 3592.74,
        "duration": 6.96,
        "text": "um what I'm doing here is explaining a"
      },
      {
        "start": 3595.64,
        "duration": 6.64,
        "text": "advanced use of of airflow in in a way"
      },
      {
        "start": 3599.7,
        "duration": 3.78,
        "text": "that I think a level can get their um"
      },
      {
        "start": 3602.28,
        "duration": 2.88,
        "text": "their mind around because there's a"
      },
      {
        "start": 3603.48,
        "duration": 3.9,
        "text": "challenge that I see all the time with"
      },
      {
        "start": 3605.16,
        "duration": 4.38,
        "text": "with people that are using Cassandra uh"
      },
      {
        "start": 3607.38,
        "duration": 5.64,
        "text": "which is they need to do data hygiene"
      },
      {
        "start": 3609.54,
        "duration": 5.759,
        "text": "which is beyond TTL and when you have"
      },
      {
        "start": 3613.02,
        "duration": 4.88,
        "text": "billions of Records across terabytes of"
      },
      {
        "start": 3615.299,
        "duration": 5.28,
        "text": "data in Cassandra you have to basically"
      },
      {
        "start": 3617.9,
        "duration": 4.6,
        "text": "uh iterate and if you don't have the"
      },
      {
        "start": 3620.579,
        "duration": 3.601,
        "text": "right indexes even then even if you have"
      },
      {
        "start": 3622.5,
        "duration": 4.98,
        "text": "the right indexes uh you have to"
      },
      {
        "start": 3624.18,
        "duration": 5.119,
        "text": "basically do a spark job to uh to do"
      },
      {
        "start": 3627.48,
        "duration": 4.379,
        "text": "these Advanced things like deletion or"
      },
      {
        "start": 3629.299,
        "duration": 4.54,
        "text": "manipulation of information right let's"
      },
      {
        "start": 3631.859,
        "duration": 3.781,
        "text": "say you found an error in your logic"
      },
      {
        "start": 3633.839,
        "duration": 3.0,
        "text": "hopefully you don't but let's say you"
      },
      {
        "start": 3635.64,
        "duration": 3.6,
        "text": "found an error in your logic you need to"
      },
      {
        "start": 3636.839,
        "duration": 4.381,
        "text": "update data across billions of rows uh"
      },
      {
        "start": 3639.24,
        "duration": 4.26,
        "text": "spark would be the way you do it"
      },
      {
        "start": 3641.22,
        "duration": 5.16,
        "text": "so"
      },
      {
        "start": 3643.5,
        "duration": 4.92,
        "text": "in Big Data there's lots of big data"
      },
      {
        "start": 3646.38,
        "duration": 4.199,
        "text": "options right you have your"
      },
      {
        "start": 3648.42,
        "duration": 6.419,
        "text": "um cold data stores and you have your"
      },
      {
        "start": 3650.579,
        "duration": 6.72,
        "text": "hot data stores and when I put star uh"
      },
      {
        "start": 3654.839,
        "duration": 5.22,
        "text": "on these items these all use cql so what"
      },
      {
        "start": 3657.299,
        "duration": 5.221,
        "text": "I'm showing you you can use with any cql"
      },
      {
        "start": 3660.059,
        "duration": 5.161,
        "text": "compliant system but the other thing is"
      },
      {
        "start": 3662.52,
        "duration": 6.299,
        "text": "that this approach with using ETL sorry"
      },
      {
        "start": 3665.22,
        "duration": 5.28,
        "text": "using spark with airflow to clean data"
      },
      {
        "start": 3668.819,
        "duration": 3.54,
        "text": "or to do some hygiene you can actually"
      },
      {
        "start": 3670.5,
        "duration": 4.619,
        "text": "take it and apply it to"
      },
      {
        "start": 3672.359,
        "duration": 5.041,
        "text": "any of your cold data stores too like a"
      },
      {
        "start": 3675.119,
        "duration": 3.841,
        "text": "data Lake right this is the same general"
      },
      {
        "start": 3677.4,
        "duration": 3.78,
        "text": "practice"
      },
      {
        "start": 3678.96,
        "duration": 4.32,
        "text": "um so it goes what to show you goes"
      },
      {
        "start": 3681.18,
        "duration": 3.36,
        "text": "beyond just Cassandra and cleaning data"
      },
      {
        "start": 3683.28,
        "duration": 2.819,
        "text": "and Cassandra it's it's actually any"
      },
      {
        "start": 3684.54,
        "duration": 4.019,
        "text": "system"
      },
      {
        "start": 3686.099,
        "duration": 5.281,
        "text": "um when you are"
      },
      {
        "start": 3688.559,
        "duration": 5.881,
        "text": "um you know doing uh Big Data generally"
      },
      {
        "start": 3691.38,
        "duration": 4.739,
        "text": "you try to clean your data before it"
      },
      {
        "start": 3694.44,
        "duration": 4.2,
        "text": "goes into your database right you remove"
      },
      {
        "start": 3696.119,
        "duration": 5.341,
        "text": "duplicates you you you you know you you"
      },
      {
        "start": 3698.64,
        "duration": 4.62,
        "text": "get rid of like extra information you"
      },
      {
        "start": 3701.46,
        "duration": 3.659,
        "text": "fix structural issues all this stuff you"
      },
      {
        "start": 3703.26,
        "duration": 4.44,
        "text": "do it as part of your data engineer"
      },
      {
        "start": 3705.119,
        "duration": 3.841,
        "text": "right your data rattling"
      },
      {
        "start": 3707.7,
        "duration": 3.139,
        "text": "um but"
      },
      {
        "start": 3708.96,
        "duration": 5.28,
        "text": "you may still even if you do everything"
      },
      {
        "start": 3710.839,
        "duration": 5.561,
        "text": "properly you may still have to"
      },
      {
        "start": 3714.24,
        "duration": 4.379,
        "text": "do something after the fact after it's"
      },
      {
        "start": 3716.4,
        "duration": 4.32,
        "text": "inside your your big data system like"
      },
      {
        "start": 3718.619,
        "duration": 4.321,
        "text": "Cassandra it could be that you have"
      },
      {
        "start": 3720.72,
        "duration": 5.339,
        "text": "revisions of information and you you"
      },
      {
        "start": 3722.94,
        "duration": 6.119,
        "text": "have a revision eviction policy that is"
      },
      {
        "start": 3726.059,
        "duration": 5.581,
        "text": "it is not simply oh let's put a TL on it"
      },
      {
        "start": 3729.059,
        "duration": 4.5,
        "text": "right or you need to move a customer's"
      },
      {
        "start": 3731.64,
        "duration": 3.06,
        "text": "information from one cluster to another"
      },
      {
        "start": 3733.559,
        "duration": 3.901,
        "text": "cluster"
      },
      {
        "start": 3734.7,
        "duration": 5.22,
        "text": "right it's a multi-tenant system"
      },
      {
        "start": 3737.46,
        "duration": 3.96,
        "text": "um you need to enforce gdpr the right to"
      },
      {
        "start": 3739.92,
        "duration": 4.5,
        "text": "be forgotten I want to delete all my"
      },
      {
        "start": 3741.42,
        "duration": 5.879,
        "text": "data from your systems right and so you"
      },
      {
        "start": 3744.42,
        "duration": 5.399,
        "text": "can't just at least not now you can't"
      },
      {
        "start": 3747.299,
        "duration": 4.921,
        "text": "just go to Cassandra cql sh and say"
      },
      {
        "start": 3749.819,
        "duration": 4.141,
        "text": "delete from this table where this is"
      },
      {
        "start": 3752.22,
        "duration": 4.139,
        "text": "equal to this and it'll magically do it"
      },
      {
        "start": 3753.96,
        "duration": 5.76,
        "text": "right it doesn't do that"
      },
      {
        "start": 3756.359,
        "duration": 5.7,
        "text": "um or you know update this thing to this"
      },
      {
        "start": 3759.72,
        "duration": 4.8,
        "text": "value and you have some like complex sub"
      },
      {
        "start": 3762.059,
        "duration": 4.8,
        "text": "query to to basically find out where to"
      },
      {
        "start": 3764.52,
        "duration": 4.079,
        "text": "update that information so so the things"
      },
      {
        "start": 3766.859,
        "duration": 3.121,
        "text": "that we were used to in the relational"
      },
      {
        "start": 3768.599,
        "duration": 4.141,
        "text": "world"
      },
      {
        "start": 3769.98,
        "duration": 4.079,
        "text": "we just can't take it for granted in"
      },
      {
        "start": 3772.74,
        "duration": 4.68,
        "text": "non-relational"
      },
      {
        "start": 3774.059,
        "duration": 6.121,
        "text": "and in SQL you would say hey delete"
      },
      {
        "start": 3777.42,
        "duration": 4.139,
        "text": "right here's my where clause and then in"
      },
      {
        "start": 3780.18,
        "duration": 3.3,
        "text": "your where Clause you have another sub"
      },
      {
        "start": 3781.559,
        "duration": 4.5,
        "text": "query and everything would just work"
      },
      {
        "start": 3783.48,
        "duration": 3.599,
        "text": "right you just do it even if it took 12"
      },
      {
        "start": 3786.059,
        "duration": 3.121,
        "text": "hours"
      },
      {
        "start": 3787.079,
        "duration": 5.901,
        "text": "I mean you have you have those things"
      },
      {
        "start": 3789.18,
        "duration": 3.8,
        "text": "right yeah it's a 12-hour job"
      },
      {
        "start": 3794.96,
        "duration": 4.96,
        "text": "you're right yeah at least you can have"
      },
      {
        "start": 3797.88,
        "duration": 3.06,
        "text": "a query which does that yeah but but"
      },
      {
        "start": 3799.92,
        "duration": 3.6,
        "text": "yeah"
      },
      {
        "start": 3800.94,
        "duration": 6.179,
        "text": "um you know you need to be a little"
      },
      {
        "start": 3803.52,
        "duration": 6.36,
        "text": "um a little bit more mindful in in you"
      },
      {
        "start": 3807.119,
        "duration": 4.68,
        "text": "know no SQL where performance matters"
      },
      {
        "start": 3809.88,
        "duration": 4.199,
        "text": "right yeah"
      },
      {
        "start": 3811.799,
        "duration": 4.5,
        "text": "yeah"
      },
      {
        "start": 3814.079,
        "duration": 4.141,
        "text": "but performance batters and it's it's"
      },
      {
        "start": 3816.299,
        "duration": 4.681,
        "text": "actually not as easy if you you mean"
      },
      {
        "start": 3818.22,
        "duration": 4.74,
        "text": "this this particular uh you know type of"
      },
      {
        "start": 3820.98,
        "duration": 3.96,
        "text": "delete operation correct it just won't"
      },
      {
        "start": 3822.96,
        "duration": 3.54,
        "text": "work it's syntactically it won't work in"
      },
      {
        "start": 3824.94,
        "duration": 5.34,
        "text": "cql right"
      },
      {
        "start": 3826.5,
        "duration": 6.359,
        "text": "um now in spark SQL"
      },
      {
        "start": 3830.28,
        "duration": 4.62,
        "text": "if you're connected to a data set"
      },
      {
        "start": 3832.859,
        "duration": 5.581,
        "text": "uh in spark"
      },
      {
        "start": 3834.9,
        "duration": 5.159,
        "text": "some data sets they support this type of"
      },
      {
        "start": 3838.44,
        "duration": 4.44,
        "text": "deletion"
      },
      {
        "start": 3840.059,
        "duration": 4.921,
        "text": "okay like for example in data breaks in"
      },
      {
        "start": 3842.88,
        "duration": 4.26,
        "text": "Apache spark inside databricks they"
      },
      {
        "start": 3844.98,
        "duration": 3.78,
        "text": "support this type of Syntax for Delta"
      },
      {
        "start": 3847.14,
        "duration": 3.719,
        "text": "Lake because you know data breaks made"
      },
      {
        "start": 3848.76,
        "duration": 3.839,
        "text": "Delta Lake and they made it right but"
      },
      {
        "start": 3850.859,
        "duration": 4.321,
        "text": "you can't rely on this type of syntax"
      },
      {
        "start": 3852.599,
        "duration": 6.24,
        "text": "even inside spark SQL"
      },
      {
        "start": 3855.18,
        "duration": 6.179,
        "text": "which is the ability to do SQL with"
      },
      {
        "start": 3858.839,
        "duration": 4.02,
        "text": "joins and Wares and all that you can do"
      },
      {
        "start": 3861.359,
        "duration": 3.48,
        "text": "that against Cassandra if you're using"
      },
      {
        "start": 3862.859,
        "duration": 4.081,
        "text": "spark right that's another whole another"
      },
      {
        "start": 3864.839,
        "duration": 3.72,
        "text": "talk but"
      },
      {
        "start": 3866.94,
        "duration": 3.3,
        "text": "um you can't rely on this delete"
      },
      {
        "start": 3868.559,
        "duration": 4.081,
        "text": "operation so what do you do then what do"
      },
      {
        "start": 3870.24,
        "duration": 4.02,
        "text": "you want to do deletes on tons and tons"
      },
      {
        "start": 3872.64,
        "duration": 3.06,
        "text": "of information how would you do about it"
      },
      {
        "start": 3874.26,
        "duration": 3.12,
        "text": "well"
      },
      {
        "start": 3875.7,
        "duration": 3.0,
        "text": "you can make a function"
      },
      {
        "start": 3877.38,
        "duration": 3.6,
        "text": "right"
      },
      {
        "start": 3878.7,
        "duration": 4.98,
        "text": "um and this is a Scala function which"
      },
      {
        "start": 3880.98,
        "duration": 5.16,
        "text": "gives it a particular where Clause it's"
      },
      {
        "start": 3883.68,
        "duration": 4.619,
        "text": "reusable and I'm going to bank on later"
      },
      {
        "start": 3886.14,
        "duration": 4.32,
        "text": "on because if you have a function that"
      },
      {
        "start": 3888.299,
        "duration": 3.661,
        "text": "predictably deletes data from a"
      },
      {
        "start": 3890.46,
        "duration": 3.54,
        "text": "particular table"
      },
      {
        "start": 3891.96,
        "duration": 3.659,
        "text": "that's one type of automation"
      },
      {
        "start": 3894.0,
        "duration": 3.299,
        "text": "but what if you want to give the ability"
      },
      {
        "start": 3895.619,
        "duration": 3.24,
        "text": "for other people to do this on a"
      },
      {
        "start": 3897.299,
        "duration": 2.941,
        "text": "recurring basis or to do this that's"
      },
      {
        "start": 3898.859,
        "duration": 2.94,
        "text": "where something like airflow comes in"
      },
      {
        "start": 3900.24,
        "duration": 5.16,
        "text": "right so"
      },
      {
        "start": 3901.799,
        "duration": 5.221,
        "text": "here you know what you want to delete"
      },
      {
        "start": 3905.4,
        "duration": 3.0,
        "text": "right where do you want to delete it and"
      },
      {
        "start": 3907.02,
        "duration": 4.74,
        "text": "you want to delete it you can kind of do"
      },
      {
        "start": 3908.4,
        "duration": 6.6,
        "text": "the same thing with with Scala I thought"
      },
      {
        "start": 3911.76,
        "duration": 5.4,
        "text": "um right now there's not Maybe I'm Wrong"
      },
      {
        "start": 3915.0,
        "duration": 4.859,
        "text": "uh data Stacks might have put it out but"
      },
      {
        "start": 3917.16,
        "duration": 5.1,
        "text": "there's not a way to do kind of scalable"
      },
      {
        "start": 3919.859,
        "duration": 5.161,
        "text": "deletes in Cassandra in this same way"
      },
      {
        "start": 3922.26,
        "duration": 4.559,
        "text": "you can do it like here uh with spark so"
      },
      {
        "start": 3925.02,
        "duration": 4.26,
        "text": "what we do is we basically say select"
      },
      {
        "start": 3926.819,
        "duration": 5.04,
        "text": "the data from this table"
      },
      {
        "start": 3929.28,
        "duration": 4.98,
        "text": "go through each partition in a"
      },
      {
        "start": 3931.859,
        "duration": 5.22,
        "text": "parallelized fashion go ahead and delete"
      },
      {
        "start": 3934.26,
        "duration": 5.579,
        "text": "it using cql and this is scalable"
      },
      {
        "start": 3937.079,
        "duration": 4.621,
        "text": "because spark will go gather all the"
      },
      {
        "start": 3939.839,
        "duration": 4.74,
        "text": "data that you want to filter from"
      },
      {
        "start": 3941.7,
        "duration": 6.24,
        "text": "get it for you and then it will run"
      },
      {
        "start": 3944.579,
        "duration": 5.76,
        "text": "these delete operations using cql and"
      },
      {
        "start": 3947.94,
        "duration": 3.96,
        "text": "token aware so you can do millions of"
      },
      {
        "start": 3950.339,
        "duration": 3.901,
        "text": "deletes like Cassandra cluster with"
      },
      {
        "start": 3951.9,
        "duration": 4.86,
        "text": "spark and it will be scalable"
      },
      {
        "start": 3954.24,
        "duration": 4.5,
        "text": "you can also use Scala and Spark to"
      },
      {
        "start": 3956.76,
        "duration": 4.559,
        "text": "dedupe information"
      },
      {
        "start": 3958.74,
        "duration": 6.0,
        "text": "this is a fairly complicated thing my"
      },
      {
        "start": 3961.319,
        "duration": 5.28,
        "text": "colleague Obi found something on step"
      },
      {
        "start": 3964.74,
        "duration": 4.5,
        "text": "back overflow that did it and we"
      },
      {
        "start": 3966.599,
        "duration": 4.5,
        "text": "operationalized it into this function"
      },
      {
        "start": 3969.24,
        "duration": 3.42,
        "text": "um and technically you can use it for"
      },
      {
        "start": 3971.099,
        "duration": 4.98,
        "text": "any type of deduping but this is like"
      },
      {
        "start": 3972.66,
        "duration": 6.12,
        "text": "for deduping documents in in casera this"
      },
      {
        "start": 3976.079,
        "duration": 5.161,
        "text": "is a common use case where you have"
      },
      {
        "start": 3978.78,
        "duration": 4.019,
        "text": "duplicate partitions duplicate you know"
      },
      {
        "start": 3981.24,
        "duration": 4.98,
        "text": "versions and you want to get rid of"
      },
      {
        "start": 3982.799,
        "duration": 4.921,
        "text": "anything uh that is not necessary this"
      },
      {
        "start": 3986.22,
        "duration": 3.599,
        "text": "is the type of thing that again it's"
      },
      {
        "start": 3987.72,
        "duration": 4.26,
        "text": "very hard in SQL"
      },
      {
        "start": 3989.819,
        "duration": 3.361,
        "text": "um and and this code is is open source"
      },
      {
        "start": 3991.98,
        "duration": 3.18,
        "text": "so you know we'll make sure it's already"
      },
      {
        "start": 3993.18,
        "duration": 3.899,
        "text": "linked um so you don't have to go"
      },
      {
        "start": 3995.16,
        "duration": 4.919,
        "text": "reinvent the wheel right so cleaning"
      },
      {
        "start": 3997.079,
        "duration": 4.621,
        "text": "data deleting it deduping data these are"
      },
      {
        "start": 4000.079,
        "duration": 4.921,
        "text": "common scenarios where we have to do it"
      },
      {
        "start": 4001.7,
        "duration": 4.8,
        "text": "over and over and over again and you"
      },
      {
        "start": 4005.0,
        "duration": 3.359,
        "text": "know we're not going to go tell a data"
      },
      {
        "start": 4006.5,
        "duration": 4.2,
        "text": "analyst like okay go log into the spark"
      },
      {
        "start": 4008.359,
        "duration": 4.381,
        "text": "cluster here's to the kingdom do your"
      },
      {
        "start": 4010.7,
        "duration": 3.599,
        "text": "damage right what we're going to do is"
      },
      {
        "start": 4012.74,
        "duration": 6.18,
        "text": "control that situation"
      },
      {
        "start": 4014.299,
        "duration": 7.441,
        "text": "in combining baked uh logic like this"
      },
      {
        "start": 4018.92,
        "duration": 4.199,
        "text": "with airflow allows you to control that"
      },
      {
        "start": 4021.74,
        "duration": 3.599,
        "text": "situation and to give people"
      },
      {
        "start": 4023.119,
        "duration": 3.0,
        "text": "self-serviceability"
      },
      {
        "start": 4025.339,
        "duration": 5.301,
        "text": "um"
      },
      {
        "start": 4026.119,
        "duration": 7.98,
        "text": "here is a complex dat which allows us to"
      },
      {
        "start": 4030.64,
        "duration": 6.58,
        "text": "move hundreds of tables across hundreds"
      },
      {
        "start": 4034.099,
        "duration": 7.02,
        "text": "of key spaces uh at scale right and the"
      },
      {
        "start": 4037.22,
        "duration": 5.76,
        "text": "way it does it is it actually is"
      },
      {
        "start": 4041.119,
        "duration": 5.781,
        "text": "um"
      },
      {
        "start": 4042.98,
        "duration": 3.92,
        "text": "keeping the metadata for which tables"
      },
      {
        "start": 4049.22,
        "duration": 5.119,
        "text": "and Rahul might have lost uh your audio"
      },
      {
        "start": 4056.599,
        "duration": 4.381,
        "text": "okay let me see if I can bring back"
      },
      {
        "start": 4058.76,
        "duration": 4.68,
        "text": "Rahul"
      },
      {
        "start": 4060.98,
        "duration": 4.879,
        "text": "please bear with me let me check what's"
      },
      {
        "start": 4063.44,
        "duration": 2.419,
        "text": "going on"
      },
      {
        "start": 4079.819,
        "duration": 3.661,
        "text": "I know he was having some issues with us"
      },
      {
        "start": 4082.22,
        "duration": 5.42,
        "text": "uh"
      },
      {
        "start": 4083.48,
        "duration": 4.16,
        "text": "network provider but uh"
      },
      {
        "start": 4092.0,
        "duration": 3.14,
        "text": "see give me a sec"
      },
      {
        "start": 4103.1,
        "duration": 4.4,
        "text": "in the meantime if you're having"
      },
      {
        "start": 4104.42,
        "duration": 3.08,
        "text": "problems let me know"
      },
      {
        "start": 4114.859,
        "duration": 3.601,
        "text": "we use Skype for an infrastructure so"
      },
      {
        "start": 4117.319,
        "duration": 4.4,
        "text": "maybe that's"
      },
      {
        "start": 4118.46,
        "duration": 3.259,
        "text": "what's going on here"
      },
      {
        "start": 4140.06,
        "duration": 5.46,
        "text": "so in the meantime if any of you are"
      },
      {
        "start": 4142.279,
        "duration": 4.92,
        "text": "having trouble with the setting up the"
      },
      {
        "start": 4145.52,
        "duration": 2.52,
        "text": "database in gitpart you know please let"
      },
      {
        "start": 4147.199,
        "duration": 1.62,
        "text": "me know"
      },
      {
        "start": 4148.04,
        "duration": 2.819,
        "text": "um"
      },
      {
        "start": 4148.819,
        "duration": 5.781,
        "text": "while I try to reach out to Rahul and"
      },
      {
        "start": 4150.859,
        "duration": 3.741,
        "text": "see if his if he can get him back"
      },
      {
        "start": 4155.779,
        "duration": 5.52,
        "text": "okay he said he just got disconnected"
      },
      {
        "start": 4158.239,
        "duration": 6.261,
        "text": "and he's gonna go to the backup line"
      },
      {
        "start": 4161.299,
        "duration": 3.201,
        "text": "I'm glad he could do that"
      },
      {
        "start": 4167.0,
        "duration": 3.839,
        "text": "if you have any questions you know feel"
      },
      {
        "start": 4168.739,
        "duration": 5.181,
        "text": "free to put it in the chat while we wait"
      },
      {
        "start": 4170.839,
        "duration": 3.081,
        "text": "for Rahul to come back"
      },
      {
        "start": 4176.66,
        "duration": 6.599,
        "text": "hey"
      },
      {
        "start": 4178.94,
        "duration": 6.2,
        "text": "you're back that's it that's a wireless"
      },
      {
        "start": 4183.259,
        "duration": 6.181,
        "text": "broadband connection"
      },
      {
        "start": 4185.14,
        "duration": 5.679,
        "text": "I think you're live already so uh if you"
      },
      {
        "start": 4189.44,
        "duration": 4.259,
        "text": "want to start sharing your presentation"
      },
      {
        "start": 4190.819,
        "duration": 4.981,
        "text": "then yeah or otherwise I can bring it up"
      },
      {
        "start": 4193.699,
        "duration": 3.48,
        "text": "I got it yeah thank you I appreciate"
      },
      {
        "start": 4195.8,
        "duration": 3.72,
        "text": "that noise"
      },
      {
        "start": 4197.179,
        "duration": 6.181,
        "text": "um sorry about that folks uh"
      },
      {
        "start": 4199.52,
        "duration": 6.12,
        "text": "I've been having issues with our primary"
      },
      {
        "start": 4203.36,
        "duration": 4.4,
        "text": "internet provider and I'm not gonna"
      },
      {
        "start": 4205.64,
        "duration": 5.039,
        "text": "gossip about which ones are good but"
      },
      {
        "start": 4207.76,
        "duration": 4.36,
        "text": "it's a different conversation"
      },
      {
        "start": 4210.679,
        "duration": 3.0,
        "text": "um but anyways what I wanted to finish"
      },
      {
        "start": 4212.12,
        "duration": 3.72,
        "text": "this thought is"
      },
      {
        "start": 4213.679,
        "duration": 3.54,
        "text": "that look you can use airflow for"
      },
      {
        "start": 4215.84,
        "duration": 3.78,
        "text": "settings you can use or complex things"
      },
      {
        "start": 4217.219,
        "duration": 4.5,
        "text": "it's just how you put it all together"
      },
      {
        "start": 4219.62,
        "duration": 5.099,
        "text": "um and you know when we do the Hands-On"
      },
      {
        "start": 4221.719,
        "duration": 4.921,
        "text": "we're doing a very simple operation but"
      },
      {
        "start": 4224.719,
        "duration": 4.201,
        "text": "if you can just imagine and think about"
      },
      {
        "start": 4226.64,
        "duration": 3.96,
        "text": "the complex processes that you have in"
      },
      {
        "start": 4228.92,
        "duration": 3.06,
        "text": "your organization to get data in get"
      },
      {
        "start": 4230.6,
        "duration": 3.119,
        "text": "data out"
      },
      {
        "start": 4231.98,
        "duration": 3.84,
        "text": "um any recurring processes that you do"
      },
      {
        "start": 4233.719,
        "duration": 3.241,
        "text": "on Big Data that's where uh we can we"
      },
      {
        "start": 4235.82,
        "duration": 4.08,
        "text": "can automate it"
      },
      {
        "start": 4236.96,
        "duration": 6.18,
        "text": "so uh you all should have this uh"
      },
      {
        "start": 4239.9,
        "duration": 5.4,
        "text": "repository uh available to you"
      },
      {
        "start": 4243.14,
        "duration": 4.8,
        "text": "um this is what Rags was using to get"
      },
      {
        "start": 4245.3,
        "duration": 4.98,
        "text": "you started and uh you know I actually"
      },
      {
        "start": 4247.94,
        "duration": 5.52,
        "text": "you know followed along with exactly"
      },
      {
        "start": 4250.28,
        "duration": 5.1,
        "text": "what Rags did and got my Astra"
      },
      {
        "start": 4253.46,
        "duration": 4.92,
        "text": "um CLI set up"
      },
      {
        "start": 4255.38,
        "duration": 4.44,
        "text": "um so I already have my CLI but um go"
      },
      {
        "start": 4258.38,
        "duration": 3.48,
        "text": "ahead and bring it up"
      },
      {
        "start": 4259.82,
        "duration": 4.94,
        "text": "um and what we're gonna do is we're"
      },
      {
        "start": 4261.86,
        "duration": 2.9,
        "text": "going to walk through"
      },
      {
        "start": 4265.82,
        "duration": 5.839,
        "text": "the airflow bit"
      },
      {
        "start": 4268.88,
        "duration": 2.779,
        "text": "there we go"
      },
      {
        "start": 4276.64,
        "duration": 4.96,
        "text": "and uh just so folks that are you know"
      },
      {
        "start": 4279.98,
        "duration": 4.259,
        "text": "new to gitpod"
      },
      {
        "start": 4281.6,
        "duration": 4.5,
        "text": "um get pod is is all based on container"
      },
      {
        "start": 4284.239,
        "duration": 4.741,
        "text": "based on uh in the background in"
      },
      {
        "start": 4286.1,
        "duration": 5.34,
        "text": "kubernetes so when you're not using your"
      },
      {
        "start": 4288.98,
        "duration": 4.38,
        "text": "git pod it just kind of like suspends it"
      },
      {
        "start": 4291.44,
        "duration": 4.86,
        "text": "but as you can see that was like"
      },
      {
        "start": 4293.36,
        "duration": 3.6,
        "text": "five seconds and it's back right"
      },
      {
        "start": 4296.3,
        "duration": 3.419,
        "text": "um"
      },
      {
        "start": 4296.96,
        "duration": 4.86,
        "text": "um yeah this is my this is one of my"
      },
      {
        "start": 4299.719,
        "duration": 4.141,
        "text": "tokens uh that you know I basically"
      },
      {
        "start": 4301.82,
        "duration": 3.359,
        "text": "dragged in here and since I'm gonna you"
      },
      {
        "start": 4303.86,
        "duration": 4.379,
        "text": "know destroy my database I don't really"
      },
      {
        "start": 4305.179,
        "duration": 6.241,
        "text": "care but but uh one of the things to to"
      },
      {
        "start": 4308.239,
        "duration": 4.861,
        "text": "note is that if you have like a file in"
      },
      {
        "start": 4311.42,
        "duration": 4.5,
        "text": "your desktop let's say you downloaded"
      },
      {
        "start": 4313.1,
        "duration": 4.26,
        "text": "the CSV or the or the Json token file"
      },
      {
        "start": 4315.92,
        "duration": 2.279,
        "text": "from"
      },
      {
        "start": 4317.36,
        "duration": 3.24,
        "text": "um"
      },
      {
        "start": 4318.199,
        "duration": 8.221,
        "text": "here like I'll show you right so here's"
      },
      {
        "start": 4320.6,
        "duration": 7.86,
        "text": "my Astro UI if I downloaded this file"
      },
      {
        "start": 4326.42,
        "duration": 2.94,
        "text": "and I put it on my desktop in fact I"
      },
      {
        "start": 4328.46,
        "duration": 3.36,
        "text": "already have this file so I'm going to"
      },
      {
        "start": 4329.36,
        "duration": 4.44,
        "text": "overwrite it"
      },
      {
        "start": 4331.82,
        "duration": 3.359,
        "text": "I can actually um you don't see it"
      },
      {
        "start": 4333.8,
        "duration": 4.68,
        "text": "because it's off screen but I can"
      },
      {
        "start": 4335.179,
        "duration": 8.06,
        "text": "actually just drag that specific file"
      },
      {
        "start": 4338.48,
        "duration": 4.759,
        "text": "from my desktop uh onto"
      },
      {
        "start": 4344.48,
        "duration": 7.14,
        "text": "my UI in my git pod and it'll be there"
      },
      {
        "start": 4347.42,
        "duration": 6.259,
        "text": "right so um anyways I I went ahead and"
      },
      {
        "start": 4351.62,
        "duration": 2.059,
        "text": "um"
      },
      {
        "start": 4354.08,
        "duration": 7.02,
        "text": "configured my my Astra CLI earlier"
      },
      {
        "start": 4357.8,
        "duration": 6.859,
        "text": "um so if you say Astra"
      },
      {
        "start": 4361.1,
        "duration": 3.559,
        "text": "oh hold on one second"
      },
      {
        "start": 4370.28,
        "duration": 6.48,
        "text": "yes bring up a new bash yeah yeah"
      },
      {
        "start": 4373.699,
        "duration": 4.141,
        "text": "that's exactly"
      },
      {
        "start": 4376.76,
        "duration": 2.88,
        "text": "um"
      },
      {
        "start": 4377.84,
        "duration": 4.68,
        "text": "well you know what I'll go ahead and"
      },
      {
        "start": 4379.64,
        "duration": 4.26,
        "text": "reset it re reinstall it because it"
      },
      {
        "start": 4382.52,
        "duration": 2.699,
        "text": "looks like it didn't"
      },
      {
        "start": 4383.9,
        "duration": 3.299,
        "text": "um you know keep my change I didn't"
      },
      {
        "start": 4385.219,
        "duration": 4.261,
        "text": "commit anything so that's why I think I"
      },
      {
        "start": 4387.199,
        "duration": 3.781,
        "text": "think the nice part about you know if"
      },
      {
        "start": 4389.48,
        "duration": 3.9,
        "text": "you're if you're falling behind in the"
      },
      {
        "start": 4390.98,
        "duration": 5.219,
        "text": "workshop it just go back to you know"
      },
      {
        "start": 4393.38,
        "duration": 4.2,
        "text": "square a like what uh you know Rahul was"
      },
      {
        "start": 4396.199,
        "duration": 4.261,
        "text": "doing here you know he's just gonna"
      },
      {
        "start": 4397.58,
        "duration": 5.76,
        "text": "install Astro again and no big deal it"
      },
      {
        "start": 4400.46,
        "duration": 3.96,
        "text": "should hopefully work I know as well so"
      },
      {
        "start": 4403.34,
        "duration": 3.6,
        "text": "if you're falling behind don't worry"
      },
      {
        "start": 4404.42,
        "duration": 6.84,
        "text": "about it yeah exactly so yeah"
      },
      {
        "start": 4406.94,
        "duration": 6.6,
        "text": "exactly so um I'm in Step 1C uh which is"
      },
      {
        "start": 4411.26,
        "duration": 4.74,
        "text": "installed all the Astro CLI"
      },
      {
        "start": 4413.54,
        "duration": 5.159,
        "text": "um you know I've already uh database"
      },
      {
        "start": 4416.0,
        "duration": 5.28,
        "text": "setup uh so it is I'll install"
      },
      {
        "start": 4418.699,
        "duration": 4.861,
        "text": "everything and uh the reason we need to"
      },
      {
        "start": 4421.28,
        "duration": 4.2,
        "text": "close this is that uh it goes into the"
      },
      {
        "start": 4423.56,
        "duration": 3.179,
        "text": "pack right so like"
      },
      {
        "start": 4425.48,
        "duration": 3.719,
        "text": "um"
      },
      {
        "start": 4426.739,
        "duration": 4.5,
        "text": "and so here's here's one of my terminals"
      },
      {
        "start": 4429.199,
        "duration": 3.421,
        "text": "which is what I'm going to use to"
      },
      {
        "start": 4431.239,
        "duration": 3.42,
        "text": "install stuff and then I'm going to run"
      },
      {
        "start": 4432.62,
        "duration": 4.2,
        "text": "my airflow commands here uh you already"
      },
      {
        "start": 4434.659,
        "duration": 5.101,
        "text": "have a split screen"
      },
      {
        "start": 4436.82,
        "duration": 7.14,
        "text": "um but I've got Astra in here right so"
      },
      {
        "start": 4439.76,
        "duration": 4.2,
        "text": "I'm going to do is say"
      },
      {
        "start": 4447.26,
        "duration": 4.88,
        "text": "and so I sourced and if you want to see"
      },
      {
        "start": 4449.36,
        "duration": 2.78,
        "text": "what that file is"
      },
      {
        "start": 4461.54,
        "duration": 6.179,
        "text": "it basically has uh you know where to"
      },
      {
        "start": 4466.04,
        "duration": 3.119,
        "text": "um you know pick up the CLI right and"
      },
      {
        "start": 4467.719,
        "duration": 2.401,
        "text": "where the home directory is going to be"
      },
      {
        "start": 4469.159,
        "duration": 3.901,
        "text": "so"
      },
      {
        "start": 4470.12,
        "duration": 7.52,
        "text": "um I've got Astro now in my command"
      },
      {
        "start": 4473.06,
        "duration": 4.58,
        "text": "right so I'm going to do Astra setup"
      },
      {
        "start": 4477.739,
        "duration": 4.621,
        "text": "and the aster token uh there's one set"
      },
      {
        "start": 4480.739,
        "duration": 2.701,
        "text": "of tokens for just you know when I make"
      },
      {
        "start": 4482.36,
        "duration": 2.76,
        "text": "the database and then there's an"
      },
      {
        "start": 4483.44,
        "duration": 2.88,
        "text": "organizational level token which I have"
      },
      {
        "start": 4485.12,
        "duration": 4.44,
        "text": "here"
      },
      {
        "start": 4486.32,
        "duration": 6.54,
        "text": "um and it's the one that has Astra"
      },
      {
        "start": 4489.56,
        "duration": 6.06,
        "text": "in it Astra CS"
      },
      {
        "start": 4492.86,
        "duration": 6.56,
        "text": "and I copied it and I just right click"
      },
      {
        "start": 4495.62,
        "duration": 3.8,
        "text": "right and it's gonna put that there"
      },
      {
        "start": 4499.699,
        "duration": 6.781,
        "text": "and I've got Astra set up and so if I do"
      },
      {
        "start": 4503.12,
        "duration": 6.18,
        "text": "Astra DB list"
      },
      {
        "start": 4506.48,
        "duration": 5.52,
        "text": "it's got my two databases"
      },
      {
        "start": 4509.3,
        "duration": 4.08,
        "text": "um I have this this old database that I"
      },
      {
        "start": 4512.0,
        "duration": 4.32,
        "text": "use for other demonstrations and I'm"
      },
      {
        "start": 4513.38,
        "duration": 5.1,
        "text": "using the same exact database name that"
      },
      {
        "start": 4516.32,
        "duration": 3.839,
        "text": "is in the demo so I'm I'm doing exactly"
      },
      {
        "start": 4518.48,
        "duration": 4.38,
        "text": "what's in there uh this is the new"
      },
      {
        "start": 4520.159,
        "duration": 6.721,
        "text": "Workshops the database that I made"
      },
      {
        "start": 4522.86,
        "duration": 7.319,
        "text": "and I made a key space also called uh"
      },
      {
        "start": 4526.88,
        "duration": 5.7,
        "text": "um airflow demo now if"
      },
      {
        "start": 4530.179,
        "duration": 5.341,
        "text": "um you know you didn't have the same"
      },
      {
        "start": 4532.58,
        "duration": 4.139,
        "text": "exact key space that's okay but um this"
      },
      {
        "start": 4535.52,
        "duration": 2.76,
        "text": "command that I'm going to paste which is"
      },
      {
        "start": 4536.719,
        "duration": 5.401,
        "text": "also in the documentation"
      },
      {
        "start": 4538.28,
        "duration": 6.419,
        "text": "it's going to create uh in the uh the"
      },
      {
        "start": 4542.12,
        "duration": 4.86,
        "text": "database at this particular key space if"
      },
      {
        "start": 4544.699,
        "duration": 6.0,
        "text": "it doesn't exist right but it already"
      },
      {
        "start": 4546.98,
        "duration": 6.0,
        "text": "does and for some of you know that quite"
      },
      {
        "start": 4550.699,
        "duration": 3.781,
        "text": "a few of you um have attended data"
      },
      {
        "start": 4552.98,
        "duration": 3.84,
        "text": "Stacks there's workshop and chances are"
      },
      {
        "start": 4554.48,
        "duration": 4.14,
        "text": "you may have already created a workshops"
      },
      {
        "start": 4556.82,
        "duration": 3.06,
        "text": "database in which case you know you"
      },
      {
        "start": 4558.62,
        "duration": 2.64,
        "text": "really don't need to do anything other"
      },
      {
        "start": 4559.88,
        "duration": 4.14,
        "text": "than you know kind of grab the token"
      },
      {
        "start": 4561.26,
        "duration": 4.439,
        "text": "right and uh and then you can you know"
      },
      {
        "start": 4564.02,
        "duration": 3.659,
        "text": "you can create the workspace because"
      },
      {
        "start": 4565.699,
        "duration": 3.96,
        "text": "even if you try to create the workshop"
      },
      {
        "start": 4567.679,
        "duration": 4.56,
        "text": "you know it's not going to create it uh"
      },
      {
        "start": 4569.659,
        "duration": 3.5,
        "text": "if it's already there okay so so all of"
      },
      {
        "start": 4572.239,
        "duration": 4.521,
        "text": "that"
      },
      {
        "start": 4573.159,
        "duration": 3.601,
        "text": "instructions are in there"
      },
      {
        "start": 4577.219,
        "duration": 3.241,
        "text": "perfect"
      },
      {
        "start": 4578.78,
        "duration": 5.04,
        "text": "um so because we're connecting to a"
      },
      {
        "start": 4580.46,
        "duration": 6.66,
        "text": "cloud database uh we have to have a"
      },
      {
        "start": 4583.82,
        "duration": 5.04,
        "text": "secure connect bundle uh and I'm just"
      },
      {
        "start": 4587.12,
        "duration": 3.539,
        "text": "you know pasting the command to download"
      },
      {
        "start": 4588.86,
        "duration": 3.9,
        "text": "the secure connect bundle for this"
      },
      {
        "start": 4590.659,
        "duration": 3.481,
        "text": "particular database"
      },
      {
        "start": 4592.76,
        "duration": 4.04,
        "text": "um and"
      },
      {
        "start": 4594.14,
        "duration": 2.66,
        "text": "here we go"
      },
      {
        "start": 4596.96,
        "duration": 4.92,
        "text": "and this is going to allow us to"
      },
      {
        "start": 4599.42,
        "duration": 4.86,
        "text": "basically talk to Astra securely from"
      },
      {
        "start": 4601.88,
        "duration": 4.08,
        "text": "our from our gitbot"
      },
      {
        "start": 4604.28,
        "duration": 3.18,
        "text": "um"
      },
      {
        "start": 4605.96,
        "duration": 4.02,
        "text": "so what are we doing with our"
      },
      {
        "start": 4607.46,
        "duration": 5.94,
        "text": "demonstration right we're going to be"
      },
      {
        "start": 4609.98,
        "duration": 4.739,
        "text": "getting data into a Cassandra table"
      },
      {
        "start": 4613.4,
        "duration": 2.88,
        "text": "where we're gonna take it out of that"
      },
      {
        "start": 4614.719,
        "duration": 3.901,
        "text": "casino we'll put into another Cassandra"
      },
      {
        "start": 4616.28,
        "duration": 4.8,
        "text": "table uh using spark"
      },
      {
        "start": 4618.62,
        "duration": 4.68,
        "text": "um and uh we need we need a table"
      },
      {
        "start": 4621.08,
        "duration": 4.02,
        "text": "structure right we need some tables in"
      },
      {
        "start": 4623.3,
        "duration": 4.26,
        "text": "there a key space does not have any data"
      },
      {
        "start": 4625.1,
        "duration": 4.44,
        "text": "it's just a container for tables so"
      },
      {
        "start": 4627.56,
        "duration": 5.34,
        "text": "um we're going to be creating"
      },
      {
        "start": 4629.54,
        "duration": 4.679,
        "text": "two uh tables one is previous employees"
      },
      {
        "start": 4632.9,
        "duration": 3.06,
        "text": "by job title and then there's another"
      },
      {
        "start": 4634.219,
        "duration": 3.661,
        "text": "one day's work by previous employees by"
      },
      {
        "start": 4635.96,
        "duration": 3.719,
        "text": "job title and this is a you know Common"
      },
      {
        "start": 4637.88,
        "duration": 4.08,
        "text": "scenario we have to do some Roll-Ups on"
      },
      {
        "start": 4639.679,
        "duration": 4.261,
        "text": "some data on a batch process maybe"
      },
      {
        "start": 4641.96,
        "duration": 4.199,
        "text": "you're doing it in real time but you"
      },
      {
        "start": 4643.94,
        "duration": 4.56,
        "text": "want to reconcile this information uh"
      },
      {
        "start": 4646.159,
        "duration": 4.381,
        "text": "and so these two tables we're going to"
      },
      {
        "start": 4648.5,
        "duration": 5.46,
        "text": "go ahead and create them you can do it"
      },
      {
        "start": 4650.54,
        "duration": 6.119,
        "text": "in the GUI uh but with Astra dbcli you"
      },
      {
        "start": 4653.96,
        "duration": 6.66,
        "text": "can just say hey in this table uh sorry"
      },
      {
        "start": 4656.659,
        "duration": 7.381,
        "text": "in this database go ahead and run this"
      },
      {
        "start": 4660.62,
        "duration": 5.039,
        "text": "file and if you're used to cql sh yes if"
      },
      {
        "start": 4664.04,
        "duration": 4.44,
        "text": "you've created a key space with another"
      },
      {
        "start": 4665.659,
        "duration": 5.101,
        "text": "name like you know Nadia flow demo um"
      },
      {
        "start": 4668.48,
        "duration": 3.96,
        "text": "you just have to change you know the"
      },
      {
        "start": 4670.76,
        "duration": 3.959,
        "text": "setup to use that particular key space"
      },
      {
        "start": 4672.44,
        "duration": 3.44,
        "text": "not a big deal exactly"
      },
      {
        "start": 4674.719,
        "duration": 3.601,
        "text": "yeah"
      },
      {
        "start": 4675.88,
        "duration": 7.06,
        "text": "exactly exactly"
      },
      {
        "start": 4678.32,
        "duration": 6.839,
        "text": "um so we've got uh uh a database"
      },
      {
        "start": 4682.94,
        "duration": 4.02,
        "text": "we've got a key space"
      },
      {
        "start": 4685.159,
        "duration": 3.661,
        "text": "and we have"
      },
      {
        "start": 4686.96,
        "duration": 4.14,
        "text": "two tables in there"
      },
      {
        "start": 4688.82,
        "duration": 4.8,
        "text": "um and you can verify"
      },
      {
        "start": 4691.1,
        "duration": 4.38,
        "text": "um you know uh those tables by you know"
      },
      {
        "start": 4693.62,
        "duration": 3.9,
        "text": "going into cql Stage on the GUI and"
      },
      {
        "start": 4695.48,
        "duration": 4.38,
        "text": "we'll see it in a second but"
      },
      {
        "start": 4697.52,
        "duration": 5.159,
        "text": "um now we're going to trust the fact"
      },
      {
        "start": 4699.86,
        "duration": 3.66,
        "text": "that we have uh these these tables in"
      },
      {
        "start": 4702.679,
        "duration": 1.921,
        "text": "there"
      },
      {
        "start": 4703.52,
        "duration": 3.3,
        "text": "um and the next thing that we're going"
      },
      {
        "start": 4704.6,
        "duration": 5.639,
        "text": "to do is we're going to set up"
      },
      {
        "start": 4706.82,
        "duration": 5.7,
        "text": "airflow this is a sample script uh"
      },
      {
        "start": 4710.239,
        "duration": 4.44,
        "text": "there's many different ways to run uh"
      },
      {
        "start": 4712.52,
        "duration": 4.26,
        "text": "airflow okay"
      },
      {
        "start": 4714.679,
        "duration": 4.321,
        "text": "um this is just running it on one"
      },
      {
        "start": 4716.78,
        "duration": 4.439,
        "text": "computer one container so all the"
      },
      {
        "start": 4719.0,
        "duration": 2.82,
        "text": "components are here okay"
      },
      {
        "start": 4721.219,
        "duration": 5.341,
        "text": "um"
      },
      {
        "start": 4721.82,
        "duration": 5.64,
        "text": "and it uses not postgres but it uses cql"
      },
      {
        "start": 4726.56,
        "duration": 2.58,
        "text": "light"
      },
      {
        "start": 4727.46,
        "duration": 3.779,
        "text": "because it's a demonstration this is a"
      },
      {
        "start": 4729.14,
        "duration": 4.62,
        "text": "perfect concept type system right so you"
      },
      {
        "start": 4731.239,
        "duration": 3.781,
        "text": "don't have to run postgres but it's"
      },
      {
        "start": 4733.76,
        "duration": 4.14,
        "text": "recommended if you go into production"
      },
      {
        "start": 4735.02,
        "duration": 4.38,
        "text": "you do a real database basically"
      },
      {
        "start": 4737.9,
        "duration": 4.08,
        "text": "um and uh you know maybe somebody will"
      },
      {
        "start": 4739.4,
        "duration": 4.62,
        "text": "commit code to run airflow on Astra or"
      },
      {
        "start": 4741.98,
        "duration": 4.679,
        "text": "Cassandra but right now in production"
      },
      {
        "start": 4744.02,
        "duration": 4.62,
        "text": "you need a postgres database and the"
      },
      {
        "start": 4746.659,
        "duration": 4.201,
        "text": "recommendation is if you're on the cloud"
      },
      {
        "start": 4748.64,
        "duration": 4.559,
        "text": "don't run postgres by yourself"
      },
      {
        "start": 4750.86,
        "duration": 3.839,
        "text": "right just use like a managed database"
      },
      {
        "start": 4753.199,
        "duration": 4.681,
        "text": "like on digitalocean we use manage"
      },
      {
        "start": 4754.699,
        "duration": 5.101,
        "text": "postgres or our AWS there's RDS so"
      },
      {
        "start": 4757.88,
        "duration": 5.4,
        "text": "what's happening here uh we're setting"
      },
      {
        "start": 4759.8,
        "duration": 5.34,
        "text": "some specific versions for airflow"
      },
      {
        "start": 4763.28,
        "duration": 3.3,
        "text": "um because if you don't pop in it"
      },
      {
        "start": 4765.14,
        "duration": 2.88,
        "text": "that'll download the latest one and we"
      },
      {
        "start": 4766.58,
        "duration": 3.119,
        "text": "don't have control over which libraries"
      },
      {
        "start": 4768.02,
        "duration": 3.42,
        "text": "got downloaded right"
      },
      {
        "start": 4769.699,
        "duration": 3.601,
        "text": "and then once it's installed it's a"
      },
      {
        "start": 4771.44,
        "duration": 4.08,
        "text": "Python program right so we're just"
      },
      {
        "start": 4773.3,
        "duration": 5.879,
        "text": "installing airflow using pip we have"
      },
      {
        "start": 4775.52,
        "duration": 6.24,
        "text": "python on our system and the DB init"
      },
      {
        "start": 4779.179,
        "duration": 4.02,
        "text": "will in our case make a a schema inside"
      },
      {
        "start": 4781.76,
        "duration": 4.02,
        "text": "sqlite"
      },
      {
        "start": 4783.199,
        "duration": 5.721,
        "text": "and then we need a user we need a first"
      },
      {
        "start": 4785.78,
        "duration": 6.419,
        "text": "user an admin user to be able to log in"
      },
      {
        "start": 4788.92,
        "duration": 5.319,
        "text": "the user system is pretty robust you"
      },
      {
        "start": 4792.199,
        "duration": 3.721,
        "text": "have roles you have permissions"
      },
      {
        "start": 4794.239,
        "duration": 3.061,
        "text": "um to give people the ability to just"
      },
      {
        "start": 4795.92,
        "duration": 3.299,
        "text": "weed"
      },
      {
        "start": 4797.3,
        "duration": 3.78,
        "text": "what's going on to see what's going on"
      },
      {
        "start": 4799.219,
        "duration": 4.44,
        "text": "to"
      },
      {
        "start": 4801.08,
        "duration": 5.94,
        "text": "um you know uh to to start and stop jobs"
      },
      {
        "start": 4803.659,
        "duration": 4.921,
        "text": "for a certain uh you know set of jobs"
      },
      {
        "start": 4807.02,
        "duration": 2.88,
        "text": "um but we're gonna have a super user"
      },
      {
        "start": 4808.58,
        "duration": 3.18,
        "text": "We're not gonna get into the you know"
      },
      {
        "start": 4809.9,
        "duration": 3.48,
        "text": "role-based access control but it has a"
      },
      {
        "start": 4811.76,
        "duration": 3.3,
        "text": "pretty robust system"
      },
      {
        "start": 4813.38,
        "duration": 3.66,
        "text": "um and once the database is create a"
      },
      {
        "start": 4815.06,
        "duration": 4.38,
        "text": "database is created once we have a user"
      },
      {
        "start": 4817.04,
        "duration": 4.619,
        "text": "in there we run the web server"
      },
      {
        "start": 4819.44,
        "duration": 6.6,
        "text": "the web server is what gives us the API"
      },
      {
        "start": 4821.659,
        "duration": 6.421,
        "text": "ability uh as well as the ability to uh"
      },
      {
        "start": 4826.04,
        "duration": 4.26,
        "text": "you know call apis uh and UI"
      },
      {
        "start": 4828.08,
        "duration": 2.82,
        "text": "simultaneously you can have it"
      },
      {
        "start": 4830.3,
        "duration": 2.22,
        "text": "um"
      },
      {
        "start": 4830.9,
        "duration": 3.42,
        "text": "and"
      },
      {
        "start": 4832.52,
        "duration": 3.48,
        "text": "most people they forget this they forget"
      },
      {
        "start": 4834.32,
        "duration": 3.72,
        "text": "that after you run the web server you"
      },
      {
        "start": 4836.0,
        "duration": 3.84,
        "text": "still need the scheduler so the"
      },
      {
        "start": 4838.04,
        "duration": 3.84,
        "text": "scheduler is what is kind of like"
      },
      {
        "start": 4839.84,
        "duration": 4.74,
        "text": "watching out for changes in your files"
      },
      {
        "start": 4841.88,
        "duration": 4.74,
        "text": "it's the one that's saying oh it's time"
      },
      {
        "start": 4844.58,
        "duration": 3.9,
        "text": "to go run this job right and it's it's"
      },
      {
        "start": 4846.62,
        "duration": 3.78,
        "text": "the whole manager for all the stuff that"
      },
      {
        "start": 4848.48,
        "duration": 4.199,
        "text": "airflow is doing for you uh but you need"
      },
      {
        "start": 4850.4,
        "duration": 5.7,
        "text": "all these components you need a database"
      },
      {
        "start": 4852.679,
        "duration": 5.04,
        "text": "right uh you need a web server and you"
      },
      {
        "start": 4856.1,
        "duration": 2.639,
        "text": "need a scheduler"
      },
      {
        "start": 4857.719,
        "duration": 2.761,
        "text": "um and"
      },
      {
        "start": 4858.739,
        "duration": 3.541,
        "text": "um it's actually pretty simple I mean"
      },
      {
        "start": 4860.48,
        "duration": 4.56,
        "text": "when you remember the diagram"
      },
      {
        "start": 4862.28,
        "duration": 4.2,
        "text": "um earlier with the airflow system runs"
      },
      {
        "start": 4865.04,
        "duration": 3.54,
        "text": "like this between the web server and the"
      },
      {
        "start": 4866.48,
        "duration": 3.48,
        "text": "scheduler this is a simple setup you can"
      },
      {
        "start": 4868.58,
        "duration": 2.94,
        "text": "have a much more complex setup later on"
      },
      {
        "start": 4869.96,
        "duration": 4.56,
        "text": "if you want to"
      },
      {
        "start": 4871.52,
        "duration": 4.44,
        "text": "so I'm going to go ahead and run"
      },
      {
        "start": 4874.52,
        "duration": 3.0,
        "text": "oops"
      },
      {
        "start": 4875.96,
        "duration": 5.48,
        "text": "I'll do that I'm going to go ahead and"
      },
      {
        "start": 4877.52,
        "duration": 3.92,
        "text": "run the setup program"
      },
      {
        "start": 4882.32,
        "duration": 5.419,
        "text": "and"
      },
      {
        "start": 4884.86,
        "duration": 4.5,
        "text": "just mind that it's going to download"
      },
      {
        "start": 4887.739,
        "duration": 6.401,
        "text": "install"
      },
      {
        "start": 4889.36,
        "duration": 6.94,
        "text": "and it's going to ask you for a password"
      },
      {
        "start": 4894.14,
        "duration": 5.22,
        "text": "this is a throwaway system I'm not that"
      },
      {
        "start": 4896.3,
        "duration": 6.419,
        "text": "concerned so you know"
      },
      {
        "start": 4899.36,
        "duration": 6.839,
        "text": "um I'm just gonna have like admin and"
      },
      {
        "start": 4902.719,
        "duration": 4.141,
        "text": "one two three four five or whatever but"
      },
      {
        "start": 4906.199,
        "duration": 1.801,
        "text": "um"
      },
      {
        "start": 4906.86,
        "duration": 3.12,
        "text": "one of the biggest security"
      },
      {
        "start": 4908.0,
        "duration": 5.64,
        "text": "vulnerabilities for airflow was that"
      },
      {
        "start": 4909.98,
        "duration": 5.88,
        "text": "people were not securing even after they"
      },
      {
        "start": 4913.64,
        "duration": 5.16,
        "text": "created a user you know a secure user"
      },
      {
        "start": 4915.86,
        "duration": 5.28,
        "text": "they weren't protecting this endpoint"
      },
      {
        "start": 4918.8,
        "duration": 3.72,
        "text": "okay and when we were recommending"
      },
      {
        "start": 4921.14,
        "duration": 2.88,
        "text": "airflow at a client they're like this"
      },
      {
        "start": 4922.52,
        "duration": 3.78,
        "text": "software is insecure I'm like no the"
      },
      {
        "start": 4924.02,
        "duration": 4.139,
        "text": "software is fine"
      },
      {
        "start": 4926.3,
        "duration": 3.66,
        "text": "you have to protect the endpoint with a"
      },
      {
        "start": 4928.159,
        "duration": 4.141,
        "text": "firewall you can't just put it out there"
      },
      {
        "start": 4929.96,
        "duration": 4.92,
        "text": "uh and that's the thing with any you"
      },
      {
        "start": 4932.3,
        "duration": 4.74,
        "text": "know open source tool I remember when"
      },
      {
        "start": 4934.88,
        "duration": 3.72,
        "text": "people were like oh uh which I"
      },
      {
        "start": 4937.04,
        "duration": 3.6,
        "text": "don't like that database but like"
      },
      {
        "start": 4938.6,
        "duration": 4.079,
        "text": "is insecure or Cassandra is insecure"
      },
      {
        "start": 4940.64,
        "duration": 4.5,
        "text": "because people can just log in using"
      },
      {
        "start": 4942.679,
        "duration": 6.241,
        "text": "Cassandra Cassandra I'm like yeah"
      },
      {
        "start": 4945.14,
        "duration": 4.86,
        "text": "that's true but anybody that knows what"
      },
      {
        "start": 4948.92,
        "duration": 3.96,
        "text": "they're doing they wouldn't just leave"
      },
      {
        "start": 4950.0,
        "duration": 4.14,
        "text": "Cassandra Cassandra as a default user"
      },
      {
        "start": 4952.88,
        "duration": 4.319,
        "text": "um and you know because you're using"
      },
      {
        "start": 4954.14,
        "duration": 4.86,
        "text": "Astra you have some security built in"
      },
      {
        "start": 4957.199,
        "duration": 3.181,
        "text": "already right people can't access that"
      },
      {
        "start": 4959.0,
        "duration": 2.699,
        "text": "database unless they have a secure"
      },
      {
        "start": 4960.38,
        "duration": 3.96,
        "text": "connect bundle and the token and"
      },
      {
        "start": 4961.699,
        "duration": 4.561,
        "text": "everything okay so a simple password"
      },
      {
        "start": 4964.34,
        "duration": 4.62,
        "text": "that I'm not going to forget"
      },
      {
        "start": 4966.26,
        "duration": 4.8,
        "text": "and that's gonna ask me again and you"
      },
      {
        "start": 4968.96,
        "duration": 4.259,
        "text": "will have to make note of that uh so"
      },
      {
        "start": 4971.06,
        "duration": 5.159,
        "text": "yeah you know keep it simple uh just for"
      },
      {
        "start": 4973.219,
        "duration": 4.5,
        "text": "because it's throw away but uh I"
      },
      {
        "start": 4976.219,
        "duration": 3.841,
        "text": "completely agree with you Rahul you know"
      },
      {
        "start": 4977.719,
        "duration": 4.98,
        "text": "there's always this balance right"
      },
      {
        "start": 4980.06,
        "duration": 5.46,
        "text": "between getting started then and running"
      },
      {
        "start": 4982.699,
        "duration": 5.841,
        "text": "a production system it's two two"
      },
      {
        "start": 4985.52,
        "duration": 3.02,
        "text": "different beats yeah"
      },
      {
        "start": 4990.56,
        "duration": 6.0,
        "text": "ephemeral right uh Astra is gonna store"
      },
      {
        "start": 4993.56,
        "duration": 4.74,
        "text": "your data which you can come back to"
      },
      {
        "start": 4996.56,
        "duration": 4.56,
        "text": "um but you know Astro databases are"
      },
      {
        "start": 4998.3,
        "duration": 5.04,
        "text": "serverless it can get suspended uh"
      },
      {
        "start": 5001.12,
        "duration": 3.72,
        "text": "gitpod is ephemeral it starts up when"
      },
      {
        "start": 5003.34,
        "duration": 4.379,
        "text": "you need it when you're not using it it"
      },
      {
        "start": 5004.84,
        "duration": 5.16,
        "text": "gets suspended in reality you're gonna"
      },
      {
        "start": 5007.719,
        "duration": 4.141,
        "text": "have a database that is always running"
      },
      {
        "start": 5010.0,
        "duration": 3.78,
        "text": "and always has data in and out so it's"
      },
      {
        "start": 5011.86,
        "duration": 2.94,
        "text": "not going to get suspended"
      },
      {
        "start": 5013.78,
        "duration": 3.6,
        "text": "um"
      },
      {
        "start": 5014.8,
        "duration": 4.56,
        "text": "you know there are some takeaways in"
      },
      {
        "start": 5017.38,
        "duration": 3.9,
        "text": "terms of like uh we'll get to that in"
      },
      {
        "start": 5019.36,
        "duration": 3.96,
        "text": "terms of how I would recommend you use"
      },
      {
        "start": 5021.28,
        "duration": 3.66,
        "text": "airflow but this is just a learning"
      },
      {
        "start": 5023.32,
        "duration": 3.54,
        "text": "experience and you can run it you know"
      },
      {
        "start": 5024.94,
        "duration": 3.6,
        "text": "you're doing this Hands-On the goal is"
      },
      {
        "start": 5026.86,
        "duration": 3.48,
        "text": "for you to understand what interflow is"
      },
      {
        "start": 5028.54,
        "duration": 3.9,
        "text": "and how to use it so"
      },
      {
        "start": 5030.34,
        "duration": 3.899,
        "text": "airflow is at this point running okay"
      },
      {
        "start": 5032.44,
        "duration": 3.6,
        "text": "and"
      },
      {
        "start": 5034.239,
        "duration": 3.721,
        "text": "um I'm just going to keep this here I'm"
      },
      {
        "start": 5036.04,
        "duration": 3.5,
        "text": "not going to turn it off but we'll"
      },
      {
        "start": 5037.96,
        "duration": 6.12,
        "text": "notice over here"
      },
      {
        "start": 5039.54,
        "duration": 8.32,
        "text": "that certain ports are exposed right and"
      },
      {
        "start": 5044.08,
        "duration": 5.34,
        "text": "port 8080 is exposed and and Port 8793"
      },
      {
        "start": 5047.86,
        "duration": 4.74,
        "text": "is exposed"
      },
      {
        "start": 5049.42,
        "duration": 5.279,
        "text": "um so if you click on this"
      },
      {
        "start": 5052.6,
        "duration": 3.48,
        "text": "it actually shows you the airflow"
      },
      {
        "start": 5054.699,
        "duration": 5.181,
        "text": "interface"
      },
      {
        "start": 5056.08,
        "duration": 3.8,
        "text": "so I'm just going to log in"
      },
      {
        "start": 5061.179,
        "duration": 3.301,
        "text": "and I'm not going to"
      },
      {
        "start": 5062.56,
        "duration": 4.139,
        "text": "do anything right now"
      },
      {
        "start": 5064.48,
        "duration": 6.06,
        "text": "but it gives us the warnings right don't"
      },
      {
        "start": 5066.699,
        "duration": 5.941,
        "text": "use SQL Lite in production it tells you"
      },
      {
        "start": 5070.54,
        "duration": 2.9,
        "text": "oh he doesn't like my password it's too"
      },
      {
        "start": 5072.64,
        "duration": 3.599,
        "text": "simple"
      },
      {
        "start": 5073.44,
        "duration": 4.719,
        "text": "but that's Chrome telling me that uh do"
      },
      {
        "start": 5076.239,
        "duration": 4.261,
        "text": "not use sequential executors a"
      },
      {
        "start": 5078.159,
        "duration": 4.681,
        "text": "sequential executor just runs the next"
      },
      {
        "start": 5080.5,
        "duration": 3.659,
        "text": "thing in in the system uh and then it'll"
      },
      {
        "start": 5082.84,
        "duration": 3.66,
        "text": "do it but as I mentioned their salary"
      },
      {
        "start": 5084.159,
        "duration": 4.02,
        "text": "executor that can do in parallel there's"
      },
      {
        "start": 5086.5,
        "duration": 3.719,
        "text": "kubernetes executor that will just"
      },
      {
        "start": 5088.179,
        "duration": 3.961,
        "text": "create containers for each job and do it"
      },
      {
        "start": 5090.219,
        "duration": 4.98,
        "text": "for you and under the hood it's using"
      },
      {
        "start": 5092.14,
        "duration": 4.38,
        "text": "the kubernetes you know job container"
      },
      {
        "start": 5095.199,
        "duration": 2.761,
        "text": "type basically"
      },
      {
        "start": 5096.52,
        "duration": 5.76,
        "text": "right"
      },
      {
        "start": 5097.96,
        "duration": 6.719,
        "text": "um here are example dags okay and uh you"
      },
      {
        "start": 5102.28,
        "duration": 5.16,
        "text": "don't even have to look at the code uh"
      },
      {
        "start": 5104.679,
        "duration": 4.621,
        "text": "in your uh Visual Studio in your git pod"
      },
      {
        "start": 5107.44,
        "duration": 5.0,
        "text": "you can actually just click on this"
      },
      {
        "start": 5109.3,
        "duration": 3.14,
        "text": "right here"
      },
      {
        "start": 5113.92,
        "duration": 6.48,
        "text": "and you go to the graph View"
      },
      {
        "start": 5117.4,
        "duration": 5.279,
        "text": "and that this is a simple uh thing but"
      },
      {
        "start": 5120.4,
        "duration": 5.4,
        "text": "you can start to see uh you know at any"
      },
      {
        "start": 5122.679,
        "duration": 5.101,
        "text": "given uh job or in this and I call it a"
      },
      {
        "start": 5125.8,
        "duration": 4.26,
        "text": "job or dag is the same"
      },
      {
        "start": 5127.78,
        "duration": 4.02,
        "text": "um you have different views uh the grid"
      },
      {
        "start": 5130.06,
        "duration": 3.84,
        "text": "basically shows you your previous"
      },
      {
        "start": 5131.8,
        "duration": 4.14,
        "text": "executions and how long it took and"
      },
      {
        "start": 5133.9,
        "duration": 5.16,
        "text": "we'll show that in a second the graph"
      },
      {
        "start": 5135.94,
        "duration": 5.04,
        "text": "shows you the current running uh dag in"
      },
      {
        "start": 5139.06,
        "duration": 3.96,
        "text": "this case there's only one task"
      },
      {
        "start": 5140.98,
        "duration": 4.739,
        "text": "um you know what is scheduled what is"
      },
      {
        "start": 5143.02,
        "duration": 4.32,
        "text": "deferred failed Etc"
      },
      {
        "start": 5145.719,
        "duration": 4.621,
        "text": "the calendar shows you previous"
      },
      {
        "start": 5147.34,
        "duration": 5.7,
        "text": "executions so if you have a job or a dag"
      },
      {
        "start": 5150.34,
        "duration": 4.62,
        "text": "running for years"
      },
      {
        "start": 5153.04,
        "duration": 3.9,
        "text": "um you can actually see the history what"
      },
      {
        "start": 5154.96,
        "duration": 3.54,
        "text": "was the success here how many were"
      },
      {
        "start": 5156.94,
        "duration": 3.299,
        "text": "successful how many failed over the"
      },
      {
        "start": 5158.5,
        "duration": 3.659,
        "text": "course of a year"
      },
      {
        "start": 5160.239,
        "duration": 3.121,
        "text": "um task duration and okay when you see"
      },
      {
        "start": 5162.159,
        "duration": 2.761,
        "text": "all this you know what I'm gonna wait"
      },
      {
        "start": 5163.36,
        "duration": 3.299,
        "text": "I'm gonna wait until we're running some"
      },
      {
        "start": 5164.92,
        "duration": 2.46,
        "text": "real jobs but"
      },
      {
        "start": 5166.659,
        "duration": 3.961,
        "text": "um"
      },
      {
        "start": 5167.38,
        "duration": 5.94,
        "text": "I continue uh to go to the spark setup"
      },
      {
        "start": 5170.62,
        "duration": 4.32,
        "text": "uh one thing that that's interesting uh"
      },
      {
        "start": 5173.32,
        "duration": 3.18,
        "text": "you want if you're curious to see what"
      },
      {
        "start": 5174.94,
        "duration": 4.14,
        "text": "this code is without even looking at"
      },
      {
        "start": 5176.5,
        "duration": 4.26,
        "text": "Visual Studio in gitpod you can see the"
      },
      {
        "start": 5179.08,
        "duration": 4.619,
        "text": "code right here"
      },
      {
        "start": 5180.76,
        "duration": 5.1,
        "text": "right it tells you which version of your"
      },
      {
        "start": 5183.699,
        "duration": 3.841,
        "text": "code is running so because"
      },
      {
        "start": 5185.86,
        "duration": 4.08,
        "text": "um the the schedule every five minutes"
      },
      {
        "start": 5187.54,
        "duration": 4.44,
        "text": "it's watching out for files okay"
      },
      {
        "start": 5189.94,
        "duration": 4.08,
        "text": "and so if you notice that your job is"
      },
      {
        "start": 5191.98,
        "duration": 3.6,
        "text": "not working as intended you can just go"
      },
      {
        "start": 5194.02,
        "duration": 3.9,
        "text": "in here and say what what version of my"
      },
      {
        "start": 5195.58,
        "duration": 4.139,
        "text": "code is driving all right okay so we're"
      },
      {
        "start": 5197.92,
        "duration": 3.9,
        "text": "gonna we're gonna put this aside we're"
      },
      {
        "start": 5199.719,
        "duration": 4.801,
        "text": "gonna come around question"
      },
      {
        "start": 5201.82,
        "duration": 5.46,
        "text": "um Again unchecked by shashikant who's"
      },
      {
        "start": 5204.52,
        "duration": 5.04,
        "text": "asking is airflow only for data related"
      },
      {
        "start": 5207.28,
        "duration": 4.62,
        "text": "ETL jobs automation or any other"
      },
      {
        "start": 5209.56,
        "duration": 5.52,
        "text": "workflows could be created what are"
      },
      {
        "start": 5211.9,
        "duration": 6.18,
        "text": "other examples and ion thanks for"
      },
      {
        "start": 5215.08,
        "duration": 6.36,
        "text": "performing it is not just for ETL think"
      },
      {
        "start": 5218.08,
        "duration": 6.96,
        "text": "of it as a workflow manager"
      },
      {
        "start": 5221.44,
        "duration": 4.68,
        "text": "yes exactly thank you Ian I appreciate"
      },
      {
        "start": 5225.04,
        "duration": 2.94,
        "text": "that"
      },
      {
        "start": 5226.12,
        "duration": 3.96,
        "text": "um so actually I'll bring this back"
      },
      {
        "start": 5227.98,
        "duration": 5.34,
        "text": "screen back okay"
      },
      {
        "start": 5230.08,
        "duration": 6.54,
        "text": "um it's not just for ETL uh anything"
      },
      {
        "start": 5233.32,
        "duration": 5.04,
        "text": "that is a long-running process that is"
      },
      {
        "start": 5236.62,
        "duration": 4.44,
        "text": "dependent on other long running"
      },
      {
        "start": 5238.36,
        "duration": 4.74,
        "text": "processes or you have subsequent long"
      },
      {
        "start": 5241.06,
        "duration": 4.079,
        "text": "running processes and you are"
      },
      {
        "start": 5243.1,
        "duration": 2.82,
        "text": "coordinating it across a distributed"
      },
      {
        "start": 5245.139,
        "duration": 3.241,
        "text": "system"
      },
      {
        "start": 5245.92,
        "duration": 6.12,
        "text": "right many many things like I want data"
      },
      {
        "start": 5248.38,
        "duration": 6.06,
        "text": "to come in from S3 into Cassandra I want"
      },
      {
        "start": 5252.04,
        "duration": 5.52,
        "text": "data to go from Cassandra into snowflake"
      },
      {
        "start": 5254.44,
        "duration": 4.199,
        "text": "DB I want some spark job to enhance this"
      },
      {
        "start": 5257.56,
        "duration": 5.4,
        "text": "information"
      },
      {
        "start": 5258.639,
        "duration": 7.681,
        "text": "so you can do the whole uh kind of range"
      },
      {
        "start": 5262.96,
        "duration": 6.9,
        "text": "of import export processing you can run"
      },
      {
        "start": 5266.32,
        "duration": 5.58,
        "text": "machine learning tasks in sequence so"
      },
      {
        "start": 5269.86,
        "duration": 4.859,
        "text": "anytime you were doing sequence of tasks"
      },
      {
        "start": 5271.9,
        "duration": 4.799,
        "text": "with data you can you can use it and and"
      },
      {
        "start": 5274.719,
        "duration": 3.301,
        "text": "this is a relatively new uh feature"
      },
      {
        "start": 5276.699,
        "duration": 4.141,
        "text": "which um"
      },
      {
        "start": 5278.02,
        "duration": 5.28,
        "text": "uh I have not done a presentation on but"
      },
      {
        "start": 5280.84,
        "duration": 3.859,
        "text": "in this case what it's showing is you"
      },
      {
        "start": 5283.3,
        "duration": 4.74,
        "text": "can have jobs"
      },
      {
        "start": 5284.699,
        "duration": 6.52,
        "text": "triggered by S3 events"
      },
      {
        "start": 5288.04,
        "duration": 5.22,
        "text": "so in this case two different S3 data"
      },
      {
        "start": 5291.219,
        "duration": 5.221,
        "text": "sets when they get updated it triggers"
      },
      {
        "start": 5293.26,
        "duration": 5.28,
        "text": "this dag and then that DAC produces this"
      },
      {
        "start": 5296.44,
        "duration": 4.62,
        "text": "S3 uh"
      },
      {
        "start": 5298.54,
        "duration": 5.099,
        "text": "um you know object then that"
      },
      {
        "start": 5301.06,
        "duration": 4.56,
        "text": "uh gets used subsequently this is one"
      },
      {
        "start": 5303.639,
        "duration": 3.421,
        "text": "kind of dag where data getting updated"
      },
      {
        "start": 5305.62,
        "duration": 3.3,
        "text": "we'll we'll run this and then still"
      },
      {
        "start": 5307.06,
        "duration": 3.78,
        "text": "update this automatically right you"
      },
      {
        "start": 5308.92,
        "duration": 3.96,
        "text": "don't have to do this on a schedule"
      },
      {
        "start": 5310.84,
        "duration": 4.319,
        "text": "um same thing here you have two data"
      },
      {
        "start": 5312.88,
        "duration": 4.02,
        "text": "sets here you have another uh file here"
      },
      {
        "start": 5315.159,
        "duration": 3.961,
        "text": "and"
      },
      {
        "start": 5316.9,
        "duration": 3.9,
        "text": "um the outputs are all getting consumed"
      },
      {
        "start": 5319.12,
        "duration": 5.66,
        "text": "eventually to have two different outputs"
      },
      {
        "start": 5320.8,
        "duration": 7.26,
        "text": "so this is an example of it could it"
      },
      {
        "start": 5324.78,
        "duration": 5.14,
        "text": "could not just be ETL it could be I want"
      },
      {
        "start": 5328.06,
        "duration": 3.119,
        "text": "to take this information and I want to"
      },
      {
        "start": 5329.92,
        "duration": 2.759,
        "text": "enhance it I want to run some machine"
      },
      {
        "start": 5331.179,
        "duration": 4.02,
        "text": "learning process to classify information"
      },
      {
        "start": 5332.679,
        "duration": 3.421,
        "text": "I don't want to save the data here"
      },
      {
        "start": 5335.199,
        "duration": 2.761,
        "text": "um"
      },
      {
        "start": 5336.1,
        "duration": 3.9,
        "text": "yes so hopefully that answers your"
      },
      {
        "start": 5337.96,
        "duration": 3.42,
        "text": "question uh here's an example of a"
      },
      {
        "start": 5340.0,
        "duration": 2.219,
        "text": "workflow and actually we have example"
      },
      {
        "start": 5341.38,
        "duration": 2.58,
        "text": "code"
      },
      {
        "start": 5342.219,
        "duration": 7.44,
        "text": "where"
      },
      {
        "start": 5343.96,
        "duration": 8.4,
        "text": "um you have a uh a airflow DAC that will"
      },
      {
        "start": 5349.659,
        "duration": 6.06,
        "text": "create a Google dataproc cluster which"
      },
      {
        "start": 5352.36,
        "duration": 5.879,
        "text": "is a spark cluster okay and it will"
      },
      {
        "start": 5355.719,
        "duration": 5.041,
        "text": "create an aster database"
      },
      {
        "start": 5358.239,
        "duration": 6.121,
        "text": "and it will go and do some data process"
      },
      {
        "start": 5360.76,
        "duration": 5.76,
        "text": "with Astra on the spark cluster and then"
      },
      {
        "start": 5364.36,
        "duration": 3.66,
        "text": "it can destroy the spark cluster so"
      },
      {
        "start": 5366.52,
        "duration": 3.119,
        "text": "airflow can actually be used to"
      },
      {
        "start": 5368.02,
        "duration": 3.24,
        "text": "coordinate some of the infrastructure"
      },
      {
        "start": 5369.639,
        "duration": 3.921,
        "text": "that you need from an ephemeral basis"
      },
      {
        "start": 5371.26,
        "duration": 4.56,
        "text": "right when you have a long-running job"
      },
      {
        "start": 5373.56,
        "duration": 3.4,
        "text": "that needs hundreds of computers you"
      },
      {
        "start": 5375.82,
        "duration": 3.06,
        "text": "don't want those hundreds of computers"
      },
      {
        "start": 5376.96,
        "duration": 3.6,
        "text": "running in your spark cluster you can"
      },
      {
        "start": 5378.88,
        "duration": 2.759,
        "text": "use airflow to say make me a temporary"
      },
      {
        "start": 5380.56,
        "duration": 3.24,
        "text": "cluster"
      },
      {
        "start": 5381.639,
        "duration": 5.301,
        "text": "run the job and then go ahead and delete"
      },
      {
        "start": 5383.8,
        "duration": 3.14,
        "text": "it okay"
      },
      {
        "start": 5387.34,
        "duration": 4.62,
        "text": "um thank you I appreciate the question"
      },
      {
        "start": 5389.8,
        "duration": 4.74,
        "text": "um so we've got airflow running we've"
      },
      {
        "start": 5391.96,
        "duration": 4.98,
        "text": "validated that"
      },
      {
        "start": 5394.54,
        "duration": 3.96,
        "text": "um we also need to run spark so I'm"
      },
      {
        "start": 5396.94,
        "duration": 5.64,
        "text": "going to make a another"
      },
      {
        "start": 5398.5,
        "duration": 7.26,
        "text": "shella here for Simplicity and um"
      },
      {
        "start": 5402.58,
        "duration": 4.68,
        "text": "so this is not the way you would"
      },
      {
        "start": 5405.76,
        "duration": 5.479,
        "text": "um"
      },
      {
        "start": 5407.26,
        "duration": 3.979,
        "text": "you know run spark in production"
      },
      {
        "start": 5411.88,
        "duration": 5.819,
        "text": "um but it's you know your your container"
      },
      {
        "start": 5415.06,
        "duration": 5.46,
        "text": "your desktop this is perfectly fine"
      },
      {
        "start": 5417.699,
        "duration": 5.701,
        "text": "um and uh you know once we run the spark"
      },
      {
        "start": 5420.52,
        "duration": 5.4,
        "text": "Master it actually runs a web server"
      },
      {
        "start": 5423.4,
        "duration": 4.92,
        "text": "that gives us information about where to"
      },
      {
        "start": 5425.92,
        "duration": 3.54,
        "text": "connect our slave processes and it's"
      },
      {
        "start": 5428.32,
        "duration": 3.12,
        "text": "actually"
      },
      {
        "start": 5429.46,
        "duration": 4.56,
        "text": "if you look here"
      },
      {
        "start": 5431.44,
        "duration": 4.98,
        "text": "um it's it's going to be Port 8081 so if"
      },
      {
        "start": 5434.02,
        "duration": 4.38,
        "text": "you click on this"
      },
      {
        "start": 5436.42,
        "duration": 3.18,
        "text": "this is spark it's running so what we're"
      },
      {
        "start": 5438.4,
        "duration": 3.6,
        "text": "going to do is we're going to grab this"
      },
      {
        "start": 5439.6,
        "duration": 4.38,
        "text": "this is our local address for our for"
      },
      {
        "start": 5442.0,
        "duration": 3.36,
        "text": "our spark master and we're going to make"
      },
      {
        "start": 5443.98,
        "duration": 3.78,
        "text": "note of this and we're going to use it"
      },
      {
        "start": 5445.36,
        "duration": 4.859,
        "text": "in a few places okay uh and I'm just"
      },
      {
        "start": 5447.76,
        "duration": 4.2,
        "text": "going to drop it in my readme for uh"
      },
      {
        "start": 5450.219,
        "duration": 4.94,
        "text": "actually you know what I'm going to make"
      },
      {
        "start": 5451.96,
        "duration": 3.199,
        "text": "a new file called scratch"
      },
      {
        "start": 5458.199,
        "duration": 3.5,
        "text": "I'm going to just drop this in here"
      },
      {
        "start": 5462.0,
        "duration": 2.92,
        "text": "and"
      },
      {
        "start": 5463.54,
        "duration": 5.04,
        "text": "um the next thing I'm going to do is I'm"
      },
      {
        "start": 5464.92,
        "duration": 6.259,
        "text": "going to start a a slave process that is"
      },
      {
        "start": 5468.58,
        "duration": 2.599,
        "text": "going to connect"
      },
      {
        "start": 5471.34,
        "duration": 2.76,
        "text": "master"
      },
      {
        "start": 5472.48,
        "duration": 3.78,
        "text": "um and by the way if you're a user of"
      },
      {
        "start": 5474.1,
        "duration": 3.72,
        "text": "data Stacks Enterprise"
      },
      {
        "start": 5476.26,
        "duration": 4.14,
        "text": "um you don't have to do any of this"
      },
      {
        "start": 5477.82,
        "duration": 4.56,
        "text": "spark just can come up on the cluster"
      },
      {
        "start": 5480.4,
        "duration": 4.56,
        "text": "you enable it and it takes care of all"
      },
      {
        "start": 5482.38,
        "duration": 4.98,
        "text": "this and technically speaking you can"
      },
      {
        "start": 5484.96,
        "duration": 3.779,
        "text": "use airflow with data sex Enterprise"
      },
      {
        "start": 5487.36,
        "duration": 2.879,
        "text": "right because it has spark and Ask"
      },
      {
        "start": 5488.739,
        "duration": 2.521,
        "text": "Cassandra in it"
      },
      {
        "start": 5490.239,
        "duration": 4.621,
        "text": "um"
      },
      {
        "start": 5491.26,
        "duration": 6.479,
        "text": "in fact we've done a lot of that so"
      },
      {
        "start": 5494.86,
        "duration": 5.779,
        "text": "so you have here in your master I'm"
      },
      {
        "start": 5497.739,
        "duration": 2.9,
        "text": "going to refresh this"
      },
      {
        "start": 5501.4,
        "duration": 7.08,
        "text": "should be coming up soon"
      },
      {
        "start": 5503.86,
        "duration": 4.62,
        "text": "happen here no you didn't know yeah"
      },
      {
        "start": 5510.219,
        "duration": 3.201,
        "text": "um let's do a refresh"
      },
      {
        "start": 5514.0,
        "duration": 3.48,
        "text": "in a second okay there you go so we have"
      },
      {
        "start": 5516.219,
        "duration": 2.041,
        "text": "a worker"
      },
      {
        "start": 5517.48,
        "duration": 3.179,
        "text": "um"
      },
      {
        "start": 5518.26,
        "duration": 3.78,
        "text": "you know in a real environment you will"
      },
      {
        "start": 5520.659,
        "duration": 5.641,
        "text": "have"
      },
      {
        "start": 5522.04,
        "duration": 6.24,
        "text": "6 10 200 workers basically that have"
      },
      {
        "start": 5526.3,
        "duration": 3.78,
        "text": "their own amount of memory their amount"
      },
      {
        "start": 5528.28,
        "duration": 3.48,
        "text": "amount of cores"
      },
      {
        "start": 5530.08,
        "duration": 3.599,
        "text": "um and it's pretty impressive I mean I"
      },
      {
        "start": 5531.76,
        "duration": 5.1,
        "text": "don't know how I got this but I've got"
      },
      {
        "start": 5533.679,
        "duration": 4.681,
        "text": "64 64 gigs in my container so I can do a"
      },
      {
        "start": 5536.86,
        "duration": 2.94,
        "text": "lot of damage which doesn't seems"
      },
      {
        "start": 5538.36,
        "duration": 3.9,
        "text": "amazing the things you can do in guitar"
      },
      {
        "start": 5539.8,
        "duration": 5.399,
        "text": "right you know it's like and"
      },
      {
        "start": 5542.26,
        "duration": 3.78,
        "text": "yeah this is a lot I mean I know you can"
      },
      {
        "start": 5545.199,
        "duration": 2.761,
        "text": "run"
      },
      {
        "start": 5546.04,
        "duration": 4.94,
        "text": "you can run Cassandra on this container"
      },
      {
        "start": 5547.96,
        "duration": 3.02,
        "text": "if you wanted to yeah"
      },
      {
        "start": 5551.56,
        "duration": 6.36,
        "text": "so we've yeah so we've got uh air flow"
      },
      {
        "start": 5555.04,
        "duration": 5.4,
        "text": "right we got that running uh we've got"
      },
      {
        "start": 5557.92,
        "duration": 5.66,
        "text": "our spark cluster running and so the"
      },
      {
        "start": 5560.44,
        "duration": 7.5,
        "text": "next thing is we want to run a process"
      },
      {
        "start": 5563.58,
        "duration": 6.82,
        "text": "that is uh gonna run a spark job for us"
      },
      {
        "start": 5567.94,
        "duration": 4.02,
        "text": "um and before I I'll get that started"
      },
      {
        "start": 5570.4,
        "duration": 3.72,
        "text": "I'm just going to go over the code right"
      },
      {
        "start": 5571.96,
        "duration": 4.739,
        "text": "so we have here a dad"
      },
      {
        "start": 5574.12,
        "duration": 4.2,
        "text": "and I'm going to go over what it's doing"
      },
      {
        "start": 5576.699,
        "duration": 3.96,
        "text": "and that and we're also have also have"
      },
      {
        "start": 5578.32,
        "duration": 5.04,
        "text": "some code that is being coordinated by"
      },
      {
        "start": 5580.659,
        "duration": 4.861,
        "text": "the data so dag is a does it take that"
      },
      {
        "start": 5583.36,
        "duration": 5.7,
        "text": "directed basically"
      },
      {
        "start": 5585.52,
        "duration": 6.119,
        "text": "right and this code right here is it's a"
      },
      {
        "start": 5589.06,
        "duration": 5.4,
        "text": "task right I have a task that's using"
      },
      {
        "start": 5591.639,
        "duration": 5.641,
        "text": "this operator bash operator you can use"
      },
      {
        "start": 5594.46,
        "duration": 4.679,
        "text": "a spark operator you can use a"
      },
      {
        "start": 5597.28,
        "duration": 3.3,
        "text": "Google Cloud operator to do specific"
      },
      {
        "start": 5599.139,
        "duration": 3.961,
        "text": "things but this is a batch operator"
      },
      {
        "start": 5600.58,
        "duration": 4.02,
        "text": "that's just running a spark command I"
      },
      {
        "start": 5603.1,
        "duration": 3.36,
        "text": "could run the same exact command on my"
      },
      {
        "start": 5604.6,
        "duration": 5.76,
        "text": "computer I could put the same exact"
      },
      {
        "start": 5606.46,
        "duration": 6.96,
        "text": "command in my uh cronjour but I'm using"
      },
      {
        "start": 5610.36,
        "duration": 6.48,
        "text": "a bash operator to do that for me and in"
      },
      {
        "start": 5613.42,
        "duration": 7.14,
        "text": "my bachelor operator I'm saying run this"
      },
      {
        "start": 5616.84,
        "duration": 6.48,
        "text": "uh you know uh uh version of the"
      },
      {
        "start": 5620.56,
        "duration": 6.179,
        "text": "connector with my spark instance use"
      },
      {
        "start": 5623.32,
        "duration": 6.839,
        "text": "this properties file and"
      },
      {
        "start": 5626.739,
        "duration": 4.4,
        "text": "here's my extract and load"
      },
      {
        "start": 5630.159,
        "duration": 3.48,
        "text": "um"
      },
      {
        "start": 5631.139,
        "duration": 4.0,
        "text": "which we'll look at in a second here's"
      },
      {
        "start": 5633.639,
        "duration": 5.04,
        "text": "another task and I could have many many"
      },
      {
        "start": 5635.139,
        "duration": 5.941,
        "text": "tasks defined like this but the way"
      },
      {
        "start": 5638.679,
        "duration": 5.941,
        "text": "it knows which one to run for second is"
      },
      {
        "start": 5641.08,
        "duration": 6.3,
        "text": "this this syntax right here right so"
      },
      {
        "start": 5644.62,
        "duration": 5.099,
        "text": "this job is dependent on this job and"
      },
      {
        "start": 5647.38,
        "duration": 6.72,
        "text": "using the syntax you can create very"
      },
      {
        "start": 5649.719,
        "duration": 6.241,
        "text": "complex uh Dax so uh I'll show you one"
      },
      {
        "start": 5654.1,
        "duration": 5.16,
        "text": "uh quickly"
      },
      {
        "start": 5655.96,
        "duration": 6.38,
        "text": "uh that's a little bit more complex it's"
      },
      {
        "start": 5659.26,
        "duration": 3.08,
        "text": "one of the tutorial ones"
      },
      {
        "start": 5662.86,
        "duration": 6.18,
        "text": "and I'm going to click on code"
      },
      {
        "start": 5666.04,
        "duration": 4.38,
        "text": "lots of things like e extract"
      },
      {
        "start": 5669.04,
        "duration": 4.98,
        "text": "transform"
      },
      {
        "start": 5670.42,
        "duration": 5.7,
        "text": "and load right and it's got all these"
      },
      {
        "start": 5674.02,
        "duration": 4.44,
        "text": "other uh you know kind of like helpers"
      },
      {
        "start": 5676.12,
        "duration": 4.14,
        "text": "in here to tell it what the uh the"
      },
      {
        "start": 5678.46,
        "duration": 4.8,
        "text": "documentation is going to look like but"
      },
      {
        "start": 5680.26,
        "duration": 4.5,
        "text": "here's here's the dag the load task is"
      },
      {
        "start": 5683.26,
        "duration": 2.879,
        "text": "dependent on the transform tag the"
      },
      {
        "start": 5684.76,
        "duration": 2.64,
        "text": "transform tag is depend on the extract"
      },
      {
        "start": 5686.139,
        "duration": 2.52,
        "text": "tag"
      },
      {
        "start": 5687.4,
        "duration": 3.839,
        "text": "okay"
      },
      {
        "start": 5688.659,
        "duration": 4.641,
        "text": "and so I'm going to go back to"
      },
      {
        "start": 5691.239,
        "duration": 2.061,
        "text": "um"
      },
      {
        "start": 5693.94,
        "duration": 3.719,
        "text": "my spark"
      },
      {
        "start": 5695.199,
        "duration": 6.781,
        "text": "so let's take a look at extract and"
      },
      {
        "start": 5697.659,
        "duration": 6.781,
        "text": "loads extract and load is a standard"
      },
      {
        "start": 5701.98,
        "duration": 4.08,
        "text": "spark Python program there's nothing"
      },
      {
        "start": 5704.44,
        "duration": 4.739,
        "text": "special about this"
      },
      {
        "start": 5706.06,
        "duration": 6.179,
        "text": "you can run this on any spark instance"
      },
      {
        "start": 5709.179,
        "duration": 4.081,
        "text": "uh connected to Cassandra without an"
      },
      {
        "start": 5712.239,
        "duration": 2.0,
        "text": "issue"
      },
      {
        "start": 5713.26,
        "duration": 3.78,
        "text": "okay"
      },
      {
        "start": 5714.239,
        "duration": 7.361,
        "text": "and this other one"
      },
      {
        "start": 5717.04,
        "duration": 7.8,
        "text": "etl.py again standard job okay"
      },
      {
        "start": 5721.6,
        "duration": 5.639,
        "text": "um so in our case in our Dag"
      },
      {
        "start": 5724.84,
        "duration": 4.62,
        "text": "load and write is happening first which"
      },
      {
        "start": 5727.239,
        "duration": 4.201,
        "text": "is this one right extract and load and"
      },
      {
        "start": 5729.46,
        "duration": 3.3,
        "text": "then ETL which is ETL that Pi is"
      },
      {
        "start": 5731.44,
        "duration": 4.08,
        "text": "happening next so what's happening here"
      },
      {
        "start": 5732.76,
        "duration": 5.34,
        "text": "we're doing an extract and load"
      },
      {
        "start": 5735.52,
        "duration": 5.04,
        "text": "and we're taking a CSV file"
      },
      {
        "start": 5738.1,
        "duration": 4.74,
        "text": "which is here okay take a look at the"
      },
      {
        "start": 5740.56,
        "duration": 5.099,
        "text": "data a little big"
      },
      {
        "start": 5742.84,
        "duration": 5.64,
        "text": "lots of data in here"
      },
      {
        "start": 5745.659,
        "duration": 5.461,
        "text": "and we're taking it and we're selecting"
      },
      {
        "start": 5748.48,
        "duration": 5.88,
        "text": "the data from that CSV data frame and"
      },
      {
        "start": 5751.12,
        "duration": 5.88,
        "text": "we're putting it into"
      },
      {
        "start": 5754.36,
        "duration": 6.24,
        "text": "this data frame and we're saving it to"
      },
      {
        "start": 5757.0,
        "duration": 6.54,
        "text": "our Cassandra or instance or Aster"
      },
      {
        "start": 5760.6,
        "duration": 4.92,
        "text": "instance so how does it know where to to"
      },
      {
        "start": 5763.54,
        "duration": 5.82,
        "text": "put all this well there's a properties"
      },
      {
        "start": 5765.52,
        "duration": 6.6,
        "text": "file which has this information and we"
      },
      {
        "start": 5769.36,
        "duration": 4.74,
        "text": "need to update this to the right one"
      },
      {
        "start": 5772.12,
        "duration": 3.74,
        "text": "okay so we're gonna go back here and all"
      },
      {
        "start": 5774.1,
        "duration": 4.74,
        "text": "of these all of these instructions"
      },
      {
        "start": 5775.86,
        "duration": 5.26,
        "text": "anyway so including updating the"
      },
      {
        "start": 5778.84,
        "duration": 4.68,
        "text": "properties cons file yeah um just make"
      },
      {
        "start": 5781.12,
        "duration": 4.8,
        "text": "sure you have you know exactly details"
      },
      {
        "start": 5783.52,
        "duration": 4.679,
        "text": "from Astra and the master"
      },
      {
        "start": 5785.92,
        "duration": 5.219,
        "text": "um that's really what you need"
      },
      {
        "start": 5788.199,
        "duration": 4.381,
        "text": "here that's basically what you need and"
      },
      {
        "start": 5791.139,
        "duration": 3.661,
        "text": "you know you don't have to change this"
      },
      {
        "start": 5792.58,
        "duration": 3.84,
        "text": "because when we down we named it exactly"
      },
      {
        "start": 5794.8,
        "duration": 3.72,
        "text": "this right"
      },
      {
        "start": 5796.42,
        "duration": 4.319,
        "text": "um and we have our username and we have"
      },
      {
        "start": 5798.52,
        "duration": 5.34,
        "text": "our password and so this information is"
      },
      {
        "start": 5800.739,
        "duration": 5.94,
        "text": "going to come from that um the generated"
      },
      {
        "start": 5803.86,
        "duration": 5.7,
        "text": "information right the generated token so"
      },
      {
        "start": 5806.679,
        "duration": 4.681,
        "text": "we've got our"
      },
      {
        "start": 5809.56,
        "duration": 4.7,
        "text": "client ID and client secret so I'm going"
      },
      {
        "start": 5811.36,
        "duration": 2.9,
        "text": "to just go ahead and grab this"
      },
      {
        "start": 5814.78,
        "duration": 5.18,
        "text": "good thing I dropped that file in there"
      },
      {
        "start": 5816.639,
        "duration": 6.361,
        "text": "right uh you wouldn't leave it in there"
      },
      {
        "start": 5819.96,
        "duration": 4.42,
        "text": "right don't leave passwords in Source"
      },
      {
        "start": 5823.0,
        "duration": 3.48,
        "text": "control"
      },
      {
        "start": 5824.38,
        "duration": 4.259,
        "text": "yeah and and"
      },
      {
        "start": 5826.48,
        "duration": 4.86,
        "text": "even if you have it temporarily you know"
      },
      {
        "start": 5828.639,
        "duration": 4.861,
        "text": "remember not to check them in"
      },
      {
        "start": 5831.34,
        "duration": 3.899,
        "text": "exactly"
      },
      {
        "start": 5833.5,
        "duration": 4.26,
        "text": "um in fact I should put that in the git"
      },
      {
        "start": 5835.239,
        "duration": 5.121,
        "text": "ignore first leave out any generated"
      },
      {
        "start": 5837.76,
        "duration": 2.6,
        "text": "tokens"
      },
      {
        "start": 5842.38,
        "duration": 4.62,
        "text": "and so it it goes uh essentially"
      },
      {
        "start": 5846.159,
        "duration": 4.741,
        "text": "um"
      },
      {
        "start": 5847.0,
        "duration": 6.679,
        "text": "I think it's okay to do this in here"
      },
      {
        "start": 5850.9,
        "duration": 2.779,
        "text": "I feel like"
      },
      {
        "start": 5855.219,
        "duration": 3.781,
        "text": "um"
      },
      {
        "start": 5856.659,
        "duration": 3.841,
        "text": "everything else is set up for you so you"
      },
      {
        "start": 5859.0,
        "duration": 3.12,
        "text": "don't have to you know mess with it but"
      },
      {
        "start": 5860.5,
        "duration": 3.84,
        "text": "you can kind of it's self-explanatory"
      },
      {
        "start": 5862.12,
        "duration": 4.8,
        "text": "there's documentation on the spark"
      },
      {
        "start": 5864.34,
        "duration": 3.839,
        "text": "connector on GitHub uh data Stacks has"
      },
      {
        "start": 5866.92,
        "duration": 2.64,
        "text": "released that"
      },
      {
        "start": 5868.179,
        "duration": 3.361,
        "text": "um and then you know the rest of the"
      },
      {
        "start": 5869.56,
        "duration": 5.159,
        "text": "stuff again it's it's all spark stuff"
      },
      {
        "start": 5871.54,
        "duration": 4.86,
        "text": "but bear with me this is not a spark"
      },
      {
        "start": 5874.719,
        "duration": 4.44,
        "text": "talk it's about airflow I don't want to"
      },
      {
        "start": 5876.4,
        "duration": 5.58,
        "text": "get too uh too much into details and so"
      },
      {
        "start": 5879.159,
        "duration": 6.0,
        "text": "we take the data from the file put it"
      },
      {
        "start": 5881.98,
        "duration": 5.219,
        "text": "into the key space name that we have as"
      },
      {
        "start": 5885.159,
        "duration": 3.601,
        "text": "a configuration property and we put into"
      },
      {
        "start": 5887.199,
        "duration": 4.801,
        "text": "this table so once it's inside previous"
      },
      {
        "start": 5888.76,
        "duration": 4.86,
        "text": "employees by by job title the next job"
      },
      {
        "start": 5892.0,
        "duration": 2.94,
        "text": "will be run for us"
      },
      {
        "start": 5893.62,
        "duration": 4.559,
        "text": "which is"
      },
      {
        "start": 5894.94,
        "duration": 5.04,
        "text": "etl.py so what etl.py is doing is saying"
      },
      {
        "start": 5898.179,
        "duration": 5.221,
        "text": "okay well let's go ahead and connect to"
      },
      {
        "start": 5899.98,
        "duration": 7.259,
        "text": "the e-space and the database"
      },
      {
        "start": 5903.4,
        "duration": 8.759,
        "text": "and we use this tool to make it easy for"
      },
      {
        "start": 5907.239,
        "duration": 6.661,
        "text": "us to run spark SQL so in spark SQL you"
      },
      {
        "start": 5912.159,
        "duration": 3.361,
        "text": "have to connect it to account"
      },
      {
        "start": 5913.9,
        "duration": 2.759,
        "text": "catalog again if you're using data"
      },
      {
        "start": 5915.52,
        "duration": 2.76,
        "text": "Stacks Enterprise you don't have to do"
      },
      {
        "start": 5916.659,
        "duration": 3.121,
        "text": "any of this you just go into spark and"
      },
      {
        "start": 5918.28,
        "duration": 3.18,
        "text": "you can use it"
      },
      {
        "start": 5919.78,
        "duration": 3.959,
        "text": "um but what we're doing is we're doing"
      },
      {
        "start": 5921.46,
        "duration": 4.02,
        "text": "an SQL query that you could not do in"
      },
      {
        "start": 5923.739,
        "duration": 4.021,
        "text": "cql"
      },
      {
        "start": 5925.48,
        "duration": 6.0,
        "text": "right we're running in at like a"
      },
      {
        "start": 5927.76,
        "duration": 6.36,
        "text": "legitimate SQL query with some uh you"
      },
      {
        "start": 5931.48,
        "duration": 4.02,
        "text": "know absolute value uh with a date"
      },
      {
        "start": 5934.12,
        "duration": 3.0,
        "text": "difference between the first day and the"
      },
      {
        "start": 5935.5,
        "duration": 3.9,
        "text": "last day and it's calculating this for"
      },
      {
        "start": 5937.12,
        "duration": 3.599,
        "text": "us right you couldn't do that I guess"
      },
      {
        "start": 5939.4,
        "duration": 3.72,
        "text": "you could but you wouldn't want to do it"
      },
      {
        "start": 5940.719,
        "duration": 4.92,
        "text": "in cql sh with user-defined functions"
      },
      {
        "start": 5943.12,
        "duration": 4.559,
        "text": "um and we see we take that data"
      },
      {
        "start": 5945.639,
        "duration": 3.721,
        "text": "and then we save it to this other table"
      },
      {
        "start": 5947.679,
        "duration": 4.5,
        "text": "right that's it that's all the whole"
      },
      {
        "start": 5949.36,
        "duration": 5.46,
        "text": "thing is doing get data from CSV dump it"
      },
      {
        "start": 5952.179,
        "duration": 4.201,
        "text": "into one table do some calculation and"
      },
      {
        "start": 5954.82,
        "duration": 4.2,
        "text": "then put into another table"
      },
      {
        "start": 5956.38,
        "duration": 4.859,
        "text": "right and you would think well you know"
      },
      {
        "start": 5959.02,
        "duration": 3.659,
        "text": "what I don't I don't need anything I"
      },
      {
        "start": 5961.239,
        "duration": 3.181,
        "text": "could just do it as part of a spark job"
      },
      {
        "start": 5962.679,
        "duration": 4.98,
        "text": "but then what if this needs to happen on"
      },
      {
        "start": 5964.42,
        "duration": 4.62,
        "text": "a cyclical basis every six hours a Cron"
      },
      {
        "start": 5967.659,
        "duration": 3.841,
        "text": "job"
      },
      {
        "start": 5969.04,
        "duration": 4.679,
        "text": "um is not distributed it runs only on"
      },
      {
        "start": 5971.5,
        "duration": 4.739,
        "text": "that one computer well if you have a"
      },
      {
        "start": 5973.719,
        "duration": 5.341,
        "text": "high availability airflow setup with"
      },
      {
        "start": 5976.239,
        "duration": 5.46,
        "text": "postgres with or salary uh with many"
      },
      {
        "start": 5979.06,
        "duration": 4.619,
        "text": "many workers that job will get executed"
      },
      {
        "start": 5981.699,
        "duration": 3.241,
        "text": "no matter what it will make sure it gets"
      },
      {
        "start": 5983.679,
        "duration": 4.5,
        "text": "done right"
      },
      {
        "start": 5984.94,
        "duration": 4.98,
        "text": "so we've got our property set up we know"
      },
      {
        "start": 5988.179,
        "duration": 3.661,
        "text": "what our dags are"
      },
      {
        "start": 5989.92,
        "duration": 3.779,
        "text": "um but there's one missing piece in"
      },
      {
        "start": 5991.84,
        "duration": 6.12,
        "text": "order for"
      },
      {
        "start": 5993.699,
        "duration": 6.721,
        "text": "um airflow to know about your your job"
      },
      {
        "start": 5997.96,
        "duration": 6.06,
        "text": "um you have to copy this file into the"
      },
      {
        "start": 6000.42,
        "duration": 5.04,
        "text": "right folder and this is uh you know I"
      },
      {
        "start": 6004.02,
        "duration": 2.82,
        "text": "used to be like why do I have to copy"
      },
      {
        "start": 6005.46,
        "duration": 3.6,
        "text": "this file why doesn't this look for it"
      },
      {
        "start": 6006.84,
        "duration": 4.92,
        "text": "well this is what allows you to edit"
      },
      {
        "start": 6009.06,
        "duration": 4.56,
        "text": "files play with them uh make your"
      },
      {
        "start": 6011.76,
        "duration": 4.919,
        "text": "changes and then when you're ready you"
      },
      {
        "start": 6013.62,
        "duration": 5.46,
        "text": "copy it and so from a devops perspective"
      },
      {
        "start": 6016.679,
        "duration": 5.281,
        "text": "you can set up a process where in GitHub"
      },
      {
        "start": 6019.08,
        "duration": 5.34,
        "text": "or gitlab right you can have a branch"
      },
      {
        "start": 6021.96,
        "duration": 3.84,
        "text": "that if you do a pull request or a merge"
      },
      {
        "start": 6024.42,
        "duration": 2.819,
        "text": "request into that Branch they can"
      },
      {
        "start": 6025.8,
        "duration": 3.54,
        "text": "automatically do what I'm about to do"
      },
      {
        "start": 6027.239,
        "duration": 5.521,
        "text": "for you and that's the way we do it"
      },
      {
        "start": 6029.34,
        "duration": 5.879,
        "text": "right a merge request gets approved it"
      },
      {
        "start": 6032.76,
        "duration": 3.84,
        "text": "gets copied into the right folder and"
      },
      {
        "start": 6035.219,
        "duration": 4.201,
        "text": "all of a sudden the users can start"
      },
      {
        "start": 6036.6,
        "duration": 4.92,
        "text": "using that so where does this need to go"
      },
      {
        "start": 6039.42,
        "duration": 4.14,
        "text": "uh there's a folder that we're going to"
      },
      {
        "start": 6041.52,
        "duration": 3.42,
        "text": "make which is uh airflow is already"
      },
      {
        "start": 6043.56,
        "duration": 2.7,
        "text": "configured to look for it but we don't"
      },
      {
        "start": 6044.94,
        "duration": 4.5,
        "text": "have the folder"
      },
      {
        "start": 6046.26,
        "duration": 4.379,
        "text": "and we're going to just say it's"
      },
      {
        "start": 6049.44,
        "duration": 2.82,
        "text": "um oops"
      },
      {
        "start": 6050.639,
        "duration": 2.941,
        "text": "we don't have it in there so I'm just"
      },
      {
        "start": 6052.26,
        "duration": 2.939,
        "text": "going to make"
      },
      {
        "start": 6053.58,
        "duration": 4.92,
        "text": "sure"
      },
      {
        "start": 6055.199,
        "duration": 4.98,
        "text": "okay and I'm going to copy this spark"
      },
      {
        "start": 6058.5,
        "duration": 4.5,
        "text": "dag which I showed you earlier to there"
      },
      {
        "start": 6060.179,
        "duration": 5.52,
        "text": "so I'm just going to say copy"
      },
      {
        "start": 6063.0,
        "duration": 5.52,
        "text": "I'm not going to move it uh okay you"
      },
      {
        "start": 6065.699,
        "duration": 4.561,
        "text": "know what I'll move it yeah I'll do with"
      },
      {
        "start": 6068.52,
        "duration": 3.92,
        "text": "instructions say that should be okay as"
      },
      {
        "start": 6070.26,
        "duration": 2.18,
        "text": "well yeah"
      },
      {
        "start": 6074.639,
        "duration": 6.481,
        "text": "okay I moved it and what we're going to"
      },
      {
        "start": 6078.36,
        "duration": 5.52,
        "text": "do is we're going to be waiting on"
      },
      {
        "start": 6081.12,
        "duration": 3.96,
        "text": "uh this you we don't want to wait five"
      },
      {
        "start": 6083.88,
        "duration": 2.819,
        "text": "minutes so what you can do is you can"
      },
      {
        "start": 6085.08,
        "duration": 4.02,
        "text": "just say refresh"
      },
      {
        "start": 6086.699,
        "duration": 3.601,
        "text": "and then a few seconds"
      },
      {
        "start": 6089.1,
        "duration": 4.26,
        "text": "um"
      },
      {
        "start": 6090.3,
        "duration": 4.319,
        "text": "we should see our dag so how do you know"
      },
      {
        "start": 6093.36,
        "duration": 3.48,
        "text": "what to look for"
      },
      {
        "start": 6094.619,
        "duration": 4.681,
        "text": "oh you have named it very specifically"
      },
      {
        "start": 6096.84,
        "duration": 5.339,
        "text": "we have named it as example Cassandra"
      },
      {
        "start": 6099.3,
        "duration": 5.04,
        "text": "ETL right and"
      },
      {
        "start": 6102.179,
        "duration": 4.761,
        "text": "I can just look forward to Sandra"
      },
      {
        "start": 6104.34,
        "duration": 2.6,
        "text": "because I think"
      },
      {
        "start": 6107.04,
        "duration": 4.32,
        "text": "yeah so I'm just going to go ahead and"
      },
      {
        "start": 6108.239,
        "duration": 5.581,
        "text": "do refresh a few seconds"
      },
      {
        "start": 6111.36,
        "duration": 4.319,
        "text": "and it it takes a few uh seconds"
      },
      {
        "start": 6113.82,
        "duration": 4.62,
        "text": "sometimes because this is not the high"
      },
      {
        "start": 6115.679,
        "duration": 4.081,
        "text": "availability uh version of it"
      },
      {
        "start": 6118.44,
        "duration": 4.46,
        "text": "um but"
      },
      {
        "start": 6119.76,
        "duration": 3.14,
        "text": "it does come up"
      },
      {
        "start": 6124.86,
        "duration": 2.66,
        "text": "um come on"
      },
      {
        "start": 6129.06,
        "duration": 4.44,
        "text": "give it a second and while we're waiting"
      },
      {
        "start": 6131.4,
        "duration": 4.319,
        "text": "for it to come up"
      },
      {
        "start": 6133.5,
        "duration": 6.84,
        "text": "um we do need to"
      },
      {
        "start": 6135.719,
        "duration": 6.541,
        "text": "um set up a connection inside uh airflow"
      },
      {
        "start": 6140.34,
        "duration": 5.279,
        "text": "um so"
      },
      {
        "start": 6142.26,
        "duration": 6.84,
        "text": "back here when we're looking at the DAC"
      },
      {
        "start": 6145.619,
        "duration": 4.921,
        "text": "okay there is a value uh we were using"
      },
      {
        "start": 6149.1,
        "duration": 4.32,
        "text": "for"
      },
      {
        "start": 6150.54,
        "duration": 7.02,
        "text": "um the connection okay and a connection"
      },
      {
        "start": 6153.42,
        "duration": 6.42,
        "text": "is basically something that is reused as"
      },
      {
        "start": 6157.56,
        "duration": 4.26,
        "text": "a variable but it's a special type of"
      },
      {
        "start": 6159.84,
        "duration": 4.68,
        "text": "variable it's a it's a variable that is"
      },
      {
        "start": 6161.82,
        "duration": 4.5,
        "text": "used for operators that have data in"
      },
      {
        "start": 6164.52,
        "duration": 3.3,
        "text": "there okay but it could be a system or a"
      },
      {
        "start": 6166.32,
        "duration": 3.2,
        "text": "service as well"
      },
      {
        "start": 6167.82,
        "duration": 5.819,
        "text": "um so"
      },
      {
        "start": 6169.52,
        "duration": 7.199,
        "text": "in my let's see"
      },
      {
        "start": 6173.639,
        "duration": 3.08,
        "text": "here we go"
      },
      {
        "start": 6201.3,
        "duration": 3.899,
        "text": "um"
      },
      {
        "start": 6202.26,
        "duration": 5.22,
        "text": "anyways it's it's a it's a variable in"
      },
      {
        "start": 6205.199,
        "duration": 4.02,
        "text": "our case which is like our spark master"
      },
      {
        "start": 6207.48,
        "duration": 4.44,
        "text": "of default"
      },
      {
        "start": 6209.219,
        "duration": 4.5,
        "text": "um and we're basically going to set it"
      },
      {
        "start": 6211.92,
        "duration": 3.36,
        "text": "in airflow so we can reuse that data"
      },
      {
        "start": 6213.719,
        "duration": 4.44,
        "text": "later on okay"
      },
      {
        "start": 6215.28,
        "duration": 6.3,
        "text": "um so admin section has variables"
      },
      {
        "start": 6218.159,
        "duration": 4.861,
        "text": "configurations and connections amongst"
      },
      {
        "start": 6221.58,
        "duration": 3.96,
        "text": "other things we're going to go to"
      },
      {
        "start": 6223.02,
        "duration": 5.3,
        "text": "connections"
      },
      {
        "start": 6225.54,
        "duration": 2.78,
        "text": "and"
      },
      {
        "start": 6229.56,
        "duration": 3.48,
        "text": "right now it just has yarn assuming that"
      },
      {
        "start": 6231.719,
        "duration": 2.46,
        "text": "it's going to run on a Hadoop cluster"
      },
      {
        "start": 6233.04,
        "duration": 3.32,
        "text": "but we're going to go ahead and edit"
      },
      {
        "start": 6234.179,
        "duration": 2.181,
        "text": "this"
      },
      {
        "start": 6236.639,
        "duration": 5.701,
        "text": "and we're just going to update the host"
      },
      {
        "start": 6239.46,
        "duration": 5.659,
        "text": "to our"
      },
      {
        "start": 6242.34,
        "duration": 2.779,
        "text": "product master"
      },
      {
        "start": 6247.5,
        "duration": 2.36,
        "text": "okay"
      },
      {
        "start": 6253.139,
        "duration": 3.181,
        "text": "and you know in the background what it's"
      },
      {
        "start": 6255.0,
        "duration": 2.639,
        "text": "done for us it's kind of set out an"
      },
      {
        "start": 6256.32,
        "duration": 4.2,
        "text": "environment variables so whenever we're"
      },
      {
        "start": 6257.639,
        "duration": 4.98,
        "text": "running spark from bash it'll always"
      },
      {
        "start": 6260.52,
        "duration": 3.48,
        "text": "send it to that particular uh spark"
      },
      {
        "start": 6262.619,
        "duration": 4.321,
        "text": "master and if you notice there's many"
      },
      {
        "start": 6264.0,
        "duration": 5.219,
        "text": "different types of connections web hdfs"
      },
      {
        "start": 6266.94,
        "duration": 5.34,
        "text": "redshift"
      },
      {
        "start": 6269.219,
        "duration": 5.281,
        "text": "um you know Livy uh Cassandra right"
      },
      {
        "start": 6272.28,
        "duration": 3.72,
        "text": "we're not using Cassandra directly we're"
      },
      {
        "start": 6274.5,
        "duration": 3.3,
        "text": "using spark but you can you can connect"
      },
      {
        "start": 6276.0,
        "duration": 3.719,
        "text": "it to Cassandra cluster"
      },
      {
        "start": 6277.8,
        "duration": 3.66,
        "text": "um lots of different existing types of"
      },
      {
        "start": 6279.719,
        "duration": 3.721,
        "text": "connectors in here"
      },
      {
        "start": 6281.46,
        "duration": 3.84,
        "text": "um variables are things that you would"
      },
      {
        "start": 6283.44,
        "duration": 3.54,
        "text": "use over and over again that are not"
      },
      {
        "start": 6285.3,
        "duration": 3.3,
        "text": "connections"
      },
      {
        "start": 6286.98,
        "duration": 4.86,
        "text": "um and then configurations are"
      },
      {
        "start": 6288.6,
        "duration": 5.639,
        "text": "specifically inside airflow.cfg so in"
      },
      {
        "start": 6291.84,
        "duration": 4.08,
        "text": "this case for security reasons we're not"
      },
      {
        "start": 6294.239,
        "duration": 3.361,
        "text": "exposing it but if you did you would be"
      },
      {
        "start": 6295.92,
        "duration": 6.779,
        "text": "able to see the contents of this airflow"
      },
      {
        "start": 6297.6,
        "duration": 7.26,
        "text": "CFG okay remember this connections"
      },
      {
        "start": 6302.699,
        "duration": 5.781,
        "text": "all right let's see if our Cassandra job"
      },
      {
        "start": 6304.86,
        "duration": 3.62,
        "text": "uh got shown here"
      },
      {
        "start": 6310.739,
        "duration": 9.301,
        "text": "make sure our uh scheduler"
      },
      {
        "start": 6315.48,
        "duration": 6.0,
        "text": "yep okay it's still running and it is uh"
      },
      {
        "start": 6320.04,
        "duration": 2.88,
        "text": "always kind of like looking in the"
      },
      {
        "start": 6321.48,
        "duration": 2.94,
        "text": "background to see if there's stuff in"
      },
      {
        "start": 6322.92,
        "duration": 5.52,
        "text": "here"
      },
      {
        "start": 6324.42,
        "duration": 5.88,
        "text": "we go I told you uh so we can filter it"
      },
      {
        "start": 6328.44,
        "duration": 5.1,
        "text": "and if I find the exact thing it'll"
      },
      {
        "start": 6330.3,
        "duration": 6.0,
        "text": "bring it up here so we have now the code"
      },
      {
        "start": 6333.54,
        "duration": 6.119,
        "text": "that we uh were looking at earlier you"
      },
      {
        "start": 6336.3,
        "duration": 5.7,
        "text": "know instance right so we've got an ETL"
      },
      {
        "start": 6339.659,
        "duration": 4.741,
        "text": "job this is one View"
      },
      {
        "start": 6342.0,
        "duration": 4.44,
        "text": "and we have the graph"
      },
      {
        "start": 6344.4,
        "duration": 4.259,
        "text": "it's not a complicated job it's just one"
      },
      {
        "start": 6346.44,
        "duration": 4.44,
        "text": "and then the next thing"
      },
      {
        "start": 6348.659,
        "duration": 4.98,
        "text": "um and if I want to double check to make"
      },
      {
        "start": 6350.88,
        "duration": 4.259,
        "text": "sure like is it the right code yeah it"
      },
      {
        "start": 6353.639,
        "duration": 3.301,
        "text": "looks exactly like the code that I was"
      },
      {
        "start": 6355.139,
        "duration": 4.321,
        "text": "looking at earlier right"
      },
      {
        "start": 6356.94,
        "duration": 4.259,
        "text": "um so what's the interval so in this"
      },
      {
        "start": 6359.46,
        "duration": 4.56,
        "text": "case we did not put an interval because"
      },
      {
        "start": 6361.199,
        "duration": 5.221,
        "text": "we want to run this ad hoc but we can"
      },
      {
        "start": 6364.02,
        "duration": 4.98,
        "text": "look at other examples to say well how"
      },
      {
        "start": 6366.42,
        "duration": 4.199,
        "text": "could uh you know what does that job"
      },
      {
        "start": 6369.0,
        "duration": 4.02,
        "text": "look like that I want to run daily or"
      },
      {
        "start": 6370.619,
        "duration": 3.661,
        "text": "what I want to run every minute or"
      },
      {
        "start": 6373.02,
        "duration": 3.3,
        "text": "something like that right so these are"
      },
      {
        "start": 6374.28,
        "duration": 5.72,
        "text": "Chron syntaxes"
      },
      {
        "start": 6376.32,
        "duration": 3.68,
        "text": "and if I look in here"
      },
      {
        "start": 6383.4,
        "duration": 5.58,
        "text": "that is basically being set at your"
      },
      {
        "start": 6386.219,
        "duration": 4.261,
        "text": "schedule right it's one way to set it um"
      },
      {
        "start": 6388.98,
        "duration": 3.659,
        "text": "or or when is the next scheduled"
      },
      {
        "start": 6390.48,
        "duration": 4.679,
        "text": "instance so"
      },
      {
        "start": 6392.639,
        "duration": 3.841,
        "text": "lots to dig deep into the uh the"
      },
      {
        "start": 6395.159,
        "duration": 3.54,
        "text": "configuration of a DAC but I'm just"
      },
      {
        "start": 6396.48,
        "duration": 4.5,
        "text": "going to go ahead and run my DAC and"
      },
      {
        "start": 6398.699,
        "duration": 4.401,
        "text": "cross my fingers that"
      },
      {
        "start": 6400.98,
        "duration": 4.679,
        "text": "is so I'm pretty sure it's gonna work"
      },
      {
        "start": 6403.1,
        "duration": 4.96,
        "text": "rags and Stefano testing the heck out of"
      },
      {
        "start": 6405.659,
        "duration": 4.861,
        "text": "it if you want what I can do is you can"
      },
      {
        "start": 6408.06,
        "duration": 4.44,
        "text": "go to Astra and make sure nothing you"
      },
      {
        "start": 6410.52,
        "duration": 4.56,
        "text": "know that no table is created right and"
      },
      {
        "start": 6412.5,
        "duration": 5.46,
        "text": "then you know once it runs you can show"
      },
      {
        "start": 6415.08,
        "duration": 4.559,
        "text": "that you know the tables got created up"
      },
      {
        "start": 6417.96,
        "duration": 5.04,
        "text": "to you yeah"
      },
      {
        "start": 6419.639,
        "duration": 7.201,
        "text": "yeah no exactly so uh you know I'm gonna"
      },
      {
        "start": 6423.0,
        "duration": 9.179,
        "text": "run a um to see a run a command"
      },
      {
        "start": 6426.84,
        "duration": 6.899,
        "text": "um in my uh Master shell"
      },
      {
        "start": 6432.179,
        "duration": 2.281,
        "text": "right"
      },
      {
        "start": 6433.739,
        "duration": 6.601,
        "text": "um"
      },
      {
        "start": 6434.46,
        "duration": 5.88,
        "text": "and I'm gonna just say select Star right"
      },
      {
        "start": 6445.26,
        "duration": 3.78,
        "text": "all right there's nothing in there yeah"
      },
      {
        "start": 6447.6,
        "duration": 2.82,
        "text": "so we know there's nothing there the"
      },
      {
        "start": 6449.04,
        "duration": 3.36,
        "text": "table is there though"
      },
      {
        "start": 6450.42,
        "duration": 3.719,
        "text": "right good"
      },
      {
        "start": 6452.4,
        "duration": 4.38,
        "text": "and um"
      },
      {
        "start": 6454.139,
        "duration": 5.821,
        "text": "um you can enable and run the the job"
      },
      {
        "start": 6456.78,
        "duration": 5.64,
        "text": "from here or you can enable and run the"
      },
      {
        "start": 6459.96,
        "duration": 4.38,
        "text": "job from here if you if you find it in"
      },
      {
        "start": 6462.42,
        "duration": 6.18,
        "text": "your big list"
      },
      {
        "start": 6464.34,
        "duration": 6.48,
        "text": "so you can enable it so let's unpause it"
      },
      {
        "start": 6468.6,
        "duration": 5.039,
        "text": "and unpausing it won't do anything for"
      },
      {
        "start": 6470.82,
        "duration": 5.46,
        "text": "us because it's not a an a schedule to"
      },
      {
        "start": 6473.639,
        "duration": 5.821,
        "text": "do this in any particular interval so"
      },
      {
        "start": 6476.28,
        "duration": 4.98,
        "text": "I'm going to go ahead and"
      },
      {
        "start": 6479.46,
        "duration": 4.44,
        "text": "trigger it"
      },
      {
        "start": 6481.26,
        "duration": 4.08,
        "text": "and I can trigger it as is"
      },
      {
        "start": 6483.9,
        "duration": 3.239,
        "text": "where I can trigger it with"
      },
      {
        "start": 6485.34,
        "duration": 4.5,
        "text": "configuration meaning I can override"
      },
      {
        "start": 6487.139,
        "duration": 4.381,
        "text": "certain settings if I wanted it to"
      },
      {
        "start": 6489.84,
        "duration": 3.06,
        "text": "to run slightly differently like a"
      },
      {
        "start": 6491.52,
        "duration": 5.28,
        "text": "different table or different key space"
      },
      {
        "start": 6492.9,
        "duration": 6.0,
        "text": "and I haven't written the uh the dag in"
      },
      {
        "start": 6496.8,
        "duration": 3.96,
        "text": "a way to take external configuration or"
      },
      {
        "start": 6498.9,
        "duration": 3.239,
        "text": "external variables like that but you can"
      },
      {
        "start": 6500.76,
        "duration": 5.28,
        "text": "do that"
      },
      {
        "start": 6502.139,
        "duration": 6.361,
        "text": "um it's running right uh in here and I"
      },
      {
        "start": 6506.04,
        "duration": 4.8,
        "text": "can go ahead and click this directly and"
      },
      {
        "start": 6508.5,
        "duration": 4.5,
        "text": "it's running the dag and this is the job"
      },
      {
        "start": 6510.84,
        "duration": 3.54,
        "text": "that it's running if I wanted to see"
      },
      {
        "start": 6513.0,
        "duration": 3.56,
        "text": "review"
      },
      {
        "start": 6514.38,
        "duration": 6.96,
        "text": "all right"
      },
      {
        "start": 6516.56,
        "duration": 6.639,
        "text": "so again does a you know a question"
      },
      {
        "start": 6521.34,
        "duration": 3.299,
        "text": "about one question about sparked"
      },
      {
        "start": 6523.199,
        "duration": 3.781,
        "text": "Cassandra integration typically in"
      },
      {
        "start": 6524.639,
        "duration": 4.201,
        "text": "Cassandra we design tables meant for"
      },
      {
        "start": 6526.98,
        "duration": 4.259,
        "text": "specific query right you know which"
      },
      {
        "start": 6528.84,
        "duration": 4.2,
        "text": "makes absolute sense uh because you"
      },
      {
        "start": 6531.239,
        "duration": 5.821,
        "text": "design based on use cases but here we"
      },
      {
        "start": 6533.04,
        "duration": 6.84,
        "text": "are making any type of SQL query uh SQL"
      },
      {
        "start": 6537.06,
        "duration": 5.52,
        "text": "query uh what kind of load does it put"
      },
      {
        "start": 6539.88,
        "duration": 5.339,
        "text": "on Cassandra"
      },
      {
        "start": 6542.58,
        "duration": 6.48,
        "text": "great great question"
      },
      {
        "start": 6545.219,
        "duration": 7.141,
        "text": "um and so the way that spark works with"
      },
      {
        "start": 6549.06,
        "duration": 7.5,
        "text": "Cassandra with the spark connector is"
      },
      {
        "start": 6552.36,
        "duration": 8.04,
        "text": "that it is token aware generally"
      },
      {
        "start": 6556.56,
        "duration": 5.04,
        "text": "um and it will go and it'll say go to"
      },
      {
        "start": 6560.4,
        "duration": 4.799,
        "text": "the whole thing"
      },
      {
        "start": 6561.6,
        "duration": 5.039,
        "text": "with my token ranges grab the data and"
      },
      {
        "start": 6565.199,
        "duration": 3.96,
        "text": "if there's not a predicate meaning if"
      },
      {
        "start": 6566.639,
        "duration": 4.621,
        "text": "there's not an index in there"
      },
      {
        "start": 6569.159,
        "duration": 4.08,
        "text": "um it will pull all the data and it will"
      },
      {
        "start": 6571.26,
        "duration": 3.899,
        "text": "do the filter on the spark side"
      },
      {
        "start": 6573.239,
        "duration": 4.38,
        "text": "that's that's the way Big Data works if"
      },
      {
        "start": 6575.159,
        "duration": 4.5,
        "text": "you don't have a a way to filter it on"
      },
      {
        "start": 6577.619,
        "duration": 4.401,
        "text": "the on the system itself"
      },
      {
        "start": 6579.659,
        "duration": 5.701,
        "text": "um so"
      },
      {
        "start": 6582.02,
        "duration": 5.86,
        "text": "if you have a star schema you can do"
      },
      {
        "start": 6585.36,
        "duration": 5.22,
        "text": "joins in spark and there's no load but"
      },
      {
        "start": 6587.88,
        "duration": 4.38,
        "text": "when you're doing something like in our"
      },
      {
        "start": 6590.58,
        "duration": 3.84,
        "text": "case we're doing a full scan and a full"
      },
      {
        "start": 6592.26,
        "duration": 4.32,
        "text": "you know processing and a full save it's"
      },
      {
        "start": 6594.42,
        "duration": 5.58,
        "text": "getting all the data but"
      },
      {
        "start": 6596.58,
        "duration": 6.42,
        "text": "it breaks apart the work and it doesn't"
      },
      {
        "start": 6600.0,
        "duration": 5.58,
        "text": "do a cql command the way you think about"
      },
      {
        "start": 6603.0,
        "duration": 4.26,
        "text": "it right it's not doing a select star on"
      },
      {
        "start": 6605.58,
        "duration": 4.619,
        "text": "one computer it's doing a select star"
      },
      {
        "start": 6607.26,
        "duration": 4.62,
        "text": "but it's doing it by token ranges based"
      },
      {
        "start": 6610.199,
        "duration": 4.561,
        "text": "on the loan of that machine and it grabs"
      },
      {
        "start": 6611.88,
        "duration": 5.04,
        "text": "it brings it together parallelizes the"
      },
      {
        "start": 6614.76,
        "duration": 4.14,
        "text": "work dust up in memory and then it goes"
      },
      {
        "start": 6616.92,
        "duration": 4.02,
        "text": "ahead and saves it back"
      },
      {
        "start": 6618.9,
        "duration": 3.38,
        "text": "um great great question"
      },
      {
        "start": 6620.94,
        "duration": 3.6,
        "text": "um so"
      },
      {
        "start": 6622.28,
        "duration": 5.14,
        "text": "unfortunately but fortunately for us we"
      },
      {
        "start": 6624.54,
        "duration": 4.98,
        "text": "hit an error but what happened I want to"
      },
      {
        "start": 6627.42,
        "duration": 3.84,
        "text": "know what happened so let's take a look"
      },
      {
        "start": 6629.52,
        "duration": 4.86,
        "text": "at our audit log"
      },
      {
        "start": 6631.26,
        "duration": 3.72,
        "text": "and what it's saying is"
      },
      {
        "start": 6634.38,
        "duration": 4.319,
        "text": "um"
      },
      {
        "start": 6634.98,
        "duration": 8.04,
        "text": "okay it started running up here and"
      },
      {
        "start": 6638.699,
        "duration": 7.861,
        "text": "there was an issue I think probably here"
      },
      {
        "start": 6643.02,
        "duration": 5.82,
        "text": "so let's take a look at"
      },
      {
        "start": 6646.56,
        "duration": 6.96,
        "text": "what what happened it said an upstream"
      },
      {
        "start": 6648.84,
        "duration": 6.839,
        "text": "command failed right and I'm gonna go"
      },
      {
        "start": 6653.52,
        "duration": 4.139,
        "text": "ahead back to the graph View"
      },
      {
        "start": 6655.679,
        "duration": 4.44,
        "text": "and I'm going to say what happened here"
      },
      {
        "start": 6657.659,
        "duration": 5.281,
        "text": "right let's click on this and I can go"
      },
      {
        "start": 6660.119,
        "duration": 6.54,
        "text": "to the log for that particular specific"
      },
      {
        "start": 6662.94,
        "duration": 6.719,
        "text": "task and it said"
      },
      {
        "start": 6666.659,
        "duration": 3.0,
        "text": "da"
      },
      {
        "start": 6670.98,
        "duration": 4.5,
        "text": "um"
      },
      {
        "start": 6672.179,
        "duration": 6.06,
        "text": "please install it okay the example"
      },
      {
        "start": 6675.48,
        "duration": 6.36,
        "text": "executor requires"
      },
      {
        "start": 6678.239,
        "duration": 5.46,
        "text": "blah blah blah okay"
      },
      {
        "start": 6681.84,
        "duration": 3.359,
        "text": "um bye"
      },
      {
        "start": 6683.699,
        "duration": 3.361,
        "text": "I thought we had taken care of it but"
      },
      {
        "start": 6685.199,
        "duration": 2.46,
        "text": "that's fine I'll go ahead and then and"
      },
      {
        "start": 6687.06,
        "duration": 3.059,
        "text": "um"
      },
      {
        "start": 6687.659,
        "duration": 4.681,
        "text": "and run it and and this is basically"
      },
      {
        "start": 6690.119,
        "duration": 5.161,
        "text": "it's related nothing it has nothing to"
      },
      {
        "start": 6692.34,
        "duration": 4.799,
        "text": "do with what we're doing but it's just a"
      },
      {
        "start": 6695.28,
        "duration": 5.1,
        "text": "airflow saying hey there's an example"
      },
      {
        "start": 6697.139,
        "duration": 4.5,
        "text": "code here that needs this operator and"
      },
      {
        "start": 6700.38,
        "duration": 4.2,
        "text": "you don't have it so we're just going to"
      },
      {
        "start": 6701.639,
        "duration": 3.761,
        "text": "satisfy that okay and it'll take care of"
      },
      {
        "start": 6704.58,
        "duration": 1.26,
        "text": "it"
      },
      {
        "start": 6705.4,
        "duration": 1.58,
        "text": "[Music]"
      },
      {
        "start": 6705.84,
        "duration": 3.18,
        "text": "um"
      },
      {
        "start": 6706.98,
        "duration": 4.199,
        "text": "could it be related to putting the"
      },
      {
        "start": 6709.02,
        "duration": 3.48,
        "text": "double quotes you know I've never tried"
      },
      {
        "start": 6711.179,
        "duration": 3.56,
        "text": "I mean I didn't try it with the double"
      },
      {
        "start": 6712.5,
        "duration": 2.239,
        "text": "quotes"
      },
      {
        "start": 6714.84,
        "duration": 5.16,
        "text": "uh I don't think so so he would have"
      },
      {
        "start": 6717.96,
        "duration": 3.659,
        "text": "told us if this is specifically related"
      },
      {
        "start": 6720.0,
        "duration": 4.56,
        "text": "for that yeah"
      },
      {
        "start": 6721.619,
        "duration": 6.441,
        "text": "um so what we're gonna do is"
      },
      {
        "start": 6724.56,
        "duration": 3.5,
        "text": "um go back to our dad"
      },
      {
        "start": 6728.699,
        "duration": 3.861,
        "text": "trigger it again and see what happens"
      },
      {
        "start": 6734.46,
        "duration": 6.0,
        "text": "and uh this view this grid view uh is"
      },
      {
        "start": 6738.06,
        "duration": 5.639,
        "text": "one of the newer views what it's doing"
      },
      {
        "start": 6740.46,
        "duration": 5.46,
        "text": "is showing us hey this one thing failed"
      },
      {
        "start": 6743.699,
        "duration": 3.48,
        "text": "right and it took 26 seconds and this"
      },
      {
        "start": 6745.92,
        "duration": 2.759,
        "text": "other one didn't run because this"
      },
      {
        "start": 6747.179,
        "duration": 4.261,
        "text": "Upstream one failed so it's like a"
      },
      {
        "start": 6748.679,
        "duration": 5.04,
        "text": "sequential view of the tasks and it"
      },
      {
        "start": 6751.44,
        "duration": 5.04,
        "text": "looks like it's working it didn't fail"
      },
      {
        "start": 6753.719,
        "duration": 4.94,
        "text": "yet you know hopefully it's fine"
      },
      {
        "start": 6756.48,
        "duration": 2.179,
        "text": "um"
      },
      {
        "start": 6761.48,
        "duration": 4.679,
        "text": "let's take a look let's take a look"
      },
      {
        "start": 6766.26,
        "duration": 3.84,
        "text": "and and this is the part where you know"
      },
      {
        "start": 6768.3,
        "duration": 5.52,
        "text": "airflow starts to really give us the"
      },
      {
        "start": 6770.1,
        "duration": 4.8,
        "text": "value of it's managing this work for us"
      },
      {
        "start": 6773.82,
        "duration": 4.64,
        "text": "um"
      },
      {
        "start": 6774.9,
        "duration": 3.56,
        "text": "let's see uh"
      },
      {
        "start": 6782.04,
        "duration": 3.5,
        "text": "enter location"
      },
      {
        "start": 6792.78,
        "duration": 5.72,
        "text": "okay so right girl you may be"
      },
      {
        "start": 6798.659,
        "duration": 4.46,
        "text": "let's go back and change this to"
      },
      {
        "start": 6806.76,
        "duration": 5.04,
        "text": "there we go"
      },
      {
        "start": 6809.1,
        "duration": 4.68,
        "text": "and we've saved our file"
      },
      {
        "start": 6811.8,
        "duration": 4.62,
        "text": "we haven't made any changes to our spark"
      },
      {
        "start": 6813.78,
        "duration": 5.0,
        "text": "deck right so we should be fine and then"
      },
      {
        "start": 6816.42,
        "duration": 2.36,
        "text": "go back"
      },
      {
        "start": 6819.119,
        "duration": 3.0,
        "text": "and"
      },
      {
        "start": 6823.02,
        "duration": 3.8,
        "text": "let's hit back here"
      },
      {
        "start": 6827.52,
        "duration": 3.48,
        "text": "and I can just go ahead and run"
      },
      {
        "start": 6829.619,
        "duration": 4.381,
        "text": "right I don't even have to go to the"
      },
      {
        "start": 6831.0,
        "duration": 4.8,
        "text": "trigger I can just say run"
      },
      {
        "start": 6834.0,
        "duration": 5.78,
        "text": "um oh that only works with this so fine"
      },
      {
        "start": 6835.8,
        "duration": 3.98,
        "text": "we'll we'll have to run the whole thing"
      },
      {
        "start": 6840.239,
        "duration": 6.181,
        "text": "because it is basically saying I can run"
      },
      {
        "start": 6844.5,
        "duration": 3.6,
        "text": "that particular task for you right now"
      },
      {
        "start": 6846.42,
        "duration": 4.92,
        "text": "but in this case because that's only a"
      },
      {
        "start": 6848.1,
        "duration": 4.8,
        "text": "feature in the door so let's see I got"
      },
      {
        "start": 6851.34,
        "duration": 5.839,
        "text": "third run"
      },
      {
        "start": 6852.9,
        "duration": 4.279,
        "text": "running it's running"
      },
      {
        "start": 6860.28,
        "duration": 5.22,
        "text": "yeah and that CSV has a lot of stuff in"
      },
      {
        "start": 6862.619,
        "duration": 5.301,
        "text": "there so it's not a you know it's not a"
      },
      {
        "start": 6865.5,
        "duration": 2.42,
        "text": "small file"
      },
      {
        "start": 6868.619,
        "duration": 2.901,
        "text": "fingers crossed"
      },
      {
        "start": 6877.5,
        "duration": 4.98,
        "text": "yeah good so this means that actually"
      },
      {
        "start": 6880.08,
        "duration": 3.42,
        "text": "it's doing stuff"
      },
      {
        "start": 6882.48,
        "duration": 4.199,
        "text": "um"
      },
      {
        "start": 6883.5,
        "duration": 6.42,
        "text": "and you know while that's running we can"
      },
      {
        "start": 6886.679,
        "duration": 5.641,
        "text": "look at task duration right so you know"
      },
      {
        "start": 6889.92,
        "duration": 4.319,
        "text": "how long did this thing take over time"
      },
      {
        "start": 6892.32,
        "duration": 4.62,
        "text": "you get to see ups and downs for a"
      },
      {
        "start": 6894.239,
        "duration": 5.281,
        "text": "particular uh you know job"
      },
      {
        "start": 6896.94,
        "duration": 6.0,
        "text": "um and then you can see for the last 100"
      },
      {
        "start": 6899.52,
        "duration": 5.46,
        "text": "runs or the last 365 run uh it also"
      },
      {
        "start": 6902.94,
        "duration": 3.54,
        "text": "shows you if you have configured it how"
      },
      {
        "start": 6904.98,
        "duration": 3.179,
        "text": "many retries did it take for a"
      },
      {
        "start": 6906.48,
        "duration": 4.02,
        "text": "particular task you can configure that"
      },
      {
        "start": 6908.159,
        "duration": 4.02,
        "text": "you can say this particular task go up"
      },
      {
        "start": 6910.5,
        "duration": 3.96,
        "text": "to five times go up to ten times and"
      },
      {
        "start": 6912.179,
        "duration": 5.161,
        "text": "that's just a part of the the function"
      },
      {
        "start": 6914.46,
        "duration": 5.58,
        "text": "that you wrap your command in okay"
      },
      {
        "start": 6917.34,
        "duration": 6.899,
        "text": "um Landing times how long did it take"
      },
      {
        "start": 6920.04,
        "duration": 7.079,
        "text": "overall right in this case uh took about"
      },
      {
        "start": 6924.239,
        "duration": 5.721,
        "text": "63 seconds which means that something is"
      },
      {
        "start": 6927.119,
        "duration": 2.841,
        "text": "finished right"
      },
      {
        "start": 6931.44,
        "duration": 4.679,
        "text": "so"
      },
      {
        "start": 6933.119,
        "duration": 5.461,
        "text": "it the first step finished"
      },
      {
        "start": 6936.119,
        "duration": 5.52,
        "text": "and then the second step has started"
      },
      {
        "start": 6938.58,
        "duration": 6.0,
        "text": "right taking some time remember it's a"
      },
      {
        "start": 6941.639,
        "duration": 6.121,
        "text": "it's it's not a lot of information"
      },
      {
        "start": 6944.58,
        "duration": 4.98,
        "text": "but it's it's big enough for you to"
      },
      {
        "start": 6947.76,
        "duration": 3.54,
        "text": "understand the value of spark you know"
      },
      {
        "start": 6949.56,
        "duration": 4.32,
        "text": "and so this whole thing was successful"
      },
      {
        "start": 6951.3,
        "duration": 3.96,
        "text": "so if I go to the graph view I can see"
      },
      {
        "start": 6953.88,
        "duration": 3.359,
        "text": "these are green"
      },
      {
        "start": 6955.26,
        "duration": 2.879,
        "text": "good green is good"
      },
      {
        "start": 6957.239,
        "duration": 2.581,
        "text": "um"
      },
      {
        "start": 6958.139,
        "duration": 5.0,
        "text": "and I'm not just talking about American"
      },
      {
        "start": 6959.82,
        "duration": 3.319,
        "text": "dollars uh"
      },
      {
        "start": 6963.54,
        "duration": 4.38,
        "text": "um and if I look for"
      },
      {
        "start": 6966.54,
        "duration": 4.26,
        "text": "um actually I want to see it in the in"
      },
      {
        "start": 6967.92,
        "duration": 5.279,
        "text": "the in the uh the master view here let's"
      },
      {
        "start": 6970.8,
        "duration": 4.46,
        "text": "just scroll down to"
      },
      {
        "start": 6973.199,
        "duration": 2.061,
        "text": "um"
      },
      {
        "start": 6976.56,
        "duration": 2.659,
        "text": "there you go"
      },
      {
        "start": 6979.5,
        "duration": 4.92,
        "text": "so here you go so it shows us a history"
      },
      {
        "start": 6981.84,
        "duration": 3.96,
        "text": "on the dashboard of hey there were some"
      },
      {
        "start": 6984.42,
        "duration": 2.52,
        "text": "failures"
      },
      {
        "start": 6985.8,
        "duration": 3.18,
        "text": "right"
      },
      {
        "start": 6986.94,
        "duration": 3.719,
        "text": "and there's zero running"
      },
      {
        "start": 6988.98,
        "duration": 3.06,
        "text": "how many successes there were and how"
      },
      {
        "start": 6990.659,
        "duration": 3.121,
        "text": "many are queued up"
      },
      {
        "start": 6992.04,
        "duration": 4.32,
        "text": "and that's what these mean queued"
      },
      {
        "start": 6993.78,
        "duration": 4.68,
        "text": "success running and failed and then"
      },
      {
        "start": 6996.36,
        "duration": 5.1,
        "text": "these are basically telling you what is"
      },
      {
        "start": 6998.46,
        "duration": 6.719,
        "text": "the number of statuses so there's been"
      },
      {
        "start": 7001.46,
        "duration": 6.06,
        "text": "two successes uh new zero deferred so I"
      },
      {
        "start": 7005.179,
        "duration": 4.02,
        "text": "could continue doing this here"
      },
      {
        "start": 7007.52,
        "duration": 3.96,
        "text": "um I could jump directly to one of these"
      },
      {
        "start": 7009.199,
        "duration": 4.081,
        "text": "interfaces so this this master view is"
      },
      {
        "start": 7011.48,
        "duration": 3.54,
        "text": "actually a nice shortcut to see what you"
      },
      {
        "start": 7013.28,
        "duration": 3.72,
        "text": "need and get to exactly what you need to"
      },
      {
        "start": 7015.02,
        "duration": 6.42,
        "text": "run so if there was an error I could"
      },
      {
        "start": 7017.0,
        "duration": 6.679,
        "text": "just say you know uh go to the graph"
      },
      {
        "start": 7021.44,
        "duration": 2.239,
        "text": "View"
      },
      {
        "start": 7023.739,
        "duration": 4.781,
        "text": "click on it"
      },
      {
        "start": 7026.84,
        "duration": 2.879,
        "text": "go to the log"
      },
      {
        "start": 7028.52,
        "duration": 2.94,
        "text": "right"
      },
      {
        "start": 7029.719,
        "duration": 3.781,
        "text": "and"
      },
      {
        "start": 7031.46,
        "duration": 4.38,
        "text": "that's it that's airflow for you to be"
      },
      {
        "start": 7033.5,
        "duration": 4.86,
        "text": "able to organize your tasks and execute"
      },
      {
        "start": 7035.84,
        "duration": 4.799,
        "text": "them let's verify first of all that did"
      },
      {
        "start": 7038.36,
        "duration": 6.72,
        "text": "this do what we wanted it to do"
      },
      {
        "start": 7040.639,
        "duration": 7.741,
        "text": "um so in my other department right back"
      },
      {
        "start": 7045.08,
        "duration": 6.3,
        "text": "yep we moved the data to Cassandra and"
      },
      {
        "start": 7048.38,
        "duration": 5.04,
        "text": "we then processed it from Cassandra into"
      },
      {
        "start": 7051.38,
        "duration": 4.739,
        "text": "another casino table"
      },
      {
        "start": 7053.42,
        "duration": 5.1,
        "text": "this is one command uh this is the data"
      },
      {
        "start": 7056.119,
        "duration": 4.5,
        "text": "that we inputted from the CSV file"
      },
      {
        "start": 7058.52,
        "duration": 5.639,
        "text": "and then"
      },
      {
        "start": 7060.619,
        "duration": 5.881,
        "text": "the data that we processed into the uh"
      },
      {
        "start": 7064.159,
        "duration": 4.56,
        "text": "days worked by previous employees by job"
      },
      {
        "start": 7066.5,
        "duration": 4.679,
        "text": "title right so put me back to straight"
      },
      {
        "start": 7068.719,
        "duration": 4.861,
        "text": "Count's question right these two tables"
      },
      {
        "start": 7071.179,
        "duration": 4.5,
        "text": "were designed in the way that Cassandra"
      },
      {
        "start": 7073.58,
        "duration": 4.94,
        "text": "table should be designed"
      },
      {
        "start": 7075.679,
        "duration": 4.98,
        "text": "and Spark is doing the hard work of"
      },
      {
        "start": 7078.52,
        "duration": 3.579,
        "text": "materializing this view so basically you"
      },
      {
        "start": 7080.659,
        "duration": 3.241,
        "text": "need to keep the partition key in mind"
      },
      {
        "start": 7082.099,
        "duration": 5.1,
        "text": "when you're designing the tables correct"
      },
      {
        "start": 7083.9,
        "duration": 6.54,
        "text": "kind of it uh with Cassandra"
      },
      {
        "start": 7087.199,
        "duration": 5.761,
        "text": "exactly exactly so"
      },
      {
        "start": 7090.44,
        "duration": 5.279,
        "text": "this is the the demonstration"
      },
      {
        "start": 7092.96,
        "duration": 4.8,
        "text": "um you know we have a lot of examples on"
      },
      {
        "start": 7095.719,
        "duration": 4.02,
        "text": "uh on our GitHub in fact I'll show you"
      },
      {
        "start": 7097.76,
        "duration": 4.08,
        "text": "the I'll go back to the presentation so"
      },
      {
        "start": 7099.739,
        "duration": 4.321,
        "text": "that I can walk you through"
      },
      {
        "start": 7101.84,
        "duration": 4.14,
        "text": "um you know what are the examples there"
      },
      {
        "start": 7104.06,
        "duration": 4.86,
        "text": "um and this is for you guys to play with"
      },
      {
        "start": 7105.98,
        "duration": 5.52,
        "text": "right it's not it's it's all open source"
      },
      {
        "start": 7108.92,
        "duration": 3.6,
        "text": "and and again you know if you want to do"
      },
      {
        "start": 7111.5,
        "duration": 3.239,
        "text": "it yourself"
      },
      {
        "start": 7112.52,
        "duration": 3.78,
        "text": "um the instructions are there um if"
      },
      {
        "start": 7114.739,
        "duration": 3.241,
        "text": "you've already set up gitpart it'll be"
      },
      {
        "start": 7116.3,
        "duration": 4.26,
        "text": "around for a little bit if you want to"
      },
      {
        "start": 7117.98,
        "duration": 4.44,
        "text": "be sure you can just pin it and you can"
      },
      {
        "start": 7120.56,
        "duration": 4.02,
        "text": "try it out you know anytime uh"
      },
      {
        "start": 7122.42,
        "duration": 3.9,
        "text": "unfortunately we're running out of time"
      },
      {
        "start": 7124.58,
        "duration": 4.579,
        "text": "um so we'll have to keep going yes"
      },
      {
        "start": 7126.32,
        "duration": 2.839,
        "text": "exactly"
      },
      {
        "start": 7129.32,
        "duration": 5.64,
        "text": "and here are just you go to the the"
      },
      {
        "start": 7132.08,
        "duration": 4.2,
        "text": "github.com and you just look for airflow"
      },
      {
        "start": 7134.96,
        "duration": 2.82,
        "text": "there's like seven different"
      },
      {
        "start": 7136.28,
        "duration": 3.66,
        "text": "repositories about doing different"
      },
      {
        "start": 7137.78,
        "duration": 4.74,
        "text": "things like even one with Cassandra on"
      },
      {
        "start": 7139.94,
        "duration": 4.98,
        "text": "Presto one with DVT the one I mentioned"
      },
      {
        "start": 7142.52,
        "duration": 5.219,
        "text": "with Google dataproc and Astra uh one"
      },
      {
        "start": 7144.92,
        "duration": 5.16,
        "text": "that uses uh amundsen uh with Cassandra"
      },
      {
        "start": 7147.739,
        "duration": 3.721,
        "text": "so lots of other examples for you to"
      },
      {
        "start": 7150.08,
        "duration": 2.039,
        "text": "take a look at"
      },
      {
        "start": 7151.46,
        "duration": 3.36,
        "text": "um"
      },
      {
        "start": 7152.119,
        "duration": 4.02,
        "text": "pick things you don't have to run your"
      },
      {
        "start": 7154.82,
        "duration": 4.68,
        "text": "own Spark"
      },
      {
        "start": 7156.139,
        "duration": 4.5,
        "text": "okay just just think about it if you if"
      },
      {
        "start": 7159.5,
        "duration": 3.48,
        "text": "you're running"
      },
      {
        "start": 7160.639,
        "duration": 4.08,
        "text": "complex spark jobs on hundreds and"
      },
      {
        "start": 7162.98,
        "duration": 3.48,
        "text": "hundreds of jobs managing your spark"
      },
      {
        "start": 7164.719,
        "duration": 4.5,
        "text": "cluster is tough so"
      },
      {
        "start": 7166.46,
        "duration": 5.34,
        "text": "um think about using a managed one"
      },
      {
        "start": 7169.219,
        "duration": 4.261,
        "text": "um not all spark code is created equal"
      },
      {
        "start": 7171.8,
        "duration": 2.879,
        "text": "don't let anybody just say here's my"
      },
      {
        "start": 7173.48,
        "duration": 3.48,
        "text": "spark job can you put it as an"
      },
      {
        "start": 7174.679,
        "duration": 5.04,
        "text": "automation"
      },
      {
        "start": 7176.96,
        "duration": 5.82,
        "text": "um if you have a compiled spark job it"
      },
      {
        "start": 7179.719,
        "duration": 4.92,
        "text": "allows you to have control over uh the"
      },
      {
        "start": 7182.78,
        "duration": 4.2,
        "text": "quality of that spark job"
      },
      {
        "start": 7184.639,
        "duration": 4.08,
        "text": "on the airflow side"
      },
      {
        "start": 7186.98,
        "duration": 5.219,
        "text": "um you don't have to run it"
      },
      {
        "start": 7188.719,
        "duration": 7.701,
        "text": "by yourself if you don't want to uh you"
      },
      {
        "start": 7192.199,
        "duration": 7.081,
        "text": "can run it on so Google uh composer"
      },
      {
        "start": 7196.42,
        "duration": 5.14,
        "text": "managed workspace manage workflows for"
      },
      {
        "start": 7199.28,
        "duration": 3.78,
        "text": "uh Apache airflow from Amazon and then"
      },
      {
        "start": 7201.56,
        "duration": 3.3,
        "text": "astronomer makes it easy to use on any"
      },
      {
        "start": 7203.06,
        "duration": 4.44,
        "text": "cloud"
      },
      {
        "start": 7204.86,
        "duration": 4.5,
        "text": "um not all dags just work so sometimes"
      },
      {
        "start": 7207.5,
        "duration": 3.78,
        "text": "you really have to think about"
      },
      {
        "start": 7209.36,
        "duration": 3.779,
        "text": "different Stacks different applications"
      },
      {
        "start": 7211.28,
        "duration": 3.359,
        "text": "different customers"
      },
      {
        "start": 7213.139,
        "duration": 3.181,
        "text": "um in terms of inside the organization"
      },
      {
        "start": 7214.639,
        "duration": 4.381,
        "text": "sometimes you may have to have different"
      },
      {
        "start": 7216.32,
        "duration": 4.74,
        "text": "airflow instances for security reason"
      },
      {
        "start": 7219.02,
        "duration": 3.9,
        "text": "um the same dag May to see is it running"
      },
      {
        "start": 7221.06,
        "duration": 3.42,
        "text": "okay right and we may have to put some"
      },
      {
        "start": 7222.92,
        "duration": 3.54,
        "text": "retries in there it's not a set it and"
      },
      {
        "start": 7224.48,
        "duration": 4.5,
        "text": "forget it but once you figure this out"
      },
      {
        "start": 7226.46,
        "duration": 5.04,
        "text": "it is a set it and forget it uh key"
      },
      {
        "start": 7228.98,
        "duration": 4.139,
        "text": "takeaways don't reinvent the wheel use"
      },
      {
        "start": 7231.5,
        "duration": 3.78,
        "text": "the passion spark like literally that's"
      },
      {
        "start": 7233.119,
        "duration": 3.54,
        "text": "the best tool for Big Data operations on"
      },
      {
        "start": 7235.28,
        "duration": 3.24,
        "text": "Cassandra"
      },
      {
        "start": 7236.659,
        "duration": 3.361,
        "text": "um you can do other stuff but Apache"
      },
      {
        "start": 7238.52,
        "duration": 5.46,
        "text": "spark is a very common way of doing it"
      },
      {
        "start": 7240.02,
        "duration": 6.78,
        "text": "use a scheduler so Apache airflow with"
      },
      {
        "start": 7243.98,
        "duration": 4.739,
        "text": "python and python spark is a very quick"
      },
      {
        "start": 7246.8,
        "duration": 4.5,
        "text": "and iterative process to do these things"
      },
      {
        "start": 7248.719,
        "duration": 5.641,
        "text": "right you notice I can make a coach"
      },
      {
        "start": 7251.3,
        "duration": 4.56,
        "text": "range copy it and then run it and then"
      },
      {
        "start": 7254.36,
        "duration": 5.16,
        "text": "once it's baked in I can maybe think"
      },
      {
        "start": 7255.86,
        "duration": 5.879,
        "text": "about a compile job as well"
      },
      {
        "start": 7259.52,
        "duration": 4.98,
        "text": "um thank you and I'm going to pass it"
      },
      {
        "start": 7261.739,
        "duration": 4.92,
        "text": "back to right all right"
      },
      {
        "start": 7264.5,
        "duration": 3.48,
        "text": "um so I know that we are going to do"
      },
      {
        "start": 7266.659,
        "duration": 4.44,
        "text": "minty"
      },
      {
        "start": 7267.98,
        "duration": 5.4,
        "text": "um and we are almost there uh and what"
      },
      {
        "start": 7271.099,
        "duration": 4.56,
        "text": "we're gonna do here is"
      },
      {
        "start": 7273.38,
        "duration": 4.44,
        "text": "um you know usually we have a form where"
      },
      {
        "start": 7275.659,
        "duration": 4.02,
        "text": "you can submit your uh"
      },
      {
        "start": 7277.82,
        "duration": 4.379,
        "text": "um you know Windows supplementary but"
      },
      {
        "start": 7279.679,
        "duration": 7.52,
        "text": "but what we're going to do here is"
      },
      {
        "start": 7282.199,
        "duration": 9.54,
        "text": "you're gonna mail it to um rahu it's"
      },
      {
        "start": 7287.199,
        "duration": 7.0,
        "text": "s-i-n-g-h correct uh yeah you hold that"
      },
      {
        "start": 7291.739,
        "duration": 4.561,
        "text": "sing it on that us okay and and and and"
      },
      {
        "start": 7294.199,
        "duration": 4.741,
        "text": "you can put it in chat as well just to"
      },
      {
        "start": 7296.3,
        "duration": 4.62,
        "text": "you know make sure um that it's there"
      },
      {
        "start": 7298.94,
        "duration": 5.04,
        "text": "um and it's really a cool looking shirt"
      },
      {
        "start": 7300.92,
        "duration": 8.219,
        "text": "there no architect as you can see okay"
      },
      {
        "start": 7303.98,
        "duration": 8.52,
        "text": "uh all right so let me grab the screen"
      },
      {
        "start": 7309.139,
        "duration": 4.861,
        "text": "all right and let me make sure that you"
      },
      {
        "start": 7312.5,
        "duration": 3.96,
        "text": "are"
      },
      {
        "start": 7314.0,
        "duration": 5.06,
        "text": "okay that's good"
      },
      {
        "start": 7316.46,
        "duration": 5.279,
        "text": "all right"
      },
      {
        "start": 7319.06,
        "duration": 4.599,
        "text": "so um you know we are just about to jump"
      },
      {
        "start": 7321.739,
        "duration": 4.92,
        "text": "to mentee you just give me a second so"
      },
      {
        "start": 7323.659,
        "duration": 6.0,
        "text": "that I'll be ready to go all right"
      },
      {
        "start": 7326.659,
        "duration": 3.0,
        "text": "and"
      },
      {
        "start": 7330.02,
        "duration": 6.42,
        "text": "okay Chris time you guys ready"
      },
      {
        "start": 7333.8,
        "duration": 3.48,
        "text": "um please feel free to join again if you"
      },
      {
        "start": 7336.44,
        "duration": 3.719,
        "text": "haven't"
      },
      {
        "start": 7337.28,
        "duration": 4.8,
        "text": "three five one eight zero six nine"
      },
      {
        "start": 7340.159,
        "duration": 4.44,
        "text": "okay"
      },
      {
        "start": 7342.08,
        "duration": 4.92,
        "text": "and I'm gonna put this"
      },
      {
        "start": 7344.599,
        "duration": 5.06,
        "text": "three five one eight zero six nine oh I"
      },
      {
        "start": 7347.0,
        "duration": 2.659,
        "text": "don't have the"
      },
      {
        "start": 7350.0,
        "duration": 4.04,
        "text": "yeah it's still showing my miles"
      },
      {
        "start": 7354.619,
        "duration": 4.681,
        "text": "what did I do okay there you go"
      },
      {
        "start": 7357.44,
        "duration": 4.199,
        "text": "so it's three five one eight zero six"
      },
      {
        "start": 7359.3,
        "duration": 6.859,
        "text": "nine okay and let me start the quiz"
      },
      {
        "start": 7361.639,
        "duration": 4.52,
        "text": "music just to have a little bit of fun"
      },
      {
        "start": 7367.219,
        "duration": 5.121,
        "text": "if it's too loud let me know I will try"
      },
      {
        "start": 7370.099,
        "duration": 2.241,
        "text": "to"
      },
      {
        "start": 7372.44,
        "duration": 3.679,
        "text": "okay you guys ready"
      },
      {
        "start": 7376.88,
        "duration": 6.9,
        "text": "again three five one eight zero six nine"
      },
      {
        "start": 7380.36,
        "duration": 6.02,
        "text": "and let me see what I am"
      },
      {
        "start": 7383.78,
        "duration": 2.6,
        "text": "okay"
      },
      {
        "start": 7389.659,
        "duration": 2.721,
        "text": "okay all right"
      },
      {
        "start": 7393.5,
        "duration": 4.86,
        "text": "we have a few people joining question"
      },
      {
        "start": 7395.54,
        "duration": 5.52,
        "text": "number one of six and rahula did I lose"
      },
      {
        "start": 7398.36,
        "duration": 4.98,
        "text": "you are you still around"
      },
      {
        "start": 7401.06,
        "duration": 4.619,
        "text": "uh looks like we lost Ronald again"
      },
      {
        "start": 7403.34,
        "duration": 4.56,
        "text": "um give it a second and I'll make sure"
      },
      {
        "start": 7405.679,
        "duration": 5.161,
        "text": "that he's gonna be joining I'll in the"
      },
      {
        "start": 7407.9,
        "duration": 5.64,
        "text": "meantime you know you people can join in"
      },
      {
        "start": 7410.84,
        "duration": 5.54,
        "text": "looks like gravel has some connection"
      },
      {
        "start": 7413.54,
        "duration": 2.84,
        "text": "issues again"
      },
      {
        "start": 7418.76,
        "duration": 3.439,
        "text": "Riley or maybe a mute"
      },
      {
        "start": 7422.9,
        "duration": 6.92,
        "text": "that's what I see here"
      },
      {
        "start": 7425.719,
        "duration": 4.101,
        "text": "let me make sure all is there"
      },
      {
        "start": 7438.26,
        "duration": 4.22,
        "text": "otherwise we can just proceed if it"
      },
      {
        "start": 7454.159,
        "duration": 5.94,
        "text": "okay question number one you guys ready"
      },
      {
        "start": 7458.3,
        "duration": 3.06,
        "text": "again you know for those of you who are"
      },
      {
        "start": 7460.099,
        "duration": 3.241,
        "text": "joining there are quite a few who are"
      },
      {
        "start": 7461.36,
        "duration": 4.14,
        "text": "there uh"
      },
      {
        "start": 7463.34,
        "duration": 5.24,
        "text": "all right"
      },
      {
        "start": 7465.5,
        "duration": 3.08,
        "text": "so very first question"
      },
      {
        "start": 7472.52,
        "duration": 4.079,
        "text": "answer fast but most importantly answer"
      },
      {
        "start": 7474.739,
        "duration": 4.101,
        "text": "it correctly the best that we use today"
      },
      {
        "start": 7476.599,
        "duration": 5.221,
        "text": "pretty straightforward question"
      },
      {
        "start": 7478.84,
        "duration": 5.98,
        "text": "is it Hadoop is it Astra is it good part"
      },
      {
        "start": 7481.82,
        "duration": 4.859,
        "text": "is it none remember it's a database as a"
      },
      {
        "start": 7484.82,
        "duration": 5.04,
        "text": "service that we use today"
      },
      {
        "start": 7486.679,
        "duration": 5.161,
        "text": "what did we use today"
      },
      {
        "start": 7489.86,
        "duration": 4.08,
        "text": "we did not use anything or did we use"
      },
      {
        "start": 7491.84,
        "duration": 4.56,
        "text": "git power did we use Astra or had you"
      },
      {
        "start": 7493.94,
        "duration": 5.58,
        "text": "which one of these is a DB as a service"
      },
      {
        "start": 7496.4,
        "duration": 6.239,
        "text": "I'm sure we used all of them right"
      },
      {
        "start": 7499.52,
        "duration": 6.06,
        "text": "all right nobody got it wrong which is"
      },
      {
        "start": 7502.639,
        "duration": 5.901,
        "text": "cool but let's see how fast"
      },
      {
        "start": 7505.58,
        "duration": 2.96,
        "text": "um people answer it"
      },
      {
        "start": 7509.0,
        "duration": 4.82,
        "text": "uh Rahul is back"
      },
      {
        "start": 7511.219,
        "duration": 4.321,
        "text": "yeah perfect"
      },
      {
        "start": 7513.82,
        "duration": 6.16,
        "text": "okay"
      },
      {
        "start": 7515.54,
        "duration": 7.199,
        "text": "Joker is the fastest nice all right cool"
      },
      {
        "start": 7519.98,
        "duration": 6.36,
        "text": "but it could be anybody's game Simba"
      },
      {
        "start": 7522.739,
        "duration": 8.36,
        "text": "Shashi marks Ajay Ricardo shaudia"
      },
      {
        "start": 7526.34,
        "duration": 4.759,
        "text": "anybody okay moving on next question"
      },
      {
        "start": 7531.8,
        "duration": 3.08,
        "text": "two of six"
      },
      {
        "start": 7535.099,
        "duration": 6.961,
        "text": "answer fast but make sure you get it"
      },
      {
        "start": 7538.76,
        "duration": 5.1,
        "text": "right which of the following is not a"
      },
      {
        "start": 7542.06,
        "duration": 5.159,
        "text": "characteristic of Astra"
      },
      {
        "start": 7543.86,
        "duration": 6.359,
        "text": "it's Apache Cassandra in the cloud"
      },
      {
        "start": 7547.219,
        "duration": 5.46,
        "text": "it's Cloud agnostic"
      },
      {
        "start": 7550.219,
        "duration": 5.281,
        "text": "is it single tenant"
      },
      {
        "start": 7552.679,
        "duration": 5.881,
        "text": "and there is no free tier remember this"
      },
      {
        "start": 7555.5,
        "duration": 5.76,
        "text": "is a little bit backwards question"
      },
      {
        "start": 7558.56,
        "duration": 4.8,
        "text": "so let's see how how people are gonna"
      },
      {
        "start": 7561.26,
        "duration": 3.6,
        "text": "answer this"
      },
      {
        "start": 7563.36,
        "duration": 4.62,
        "text": "perfect"
      },
      {
        "start": 7564.86,
        "duration": 7.08,
        "text": "I think I made it too easy this is not"
      },
      {
        "start": 7567.98,
        "duration": 6.84,
        "text": "good nice nice yeah this is not good"
      },
      {
        "start": 7571.94,
        "duration": 4.5,
        "text": "all right let's see who got it first"
      },
      {
        "start": 7574.82,
        "duration": 5.46,
        "text": "again you know you've got to be fast"
      },
      {
        "start": 7576.44,
        "duration": 6.48,
        "text": "right and I think"
      },
      {
        "start": 7580.28,
        "duration": 5.16,
        "text": "Nick was the fastest"
      },
      {
        "start": 7582.92,
        "duration": 3.9,
        "text": "nice yeah good job Nick yeah I think"
      },
      {
        "start": 7585.44,
        "duration": 3.179,
        "text": "it's our name"
      },
      {
        "start": 7586.82,
        "duration": 3.96,
        "text": "um you know you know it's funny uh at"
      },
      {
        "start": 7588.619,
        "duration": 4.98,
        "text": "Cassandra day we were doing this and uh"
      },
      {
        "start": 7590.78,
        "duration": 4.879,
        "text": "I was playing against Mick the PMC of"
      },
      {
        "start": 7593.599,
        "duration": 5.341,
        "text": "Apache Cassandra and we were just like"
      },
      {
        "start": 7595.659,
        "duration": 7.54,
        "text": "that was that was that was fun Nick is"
      },
      {
        "start": 7598.94,
        "duration": 6.779,
        "text": "fast all right now now we go to more"
      },
      {
        "start": 7603.199,
        "duration": 4.801,
        "text": "you know tough questions what is Apache"
      },
      {
        "start": 7605.719,
        "duration": 6.181,
        "text": "airflow"
      },
      {
        "start": 7608.0,
        "duration": 6.84,
        "text": "is it a workflow tool to manage any ETL"
      },
      {
        "start": 7611.9,
        "duration": 4.739,
        "text": "operation and data pipeline or is it the"
      },
      {
        "start": 7614.84,
        "duration": 4.44,
        "text": "infrastructure as code to"
      },
      {
        "start": 7616.639,
        "duration": 4.441,
        "text": "is it really a big data framework or is"
      },
      {
        "start": 7619.28,
        "duration": 4.76,
        "text": "it stream processing"
      },
      {
        "start": 7621.08,
        "duration": 2.96,
        "text": "let's see"
      },
      {
        "start": 7624.199,
        "duration": 3.801,
        "text": "all right time's up"
      },
      {
        "start": 7628.82,
        "duration": 3.779,
        "text": "and man we can't seem to stump our"
      },
      {
        "start": 7631.4,
        "duration": 3.18,
        "text": "audience here"
      },
      {
        "start": 7632.599,
        "duration": 4.08,
        "text": "I was hoping at least you would get some"
      },
      {
        "start": 7634.58,
        "duration": 5.039,
        "text": "wrong right you know our audience is"
      },
      {
        "start": 7636.679,
        "duration": 5.281,
        "text": "really getting everything here it's cool"
      },
      {
        "start": 7639.619,
        "duration": 5.181,
        "text": "it's basically a workflow it means that"
      },
      {
        "start": 7641.96,
        "duration": 5.58,
        "text": "we did a good job"
      },
      {
        "start": 7644.8,
        "duration": 4.54,
        "text": "but but we failed in setting the"
      },
      {
        "start": 7647.54,
        "duration": 3.599,
        "text": "questions to be a little bit tough right"
      },
      {
        "start": 7649.34,
        "duration": 4.14,
        "text": "you know we'll see we'll see how it"
      },
      {
        "start": 7651.139,
        "duration": 5.6,
        "text": "progresses but but we'll maybe there is"
      },
      {
        "start": 7653.48,
        "duration": 3.259,
        "text": "a change to the leaderboard that's"
      },
      {
        "start": 7659.179,
        "duration": 4.641,
        "text": "might be right"
      },
      {
        "start": 7661.4,
        "duration": 4.739,
        "text": "maybe not Nick was still the fastest"
      },
      {
        "start": 7663.82,
        "duration": 5.14,
        "text": "wow it's cool"
      },
      {
        "start": 7666.139,
        "duration": 4.381,
        "text": "and Joker is still in contention uh so"
      },
      {
        "start": 7668.96,
        "duration": 3.6,
        "text": "we'll see how it goes okay"
      },
      {
        "start": 7670.52,
        "duration": 5.699,
        "text": "[Music]"
      },
      {
        "start": 7672.56,
        "duration": 5.94,
        "text": "Let's see we have three more questions"
      },
      {
        "start": 7676.219,
        "duration": 4.201,
        "text": "four or five and six"
      },
      {
        "start": 7678.5,
        "duration": 2.639,
        "text": "and so fast but most importantly get it"
      },
      {
        "start": 7680.42,
        "duration": 1.679,
        "text": "right"
      },
      {
        "start": 7681.139,
        "duration": 4.441,
        "text": "um"
      },
      {
        "start": 7682.099,
        "duration": 5.52,
        "text": "is it an ATL tool this is kind of I"
      },
      {
        "start": 7685.58,
        "duration": 4.139,
        "text": "think it's a tough question in my"
      },
      {
        "start": 7687.619,
        "duration": 3.421,
        "text": "opinion even though answers are very"
      },
      {
        "start": 7689.719,
        "duration": 5.821,
        "text": "straightforward right"
      },
      {
        "start": 7691.04,
        "duration": 10.28,
        "text": "is it really an ATL too maybe not could"
      },
      {
        "start": 7695.54,
        "duration": 5.78,
        "text": "be should be uh oh it's confusing"
      },
      {
        "start": 7703.82,
        "duration": 6.779,
        "text": "I was talking to rags earlier it's like"
      },
      {
        "start": 7706.04,
        "duration": 5.88,
        "text": "can Cassandra be a q should it be a cube"
      },
      {
        "start": 7710.599,
        "duration": 2.301,
        "text": "exactly"
      },
      {
        "start": 7711.92,
        "duration": 4.5,
        "text": "yeah"
      },
      {
        "start": 7712.9,
        "duration": 5.44,
        "text": "so I I'll let you know you you talk"
      },
      {
        "start": 7716.42,
        "duration": 6.36,
        "text": "about this"
      },
      {
        "start": 7718.34,
        "duration": 6.0,
        "text": "yeah so um I'm I'm glad that you know uh"
      },
      {
        "start": 7722.78,
        "duration": 4.02,
        "text": "some people you know were thinking that"
      },
      {
        "start": 7724.34,
        "duration": 4.879,
        "text": "yeah I could it is a TTL tool or it"
      },
      {
        "start": 7726.8,
        "duration": 5.76,
        "text": "could be the thing is that"
      },
      {
        "start": 7729.219,
        "duration": 6.281,
        "text": "airflow's purpose is not to do the ETL"
      },
      {
        "start": 7732.56,
        "duration": 4.559,
        "text": "your code is doing the ETF your code is"
      },
      {
        "start": 7735.5,
        "duration": 4.98,
        "text": "doing the extraction your code is doing"
      },
      {
        "start": 7737.119,
        "duration": 5.58,
        "text": "the the transformation what it's doing"
      },
      {
        "start": 7740.48,
        "duration": 4.4,
        "text": "for you is it's wrapping it it's like a"
      },
      {
        "start": 7742.699,
        "duration": 5.4,
        "text": "Cron job can you use it"
      },
      {
        "start": 7744.88,
        "duration": 6.1,
        "text": "yeah exactly just scheduling it exactly"
      },
      {
        "start": 7748.099,
        "duration": 4.56,
        "text": "perfect yeah okay at least we managed to"
      },
      {
        "start": 7750.98,
        "duration": 5.06,
        "text": "stump a few people let's see if there's"
      },
      {
        "start": 7752.659,
        "duration": 3.381,
        "text": "big changes"
      },
      {
        "start": 7756.98,
        "duration": 6.3,
        "text": "oh it looks like it"
      },
      {
        "start": 7759.34,
        "duration": 5.68,
        "text": "Joker it re-establishes the lead it was"
      },
      {
        "start": 7763.28,
        "duration": 4.02,
        "text": "the fastest but it could be anybody's"
      },
      {
        "start": 7765.02,
        "duration": 5.76,
        "text": "game including Santa I'm looking for"
      },
      {
        "start": 7767.3,
        "duration": 6.02,
        "text": "Santa in December right so let's see all"
      },
      {
        "start": 7770.78,
        "duration": 2.54,
        "text": "right guys"
      },
      {
        "start": 7775.159,
        "duration": 3.741,
        "text": "five or six we have two more"
      },
      {
        "start": 7779.48,
        "duration": 6.719,
        "text": "what is not a component of airflow"
      },
      {
        "start": 7783.32,
        "duration": 6.06,
        "text": "you saw a bunch of these"
      },
      {
        "start": 7786.199,
        "duration": 6.9,
        "text": "um it was a database schema manager"
      },
      {
        "start": 7789.38,
        "duration": 6.62,
        "text": "worker scheduler which is again not a"
      },
      {
        "start": 7793.099,
        "duration": 2.901,
        "text": "component of airflow"
      },
      {
        "start": 7800.3,
        "duration": 4.2,
        "text": "two one"
      },
      {
        "start": 7802.46,
        "duration": 5.279,
        "text": "time's up"
      },
      {
        "start": 7804.5,
        "duration": 5.639,
        "text": "oh this guy totally distributed huh"
      },
      {
        "start": 7807.739,
        "duration": 4.5,
        "text": "maybe not yeah maybe it's the same"
      },
      {
        "start": 7810.139,
        "duration": 4.861,
        "text": "salmon who got it right right"
      },
      {
        "start": 7812.239,
        "duration": 4.201,
        "text": "so the schema manager is not a component"
      },
      {
        "start": 7815.0,
        "duration": 4.26,
        "text": "of airflow and I don't think we ever"
      },
      {
        "start": 7816.44,
        "duration": 6.799,
        "text": "talked about that right so um yeah um"
      },
      {
        "start": 7819.26,
        "duration": 3.979,
        "text": "anything you want to add there or"
      },
      {
        "start": 7823.58,
        "duration": 6.36,
        "text": "yeah it you can you can consider"
      },
      {
        "start": 7826.88,
        "duration": 4.38,
        "text": "part of your data operations that at"
      },
      {
        "start": 7829.94,
        "duration": 4.62,
        "text": "some point you have to make schema"
      },
      {
        "start": 7831.26,
        "duration": 5.28,
        "text": "changes right and it is possible to to"
      },
      {
        "start": 7834.56,
        "duration": 4.8,
        "text": "do a schema change and do a migration"
      },
      {
        "start": 7836.54,
        "duration": 5.699,
        "text": "with airflow right but airflow is not"
      },
      {
        "start": 7839.36,
        "duration": 5.879,
        "text": "doing that for you it's not managing the"
      },
      {
        "start": 7842.239,
        "duration": 5.4,
        "text": "the tables and columns and stuff for you"
      },
      {
        "start": 7845.239,
        "duration": 4.081,
        "text": "um and it would take a lot of work to"
      },
      {
        "start": 7847.639,
        "duration": 3.901,
        "text": "kind of rig it to do that so it's just"
      },
      {
        "start": 7849.32,
        "duration": 4.14,
        "text": "definitely not part of the whole uh"
      },
      {
        "start": 7851.54,
        "duration": 4.679,
        "text": "airflow feature set"
      },
      {
        "start": 7853.46,
        "duration": 4.98,
        "text": "last question for all the marbles let's"
      },
      {
        "start": 7856.219,
        "duration": 4.561,
        "text": "see how the"
      },
      {
        "start": 7858.44,
        "duration": 6.179,
        "text": "ooh is it shaurya is going to be in the"
      },
      {
        "start": 7860.78,
        "duration": 4.919,
        "text": "lead maybe just a jingle Joker it looks"
      },
      {
        "start": 7864.619,
        "duration": 4.5,
        "text": "like"
      },
      {
        "start": 7865.699,
        "duration": 5.161,
        "text": "um but it's still anybody's game uh Nick"
      },
      {
        "start": 7869.119,
        "duration": 4.321,
        "text": "who's the fastest"
      },
      {
        "start": 7870.86,
        "duration": 5.66,
        "text": "could still jump in there last question"
      },
      {
        "start": 7873.44,
        "duration": 3.08,
        "text": "question six of six"
      },
      {
        "start": 7877.58,
        "duration": 4.26,
        "text": "all right"
      },
      {
        "start": 7879.32,
        "duration": 5.22,
        "text": "you know the drill"
      },
      {
        "start": 7881.84,
        "duration": 5.16,
        "text": "how can you reuse the same database"
      },
      {
        "start": 7884.54,
        "duration": 5.699,
        "text": "system without hardcoding the values"
      },
      {
        "start": 7887.0,
        "duration": 5.639,
        "text": "this seems like a tough one right"
      },
      {
        "start": 7890.239,
        "duration": 4.561,
        "text": "is it using data set is it using"
      },
      {
        "start": 7892.639,
        "duration": 4.141,
        "text": "configuration use it using setting is it"
      },
      {
        "start": 7894.8,
        "duration": 3.899,
        "text": "using connection I don't know if well"
      },
      {
        "start": 7896.78,
        "duration": 6.06,
        "text": "you talked about this right I think you"
      },
      {
        "start": 7898.699,
        "duration": 8.101,
        "text": "did you said pay attention yeah I did"
      },
      {
        "start": 7902.84,
        "duration": 6.299,
        "text": "yeah and the answer is"
      },
      {
        "start": 7906.8,
        "duration": 3.6,
        "text": "it's connection actually"
      },
      {
        "start": 7909.139,
        "duration": 4.801,
        "text": "right"
      },
      {
        "start": 7910.4,
        "duration": 6.54,
        "text": "and eight people got it wrong yes wow"
      },
      {
        "start": 7913.94,
        "duration": 3.0,
        "text": "right"
      },
      {
        "start": 7918.619,
        "duration": 5.04,
        "text": "um so I I guess could you do it with the"
      },
      {
        "start": 7922.099,
        "duration": 4.02,
        "text": "configuration"
      },
      {
        "start": 7923.659,
        "duration": 6.48,
        "text": "yeah"
      },
      {
        "start": 7926.119,
        "duration": 5.821,
        "text": "so in a configuration and airflow is an"
      },
      {
        "start": 7930.139,
        "duration": 4.861,
        "text": "airflow airflow configuration meaning"
      },
      {
        "start": 7931.94,
        "duration": 5.64,
        "text": "what it what it takes to run uh air flow"
      },
      {
        "start": 7935.0,
        "duration": 5.639,
        "text": "so it's definitely not that now there"
      },
      {
        "start": 7937.58,
        "duration": 4.68,
        "text": "are data sets uh which we saw which are"
      },
      {
        "start": 7940.639,
        "duration": 3.241,
        "text": "like you know how do you connect data"
      },
      {
        "start": 7942.26,
        "duration": 4.32,
        "text": "sets to jobs"
      },
      {
        "start": 7943.88,
        "duration": 5.1,
        "text": "um but but tends to be that that is a"
      },
      {
        "start": 7946.58,
        "duration": 3.659,
        "text": "object that is that can be then fed by a"
      },
      {
        "start": 7948.98,
        "duration": 3.06,
        "text": "connection"
      },
      {
        "start": 7950.239,
        "duration": 3.48,
        "text": "um in a setting"
      },
      {
        "start": 7952.04,
        "duration": 4.139,
        "text": "um again the setting is very generic"
      },
      {
        "start": 7953.719,
        "duration": 4.92,
        "text": "it's really just top level airflow"
      },
      {
        "start": 7956.179,
        "duration": 4.861,
        "text": "setting uh which you can say setting and"
      },
      {
        "start": 7958.639,
        "duration": 5.52,
        "text": "configuration is synonymous but the the"
      },
      {
        "start": 7961.04,
        "duration": 6.119,
        "text": "right approach for reusing the same"
      },
      {
        "start": 7964.159,
        "duration": 4.861,
        "text": "database or or connection string is in"
      },
      {
        "start": 7967.159,
        "duration": 3.601,
        "text": "connections and in fact in the database"
      },
      {
        "start": 7969.02,
        "duration": 3.84,
        "text": "world we we say connections training"
      },
      {
        "start": 7970.76,
        "duration": 3.66,
        "text": "right and that's what the connection is"
      },
      {
        "start": 7972.86,
        "duration": 3.0,
        "text": "for connecting to an existing system"
      },
      {
        "start": 7974.42,
        "duration": 2.52,
        "text": "that you're going to reuse over and over"
      },
      {
        "start": 7975.86,
        "duration": 3.719,
        "text": "again"
      },
      {
        "start": 7976.94,
        "duration": 3.719,
        "text": "um and having it in a place if you can"
      },
      {
        "start": 7979.579,
        "duration": 2.881,
        "text": "think about it"
      },
      {
        "start": 7980.659,
        "duration": 3.901,
        "text": "you have a developing environment you"
      },
      {
        "start": 7982.46,
        "duration": 4.219,
        "text": "have a production environment"
      },
      {
        "start": 7984.56,
        "duration": 7.019,
        "text": "um you can have the same connection"
      },
      {
        "start": 7986.679,
        "duration": 6.641,
        "text": "token in airflow and then have uh your"
      },
      {
        "start": 7991.579,
        "duration": 3.781,
        "text": "users practice in the dev environment"
      },
      {
        "start": 7993.32,
        "duration": 4.5,
        "text": "against the dev connections and then"
      },
      {
        "start": 7995.36,
        "duration": 5.4,
        "text": "when their code goes to the higher"
      },
      {
        "start": 7997.82,
        "duration": 3.72,
        "text": "environment it would be using a"
      },
      {
        "start": 8000.76,
        "duration": 2.459,
        "text": "um"
      },
      {
        "start": 8001.54,
        "duration": 4.92,
        "text": "production token yeah production perfect"
      },
      {
        "start": 8003.219,
        "duration": 6.061,
        "text": "so I know we saved the hardest for the"
      },
      {
        "start": 8006.46,
        "duration": 5.719,
        "text": "last right but everybody is eagerly"
      },
      {
        "start": 8009.28,
        "duration": 6.02,
        "text": "waiting for who's the winner let's see"
      },
      {
        "start": 8012.179,
        "duration": 3.121,
        "text": "do do"
      },
      {
        "start": 8015.94,
        "duration": 3.139,
        "text": "all right"
      },
      {
        "start": 8019.239,
        "duration": 5.541,
        "text": "nice wow Simba came in French"
      },
      {
        "start": 8025.139,
        "duration": 7.181,
        "text": "congratulations what I want"
      },
      {
        "start": 8028.9,
        "duration": 6.42,
        "text": "good job so what I would like you uh the"
      },
      {
        "start": 8032.32,
        "duration": 7.2,
        "text": "top three to do um are you gonna give"
      },
      {
        "start": 8035.32,
        "duration": 6.66,
        "text": "out three t-shirts or uh yeah yeah so"
      },
      {
        "start": 8039.52,
        "duration": 5.04,
        "text": "three of them that is shaurya Joker and"
      },
      {
        "start": 8041.98,
        "duration": 3.9,
        "text": "Ajay when uh the cool looking architect"
      },
      {
        "start": 8044.56,
        "duration": 3.78,
        "text": "shirts"
      },
      {
        "start": 8045.88,
        "duration": 4.859,
        "text": "um and I will let you know in a moment"
      },
      {
        "start": 8048.34,
        "duration": 4.14,
        "text": "how you can do that but but please take"
      },
      {
        "start": 8050.739,
        "duration": 6.241,
        "text": "a screenshot of this"
      },
      {
        "start": 8052.48,
        "duration": 5.34,
        "text": "um you know you uh we don't want you"
      },
      {
        "start": 8056.98,
        "duration": 3.179,
        "text": "um"
      },
      {
        "start": 8057.82,
        "duration": 4.919,
        "text": "giving a screenshot somewhere else but"
      },
      {
        "start": 8060.159,
        "duration": 4.381,
        "text": "but take it on your device okay um take"
      },
      {
        "start": 8062.739,
        "duration": 3.84,
        "text": "it on your device where it tells you"
      },
      {
        "start": 8064.54,
        "duration": 3.599,
        "text": "you're the winner okay so don't take a"
      },
      {
        "start": 8066.579,
        "duration": 3.721,
        "text": "screenshot of this I'm sorry I was wrong"
      },
      {
        "start": 8068.139,
        "duration": 5.221,
        "text": "okay so take a screenshot on your device"
      },
      {
        "start": 8070.3,
        "duration": 5.879,
        "text": "and save it for a moment and then we"
      },
      {
        "start": 8073.36,
        "duration": 4.62,
        "text": "will let you know how to get your swag"
      },
      {
        "start": 8076.179,
        "duration": 4.381,
        "text": "okay that's it"
      },
      {
        "start": 8077.98,
        "duration": 4.679,
        "text": "a couple things and mentee"
      },
      {
        "start": 8080.56,
        "duration": 5.159,
        "text": "um we'd like to get your feedback you"
      },
      {
        "start": 8082.659,
        "duration": 5.161,
        "text": "know um how would you like to"
      },
      {
        "start": 8085.719,
        "duration": 5.101,
        "text": "um improve our next Workshop you know"
      },
      {
        "start": 8087.82,
        "duration": 5.279,
        "text": "for instance we need to be on time but"
      },
      {
        "start": 8090.82,
        "duration": 4.56,
        "text": "but there was a lot of material to cover"
      },
      {
        "start": 8093.099,
        "duration": 5.04,
        "text": "um and you know it's not like we"
      },
      {
        "start": 8095.38,
        "duration": 6.08,
        "text": "taken too long but but hopefully you"
      },
      {
        "start": 8098.139,
        "duration": 3.321,
        "text": "know it's been enjoyable so far"
      },
      {
        "start": 8101.86,
        "duration": 6.0,
        "text": "please put in what you'd like us to"
      },
      {
        "start": 8104.32,
        "duration": 7.62,
        "text": "improve okay or"
      },
      {
        "start": 8107.86,
        "duration": 6.54,
        "text": "do that we thank you for the mentee"
      },
      {
        "start": 8111.94,
        "duration": 4.199,
        "text": "all right so now moving on to the"
      },
      {
        "start": 8114.4,
        "duration": 4.56,
        "text": "presentation"
      },
      {
        "start": 8116.139,
        "duration": 4.801,
        "text": "um what you will do is uh take a"
      },
      {
        "start": 8118.96,
        "duration": 3.6,
        "text": "screenshot of your midi screen right"
      },
      {
        "start": 8120.94,
        "duration": 3.6,
        "text": "congratulations again to the first"
      },
      {
        "start": 8122.56,
        "duration": 4.5,
        "text": "second and third place"
      },
      {
        "start": 8124.54,
        "duration": 5.22,
        "text": "um and send it to uh and I'll put it in"
      },
      {
        "start": 8127.06,
        "duration": 4.079,
        "text": "here I'll put it in chat I think it"
      },
      {
        "start": 8129.76,
        "duration": 3.18,
        "text": "should be there"
      },
      {
        "start": 8131.139,
        "duration": 3.121,
        "text": "if you just do a swag you should be able"
      },
      {
        "start": 8132.94,
        "duration": 4.34,
        "text": "to find it"
      },
      {
        "start": 8134.26,
        "duration": 3.02,
        "text": "let's see if it works"
      },
      {
        "start": 8139.0,
        "duration": 3.079,
        "text": "thanks shaurya"
      },
      {
        "start": 8144.46,
        "duration": 6.779,
        "text": "have some extra"
      },
      {
        "start": 8146.94,
        "duration": 6.34,
        "text": "data Stacks uh glass cups"
      },
      {
        "start": 8151.239,
        "duration": 4.561,
        "text": "uh sorry the steel cups I'll send those"
      },
      {
        "start": 8153.28,
        "duration": 4.26,
        "text": "along too we have a lot of extra ones"
      },
      {
        "start": 8155.8,
        "duration": 3.12,
        "text": "from before so"
      },
      {
        "start": 8157.54,
        "duration": 5.099,
        "text": "cool"
      },
      {
        "start": 8158.92,
        "duration": 5.819,
        "text": "is it us does that work you know maybe I"
      },
      {
        "start": 8162.639,
        "duration": 3.48,
        "text": "got it but okay okay yep"
      },
      {
        "start": 8164.739,
        "duration": 4.141,
        "text": "all right"
      },
      {
        "start": 8166.119,
        "duration": 4.861,
        "text": "yeah oh definitely okay and if you want"
      },
      {
        "start": 8168.88,
        "duration": 4.739,
        "text": "you want that again you know just put in"
      },
      {
        "start": 8170.98,
        "duration": 4.98,
        "text": "um exclamation swag and you'll you'll be"
      },
      {
        "start": 8173.619,
        "duration": 4.381,
        "text": "able to see again for more detailed"
      },
      {
        "start": 8175.96,
        "duration": 3.42,
        "text": "discussions you know jump on discard um"
      },
      {
        "start": 8178.0,
        "duration": 3.659,
        "text": "and and there are lots of people"
      },
      {
        "start": 8179.38,
        "duration": 4.08,
        "text": "answering questions uh I don't know if"
      },
      {
        "start": 8181.659,
        "duration": 3.48,
        "text": "you are in Discord"
      },
      {
        "start": 8183.46,
        "duration": 4.92,
        "text": "um you know but uh I'll get on there"
      },
      {
        "start": 8185.139,
        "duration": 4.5,
        "text": "yeah yeah you can watch out for anything"
      },
      {
        "start": 8188.38,
        "duration": 2.04,
        "text": "else"
      },
      {
        "start": 8189.639,
        "duration": 2.821,
        "text": "um"
      },
      {
        "start": 8190.42,
        "duration": 4.979,
        "text": "we do have a lot of workshops Wednesday"
      },
      {
        "start": 8192.46,
        "duration": 4.019,
        "text": "workshops I know that uh Eon I think you"
      },
      {
        "start": 8195.399,
        "duration": 3.061,
        "text": "know mentioned that he's going to be"
      },
      {
        "start": 8196.479,
        "duration": 5.16,
        "text": "back uh but you know"
      },
      {
        "start": 8198.46,
        "duration": 4.979,
        "text": "um others please free to join as well"
      },
      {
        "start": 8201.639,
        "duration": 5.161,
        "text": "um we have a casabra summit and it's"
      },
      {
        "start": 8203.439,
        "duration": 6.42,
        "text": "coming up on March 13th 14th I know I've"
      },
      {
        "start": 8206.8,
        "duration": 4.379,
        "text": "been working with rahu to get him to do"
      },
      {
        "start": 8209.859,
        "duration": 3.66,
        "text": "some submissions I don't know if you"
      },
      {
        "start": 8211.179,
        "duration": 4.8,
        "text": "already did Raul otherwise treat this as"
      },
      {
        "start": 8213.519,
        "duration": 6.84,
        "text": "you know again you know nagging you here"
      },
      {
        "start": 8215.979,
        "duration": 6.721,
        "text": "okay uh but uh cfp closes uh in in about"
      },
      {
        "start": 8220.359,
        "duration": 5.101,
        "text": "uh three or four days"
      },
      {
        "start": 8222.7,
        "duration": 6.54,
        "text": "um so don't be left out uh put in put in"
      },
      {
        "start": 8225.46,
        "duration": 5.099,
        "text": "a um a submission uh and and um you know"
      },
      {
        "start": 8229.24,
        "duration": 4.68,
        "text": "we're gonna build on the momentum that"
      },
      {
        "start": 8230.559,
        "duration": 5.34,
        "text": "we've had with the community and um you"
      },
      {
        "start": 8233.92,
        "duration": 4.679,
        "text": "know this is going to be happening March"
      },
      {
        "start": 8235.899,
        "duration": 4.2,
        "text": "13th 14th at San Jose uh there's gonna"
      },
      {
        "start": 8238.599,
        "duration": 2.76,
        "text": "be a training day followed by a number"
      },
      {
        "start": 8240.099,
        "duration": 4.561,
        "text": "of different"
      },
      {
        "start": 8241.359,
        "duration": 5.401,
        "text": "um you know sessions not just by data"
      },
      {
        "start": 8244.66,
        "duration": 3.12,
        "text": "Stacks but pretty much by the entire"
      },
      {
        "start": 8246.76,
        "duration": 3.12,
        "text": "community"
      },
      {
        "start": 8247.78,
        "duration": 4.619,
        "text": "and with that said"
      },
      {
        "start": 8249.88,
        "duration": 3.96,
        "text": "I want to thank you all very much thank"
      },
      {
        "start": 8252.399,
        "duration": 3.181,
        "text": "you"
      },
      {
        "start": 8253.84,
        "duration": 3.66,
        "text": "and we are off"
      },
      {
        "start": 8255.58,
        "duration": 4.519,
        "text": "thank you thanks for having us I"
      },
      {
        "start": 8257.5,
        "duration": 2.599,
        "text": "appreciate it"
      },
      {
        "start": 8263.62,
        "duration": 4.439,
        "text": "and as always don't forget to click that"
      },
      {
        "start": 8265.899,
        "duration": 4.08,
        "text": "subscribe button and ring that Bell to"
      },
      {
        "start": 8268.059,
        "duration": 4.62,
        "text": "get notifications for all of our future"
      },
      {
        "start": 8269.979,
        "duration": 5.641,
        "text": "upcoming imagine a being gifted with"
      },
      {
        "start": 8272.679,
        "duration": 5.281,
        "text": "powers from the goddess of Cassandra"
      },
      {
        "start": 8275.62,
        "duration": 4.8,
        "text": "those Powers until she can multiply it"
      },
      {
        "start": 8277.96,
        "duration": 4.74,
        "text": "will move with Limitless speed and"
      },
      {
        "start": 8280.42,
        "duration": 4.259,
        "text": "unmask hidden knowledge with those"
      },
      {
        "start": 8282.7,
        "duration": 4.859,
        "text": "Powers she was able to fully understand"
      },
      {
        "start": 8284.679,
        "duration": 5.46,
        "text": "the connectedness of the world what she"
      },
      {
        "start": 8287.559,
        "duration": 5.34,
        "text": "saw was a world in need of understanding"
      },
      {
        "start": 8290.139,
        "duration": 4.8,
        "text": "from that day forward she sought to"
      },
      {
        "start": 8292.899,
        "duration": 4.441,
        "text": "bestow her powers on all who came into"
      },
      {
        "start": 8294.939,
        "duration": 5.841,
        "text": "contact with her empowering them to"
      },
      {
        "start": 8297.34,
        "duration": 3.44,
        "text": "achieve wondrous feats"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T18:28:02.757139+00:00"
}