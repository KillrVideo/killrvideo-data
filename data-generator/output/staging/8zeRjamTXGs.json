{
  "video_id": "8zeRjamTXGs",
  "title": "Spark-Cassandra Connector 2.5 with Russ Spitzer | DataStax",
  "description": "Russ Spitzer returns to the show to discuss the benefits of combining Cassandra and Spark, the new release of the DataStax Spark-Cassandra connector, and why the proprietary features that used to ship with DataStax Enterprise have now been merged into the open-source project. We also check in on the Spark 3.0 Beta and discuss the tradeoffs in various ways you can deploy Spark and Cassandra together.",
  "published_at": "2020-06-05T08:39:26Z",
  "thumbnail": "https://i.ytimg.com/vi/8zeRjamTXGs/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=8zeRjamTXGs",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] from data stacks this is the distributed data show podcast on this week's episode of the distributed data show I'm talking with Russell Spitzer that's Russell with two L's not one just in case in case you have any question about that this is Jeff carpenter developer relations at data stacks welcome to the show arrests hi yeah it's great to be here we've had you on before talking about spark but it's been a while so what I thought we could do is do a little bit of a recap on you know what a spark what is spark plus Cassandra why do people do that and then just to set a little bit of context real quick bring everybody up to speed and then we're gonna dive into some exciting new stuff that you've been working on does that sound like a plan yeah that sounds great okay so Kay so yeah tell me a little bit about spark maybe a little your background from it and then then why we get into this world of Cassandra cliff spark yeah sure so you know a my history is actually started when in biological computing I got my degrees and computer science and math and then got a grad degree in bioinformatics and that's when I knew I really wanted to do distributed computing and batch analytics and that's why I joined data stacks and after many years of being there I nest Bend a lot of my time working with the integration between SPARC and Cassandra so the de sacs for Cassandra connector is now pretty much the only in most popular way of connecting to Sandra and SPARC and it basically provides an avenue for taking your data that's inside of Cassandra and using it inside of a spark environment I actually recently did a talk on this I'm forgetting what the name of it is but for scale by the bay I believe likewise Park I actually think I also did this at a the data sex accelerate last here I guess you do a lot of talk let's be fair yeah I I boiled this down into why why would you want to use spark and Cassandra in the first place and I kind of have three main categories that I like to think about first is is ETL which is that I have data somewhere else and they need to have it in Cassandra or I have it in Cassandra and I need it somewhere else and one of the great things about SPARC is that because it has all of these different connectors to all of these different systems once you have a connector for both systems that you want to use moving data between them is almost trivial I mean you basically set up your reader and writer for each side and SPARC does all the checking for error correction make sure that everything succeeds successfully make sure that everything is in the correct formats and of the correct casts and data types and all that kind of stuff for you so it's really a great tool for moving a huge amount of data from one place to another and making sure it's done correctly regardless of where you're taking it from and sending it to okay gotcha that's actually kind of a maybe an under underrated aspect of using spark as a tool for ETL I don't hear that quite as often so that's really good to highlight that yeah I mean it's it's it's huge because that's usually the entry point for a lot of folks they start doing that and then they realize wait there's things I want to do to the data in flight and that gets to my right second big category which is doing kind of batch analytics which is like I'm taking all this data out but I don't really need it all copied directly from one source to another maybe I just need it in another format or generated into reports where I have like summaries of how the data is represented things like that so this is where you can use spark as basically an analytics framework on top of your Cassandra data because normally you can't ask questions of Cassandra like of all of my customers last year how many bought more than a hundred pounds of cheese or something because Cassandra isn't set up to answer like broad questions like that right it's a very much for your queries it's very much about questions like I would much did customer a by last week between the hours of 5:00 and yeah it's getting very good to its best on that targeted queries that are gonna support directly actions in your application right yeah but spark kind of expands that it basically opens it up to everything else so any other question you might need to ask you can start asking with spark one of the cool things that spark has is this spark thrift server which is kind of grown out of the hive thrift server and in DSC we package it is something called a always on SQL server we give an H a component and basically that lets you connect a JDBC client to your Cassandra data with no fuss so basically now you can run ansi SQL against Cassandra and it will distribute all the things that need to be distributed so if you ask for something that requires like a giant join and shuffle and all the stuff SPARC will just do that all for you so it's a great way of providing access to everything in your data right and not just little bits of it I mean this is the like this is the secret cheat code for doing SQL on top of your Cassandra Day there yeah yeah and then you know the last big category that I like to tell people about for SPARC is is using it for streaming use cases because it's got a great streaming engine and because of its connect ability because of how many different connectors it has four different sources it provides a great platform for integrating data between things so for example if I've got a stream of data coming out of Kafka and this data needs to read a subset of my Cassandra data over and over and over because like I have a catalog and I got 40,000 customer requests and I need just those 40,000 partitions SPARC is a way to automate all of that fetching requesting managing concurrency all the things to make it performance and efficient and as a user you don't actually have to know how all of those things work so it's a great platform for connecting different data sources and streaming environments as well nice okay so I'm gonna add another question to my list I'm gonna pop it on to the end here because I really want to get to the stuff about the Sparky SATA connector but let's circle back later on and talk about this combination of SPARC Cassandra and Kafka you know huh how those three interplay together because I think that's somebody that people can get a little bit confused so let's well we'll set that to the side for just a second and we'll come back to it you know the main reason I wanted to have you on the show is of course to talk about the new release of the spark Cassandra connector 2.5 release right so what's what are the highlights yeah so this is a new new frontier for data stat so as you know the for a while data sex was doing a lot of work you know only inside of our enterprise product and we actually had a bunch of really awesome features that we had only a pact with data sex enterprise and smart connects and ER connected 2.5 is us taking all of them taking everything that we had internally and putting it all out of the open again so now all of the things that you could have only gotten if you were using data sex enterprise are now available in the open source connector in addition we've also added the full upgrade to the new Java driver for dot o and connections and the ability to work with Astra so you can actually now connect with the same connector open source Cassandra DSE Astra all at the same time same codebase same configuration it's nice okay so - previously - releases of the connector an open source based release and a DSC based release is alright yeah it was a little more confusing but basically if you got data stacks Enterprise there was a version of the spark eccentric connector that was included with it yeah and that differed from the open source version okay so it wasn't physically separate version or distinct version it was physically mystic yeah what's it what does the upgrade path look like for somebody that wants to either coming from previous version of open source connector or the the older DSE connector to this new app so the the most difficult part of the upgrade path because we basically made so everything that works previously still works okay but there are some changes if you are manually using the Java driver so as you know when we upgrade the Java driver guava is removed the entire future operations are all changed around async code has changed because of that if you were manually using the Java driver and you were using Java driver 300 code that no longer will work you have to change to Java driver for dot no code but that's really the only case where you have to upgrade if you were just using normal spark api's everything works exactly the same so it should be a source code compatible upgrade so you just take your code compile it against the new artifacts that should be it yeah ok so and and and I happen to know I monitor the internal slack channels so I happen to know that there was an effort to you know of kind of some of the some of the things that you went through in the process of updating from updating the spark Cassandra connector from the our data Sox Java 3 driver to the Deus Ex Java four driver I want to make sure I get on my version numbers right I think I did it the the DC version is one to two but now 2 is gone and there's only open source for doesn't really matter but yeah it's a it's a bit of a rough upgrade if you have a lot of code run configuration ok gotcha but apparently it well I mean that's the thing as the spark Cassandra connector had its own set of configuration parameters and things like that let's go because of that all the parameters that you use to configure the spark estándar connector are the exact same ones but the underlying code that sets up the connection the underlying code that handles futures all of that stuff had to change but it's all internal to the connector so you you as a user don't really have to know anything about it the cool thing though is that we also added in the additional hooks so that you can do that same file based configuration so ok this bar connector you've got your application configuration file you can just use that as your configuration for the spark standard connector awesome yeah ok so good and I would say I would guess that you would say like totally worth doing lots of in terms of the updates within the open source java for driver you know basically paying down a couple of years worth of technical debt like API redesign you went through the work of the upgrade and kind of know what was involved but not to put words in your mouth but I hate hopefully the end result is a little cleaner our little works a little bit better yeah I mean it's one of those things where it's very it was it's painful to get to that point for a lot of code for some applications I think it's actually very easy to do that upgrade but if you have a lot of custom connection and configuration code you basically have to rewrite it if you want it to behave the same way that it used to behave right but once you're done everything is much simpler to maintain I mean one of the huge things in this upgrade is that there's no guava dependency and because of that so yeah build binaries that are compatible with spark I mean this is the kind of stuff that was a big pain previously nice did I understand event you use a asynchronous API under the hood of the Java grab yeah so we use asynchronous API is in several places basically so spark has a couple levels of concurrency one is that it spawns up individual tasks that run separately those are independent chunks of work and it basically gives one thread to each of those and then within that you're actually allowed to of course do more concurrency or parallelism if you want and it doesn't make sense with Cassandra to have a single thread like execute a query and then wait execute a query and then wait you want to be proactively fetching data basically as quickly as you can so we have a couple parameters that allow you to tune how many simultaneous requests are happening at a time and we use we use our own semaphores and things like that to limit how many asynchronous requests are happening at the time but it's it's using the asynchronous API okay now are there any significant API changes in the 2.5 version of the connector so yeah again for the for an end user there really aren't any ok like all of the cool DC stuff is mostly around catalyst which is sparks internal optimization engine and basically what we have is a bunch of special rules special analysis patterns that look at how the data is about to be executed how the is about to be executed and rewrite it if it makes more sense in Cassandra to do it differently so one of the big examples is something we call the direct joy and the direct join is something that makes a lot of sense if you know a lot about Cassandra which is say I've got a whole large set of partition keys you know maybe like a hundred thousand partition keys and then you say I have got a Cassandra table that has a billion records well what's the fastest way to get those hundred thousand partition keys well it's not to read all of the Cassandra data writes keys and then compare them because we know in Cassandra the easiest way to look up a key is to just look it up it's one of the fastest patterns in Cassandra so one of the problems with the way spark was is constructed is that it doesn't have that notion of like this join where you just ask all for all of these pieces individually very quickly so what we did is we wrote a series of catalyst rules that basically looks at the join that's about to do analyzes the components of the joint and says this is the kind of join that Cassandra can do without any kind of scanning or shuffling or big data manipulation and we writes the plan and produces the plan that instead takes all those keys and asynchronously request them so basically you turn an operation that could have been a full table scan into a hundred thousand async and asynchronous key lookups which is going to be incredibly faster we got a bunch of parameters around tuning when this optimization takes place you can all see those details in the documentation cool things we did I know I know I have friends that want to dig into those details and ask I don't know if I know to ask the right questions about the data structures and algorithms that are in use now we're into the area of writing query optimizers maybe that's where I jumped off the path of computer science I'm not sure we we have I mean this is the one of the other big features that we included in this is the ability to get TTL and right time out of spark SQL which is actually more confusing than you would think in spark as well because they are real columns right I can't ask for the right time of just any column because it doesn't exist there's only right time and TTL for specific columns and if I make them present all the time if I say select star and I always pop up the right time in TTL alongside each column that's like a really bad user experience so you're talking about pulling in the underlying from from the underlying Cassandra via the the right time and TTL some of which is row level I guess for a partition key for for any column that's part of the key it's probably at that at that level at the row level and then individual columns beyond that yeah I made it's not harder than it was probably it's it's kind of like that I mean especially if you get into the differences between how collections are treated between various point releases of Cassandra they're like between three eight and three nine collections go from having a single value to a array of values and things like that but basically in order to support that sort of thing we actually had to do a very similar thing where we wrote custom plan information where we said if you're asking for TTL if we see this function that looks like it's asking for TTL find the underlying Cassandra column make sure it's valid if it's valid to get the TTL from that rewrite the plan so that function doesn't exist and instead we'll pretend there's a column there right now and you can pull it out so we have to do some some hoop hopping to kind of allow things that are very easy in cql but now if you're using the Sparky Santa connector 2.5 these things are possible you can select TTL and write time just like you were doing it in cql in sq out that's part that's very cool very cool and this is why I love talking to you because not only do I learn things about spark I also learned things about Cassandra so many good things okay well tell me what's going on with spark 3.0 this is on the horizon right I see I thought it's a beta release pop up on the radar recently what's happening so RC RC 2 is just voted down on but RC 3 is coming very soon it's we're getting very close to the thing line on the new spark release and the the main thing that's holding it up is workaround data source v2 which is also the most exciting thing about spark 3.0 okay let's do it so we're expecting to get it very soon so we're actually working on the patch right now for the spark estándar connected to do this and what spark 3.0 brings is this data source v2 ideal which is basically that the way doing the original implement implementation of doing data sources and spark work is it was very opaque to spark it was very hard for the data source to tell spark details about how the data was arranged how it was cataloged metadata about it data source v2 is trying to implement a more flexible approach so you're able to expose more information to spark so spark can make better decisions in its own core planning you know hopefully we get to the point where I don't have to do a lot of behind the scenes work to like make direct joint work and then eventually we can have joint optimizations just be something that data sources can expose the spark without a special path but what this really means for an end user is that the connection between a data source and spark is going to be much tighter than it's ever been before so previously if you wanted to access a table and Cassandra from spark you have to write a little bit of metadata that explains like this is what the table is called these are the connection parameters to connect to the Cassandra cluster so one is so bright right key space name and table name right base table name connection host yeah user credentials all this stuff you have to specify all the time connecting to multiple clusters is kind of hard because you know for each of these tables you're continually setting all those parameters and all sorts of stuff and even worse is that spark doesn't intrinsically know about all of the tables that are in your Cassandra like if you say I'm gonna connect to the cluster here it doesn't know to look up all of the metadata and represent them inside of spark SQL you have to register all of the tables individually yourself inside of DSC yes inside of DSC we actually had a few hooks to like automatically populate tables with these fake entries that were like you know these are the wrappers that would have made that refer to all these tables so for a lot of DSE users they got the experience that the tables were automatically known to spark but it was really kind of a wrapper around a bet hey so you were behind the scenes going in and doing like describe key space and describe tables and that's they are the equivalent of that forget it we were doing all of that and napping all of the things okay so like we were we were writing all these statements that otherwise you would have had stuff in spark 3.0 with the new data source API we can do a direct representation of the catalog inside of spark so what this means is you don't list what the table is called in the table a key space name instead what you do is you specify this is where my Cassandra cluster is and this is how you connect to it and once you've done that every table and key space is automatically registered so now there's no extra work you just say this is where this cluster is and this is how I describe it so you might say like Cassandra cluster one is specified by this connection host these user names and blah blah blah and then when you write spark SQL you just write select from Cassandra cluster one key space dot table and that's now they'll that's so much so you've totally decoupled the how do I connect to it from get me the data mm-hmm so not only is the catalog there it's also fully modifiable in both directions by spark so that means that if you change the key space or the tables inside of cql or any other process they're reflected immediately inside a spark and in spark you can do the same thing you can change the DDL of your Cassandra cluster now from spark sqi nice so it means if you want to make a new table you can make a new table in using SQL in spark so you can you can do all this stuff that used to be impossible so for example say I've got some data in a MongoDB cluster and I really want to make a Cassandra table about cassandra table with that data before I would have to go in to seek you out I'd have to make my key space I'd have to make my table I set this all up I make sure all the schemas and then I would go to spark and I would read from and right into that table so I have to use two different tools I have to make sure things match up although all the longhairs I have to make mine your data I up and right now what you can do is you can write create table as select and that will automatically make the table in Cassandra using the schema that you get from and all you have to do is specify what the partition key and clustering columns are going to be and that's it and it will do all the work of making all the metadata making all the the schema information and moving everything in one command so it's gonna be a whole new world for doing these kind of operations everything will be much easier and you won't need multiple tools anymore so that's very exciting for me it's pretty awesome so let me we we use you mouth just a little bit on the larger context of what is in spark 2 3 because I know that there's sort of a running joke on on past work releases that there's like oh it's a new release there's a new API you know we don't we don't use rdd's anymore now it's all about daydreams like what's a is there any of those kind of big-ticket surprises in in spark 3 I mean that's that's one of the main reasons that sparks having a hard time getting out those next releases there's some discussions about how the SQL dialect has to change to fit some of these new ideas right because previously the create table syntax was basically hi it's create table syntax which was a little limited because it could only express certain things and we want to have a much more generic language for creating tables now because we want to support any any provider who wants to make a catalog implementation so you want to be able to write a create statement that makes a MongoDB table and also makes the cassandra table and also can make a park a file and also can you know you imagine the engine and should also be able to work with that so that's the only thing folks may have to look out for because this upgrade to data source for YouTube does change some of those ideas a few things that used to be possible are now are no longer possible but there are things that didn't really make a lot of sense so something I'm sure a lot of folks are not familiar with is that smart too and one had a single at a set of modes for writing data frames you had a mode called error if exists you in Ana mode called a pen you had a mode called overwrite and you had a motor called ignore now what do you think that overwrite might do if it did exists you're gonna overwrite what was there before it will truncate the entire source that you are writing to and rewrite it entire and that's not necessarily what I would have expected so what does it overwrite all yes so what what might append to do if that's what overwrite does well I I guess I'm gonna hope that a pen doesn't do a truncate but it's just gonna happen to it's already there it will add but it could also overwrite was basically the way of doing adding additional data but for certain sources append is a insert which can remove the previous data so yeah there's a little semantic mismatch there isn't there so basically everything all of those modes only make sense if you're thinking about files right so if I'm thinking about files writing a file obviously I have to delete the old right appending I can't like just add data in the middle of the file so I'm always just adding more files yeah so that makes sense so those those were a little confusing and one of the the things that datasource b2 does is basically say indeed a source v2 we're just not gonna have these weird behaviors like you were gonna have a behavior where you add data and you're gonna have you know an overwrite operation but the override operations specifically you have to say what you're overriding you know things like that so things that make a lot more sense there should be a lot less gacho's one thing we actually did is in the spark Cassandra connector we said that if you use the overwrite mode we send up a warning first that's like we won't let you do this unless you also set an edition option that says that you know that this is going to do a truncate because too many people you know are like oh well I want to do up certs right I want to up cert into the tape right but the cement of the operation is drop table right okay so I it occurs to me that you know a number of our listening viewing audience is going to want to try some of this stuff out so you know both the SPARC three auto release but also the spark you standard connector 2-5 how do I get a nice demo with maybe a pre-built data set is that such a thing available oh man I wish I had something I'm pretty sure Alex OTT who's our coworker here at data stacks has got actually something for pulled up prepared up as a as a coordinated release with Zeppelin so you actually have everything set up with Zeppelin but we'll have to ping him and hopefully add a link at some point okay but you gotta maybe let's do we get Alex on and talk about the demo we're investing a lot more lately in examples we're actually building out the whole examples section of the website to to populate that with stuff that is well curated and even better maintained so because there's plenty of old example code out there that applies to past releases so anyway Rebecca with us here and Deborah is working a bunch on that so we'll be talking a bunch more about that coming up sorry for the I get to do ads for my own stuff on my own episode that's your prerogative nice so is there any uh is there anything else interesting in terms of that spark Zeppelin integration that that's worth I've against you is that is the go-to combo toolsets it's really common I mean so if you look at what what data bricks as a company is like built their entire model on is we are going to provide this notebook experience directly tied to a cluster and that's you know that's their big selling point and I think that makes a lot of sense because these notebooks are a really great way to not only like write code but also to or what you've done there a really good platform for you know yes marking up this is what this code does this is the result from this part I'm producing this and this and this so I think it's really good for prototyping all my code is totally self documenting by the way I'm not sure what you're talking about yeah I mean definitely not for me I just got a review a little my my PRS for this data source commute to work and and yarek was telling me because I was using a fold right at some point and he's like why are you folding right instead of folding left and I was just like I don't even wait you know I'm like I'm right the ended I think the moments was just like you know what we should go from that end instead of this good catch and very cool I have one more thing I want to ask you which is because it comes up from time to time sorry if it's a non stick order out of left field but I was here kind of there's a there seems to be a debate or a question that especially me people who the area have about okay I want to I want to most effectively combine spark and Cassandra from a deployment perspective and one model is okay I have my existing Cassandra cluster I'm just gonna spin up a separate spark cluster and point it at the Cassandra cluster you know by means of the connector and operate that way and then something that I've heard us articulate espoused from the DSE perspective is okay I'm going to actually set up my standard cluster with a separate data center so it's the say it's it's one conceptual cluster but replicating the data to a separate data center and then I can pound the crap out of that by co-locating spark on that Cassandra cluster and go to town and have operational analytics and don't affect the performance on my source cluster so like what are the pros and cons of these different things or you know how do I know it's the right deployment model yeah I mean this is a hard question it's really about what is easiest your organization because the different models have severely different pros and cons so obviously the DSC approach is based around the idea of the simplest possible deployment you have the same software installed on every machine when you start it up it's just very easy to have spark there at the same time the trade-offs are you aren't able to scale the analytics portion as easily as you might like and you can't have a transient analytics because you have to basically have this data center that's up all the time yes but you also could you get that you know that second replication of your data so it's easier to not affect your OLTP work we also have locality built into the spark estándar connector so it will take advantage of code lake located spark and Cassandra nodes but something I'm seeing a lot more now especially with the rise of more cloud-based spark providers is that people want a more transient spark environment or they're working with you know within their organization their organization has like a large spark cluster somewhere and they want to use resources from that so in that case I think it's you know it makes sense to just keep it you know not co-located because you have someone else who's taking care of the spark portion for you you really only want to responsible for the under part still probably make sense to separate out a DC that you're gonna read from and of course if you really are trying to avoid constantly hitting your Cassandra database do daily pulls of all your data and move it into park' files on s3 or something like that so that you have a static piece of the data that you can work on without impacting Cassandra load at all so those are like basically your two ideas as are you as a team managing spark and Cassandra if so CO deployment probably makes most sense for you because it's the least things you have to take care of while if you're part of an organization that already has a lot of institutional spark knowledge is already providing spark as a service that it makes sense to use that instead and not do colocation and I think for a lot of a lot of groups starting up now it makes a lot sense to keep your spark in some kind of cloud environment and keep your Cassandra you know somewhere else well you don't even have that option so like if you're using Astra then you want to use for spark you want to use like EMF or Asher's I forget address brand name for there right as a service or day to Brick's cloud I actually maybe this day to Brits that I think they branded day to Brick's cloud as nature's spark as a service but yeah that's basically what I'm saying is don't lose any sleep because you can't locate local okay I think these days the majority of the benefit is in operational simplicity for people managing everything the gains you get from the colocation are great but I think compared to the operational gains it's not not as valuable right no good thing this is a really great explanation of the trade-offs that you're making here and and I would guess that because of the improvements in the spark Cassandra connector you know the the performance of non collocated spark and Cassandra is as good as it probably can be given the reflash I mean we're we're stuck right now really with Cassandra's our bottleneck most of the time there are very few instances we've seen where the network is really the bottleneck for this sort of thing of course your mileage may vary depending on where your egressing and you know moving data to and from it may make sense to limit that if at all possible I mean we all know that if you've got a cluster that's not on AWS moving data to AWS I mean it's not the rental cost that's gonna cost you it's that data interesting yet it's pass where they get you so you know it's there's trade-offs all over the place and you really have to figure out what are what are your optimization goals I think I still think doing DSC as a little package is probably the fastest way to get everything set up and in an environment where you can work on it all easily but for a production environment where I'm at a company where we've got a thousand node smart cluster it doesn't make sense to Cole oh right right of course yeah there's always I mean larger the enterprise the more of these decisions have already been made on your behalf and you just have an environment a self-service environment which is great to kind of play into you so yes trade offs everywhere well thank you very much Russ Russell with two L's 102 else but very much appreciate you coming back on the show and hanging out with us and we will see you next week on another distributed data fill all right thanks so much for having me thanks for listening to the distributed data show please subscribe with your favorite podcast app give us a rating",
    "segments": [
      {
        "start": 9.44,
        "duration": 5.56,
        "text": "[Music]"
      },
      {
        "start": 11.45,
        "duration": 11.89,
        "text": "from data stacks this is the distributed"
      },
      {
        "start": 15.0,
        "duration": 11.189,
        "text": "data show podcast on this week's episode"
      },
      {
        "start": 23.34,
        "duration": 5.55,
        "text": "of the distributed data show I'm talking"
      },
      {
        "start": 26.189,
        "duration": 6.331,
        "text": "with Russell Spitzer that's Russell with"
      },
      {
        "start": 28.89,
        "duration": 4.95,
        "text": "two L's not one just in case in case you"
      },
      {
        "start": 32.52,
        "duration": 3.629,
        "text": "have any question about that this is"
      },
      {
        "start": 33.84,
        "duration": 3.51,
        "text": "Jeff carpenter developer relations at"
      },
      {
        "start": 36.149,
        "duration": 5.851,
        "text": "data stacks"
      },
      {
        "start": 37.35,
        "duration": 6.09,
        "text": "welcome to the show arrests hi yeah it's"
      },
      {
        "start": 42.0,
        "duration": 4.05,
        "text": "great to be here we've had you on before"
      },
      {
        "start": 43.44,
        "duration": 5.34,
        "text": "talking about spark but it's been a"
      },
      {
        "start": 46.05,
        "duration": 5.25,
        "text": "while so what I thought we could do is"
      },
      {
        "start": 48.78,
        "duration": 4.049,
        "text": "do a little bit of a recap on you know"
      },
      {
        "start": 51.3,
        "duration": 3.77,
        "text": "what a spark what is spark plus"
      },
      {
        "start": 52.829,
        "duration": 4.8,
        "text": "Cassandra why do people do that and then"
      },
      {
        "start": 55.07,
        "duration": 4.419,
        "text": "just to set a little bit of context real"
      },
      {
        "start": 57.629,
        "duration": 3.241,
        "text": "quick bring everybody up to speed and"
      },
      {
        "start": 59.489,
        "duration": 3.031,
        "text": "then we're gonna dive into some exciting"
      },
      {
        "start": 60.87,
        "duration": 4.079,
        "text": "new stuff that you've been working on"
      },
      {
        "start": 62.52,
        "duration": 2.94,
        "text": "does that sound like a plan yeah that"
      },
      {
        "start": 64.949,
        "duration": 2.911,
        "text": "sounds great"
      },
      {
        "start": 65.46,
        "duration": 3.75,
        "text": "okay so Kay so yeah tell me a little bit"
      },
      {
        "start": 67.86,
        "duration": 4.32,
        "text": "about spark maybe a little your"
      },
      {
        "start": 69.21,
        "duration": 4.47,
        "text": "background from it and then then why we"
      },
      {
        "start": 72.18,
        "duration": 7.049,
        "text": "get into this world of Cassandra cliff"
      },
      {
        "start": 73.68,
        "duration": 8.82,
        "text": "spark yeah sure so you know a my history"
      },
      {
        "start": 79.229,
        "duration": 6.841,
        "text": "is actually started when in biological"
      },
      {
        "start": 82.5,
        "duration": 4.89,
        "text": "computing I got my degrees and computer"
      },
      {
        "start": 86.07,
        "duration": 3.72,
        "text": "science and math and then got a grad"
      },
      {
        "start": 87.39,
        "duration": 4.5,
        "text": "degree in bioinformatics and that's when"
      },
      {
        "start": 89.79,
        "duration": 4.71,
        "text": "I knew I really wanted to do distributed"
      },
      {
        "start": 91.89,
        "duration": 6.03,
        "text": "computing and batch analytics and that's"
      },
      {
        "start": 94.5,
        "duration": 5.97,
        "text": "why I joined data stacks and after many"
      },
      {
        "start": 97.92,
        "duration": 3.989,
        "text": "years of being there I nest Bend a lot"
      },
      {
        "start": 100.47,
        "duration": 3.78,
        "text": "of my time working with the integration"
      },
      {
        "start": 101.909,
        "duration": 5.25,
        "text": "between SPARC and Cassandra so the de"
      },
      {
        "start": 104.25,
        "duration": 4.92,
        "text": "sacs for Cassandra connector is now"
      },
      {
        "start": 107.159,
        "duration": 4.861,
        "text": "pretty much the only in most popular way"
      },
      {
        "start": 109.17,
        "duration": 4.89,
        "text": "of connecting to Sandra and SPARC and it"
      },
      {
        "start": 112.02,
        "duration": 3.959,
        "text": "basically provides an avenue for taking"
      },
      {
        "start": 114.06,
        "duration": 3.9,
        "text": "your data that's inside of Cassandra and"
      },
      {
        "start": 115.979,
        "duration": 8.521,
        "text": "using it inside of a spark environment I"
      },
      {
        "start": 117.96,
        "duration": 7.979,
        "text": "actually recently did a talk on this I'm"
      },
      {
        "start": 124.5,
        "duration": 4.379,
        "text": "forgetting what the name of it is but"
      },
      {
        "start": 125.939,
        "duration": 5.221,
        "text": "for scale by the bay I believe likewise"
      },
      {
        "start": 128.879,
        "duration": 4.44,
        "text": "Park I actually think I also did this at"
      },
      {
        "start": 131.16,
        "duration": 4.98,
        "text": "a the data sex accelerate last"
      },
      {
        "start": 133.319,
        "duration": 6.03,
        "text": "here I guess you do a lot of talk let's"
      },
      {
        "start": 136.14,
        "duration": 5.37,
        "text": "be fair yeah I I boiled this down into"
      },
      {
        "start": 139.349,
        "duration": 3.931,
        "text": "why why would you want to use spark and"
      },
      {
        "start": 141.51,
        "duration": 3.509,
        "text": "Cassandra in the first place and I kind"
      },
      {
        "start": 143.28,
        "duration": 6.329,
        "text": "of have three main categories that I"
      },
      {
        "start": 145.019,
        "duration": 7.05,
        "text": "like to think about first is is ETL"
      },
      {
        "start": 149.609,
        "duration": 4.321,
        "text": "which is that I have data somewhere else"
      },
      {
        "start": 152.069,
        "duration": 3.331,
        "text": "and they need to have it in Cassandra or"
      },
      {
        "start": 153.93,
        "duration": 3.059,
        "text": "I have it in Cassandra and I need it"
      },
      {
        "start": 155.4,
        "duration": 3.78,
        "text": "somewhere else and one of the great"
      },
      {
        "start": 156.989,
        "duration": 3.75,
        "text": "things about SPARC is that because it"
      },
      {
        "start": 159.18,
        "duration": 4.38,
        "text": "has all of these different connectors to"
      },
      {
        "start": 160.739,
        "duration": 4.83,
        "text": "all of these different systems once you"
      },
      {
        "start": 163.56,
        "duration": 4.2,
        "text": "have a connector for both systems that"
      },
      {
        "start": 165.569,
        "duration": 4.221,
        "text": "you want to use moving data between them"
      },
      {
        "start": 167.76,
        "duration": 4.5,
        "text": "is almost trivial I mean you basically"
      },
      {
        "start": 169.79,
        "duration": 5.259,
        "text": "set up your reader and writer for each"
      },
      {
        "start": 172.26,
        "duration": 4.17,
        "text": "side and SPARC does all the checking for"
      },
      {
        "start": 175.049,
        "duration": 4.5,
        "text": "error correction make sure that"
      },
      {
        "start": 176.43,
        "duration": 4.83,
        "text": "everything succeeds successfully make"
      },
      {
        "start": 179.549,
        "duration": 3.601,
        "text": "sure that everything is in the correct"
      },
      {
        "start": 181.26,
        "duration": 3.27,
        "text": "formats and of the correct casts and"
      },
      {
        "start": 183.15,
        "duration": 3.569,
        "text": "data types and all that kind of stuff"
      },
      {
        "start": 184.53,
        "duration": 3.989,
        "text": "for you so it's really a great tool for"
      },
      {
        "start": 186.719,
        "duration": 3.091,
        "text": "moving a huge amount of data from one"
      },
      {
        "start": 188.519,
        "duration": 3.06,
        "text": "place to another and making sure it's"
      },
      {
        "start": 189.81,
        "duration": 3.329,
        "text": "done correctly regardless of where"
      },
      {
        "start": 191.579,
        "duration": 3.451,
        "text": "you're taking it from and sending it to"
      },
      {
        "start": 193.139,
        "duration": 4.77,
        "text": "okay gotcha that's actually kind of a"
      },
      {
        "start": 195.03,
        "duration": 5.639,
        "text": "maybe an under underrated aspect of"
      },
      {
        "start": 197.909,
        "duration": 4.17,
        "text": "using spark as a tool for ETL I don't"
      },
      {
        "start": 200.669,
        "duration": 3.69,
        "text": "hear that quite as often so that's"
      },
      {
        "start": 202.079,
        "duration": 6.15,
        "text": "really good to highlight that yeah I"
      },
      {
        "start": 204.359,
        "duration": 5.52,
        "text": "mean it's it's it's huge because that's"
      },
      {
        "start": 208.229,
        "duration": 3.0,
        "text": "usually the entry point for a lot of"
      },
      {
        "start": 209.879,
        "duration": 2.82,
        "text": "folks they start doing that and then"
      },
      {
        "start": 211.229,
        "duration": 3.12,
        "text": "they realize wait there's things I want"
      },
      {
        "start": 212.699,
        "duration": 5.19,
        "text": "to do to the data in flight and that"
      },
      {
        "start": 214.349,
        "duration": 5.31,
        "text": "gets to my right second big category"
      },
      {
        "start": 217.889,
        "duration": 3.421,
        "text": "which is doing kind of batch analytics"
      },
      {
        "start": 219.659,
        "duration": 3.3,
        "text": "which is like I'm taking all this data"
      },
      {
        "start": 221.31,
        "duration": 3.72,
        "text": "out but I don't really need it all"
      },
      {
        "start": 222.959,
        "duration": 3.691,
        "text": "copied directly from one source to"
      },
      {
        "start": 225.03,
        "duration": 4.889,
        "text": "another maybe I just need it in another"
      },
      {
        "start": 226.65,
        "duration": 5.22,
        "text": "format or generated into reports where I"
      },
      {
        "start": 229.919,
        "duration": 5.641,
        "text": "have like summaries of how the data is"
      },
      {
        "start": 231.87,
        "duration": 6.329,
        "text": "represented things like that so this is"
      },
      {
        "start": 235.56,
        "duration": 4.379,
        "text": "where you can use spark as basically an"
      },
      {
        "start": 238.199,
        "duration": 3.27,
        "text": "analytics framework on top of your"
      },
      {
        "start": 239.939,
        "duration": 3.931,
        "text": "Cassandra data because normally you"
      },
      {
        "start": 241.469,
        "duration": 5.76,
        "text": "can't ask questions of Cassandra like of"
      },
      {
        "start": 243.87,
        "duration": 5.369,
        "text": "all of my customers last year how many"
      },
      {
        "start": 247.229,
        "duration": 4.021,
        "text": "bought more than a hundred pounds of"
      },
      {
        "start": 249.239,
        "duration": 3.72,
        "text": "cheese or something because Cassandra"
      },
      {
        "start": 251.25,
        "duration": 3.15,
        "text": "isn't set up to answer like broad"
      },
      {
        "start": 252.959,
        "duration": 3.9,
        "text": "questions like that right it's a very"
      },
      {
        "start": 254.4,
        "duration": 4.589,
        "text": "much for your queries it's very much"
      },
      {
        "start": 256.859,
        "duration": 4.53,
        "text": "about questions like I would much did"
      },
      {
        "start": 258.989,
        "duration": 4.021,
        "text": "customer a by last week between the"
      },
      {
        "start": 261.389,
        "duration": 3.27,
        "text": "hours of 5:00 and yeah it's getting very"
      },
      {
        "start": 263.01,
        "duration": 2.97,
        "text": "good to its best on that targeted"
      },
      {
        "start": 264.659,
        "duration": 4.201,
        "text": "queries that are gonna"
      },
      {
        "start": 265.98,
        "duration": 4.07,
        "text": "support directly actions in your"
      },
      {
        "start": 268.86,
        "duration": 5.31,
        "text": "application right"
      },
      {
        "start": 270.05,
        "duration": 6.64,
        "text": "yeah but spark kind of expands that it"
      },
      {
        "start": 274.17,
        "duration": 4.56,
        "text": "basically opens it up to everything else"
      },
      {
        "start": 276.69,
        "duration": 5.58,
        "text": "so any other question you might need to"
      },
      {
        "start": 278.73,
        "duration": 6.42,
        "text": "ask you can start asking with spark one"
      },
      {
        "start": 282.27,
        "duration": 6.18,
        "text": "of the cool things that spark has is"
      },
      {
        "start": 285.15,
        "duration": 4.92,
        "text": "this spark thrift server which is kind"
      },
      {
        "start": 288.45,
        "duration": 3.36,
        "text": "of grown out of the hive thrift server"
      },
      {
        "start": 290.07,
        "duration": 4.38,
        "text": "and in DSC we package it is something"
      },
      {
        "start": 291.81,
        "duration": 5.01,
        "text": "called a always on SQL server we give an"
      },
      {
        "start": 294.45,
        "duration": 5.88,
        "text": "H a component and basically that lets"
      },
      {
        "start": 296.82,
        "duration": 6.09,
        "text": "you connect a JDBC client to your"
      },
      {
        "start": 300.33,
        "duration": 5.58,
        "text": "Cassandra data with no fuss so basically"
      },
      {
        "start": 302.91,
        "duration": 5.07,
        "text": "now you can run ansi SQL against"
      },
      {
        "start": 305.91,
        "duration": 3.54,
        "text": "Cassandra and it will distribute all the"
      },
      {
        "start": 307.98,
        "duration": 3.0,
        "text": "things that need to be distributed so if"
      },
      {
        "start": 309.45,
        "duration": 3.51,
        "text": "you ask for something that requires like"
      },
      {
        "start": 310.98,
        "duration": 3.72,
        "text": "a giant join and shuffle and all the"
      },
      {
        "start": 312.96,
        "duration": 4.44,
        "text": "stuff SPARC will just do that all for"
      },
      {
        "start": 314.7,
        "duration": 4.77,
        "text": "you so it's a great way of providing"
      },
      {
        "start": 317.4,
        "duration": 4.89,
        "text": "access to everything in your data right"
      },
      {
        "start": 319.47,
        "duration": 4.56,
        "text": "and not just little bits of it I mean"
      },
      {
        "start": 322.29,
        "duration": 5.31,
        "text": "this is the like this is the secret"
      },
      {
        "start": 324.03,
        "duration": 7.26,
        "text": "cheat code for doing SQL on top of your"
      },
      {
        "start": 327.6,
        "duration": 5.64,
        "text": "Cassandra Day there yeah yeah and then"
      },
      {
        "start": 331.29,
        "duration": 3.99,
        "text": "you know the last big category that I"
      },
      {
        "start": 333.24,
        "duration": 4.26,
        "text": "like to tell people about for SPARC is"
      },
      {
        "start": 335.28,
        "duration": 4.5,
        "text": "is using it for streaming use cases"
      },
      {
        "start": 337.5,
        "duration": 3.66,
        "text": "because it's got a great streaming"
      },
      {
        "start": 339.78,
        "duration": 2.61,
        "text": "engine and because of its connect"
      },
      {
        "start": 341.16,
        "duration": 2.94,
        "text": "ability because of how many different"
      },
      {
        "start": 342.39,
        "duration": 3.78,
        "text": "connectors it has four different sources"
      },
      {
        "start": 344.1,
        "duration": 4.11,
        "text": "it provides a great platform for"
      },
      {
        "start": 346.17,
        "duration": 4.17,
        "text": "integrating data between things so for"
      },
      {
        "start": 348.21,
        "duration": 4.74,
        "text": "example if I've got a stream of data"
      },
      {
        "start": 350.34,
        "duration": 4.8,
        "text": "coming out of Kafka and this data needs"
      },
      {
        "start": 352.95,
        "duration": 4.38,
        "text": "to read a subset of my Cassandra data"
      },
      {
        "start": 355.14,
        "duration": 4.68,
        "text": "over and over and over because like I"
      },
      {
        "start": 357.33,
        "duration": 4.71,
        "text": "have a catalog and I got 40,000 customer"
      },
      {
        "start": 359.82,
        "duration": 5.13,
        "text": "requests and I need just those 40,000"
      },
      {
        "start": 362.04,
        "duration": 5.7,
        "text": "partitions SPARC is a way to automate"
      },
      {
        "start": 364.95,
        "duration": 4.77,
        "text": "all of that fetching requesting managing"
      },
      {
        "start": 367.74,
        "duration": 4.5,
        "text": "concurrency all the things to make it"
      },
      {
        "start": 369.72,
        "duration": 4.05,
        "text": "performance and efficient and as a user"
      },
      {
        "start": 372.24,
        "duration": 3.03,
        "text": "you don't actually have to know how all"
      },
      {
        "start": 373.77,
        "duration": 3.3,
        "text": "of those things work so it's a great"
      },
      {
        "start": 375.27,
        "duration": 3.39,
        "text": "platform for connecting different data"
      },
      {
        "start": 377.07,
        "duration": 4.98,
        "text": "sources and streaming environments as"
      },
      {
        "start": 378.66,
        "duration": 5.1,
        "text": "well nice okay so I'm gonna add another"
      },
      {
        "start": 382.05,
        "duration": 4.32,
        "text": "question to my list I'm gonna pop it on"
      },
      {
        "start": 383.76,
        "duration": 4.26,
        "text": "to the end here because I really want to"
      },
      {
        "start": 386.37,
        "duration": 3.27,
        "text": "get to the stuff about the Sparky SATA"
      },
      {
        "start": 388.02,
        "duration": 4.47,
        "text": "connector but let's circle back later on"
      },
      {
        "start": 389.64,
        "duration": 6.54,
        "text": "and talk about this combination of SPARC"
      },
      {
        "start": 392.49,
        "duration": 5.1,
        "text": "Cassandra and Kafka you know huh how"
      },
      {
        "start": 396.18,
        "duration": 2.64,
        "text": "those three interplay together because I"
      },
      {
        "start": 397.59,
        "duration": 2.139,
        "text": "think that's somebody that people can"
      },
      {
        "start": 398.82,
        "duration": 2.89,
        "text": "get a little bit confused"
      },
      {
        "start": 399.729,
        "duration": 3.87,
        "text": "so let's well we'll set that to the side"
      },
      {
        "start": 401.71,
        "duration": 4.349,
        "text": "for just a second and we'll come back to"
      },
      {
        "start": 403.599,
        "duration": 4.261,
        "text": "it you know the main reason I wanted to"
      },
      {
        "start": 406.059,
        "duration": 4.29,
        "text": "have you on the show is of course to"
      },
      {
        "start": 407.86,
        "duration": 5.549,
        "text": "talk about the new release of the spark"
      },
      {
        "start": 410.349,
        "duration": 5.85,
        "text": "Cassandra connector 2.5 release right so"
      },
      {
        "start": 413.409,
        "duration": 6.361,
        "text": "what's what are the highlights yeah so"
      },
      {
        "start": 416.199,
        "duration": 6.3,
        "text": "this is a new new frontier for data stat"
      },
      {
        "start": 419.77,
        "duration": 5.069,
        "text": "so as you know the for a while data sex"
      },
      {
        "start": 422.499,
        "duration": 4.111,
        "text": "was doing a lot of work you know only"
      },
      {
        "start": 424.839,
        "duration": 3.75,
        "text": "inside of our enterprise product and we"
      },
      {
        "start": 426.61,
        "duration": 4.949,
        "text": "actually had a bunch of really awesome"
      },
      {
        "start": 428.589,
        "duration": 5.281,
        "text": "features that we had only a pact with"
      },
      {
        "start": 431.559,
        "duration": 5.13,
        "text": "data sex enterprise and smart connects"
      },
      {
        "start": 433.87,
        "duration": 4.879,
        "text": "and ER connected 2.5 is us taking all of"
      },
      {
        "start": 436.689,
        "duration": 4.801,
        "text": "them taking everything that we had"
      },
      {
        "start": 438.749,
        "duration": 4.51,
        "text": "internally and putting it all out of the"
      },
      {
        "start": 441.49,
        "duration": 3.299,
        "text": "open again so now all of the things that"
      },
      {
        "start": 443.259,
        "duration": 3.03,
        "text": "you could have only gotten if you were"
      },
      {
        "start": 444.789,
        "duration": 3.75,
        "text": "using data sex enterprise are now"
      },
      {
        "start": 446.289,
        "duration": 4.77,
        "text": "available in the open source connector"
      },
      {
        "start": 448.539,
        "duration": 4.44,
        "text": "in addition we've also added the full"
      },
      {
        "start": 451.059,
        "duration": 4.92,
        "text": "upgrade to the new Java driver for dot o"
      },
      {
        "start": 452.979,
        "duration": 5.131,
        "text": "and connections and the ability to work"
      },
      {
        "start": 455.979,
        "duration": 4.321,
        "text": "with Astra so you can actually now"
      },
      {
        "start": 458.11,
        "duration": 5.579,
        "text": "connect with the same connector open"
      },
      {
        "start": 460.3,
        "duration": 5.339,
        "text": "source Cassandra DSE Astra all at the"
      },
      {
        "start": 463.689,
        "duration": 4.141,
        "text": "same time same codebase same"
      },
      {
        "start": 465.639,
        "duration": 6.45,
        "text": "configuration it's nice okay so -"
      },
      {
        "start": 467.83,
        "duration": 7.379,
        "text": "previously - releases of the connector"
      },
      {
        "start": 472.089,
        "duration": 6.211,
        "text": "an open source based release and a DSC"
      },
      {
        "start": 475.209,
        "duration": 6.0,
        "text": "based release is alright yeah"
      },
      {
        "start": 478.3,
        "duration": 4.619,
        "text": "it was a little more confusing but"
      },
      {
        "start": 481.209,
        "duration": 3.27,
        "text": "basically if you got data stacks"
      },
      {
        "start": 482.919,
        "duration": 2.851,
        "text": "Enterprise there was a version of the"
      },
      {
        "start": 484.479,
        "duration": 4.291,
        "text": "spark eccentric connector that was"
      },
      {
        "start": 485.77,
        "duration": 4.739,
        "text": "included with it yeah and that differed"
      },
      {
        "start": 488.77,
        "duration": 3.81,
        "text": "from the open source version okay so it"
      },
      {
        "start": 490.509,
        "duration": 4.74,
        "text": "wasn't physically separate version or"
      },
      {
        "start": 492.58,
        "duration": 3.54,
        "text": "distinct version it was physically"
      },
      {
        "start": 495.249,
        "duration": 2.4,
        "text": "mystic yeah"
      },
      {
        "start": 496.12,
        "duration": 3.089,
        "text": "what's it what does the upgrade path"
      },
      {
        "start": 497.649,
        "duration": 3.66,
        "text": "look like for somebody that wants to"
      },
      {
        "start": 499.209,
        "duration": 4.71,
        "text": "either coming from previous version of"
      },
      {
        "start": 501.309,
        "duration": 6.301,
        "text": "open source connector or the the older"
      },
      {
        "start": 503.919,
        "duration": 6.39,
        "text": "DSE connector to this new app so the the"
      },
      {
        "start": 507.61,
        "duration": 4.5,
        "text": "most difficult part of the upgrade path"
      },
      {
        "start": 510.309,
        "duration": 4.051,
        "text": "because we basically made so everything"
      },
      {
        "start": 512.11,
        "duration": 4.799,
        "text": "that works previously still works okay"
      },
      {
        "start": 514.36,
        "duration": 5.369,
        "text": "but there are some changes if you are"
      },
      {
        "start": 516.909,
        "duration": 4.581,
        "text": "manually using the Java driver so as you"
      },
      {
        "start": 519.729,
        "duration": 5.1,
        "text": "know when we upgrade the Java driver"
      },
      {
        "start": 521.49,
        "duration": 5.38,
        "text": "guava is removed the entire future"
      },
      {
        "start": 524.829,
        "duration": 5.19,
        "text": "operations are all changed around async"
      },
      {
        "start": 526.87,
        "duration": 5.069,
        "text": "code has changed because of that if you"
      },
      {
        "start": 530.019,
        "duration": 3.07,
        "text": "were manually using the Java driver and"
      },
      {
        "start": 531.939,
        "duration": 4.361,
        "text": "you were using Java driver"
      },
      {
        "start": 533.089,
        "duration": 5.131,
        "text": "300 code that no longer will work you"
      },
      {
        "start": 536.3,
        "duration": 3.509,
        "text": "have to change to Java driver for dot no"
      },
      {
        "start": 538.22,
        "duration": 3.57,
        "text": "code but that's really the only case"
      },
      {
        "start": 539.809,
        "duration": 4.981,
        "text": "where you have to upgrade if you were"
      },
      {
        "start": 541.79,
        "duration": 5.969,
        "text": "just using normal spark api's everything"
      },
      {
        "start": 544.79,
        "duration": 5.19,
        "text": "works exactly the same so it should be a"
      },
      {
        "start": 547.759,
        "duration": 4.14,
        "text": "source code compatible upgrade so you"
      },
      {
        "start": 549.98,
        "duration": 4.32,
        "text": "just take your code compile it against"
      },
      {
        "start": 551.899,
        "duration": 5.281,
        "text": "the new artifacts that should be it yeah"
      },
      {
        "start": 554.3,
        "duration": 5.009,
        "text": "ok so and and and I happen to know I"
      },
      {
        "start": 557.18,
        "duration": 5.219,
        "text": "monitor the internal slack channels so I"
      },
      {
        "start": 559.309,
        "duration": 5.82,
        "text": "happen to know that there was an effort"
      },
      {
        "start": 562.399,
        "duration": 4.44,
        "text": "to you know of kind of some of the some"
      },
      {
        "start": 565.129,
        "duration": 4.171,
        "text": "of the things that you went through in"
      },
      {
        "start": 566.839,
        "duration": 4.831,
        "text": "the process of updating from updating"
      },
      {
        "start": 569.3,
        "duration": 5.909,
        "text": "the spark Cassandra connector from the"
      },
      {
        "start": 571.67,
        "duration": 5.49,
        "text": "our data Sox Java 3 driver to the Deus"
      },
      {
        "start": 575.209,
        "duration": 3.511,
        "text": "Ex Java four driver I want to make sure"
      },
      {
        "start": 577.16,
        "duration": 5.969,
        "text": "I get on my version numbers right I"
      },
      {
        "start": 578.72,
        "duration": 6.479,
        "text": "think I did it the the DC version is one"
      },
      {
        "start": 583.129,
        "duration": 5.37,
        "text": "to two but now 2 is gone and there's"
      },
      {
        "start": 585.199,
        "duration": 6.18,
        "text": "only open source for doesn't really"
      },
      {
        "start": 588.499,
        "duration": 4.65,
        "text": "matter but yeah it's a it's a bit of a"
      },
      {
        "start": 591.379,
        "duration": 4.77,
        "text": "rough upgrade if you have a lot of code"
      },
      {
        "start": 593.149,
        "duration": 6.511,
        "text": "run configuration ok gotcha but"
      },
      {
        "start": 596.149,
        "duration": 4.92,
        "text": "apparently it well I mean that's the"
      },
      {
        "start": 599.66,
        "duration": 3.179,
        "text": "thing as the spark Cassandra connector"
      },
      {
        "start": 601.069,
        "duration": 3.12,
        "text": "had its own set of configuration"
      },
      {
        "start": 602.839,
        "duration": 3.54,
        "text": "parameters and things like that"
      },
      {
        "start": 604.189,
        "duration": 3.781,
        "text": "let's go because of that all the"
      },
      {
        "start": 606.379,
        "duration": 3.18,
        "text": "parameters that you use to configure the"
      },
      {
        "start": 607.97,
        "duration": 3.869,
        "text": "spark estándar connector are the exact"
      },
      {
        "start": 609.559,
        "duration": 3.72,
        "text": "same ones but the underlying code that"
      },
      {
        "start": 611.839,
        "duration": 3.75,
        "text": "sets up the connection the underlying"
      },
      {
        "start": 613.279,
        "duration": 3.961,
        "text": "code that handles futures all of that"
      },
      {
        "start": 615.589,
        "duration": 3.75,
        "text": "stuff had to change but it's all"
      },
      {
        "start": 617.24,
        "duration": 5.25,
        "text": "internal to the connector so you"
      },
      {
        "start": 619.339,
        "duration": 3.961,
        "text": "you as a user don't really have to know"
      },
      {
        "start": 622.49,
        "duration": 2.519,
        "text": "anything about it"
      },
      {
        "start": 623.3,
        "duration": 3.539,
        "text": "the cool thing though is that we also"
      },
      {
        "start": 625.009,
        "duration": 3.63,
        "text": "added in the additional hooks so that"
      },
      {
        "start": 626.839,
        "duration": 6.0,
        "text": "you can do that same file based"
      },
      {
        "start": 628.639,
        "duration": 5.82,
        "text": "configuration so ok this bar connector"
      },
      {
        "start": 632.839,
        "duration": 4.17,
        "text": "you've got your application"
      },
      {
        "start": 634.459,
        "duration": 4.29,
        "text": "configuration file you can just use that"
      },
      {
        "start": 637.009,
        "duration": 3.81,
        "text": "as your configuration for the spark"
      },
      {
        "start": 638.749,
        "duration": 3.27,
        "text": "standard connector awesome yeah ok so"
      },
      {
        "start": 640.819,
        "duration": 3.0,
        "text": "good and I would say I would guess that"
      },
      {
        "start": 642.019,
        "duration": 6.12,
        "text": "you would say like totally worth doing"
      },
      {
        "start": 643.819,
        "duration": 8.07,
        "text": "lots of in terms of the updates within"
      },
      {
        "start": 648.139,
        "duration": 5.82,
        "text": "the open source java for driver you know"
      },
      {
        "start": 651.889,
        "duration": 4.11,
        "text": "basically paying down a couple of years"
      },
      {
        "start": 653.959,
        "duration": 5.04,
        "text": "worth of technical debt like API"
      },
      {
        "start": 655.999,
        "duration": 4.71,
        "text": "redesign you went through the work of"
      },
      {
        "start": 658.999,
        "duration": 4.56,
        "text": "the upgrade and kind of know what was"
      },
      {
        "start": 660.709,
        "duration": 4.05,
        "text": "involved but not to put words in your"
      },
      {
        "start": 663.559,
        "duration": 3.001,
        "text": "mouth but I hate hopefully the end"
      },
      {
        "start": 664.759,
        "duration": 4.071,
        "text": "result is a little cleaner"
      },
      {
        "start": 666.56,
        "duration": 5.1,
        "text": "our little works a little bit better"
      },
      {
        "start": 668.83,
        "duration": 5.47,
        "text": "yeah I mean it's one of those things"
      },
      {
        "start": 671.66,
        "duration": 5.07,
        "text": "where it's very it was it's painful to"
      },
      {
        "start": 674.3,
        "duration": 3.84,
        "text": "get to that point for a lot of code for"
      },
      {
        "start": 676.73,
        "duration": 3.9,
        "text": "some applications I think it's actually"
      },
      {
        "start": 678.14,
        "duration": 4.29,
        "text": "very easy to do that upgrade but if you"
      },
      {
        "start": 680.63,
        "duration": 3.63,
        "text": "have a lot of custom connection and"
      },
      {
        "start": 682.43,
        "duration": 3.48,
        "text": "configuration code you basically have to"
      },
      {
        "start": 684.26,
        "duration": 4.17,
        "text": "rewrite it if you want it to behave the"
      },
      {
        "start": 685.91,
        "duration": 5.46,
        "text": "same way that it used to behave right"
      },
      {
        "start": 688.43,
        "duration": 5.52,
        "text": "but once you're done everything is much"
      },
      {
        "start": 691.37,
        "duration": 3.99,
        "text": "simpler to maintain I mean one of the"
      },
      {
        "start": 693.95,
        "duration": 3.63,
        "text": "huge things in this upgrade is that"
      },
      {
        "start": 695.36,
        "duration": 5.88,
        "text": "there's no guava dependency and because"
      },
      {
        "start": 697.58,
        "duration": 5.55,
        "text": "of that so yeah build binaries that are"
      },
      {
        "start": 701.24,
        "duration": 3.36,
        "text": "compatible with spark I mean this is the"
      },
      {
        "start": 703.13,
        "duration": 4.8,
        "text": "kind of stuff that was a big pain"
      },
      {
        "start": 704.6,
        "duration": 6.66,
        "text": "previously nice did I understand event"
      },
      {
        "start": 707.93,
        "duration": 6.57,
        "text": "you use a asynchronous API under the"
      },
      {
        "start": 711.26,
        "duration": 6.29,
        "text": "hood of the Java grab yeah so we use"
      },
      {
        "start": 714.5,
        "duration": 6.45,
        "text": "asynchronous API is in several places"
      },
      {
        "start": 717.55,
        "duration": 5.56,
        "text": "basically so spark has a couple levels"
      },
      {
        "start": 720.95,
        "duration": 4.5,
        "text": "of concurrency one is that it spawns up"
      },
      {
        "start": 723.11,
        "duration": 4.38,
        "text": "individual tasks that run separately"
      },
      {
        "start": 725.45,
        "duration": 5.43,
        "text": "those are independent chunks of work and"
      },
      {
        "start": 727.49,
        "duration": 5.099,
        "text": "it basically gives one thread to each of"
      },
      {
        "start": 730.88,
        "duration": 4.07,
        "text": "those and then within that you're"
      },
      {
        "start": 732.589,
        "duration": 4.591,
        "text": "actually allowed to of course do more"
      },
      {
        "start": 734.95,
        "duration": 5.11,
        "text": "concurrency or parallelism if you want"
      },
      {
        "start": 737.18,
        "duration": 4.89,
        "text": "and it doesn't make sense with Cassandra"
      },
      {
        "start": 740.06,
        "duration": 4.05,
        "text": "to have a single thread like execute a"
      },
      {
        "start": 742.07,
        "duration": 4.53,
        "text": "query and then wait execute a query and"
      },
      {
        "start": 744.11,
        "duration": 5.22,
        "text": "then wait you want to be proactively"
      },
      {
        "start": 746.6,
        "duration": 4.71,
        "text": "fetching data basically as quickly as"
      },
      {
        "start": 749.33,
        "duration": 3.6,
        "text": "you can so we have a couple parameters"
      },
      {
        "start": 751.31,
        "duration": 3.6,
        "text": "that allow you to tune how many"
      },
      {
        "start": 752.93,
        "duration": 4.17,
        "text": "simultaneous requests are happening at a"
      },
      {
        "start": 754.91,
        "duration": 4.26,
        "text": "time and we use we use our own"
      },
      {
        "start": 757.1,
        "duration": 3.93,
        "text": "semaphores and things like that to limit"
      },
      {
        "start": 759.17,
        "duration": 5.58,
        "text": "how many asynchronous requests are"
      },
      {
        "start": 761.03,
        "duration": 7.32,
        "text": "happening at the time but it's it's"
      },
      {
        "start": 764.75,
        "duration": 6.51,
        "text": "using the asynchronous API okay now are"
      },
      {
        "start": 768.35,
        "duration": 6.57,
        "text": "there any significant API changes in the"
      },
      {
        "start": 771.26,
        "duration": 8.01,
        "text": "2.5 version of the connector so yeah"
      },
      {
        "start": 774.92,
        "duration": 6.48,
        "text": "again for the for an end user there"
      },
      {
        "start": 779.27,
        "duration": 6.87,
        "text": "really aren't any ok like all of the"
      },
      {
        "start": 781.4,
        "duration": 6.809,
        "text": "cool DC stuff is mostly around catalyst"
      },
      {
        "start": 786.14,
        "duration": 4.11,
        "text": "which is sparks internal optimization"
      },
      {
        "start": 788.209,
        "duration": 5.491,
        "text": "engine and basically what we have is a"
      },
      {
        "start": 790.25,
        "duration": 6.03,
        "text": "bunch of special rules special analysis"
      },
      {
        "start": 793.7,
        "duration": 4.26,
        "text": "patterns that look at how the data is"
      },
      {
        "start": 796.28,
        "duration": 3.869,
        "text": "about to be executed how the"
      },
      {
        "start": 797.96,
        "duration": 4.35,
        "text": "is about to be executed and rewrite it"
      },
      {
        "start": 800.149,
        "duration": 3.69,
        "text": "if it makes more sense in Cassandra to"
      },
      {
        "start": 802.31,
        "duration": 4.35,
        "text": "do it differently so one of the big"
      },
      {
        "start": 803.839,
        "duration": 6.421,
        "text": "examples is something we call the direct"
      },
      {
        "start": 806.66,
        "duration": 5.67,
        "text": "joy and the direct join is something"
      },
      {
        "start": 810.26,
        "duration": 3.93,
        "text": "that makes a lot of sense if you know a"
      },
      {
        "start": 812.33,
        "duration": 4.22,
        "text": "lot about Cassandra which is say I've"
      },
      {
        "start": 814.19,
        "duration": 4.709,
        "text": "got a whole large set of partition keys"
      },
      {
        "start": 816.55,
        "duration": 4.51,
        "text": "you know maybe like a hundred thousand"
      },
      {
        "start": 818.899,
        "duration": 4.051,
        "text": "partition keys and then you say I have"
      },
      {
        "start": 821.06,
        "duration": 4.02,
        "text": "got a Cassandra table that has a billion"
      },
      {
        "start": 822.95,
        "duration": 3.96,
        "text": "records well what's the fastest way to"
      },
      {
        "start": 825.08,
        "duration": 4.59,
        "text": "get those hundred thousand partition"
      },
      {
        "start": 826.91,
        "duration": 6.09,
        "text": "keys well it's not to read all of the"
      },
      {
        "start": 829.67,
        "duration": 5.16,
        "text": "Cassandra data writes keys and then"
      },
      {
        "start": 833.0,
        "duration": 3.87,
        "text": "compare them because we know in"
      },
      {
        "start": 834.83,
        "duration": 4.379,
        "text": "Cassandra the easiest way to look up a"
      },
      {
        "start": 836.87,
        "duration": 6.029,
        "text": "key is to just look it up it's one of"
      },
      {
        "start": 839.209,
        "duration": 5.491,
        "text": "the fastest patterns in Cassandra so one"
      },
      {
        "start": 842.899,
        "duration": 3.901,
        "text": "of the problems with the way spark was"
      },
      {
        "start": 844.7,
        "duration": 5.009,
        "text": "is constructed is that it doesn't have"
      },
      {
        "start": 846.8,
        "duration": 5.46,
        "text": "that notion of like this join where you"
      },
      {
        "start": 849.709,
        "duration": 5.25,
        "text": "just ask all for all of these pieces"
      },
      {
        "start": 852.26,
        "duration": 5.91,
        "text": "individually very quickly so what we did"
      },
      {
        "start": 854.959,
        "duration": 5.641,
        "text": "is we wrote a series of catalyst rules"
      },
      {
        "start": 858.17,
        "duration": 5.01,
        "text": "that basically looks at the join that's"
      },
      {
        "start": 860.6,
        "duration": 4.44,
        "text": "about to do analyzes the components of"
      },
      {
        "start": 863.18,
        "duration": 4.29,
        "text": "the joint and says this is the kind of"
      },
      {
        "start": 865.04,
        "duration": 5.13,
        "text": "join that Cassandra can do without any"
      },
      {
        "start": 867.47,
        "duration": 5.91,
        "text": "kind of scanning or shuffling or big"
      },
      {
        "start": 870.17,
        "duration": 6.12,
        "text": "data manipulation and we writes the plan"
      },
      {
        "start": 873.38,
        "duration": 4.38,
        "text": "and produces the plan that instead takes"
      },
      {
        "start": 876.29,
        "duration": 5.52,
        "text": "all those keys and asynchronously"
      },
      {
        "start": 877.76,
        "duration": 6.329,
        "text": "request them so basically you turn an"
      },
      {
        "start": 881.81,
        "duration": 5.37,
        "text": "operation that could have been a full"
      },
      {
        "start": 884.089,
        "duration": 5.011,
        "text": "table scan into a hundred thousand async"
      },
      {
        "start": 887.18,
        "duration": 4.89,
        "text": "and asynchronous key lookups which is"
      },
      {
        "start": 889.1,
        "duration": 4.44,
        "text": "going to be incredibly faster we got a"
      },
      {
        "start": 892.07,
        "duration": 3.69,
        "text": "bunch of parameters around tuning when"
      },
      {
        "start": 893.54,
        "duration": 3.39,
        "text": "this optimization takes place you can"
      },
      {
        "start": 895.76,
        "duration": 5.04,
        "text": "all see those details in the"
      },
      {
        "start": 896.93,
        "duration": 5.58,
        "text": "documentation cool things we did I know"
      },
      {
        "start": 900.8,
        "duration": 3.24,
        "text": "I know I have friends that want to dig"
      },
      {
        "start": 902.51,
        "duration": 3.84,
        "text": "into those details and ask I don't know"
      },
      {
        "start": 904.04,
        "duration": 3.96,
        "text": "if I know to ask the right questions"
      },
      {
        "start": 906.35,
        "duration": 4.739,
        "text": "about the data structures and algorithms"
      },
      {
        "start": 908.0,
        "duration": 6.63,
        "text": "that are in use now we're into the area"
      },
      {
        "start": 911.089,
        "duration": 4.981,
        "text": "of writing query optimizers maybe that's"
      },
      {
        "start": 914.63,
        "duration": 5.64,
        "text": "where I jumped off the path of computer"
      },
      {
        "start": 916.07,
        "duration": 5.759,
        "text": "science I'm not sure we we have I mean"
      },
      {
        "start": 920.27,
        "duration": 4.2,
        "text": "this is the one of the other big"
      },
      {
        "start": 921.829,
        "duration": 4.801,
        "text": "features that we included in this is the"
      },
      {
        "start": 924.47,
        "duration": 4.02,
        "text": "ability to get TTL and right time out of"
      },
      {
        "start": 926.63,
        "duration": 2.47,
        "text": "spark SQL which is actually more"
      },
      {
        "start": 928.49,
        "duration": 3.07,
        "text": "confusing"
      },
      {
        "start": 929.1,
        "duration": 5.1,
        "text": "than you would think in spark as well"
      },
      {
        "start": 931.56,
        "duration": 5.73,
        "text": "because they are real columns right I"
      },
      {
        "start": 934.2,
        "duration": 4.83,
        "text": "can't ask for the right time of just any"
      },
      {
        "start": 937.29,
        "duration": 3.72,
        "text": "column because it doesn't exist"
      },
      {
        "start": 939.03,
        "duration": 4.65,
        "text": "there's only right time and TTL for"
      },
      {
        "start": 941.01,
        "duration": 4.77,
        "text": "specific columns and if I make them"
      },
      {
        "start": 943.68,
        "duration": 4.77,
        "text": "present all the time if I say select"
      },
      {
        "start": 945.78,
        "duration": 4.8,
        "text": "star and I always pop up the right time"
      },
      {
        "start": 948.45,
        "duration": 4.17,
        "text": "in TTL alongside each column that's like"
      },
      {
        "start": 950.58,
        "duration": 3.39,
        "text": "a really bad user experience so you're"
      },
      {
        "start": 952.62,
        "duration": 3.53,
        "text": "talking about pulling in the underlying"
      },
      {
        "start": 953.97,
        "duration": 4.83,
        "text": "from from the underlying Cassandra via"
      },
      {
        "start": 956.15,
        "duration": 5.5,
        "text": "the the right time and TTL some of which"
      },
      {
        "start": 958.8,
        "duration": 5.52,
        "text": "is row level I guess for a partition key"
      },
      {
        "start": 961.65,
        "duration": 4.95,
        "text": "for for any column that's part of the"
      },
      {
        "start": 964.32,
        "duration": 4.26,
        "text": "key it's probably at that at that level"
      },
      {
        "start": 966.6,
        "duration": 4.83,
        "text": "at the row level and then individual"
      },
      {
        "start": 968.58,
        "duration": 5.7,
        "text": "columns beyond that yeah I made it's not"
      },
      {
        "start": 971.43,
        "duration": 5.64,
        "text": "harder than it was probably it's it's"
      },
      {
        "start": 974.28,
        "duration": 4.38,
        "text": "kind of like that I mean especially if"
      },
      {
        "start": 977.07,
        "duration": 3.57,
        "text": "you get into the differences between how"
      },
      {
        "start": 978.66,
        "duration": 5.28,
        "text": "collections are treated between various"
      },
      {
        "start": 980.64,
        "duration": 4.95,
        "text": "point releases of Cassandra they're like"
      },
      {
        "start": 983.94,
        "duration": 3.57,
        "text": "between three eight and three nine"
      },
      {
        "start": 985.59,
        "duration": 3.99,
        "text": "collections go from having a single"
      },
      {
        "start": 987.51,
        "duration": 5.94,
        "text": "value to a array of values and things"
      },
      {
        "start": 989.58,
        "duration": 5.22,
        "text": "like that but basically in order to"
      },
      {
        "start": 993.45,
        "duration": 3.75,
        "text": "support that sort of thing we actually"
      },
      {
        "start": 994.8,
        "duration": 4.56,
        "text": "had to do a very similar thing where we"
      },
      {
        "start": 997.2,
        "duration": 4.17,
        "text": "wrote custom plan information where we"
      },
      {
        "start": 999.36,
        "duration": 3.81,
        "text": "said if you're asking for TTL if we see"
      },
      {
        "start": 1001.37,
        "duration": 3.9,
        "text": "this function that looks like it's"
      },
      {
        "start": 1003.17,
        "duration": 4.32,
        "text": "asking for TTL find the underlying"
      },
      {
        "start": 1005.27,
        "duration": 3.87,
        "text": "Cassandra column make sure it's valid if"
      },
      {
        "start": 1007.49,
        "duration": 3.27,
        "text": "it's valid to get the TTL from that"
      },
      {
        "start": 1009.14,
        "duration": 3.63,
        "text": "rewrite the plan so that function"
      },
      {
        "start": 1010.76,
        "duration": 3.78,
        "text": "doesn't exist and instead we'll pretend"
      },
      {
        "start": 1012.77,
        "duration": 4.02,
        "text": "there's a column there right now and you"
      },
      {
        "start": 1014.54,
        "duration": 4.17,
        "text": "can pull it out so we have to do some"
      },
      {
        "start": 1016.79,
        "duration": 5.01,
        "text": "some hoop hopping to kind of allow"
      },
      {
        "start": 1018.71,
        "duration": 4.98,
        "text": "things that are very easy in cql but now"
      },
      {
        "start": 1021.8,
        "duration": 4.62,
        "text": "if you're using the Sparky Santa"
      },
      {
        "start": 1023.69,
        "duration": 4.8,
        "text": "connector 2.5 these things are possible"
      },
      {
        "start": 1026.42,
        "duration": 5.1,
        "text": "you can select TTL and write time just"
      },
      {
        "start": 1028.49,
        "duration": 4.8,
        "text": "like you were doing it in cql in sq out"
      },
      {
        "start": 1031.52,
        "duration": 3.54,
        "text": "that's part that's very cool very cool"
      },
      {
        "start": 1033.29,
        "duration": 3.36,
        "text": "and this is why I love talking to you"
      },
      {
        "start": 1035.06,
        "duration": 2.28,
        "text": "because not only do I learn things about"
      },
      {
        "start": 1036.65,
        "duration": 4.74,
        "text": "spark"
      },
      {
        "start": 1037.34,
        "duration": 5.34,
        "text": "I also learned things about Cassandra so"
      },
      {
        "start": 1041.39,
        "duration": 5.22,
        "text": "many good things okay well tell me"
      },
      {
        "start": 1042.68,
        "duration": 6.09,
        "text": "what's going on with spark 3.0 this is"
      },
      {
        "start": 1046.61,
        "duration": 5.25,
        "text": "on the horizon right I see I thought"
      },
      {
        "start": 1048.77,
        "duration": 6.63,
        "text": "it's a beta release pop up on the radar"
      },
      {
        "start": 1051.86,
        "duration": 6.21,
        "text": "recently what's happening so RC RC 2 is"
      },
      {
        "start": 1055.4,
        "duration": 6.69,
        "text": "just voted down on but RC 3 is coming"
      },
      {
        "start": 1058.07,
        "duration": 4.66,
        "text": "very soon it's we're getting very close"
      },
      {
        "start": 1062.09,
        "duration": 2.709,
        "text": "to the thing"
      },
      {
        "start": 1062.73,
        "duration": 3.75,
        "text": "line on the new spark release and the"
      },
      {
        "start": 1064.799,
        "duration": 4.471,
        "text": "the main thing that's holding it up is"
      },
      {
        "start": 1066.48,
        "duration": 5.579,
        "text": "workaround data source v2 which is also"
      },
      {
        "start": 1069.27,
        "duration": 5.34,
        "text": "the most exciting thing about spark 3.0"
      },
      {
        "start": 1072.059,
        "duration": 4.411,
        "text": "okay let's do it so we're expecting to"
      },
      {
        "start": 1074.61,
        "duration": 3.51,
        "text": "get it very soon so we're actually"
      },
      {
        "start": 1076.47,
        "duration": 3.42,
        "text": "working on the patch right now for the"
      },
      {
        "start": 1078.12,
        "duration": 5.07,
        "text": "spark estándar connected to do this and"
      },
      {
        "start": 1079.89,
        "duration": 5.99,
        "text": "what spark 3.0 brings is this data"
      },
      {
        "start": 1083.19,
        "duration": 5.57,
        "text": "source v2 ideal which is basically that"
      },
      {
        "start": 1085.88,
        "duration": 4.99,
        "text": "the way doing the original implement"
      },
      {
        "start": 1088.76,
        "duration": 4.6,
        "text": "implementation of doing data sources and"
      },
      {
        "start": 1090.87,
        "duration": 4.2,
        "text": "spark work is it was very opaque to"
      },
      {
        "start": 1093.36,
        "duration": 4.14,
        "text": "spark it was very hard for the data"
      },
      {
        "start": 1095.07,
        "duration": 4.14,
        "text": "source to tell spark details about how"
      },
      {
        "start": 1097.5,
        "duration": 5.28,
        "text": "the data was arranged how it was"
      },
      {
        "start": 1099.21,
        "duration": 5.73,
        "text": "cataloged metadata about it data source"
      },
      {
        "start": 1102.78,
        "duration": 4.08,
        "text": "v2 is trying to implement a more"
      },
      {
        "start": 1104.94,
        "duration": 4.95,
        "text": "flexible approach so you're able to"
      },
      {
        "start": 1106.86,
        "duration": 4.74,
        "text": "expose more information to spark so"
      },
      {
        "start": 1109.89,
        "duration": 3.69,
        "text": "spark can make better decisions in its"
      },
      {
        "start": 1111.6,
        "duration": 3.24,
        "text": "own core planning you know hopefully we"
      },
      {
        "start": 1113.58,
        "duration": 4.349,
        "text": "get to the point where I don't have to"
      },
      {
        "start": 1114.84,
        "duration": 4.5,
        "text": "do a lot of behind the scenes work to"
      },
      {
        "start": 1117.929,
        "duration": 2.971,
        "text": "like make direct joint work and then"
      },
      {
        "start": 1119.34,
        "duration": 3.36,
        "text": "eventually we can have joint"
      },
      {
        "start": 1120.9,
        "duration": 3.99,
        "text": "optimizations just be something that"
      },
      {
        "start": 1122.7,
        "duration": 6.15,
        "text": "data sources can expose the spark"
      },
      {
        "start": 1124.89,
        "duration": 6.39,
        "text": "without a special path but what this"
      },
      {
        "start": 1128.85,
        "duration": 4.5,
        "text": "really means for an end user is that the"
      },
      {
        "start": 1131.28,
        "duration": 4.23,
        "text": "connection between a data source and"
      },
      {
        "start": 1133.35,
        "duration": 4.23,
        "text": "spark is going to be much tighter than"
      },
      {
        "start": 1135.51,
        "duration": 3.63,
        "text": "it's ever been before so previously if"
      },
      {
        "start": 1137.58,
        "duration": 3.93,
        "text": "you wanted to access a table and"
      },
      {
        "start": 1139.14,
        "duration": 4.05,
        "text": "Cassandra from spark you have to write a"
      },
      {
        "start": 1141.51,
        "duration": 3.48,
        "text": "little bit of metadata that explains"
      },
      {
        "start": 1143.19,
        "duration": 3.479,
        "text": "like this is what the table is called"
      },
      {
        "start": 1144.99,
        "duration": 3.51,
        "text": "these are the connection parameters to"
      },
      {
        "start": 1146.669,
        "duration": 3.661,
        "text": "connect to the Cassandra cluster so one"
      },
      {
        "start": 1148.5,
        "duration": 4.29,
        "text": "is so bright right key space name and"
      },
      {
        "start": 1150.33,
        "duration": 4.83,
        "text": "table name right base table name"
      },
      {
        "start": 1152.79,
        "duration": 4.259,
        "text": "connection host yeah user credentials"
      },
      {
        "start": 1155.16,
        "duration": 4.259,
        "text": "all this stuff you have to specify all"
      },
      {
        "start": 1157.049,
        "duration": 3.841,
        "text": "the time connecting to multiple clusters"
      },
      {
        "start": 1159.419,
        "duration": 2.851,
        "text": "is kind of hard because you know for"
      },
      {
        "start": 1160.89,
        "duration": 2.64,
        "text": "each of these tables you're continually"
      },
      {
        "start": 1162.27,
        "duration": 4.47,
        "text": "setting all those parameters and all"
      },
      {
        "start": 1163.53,
        "duration": 5.97,
        "text": "sorts of stuff and even worse is that"
      },
      {
        "start": 1166.74,
        "duration": 4.41,
        "text": "spark doesn't intrinsically know about"
      },
      {
        "start": 1169.5,
        "duration": 3.36,
        "text": "all of the tables that are in your"
      },
      {
        "start": 1171.15,
        "duration": 4.23,
        "text": "Cassandra like if you say I'm gonna"
      },
      {
        "start": 1172.86,
        "duration": 4.53,
        "text": "connect to the cluster here it doesn't"
      },
      {
        "start": 1175.38,
        "duration": 4.53,
        "text": "know to look up all of the metadata and"
      },
      {
        "start": 1177.39,
        "duration": 3.99,
        "text": "represent them inside of spark SQL you"
      },
      {
        "start": 1179.91,
        "duration": 5.78,
        "text": "have to register all of the tables"
      },
      {
        "start": 1181.38,
        "duration": 7.14,
        "text": "individually yourself inside of DSC yes"
      },
      {
        "start": 1185.69,
        "duration": 5.26,
        "text": "inside of DSC we actually had a few"
      },
      {
        "start": 1188.52,
        "duration": 4.83,
        "text": "hooks to like automatically populate"
      },
      {
        "start": 1190.95,
        "duration": 3.69,
        "text": "tables with these fake entries that were"
      },
      {
        "start": 1193.35,
        "duration": 1.68,
        "text": "like you know these are the wrappers"
      },
      {
        "start": 1194.64,
        "duration": 1.86,
        "text": "that"
      },
      {
        "start": 1195.03,
        "duration": 3.87,
        "text": "would have made that refer to all these"
      },
      {
        "start": 1196.5,
        "duration": 4.74,
        "text": "tables so for a lot of DSE users they"
      },
      {
        "start": 1198.9,
        "duration": 4.41,
        "text": "got the experience that the tables were"
      },
      {
        "start": 1201.24,
        "duration": 4.98,
        "text": "automatically known to spark but it was"
      },
      {
        "start": 1203.31,
        "duration": 4.89,
        "text": "really kind of a wrapper around a bet"
      },
      {
        "start": 1206.22,
        "duration": 4.14,
        "text": "hey so you were behind the scenes going"
      },
      {
        "start": 1208.2,
        "duration": 3.87,
        "text": "in and doing like describe key space and"
      },
      {
        "start": 1210.36,
        "duration": 5.85,
        "text": "describe tables and that's they are the"
      },
      {
        "start": 1212.07,
        "duration": 5.94,
        "text": "equivalent of that forget it we were"
      },
      {
        "start": 1216.21,
        "duration": 4.23,
        "text": "doing all of that and napping all of the"
      },
      {
        "start": 1218.01,
        "duration": 3.75,
        "text": "things okay so like we were we were"
      },
      {
        "start": 1220.44,
        "duration": 3.2,
        "text": "writing all these statements that"
      },
      {
        "start": 1221.76,
        "duration": 4.5,
        "text": "otherwise you would have had stuff in"
      },
      {
        "start": 1223.64,
        "duration": 5.71,
        "text": "spark 3.0 with the new data source API"
      },
      {
        "start": 1226.26,
        "duration": 5.88,
        "text": "we can do a direct representation of the"
      },
      {
        "start": 1229.35,
        "duration": 5.07,
        "text": "catalog inside of spark so what this"
      },
      {
        "start": 1232.14,
        "duration": 4.85,
        "text": "means is you don't list what the table"
      },
      {
        "start": 1234.42,
        "duration": 5.7,
        "text": "is called in the table a key space name"
      },
      {
        "start": 1236.99,
        "duration": 4.81,
        "text": "instead what you do is you specify this"
      },
      {
        "start": 1240.12,
        "duration": 3.99,
        "text": "is where my Cassandra cluster is and"
      },
      {
        "start": 1241.8,
        "duration": 4.23,
        "text": "this is how you connect to it and once"
      },
      {
        "start": 1244.11,
        "duration": 4.71,
        "text": "you've done that every table and key"
      },
      {
        "start": 1246.03,
        "duration": 4.83,
        "text": "space is automatically registered so now"
      },
      {
        "start": 1248.82,
        "duration": 3.87,
        "text": "there's no extra work you just say this"
      },
      {
        "start": 1250.86,
        "duration": 3.15,
        "text": "is where this cluster is and this is how"
      },
      {
        "start": 1252.69,
        "duration": 4.41,
        "text": "I describe it so you might say like"
      },
      {
        "start": 1254.01,
        "duration": 4.95,
        "text": "Cassandra cluster one is specified by"
      },
      {
        "start": 1257.1,
        "duration": 3.75,
        "text": "this connection host these user names"
      },
      {
        "start": 1258.96,
        "duration": 3.75,
        "text": "and blah blah blah and then when you"
      },
      {
        "start": 1260.85,
        "duration": 5.52,
        "text": "write spark SQL you just write select"
      },
      {
        "start": 1262.71,
        "duration": 5.55,
        "text": "from Cassandra cluster one key space dot"
      },
      {
        "start": 1266.37,
        "duration": 5.31,
        "text": "table and that's now they'll that's so"
      },
      {
        "start": 1268.26,
        "duration": 5.87,
        "text": "much so you've totally decoupled the how"
      },
      {
        "start": 1271.68,
        "duration": 5.61,
        "text": "do I connect to it from get me the data"
      },
      {
        "start": 1274.13,
        "duration": 6.28,
        "text": "mm-hmm so not only is the catalog there"
      },
      {
        "start": 1277.29,
        "duration": 5.85,
        "text": "it's also fully modifiable in both"
      },
      {
        "start": 1280.41,
        "duration": 4.65,
        "text": "directions by spark so that means that"
      },
      {
        "start": 1283.14,
        "duration": 4.62,
        "text": "if you change the key space or the"
      },
      {
        "start": 1285.06,
        "duration": 4.86,
        "text": "tables inside of cql or any other"
      },
      {
        "start": 1287.76,
        "duration": 5.85,
        "text": "process they're reflected immediately"
      },
      {
        "start": 1289.92,
        "duration": 6.15,
        "text": "inside a spark and in spark you can do"
      },
      {
        "start": 1293.61,
        "duration": 5.01,
        "text": "the same thing you can change the DDL of"
      },
      {
        "start": 1296.07,
        "duration": 6.06,
        "text": "your Cassandra cluster now from spark"
      },
      {
        "start": 1298.62,
        "duration": 5.99,
        "text": "sqi nice so it means if you want to make"
      },
      {
        "start": 1302.13,
        "duration": 8.28,
        "text": "a new table you can make a new table in"
      },
      {
        "start": 1304.61,
        "duration": 7.39,
        "text": "using SQL in spark so you can you can do"
      },
      {
        "start": 1310.41,
        "duration": 3.9,
        "text": "all this stuff that used to be"
      },
      {
        "start": 1312.0,
        "duration": 4.95,
        "text": "impossible so for example say I've got"
      },
      {
        "start": 1314.31,
        "duration": 4.23,
        "text": "some data in a MongoDB cluster and I"
      },
      {
        "start": 1316.95,
        "duration": 3.69,
        "text": "really want to make a Cassandra table"
      },
      {
        "start": 1318.54,
        "duration": 4.59,
        "text": "about cassandra table with that data"
      },
      {
        "start": 1320.64,
        "duration": 4.14,
        "text": "before I would have to go in to seek you"
      },
      {
        "start": 1323.13,
        "duration": 4.14,
        "text": "out I'd have to make my key space I'd"
      },
      {
        "start": 1324.78,
        "duration": 4.02,
        "text": "have to make my table I set this all up"
      },
      {
        "start": 1327.27,
        "duration": 4.17,
        "text": "I make sure all the schemas"
      },
      {
        "start": 1328.8,
        "duration": 4.92,
        "text": "and then I would go to spark and I would"
      },
      {
        "start": 1331.44,
        "duration": 3.6,
        "text": "read from and right into that"
      },
      {
        "start": 1333.72,
        "duration": 3.6,
        "text": "table so I have to use two different"
      },
      {
        "start": 1335.04,
        "duration": 4.38,
        "text": "tools I have to make sure things match"
      },
      {
        "start": 1337.32,
        "duration": 6.9,
        "text": "up although all the longhairs I have to"
      },
      {
        "start": 1339.42,
        "duration": 7.05,
        "text": "make mine your data I up and right now"
      },
      {
        "start": 1344.22,
        "duration": 5.7,
        "text": "what you can do is you can write create"
      },
      {
        "start": 1346.47,
        "duration": 5.55,
        "text": "table as select and that will"
      },
      {
        "start": 1349.92,
        "duration": 4.5,
        "text": "automatically make the table in"
      },
      {
        "start": 1352.02,
        "duration": 4.44,
        "text": "Cassandra using the schema that you get"
      },
      {
        "start": 1354.42,
        "duration": 3.69,
        "text": "from and all you have to do is"
      },
      {
        "start": 1356.46,
        "duration": 3.12,
        "text": "specify what the partition key and"
      },
      {
        "start": 1358.11,
        "duration": 4.56,
        "text": "clustering columns are going to be and"
      },
      {
        "start": 1359.58,
        "duration": 5.16,
        "text": "that's it and it will do all the work of"
      },
      {
        "start": 1362.67,
        "duration": 4.65,
        "text": "making all the metadata making all the"
      },
      {
        "start": 1364.74,
        "duration": 5.7,
        "text": "the schema information and moving"
      },
      {
        "start": 1367.32,
        "duration": 5.25,
        "text": "everything in one command so it's gonna"
      },
      {
        "start": 1370.44,
        "duration": 3.6,
        "text": "be a whole new world for doing these"
      },
      {
        "start": 1372.57,
        "duration": 3.81,
        "text": "kind of operations everything will be"
      },
      {
        "start": 1374.04,
        "duration": 3.99,
        "text": "much easier and you won't need multiple"
      },
      {
        "start": 1376.38,
        "duration": 4.65,
        "text": "tools anymore so that's very exciting"
      },
      {
        "start": 1378.03,
        "duration": 5.25,
        "text": "for me it's pretty awesome so let me we"
      },
      {
        "start": 1381.03,
        "duration": 5.55,
        "text": "we use you mouth just a little bit on"
      },
      {
        "start": 1383.28,
        "duration": 5.76,
        "text": "the larger context of what is in spark 2"
      },
      {
        "start": 1386.58,
        "duration": 4.53,
        "text": "3 because I know that there's sort of a"
      },
      {
        "start": 1389.04,
        "duration": 4.32,
        "text": "running joke on on past work releases"
      },
      {
        "start": 1391.11,
        "duration": 4.74,
        "text": "that there's like oh it's a new release"
      },
      {
        "start": 1393.36,
        "duration": 4.74,
        "text": "there's a new API you know we don't we"
      },
      {
        "start": 1395.85,
        "duration": 5.04,
        "text": "don't use rdd's anymore now it's all"
      },
      {
        "start": 1398.1,
        "duration": 3.93,
        "text": "about daydreams like what's a is there"
      },
      {
        "start": 1400.89,
        "duration": 4.44,
        "text": "any of those kind of big-ticket"
      },
      {
        "start": 1402.03,
        "duration": 4.98,
        "text": "surprises in in spark 3 I mean that's"
      },
      {
        "start": 1405.33,
        "duration": 3.21,
        "text": "that's one of the main reasons that"
      },
      {
        "start": 1407.01,
        "duration": 2.79,
        "text": "sparks having a hard time getting out"
      },
      {
        "start": 1408.54,
        "duration": 3.75,
        "text": "those next releases there's some"
      },
      {
        "start": 1409.8,
        "duration": 4.26,
        "text": "discussions about how the SQL dialect"
      },
      {
        "start": 1412.29,
        "duration": 4.71,
        "text": "has to change to fit some of these new"
      },
      {
        "start": 1414.06,
        "duration": 5.16,
        "text": "ideas right because previously the"
      },
      {
        "start": 1417.0,
        "duration": 4.2,
        "text": "create table syntax was basically hi"
      },
      {
        "start": 1419.22,
        "duration": 3.63,
        "text": "it's create table syntax which was a"
      },
      {
        "start": 1421.2,
        "duration": 4.08,
        "text": "little limited because it could only"
      },
      {
        "start": 1422.85,
        "duration": 4.05,
        "text": "express certain things and we want to"
      },
      {
        "start": 1425.28,
        "duration": 3.51,
        "text": "have a much more generic language for"
      },
      {
        "start": 1426.9,
        "duration": 4.56,
        "text": "creating tables now because we want to"
      },
      {
        "start": 1428.79,
        "duration": 4.95,
        "text": "support any any provider who wants to"
      },
      {
        "start": 1431.46,
        "duration": 3.48,
        "text": "make a catalog implementation so you"
      },
      {
        "start": 1433.74,
        "duration": 3.24,
        "text": "want to be able to write a create"
      },
      {
        "start": 1434.94,
        "duration": 4.14,
        "text": "statement that makes a MongoDB table and"
      },
      {
        "start": 1436.98,
        "duration": 4.68,
        "text": "also makes the cassandra table and also"
      },
      {
        "start": 1439.08,
        "duration": 4.8,
        "text": "can make a park a file and also can you"
      },
      {
        "start": 1441.66,
        "duration": 4.86,
        "text": "know you imagine the engine and should"
      },
      {
        "start": 1443.88,
        "duration": 5.16,
        "text": "also be able to work with that so that's"
      },
      {
        "start": 1446.52,
        "duration": 4.38,
        "text": "the only thing folks may have to look"
      },
      {
        "start": 1449.04,
        "duration": 3.6,
        "text": "out for because this upgrade to data"
      },
      {
        "start": 1450.9,
        "duration": 4.38,
        "text": "source for YouTube does change some of"
      },
      {
        "start": 1452.64,
        "duration": 4.8,
        "text": "those ideas a few things that used to be"
      },
      {
        "start": 1455.28,
        "duration": 3.99,
        "text": "possible are now are no longer possible"
      },
      {
        "start": 1457.44,
        "duration": 5.28,
        "text": "but there are things that didn't really"
      },
      {
        "start": 1459.27,
        "duration": 6.06,
        "text": "make a lot of sense so something"
      },
      {
        "start": 1462.72,
        "duration": 6.96,
        "text": "I'm sure a lot of folks are not familiar"
      },
      {
        "start": 1465.33,
        "duration": 6.96,
        "text": "with is that smart too and one had a"
      },
      {
        "start": 1469.68,
        "duration": 5.82,
        "text": "single at a set of modes for writing"
      },
      {
        "start": 1472.29,
        "duration": 5.94,
        "text": "data frames you had a mode called error"
      },
      {
        "start": 1475.5,
        "duration": 4.32,
        "text": "if exists you in Ana mode called a pen"
      },
      {
        "start": 1478.23,
        "duration": 4.26,
        "text": "you had a mode called"
      },
      {
        "start": 1479.82,
        "duration": 7.17,
        "text": "overwrite and you had a motor called"
      },
      {
        "start": 1482.49,
        "duration": 7.83,
        "text": "ignore now what do you think that"
      },
      {
        "start": 1486.99,
        "duration": 5.16,
        "text": "overwrite might do if it did exists"
      },
      {
        "start": 1490.32,
        "duration": 5.04,
        "text": "you're gonna overwrite what was there"
      },
      {
        "start": 1492.15,
        "duration": 5.13,
        "text": "before it will truncate the entire"
      },
      {
        "start": 1495.36,
        "duration": 3.87,
        "text": "source that you are writing to and"
      },
      {
        "start": 1497.28,
        "duration": 4.91,
        "text": "rewrite it entire and that's not"
      },
      {
        "start": 1499.23,
        "duration": 7.37,
        "text": "necessarily what I would have expected"
      },
      {
        "start": 1502.19,
        "duration": 8.47,
        "text": "so what does it overwrite all"
      },
      {
        "start": 1506.6,
        "duration": 7.24,
        "text": "yes so what what might append to do if"
      },
      {
        "start": 1510.66,
        "duration": 5.22,
        "text": "that's what overwrite does well I I"
      },
      {
        "start": 1513.84,
        "duration": 2.97,
        "text": "guess I'm gonna hope that a pen doesn't"
      },
      {
        "start": 1515.88,
        "duration": 2.58,
        "text": "do a truncate"
      },
      {
        "start": 1516.81,
        "duration": 5.37,
        "text": "but it's just gonna happen to it's"
      },
      {
        "start": 1518.46,
        "duration": 9.03,
        "text": "already there it will add but it could"
      },
      {
        "start": 1522.18,
        "duration": 8.04,
        "text": "also overwrite was basically the way of"
      },
      {
        "start": 1527.49,
        "duration": 5.94,
        "text": "doing adding additional data but for"
      },
      {
        "start": 1530.22,
        "duration": 6.06,
        "text": "certain sources append is a insert which"
      },
      {
        "start": 1533.43,
        "duration": 4.38,
        "text": "can remove the previous data so yeah"
      },
      {
        "start": 1536.28,
        "duration": 4.23,
        "text": "there's a little semantic mismatch there"
      },
      {
        "start": 1537.81,
        "duration": 4.53,
        "text": "isn't there so basically everything all"
      },
      {
        "start": 1540.51,
        "duration": 3.9,
        "text": "of those modes only make sense if you're"
      },
      {
        "start": 1542.34,
        "duration": 4.53,
        "text": "thinking about files right so if I'm"
      },
      {
        "start": 1544.41,
        "duration": 4.41,
        "text": "thinking about files writing a file"
      },
      {
        "start": 1546.87,
        "duration": 5.79,
        "text": "obviously I have to delete the old"
      },
      {
        "start": 1548.82,
        "duration": 6.18,
        "text": "right appending I can't like just add"
      },
      {
        "start": 1552.66,
        "duration": 5.34,
        "text": "data in the middle of the file so I'm"
      },
      {
        "start": 1555.0,
        "duration": 6.18,
        "text": "always just adding more files yeah so"
      },
      {
        "start": 1558.0,
        "duration": 6.18,
        "text": "that makes sense so those those were a"
      },
      {
        "start": 1561.18,
        "duration": 4.83,
        "text": "little confusing and one of the the"
      },
      {
        "start": 1564.18,
        "duration": 4.11,
        "text": "things that datasource b2 does is"
      },
      {
        "start": 1566.01,
        "duration": 3.24,
        "text": "basically say indeed a source v2 we're"
      },
      {
        "start": 1568.29,
        "duration": 3.03,
        "text": "just not gonna have these weird"
      },
      {
        "start": 1569.25,
        "duration": 3.99,
        "text": "behaviors like you were gonna have a"
      },
      {
        "start": 1571.32,
        "duration": 4.53,
        "text": "behavior where you add data and you're"
      },
      {
        "start": 1573.24,
        "duration": 4.26,
        "text": "gonna have you know an overwrite"
      },
      {
        "start": 1575.85,
        "duration": 3.15,
        "text": "operation but the override operations"
      },
      {
        "start": 1577.5,
        "duration": 4.11,
        "text": "specifically you have to say what you're"
      },
      {
        "start": 1579.0,
        "duration": 5.31,
        "text": "overriding you know things like that so"
      },
      {
        "start": 1581.61,
        "duration": 4.8,
        "text": "things that make a lot more sense there"
      },
      {
        "start": 1584.31,
        "duration": 3.6,
        "text": "should be a lot less gacho's one thing"
      },
      {
        "start": 1586.41,
        "duration": 3.63,
        "text": "we actually did is in the spark"
      },
      {
        "start": 1587.91,
        "duration": 4.68,
        "text": "Cassandra connector we said that if you"
      },
      {
        "start": 1590.04,
        "duration": 4.11,
        "text": "use the overwrite mode we send up a"
      },
      {
        "start": 1592.59,
        "duration": 3.27,
        "text": "warning first that's like we won't let"
      },
      {
        "start": 1594.15,
        "duration": 2.22,
        "text": "you do this unless you also set an"
      },
      {
        "start": 1595.86,
        "duration": 2.34,
        "text": "edition"
      },
      {
        "start": 1596.37,
        "duration": 5.159,
        "text": "option that says that you know that this"
      },
      {
        "start": 1598.2,
        "duration": 4.77,
        "text": "is going to do a truncate because too"
      },
      {
        "start": 1601.529,
        "duration": 3.39,
        "text": "many people you know are like oh well I"
      },
      {
        "start": 1602.97,
        "duration": 3.839,
        "text": "want to do up certs right I want to up"
      },
      {
        "start": 1604.919,
        "duration": 7.071,
        "text": "cert into the tape right but the cement"
      },
      {
        "start": 1606.809,
        "duration": 5.181,
        "text": "of the operation is drop table right"
      },
      {
        "start": 1612.049,
        "duration": 7.721,
        "text": "okay so I it occurs to me that you know"
      },
      {
        "start": 1617.37,
        "duration": 3.99,
        "text": "a number of our listening viewing"
      },
      {
        "start": 1619.77,
        "duration": 4.8,
        "text": "audience is going to want to try some of"
      },
      {
        "start": 1621.36,
        "duration": 5.49,
        "text": "this stuff out so you know both the"
      },
      {
        "start": 1624.57,
        "duration": 4.92,
        "text": "SPARC three auto release but also the"
      },
      {
        "start": 1626.85,
        "duration": 6.9,
        "text": "spark you standard connector 2-5 how do"
      },
      {
        "start": 1629.49,
        "duration": 7.5,
        "text": "I get a nice demo with maybe a pre-built"
      },
      {
        "start": 1633.75,
        "duration": 7.62,
        "text": "data set is that such a thing available"
      },
      {
        "start": 1636.99,
        "duration": 6.809,
        "text": "oh man I wish I had something I'm pretty"
      },
      {
        "start": 1641.37,
        "duration": 3.99,
        "text": "sure Alex OTT who's our coworker here at"
      },
      {
        "start": 1643.799,
        "duration": 5.01,
        "text": "data stacks has got actually something"
      },
      {
        "start": 1645.36,
        "duration": 5.58,
        "text": "for pulled up prepared up as a as a"
      },
      {
        "start": 1648.809,
        "duration": 3.48,
        "text": "coordinated release with Zeppelin so you"
      },
      {
        "start": 1650.94,
        "duration": 3.359,
        "text": "actually have everything set up with"
      },
      {
        "start": 1652.289,
        "duration": 3.931,
        "text": "Zeppelin but we'll have to ping him and"
      },
      {
        "start": 1654.299,
        "duration": 4.591,
        "text": "hopefully add a link at some point okay"
      },
      {
        "start": 1656.22,
        "duration": 5.13,
        "text": "but you gotta maybe let's do we get Alex"
      },
      {
        "start": 1658.89,
        "duration": 6.06,
        "text": "on and talk about the demo we're"
      },
      {
        "start": 1661.35,
        "duration": 5.4,
        "text": "investing a lot more lately in examples"
      },
      {
        "start": 1664.95,
        "duration": 4.949,
        "text": "we're actually building out the whole"
      },
      {
        "start": 1666.75,
        "duration": 4.83,
        "text": "examples section of the website to to"
      },
      {
        "start": 1669.899,
        "duration": 5.731,
        "text": "populate that with stuff that is well"
      },
      {
        "start": 1671.58,
        "duration": 6.27,
        "text": "curated and even better maintained so"
      },
      {
        "start": 1675.63,
        "duration": 3.48,
        "text": "because there's plenty of old example"
      },
      {
        "start": 1677.85,
        "duration": 5.579,
        "text": "code out there that applies to past"
      },
      {
        "start": 1679.11,
        "duration": 5.819,
        "text": "releases so anyway Rebecca with us here"
      },
      {
        "start": 1683.429,
        "duration": 3.571,
        "text": "and Deborah is working a bunch on that"
      },
      {
        "start": 1684.929,
        "duration": 5.161,
        "text": "so we'll be talking a bunch more about"
      },
      {
        "start": 1687.0,
        "duration": 5.73,
        "text": "that coming up sorry for the I get to do"
      },
      {
        "start": 1690.09,
        "duration": 6.66,
        "text": "ads for my own stuff on my own episode"
      },
      {
        "start": 1692.73,
        "duration": 5.069,
        "text": "that's your prerogative nice so is there"
      },
      {
        "start": 1696.75,
        "duration": 2.85,
        "text": "any uh is there anything else"
      },
      {
        "start": 1697.799,
        "duration": 4.951,
        "text": "interesting in terms of that spark"
      },
      {
        "start": 1699.6,
        "duration": 5.37,
        "text": "Zeppelin integration that that's worth"
      },
      {
        "start": 1702.75,
        "duration": 6.899,
        "text": "I've against you is that is the go-to"
      },
      {
        "start": 1704.97,
        "duration": 6.689,
        "text": "combo toolsets it's really common I mean"
      },
      {
        "start": 1709.649,
        "duration": 3.621,
        "text": "so if you look at what what data bricks"
      },
      {
        "start": 1711.659,
        "duration": 5.13,
        "text": "as a company is like built their entire"
      },
      {
        "start": 1713.27,
        "duration": 6.07,
        "text": "model on is we are going to provide this"
      },
      {
        "start": 1716.789,
        "duration": 4.681,
        "text": "notebook experience directly tied to a"
      },
      {
        "start": 1719.34,
        "duration": 4.439,
        "text": "cluster and that's you know that's their"
      },
      {
        "start": 1721.47,
        "duration": 3.81,
        "text": "big selling point and I think that makes"
      },
      {
        "start": 1723.779,
        "duration": 4.681,
        "text": "a lot of sense because these notebooks"
      },
      {
        "start": 1725.28,
        "duration": 4.82,
        "text": "are a really great way to not only like"
      },
      {
        "start": 1728.46,
        "duration": 3.89,
        "text": "write code but also to"
      },
      {
        "start": 1730.1,
        "duration": 4.38,
        "text": "or what you've done there a really good"
      },
      {
        "start": 1732.35,
        "duration": 4.53,
        "text": "platform for you know yes marking up"
      },
      {
        "start": 1734.48,
        "duration": 3.96,
        "text": "this is what this code does this is the"
      },
      {
        "start": 1736.88,
        "duration": 5.19,
        "text": "result from this part I'm producing this"
      },
      {
        "start": 1738.44,
        "duration": 5.67,
        "text": "and this and this so I think it's really"
      },
      {
        "start": 1742.07,
        "duration": 4.14,
        "text": "good for prototyping all my code is"
      },
      {
        "start": 1744.11,
        "duration": 8.13,
        "text": "totally self documenting by the way I'm"
      },
      {
        "start": 1746.21,
        "duration": 10.079,
        "text": "not sure what you're talking about yeah"
      },
      {
        "start": 1752.24,
        "duration": 6.51,
        "text": "I mean definitely not for me I just got"
      },
      {
        "start": 1756.289,
        "duration": 5.211,
        "text": "a review a little my my PRS for this"
      },
      {
        "start": 1758.75,
        "duration": 4.98,
        "text": "data source commute to work and and"
      },
      {
        "start": 1761.5,
        "duration": 4.059,
        "text": "yarek was telling me because I was using"
      },
      {
        "start": 1763.73,
        "duration": 3.48,
        "text": "a fold right at some point and he's like"
      },
      {
        "start": 1765.559,
        "duration": 4.141,
        "text": "why are you folding right instead of"
      },
      {
        "start": 1767.21,
        "duration": 5.4,
        "text": "folding left and I was just like I don't"
      },
      {
        "start": 1769.7,
        "duration": 5.01,
        "text": "even wait you know I'm like I'm right"
      },
      {
        "start": 1772.61,
        "duration": 4.41,
        "text": "the ended I think the moments was just"
      },
      {
        "start": 1774.71,
        "duration": 6.38,
        "text": "like you know what we should go from"
      },
      {
        "start": 1777.02,
        "duration": 4.07,
        "text": "that end instead of this good catch and"
      },
      {
        "start": 1783.28,
        "duration": 6.07,
        "text": "very cool I have one more thing I want"
      },
      {
        "start": 1786.2,
        "duration": 5.219,
        "text": "to ask you which is because it comes up"
      },
      {
        "start": 1789.35,
        "duration": 5.4,
        "text": "from time to time sorry if it's a non"
      },
      {
        "start": 1791.419,
        "duration": 4.681,
        "text": "stick order out of left field but I was"
      },
      {
        "start": 1794.75,
        "duration": 3.45,
        "text": "here kind of there's a there seems to be"
      },
      {
        "start": 1796.1,
        "duration": 5.34,
        "text": "a debate or a question that especially"
      },
      {
        "start": 1798.2,
        "duration": 6.63,
        "text": "me people who the area have about okay I"
      },
      {
        "start": 1801.44,
        "duration": 6.54,
        "text": "want to I want to most effectively"
      },
      {
        "start": 1804.83,
        "duration": 5.91,
        "text": "combine spark and Cassandra from a"
      },
      {
        "start": 1807.98,
        "duration": 4.71,
        "text": "deployment perspective and one model is"
      },
      {
        "start": 1810.74,
        "duration": 3.419,
        "text": "okay I have my existing Cassandra"
      },
      {
        "start": 1812.69,
        "duration": 4.29,
        "text": "cluster I'm just gonna spin up a"
      },
      {
        "start": 1814.159,
        "duration": 5.281,
        "text": "separate spark cluster and point it at"
      },
      {
        "start": 1816.98,
        "duration": 4.71,
        "text": "the Cassandra cluster you know by means"
      },
      {
        "start": 1819.44,
        "duration": 5.69,
        "text": "of the connector and operate that way"
      },
      {
        "start": 1821.69,
        "duration": 5.969,
        "text": "and then something that I've heard us"
      },
      {
        "start": 1825.13,
        "duration": 3.94,
        "text": "articulate espoused from the DSE"
      },
      {
        "start": 1827.659,
        "duration": 4.02,
        "text": "perspective is okay I'm going to"
      },
      {
        "start": 1829.07,
        "duration": 4.83,
        "text": "actually set up my standard cluster with"
      },
      {
        "start": 1831.679,
        "duration": 5.071,
        "text": "a separate data center so it's the say"
      },
      {
        "start": 1833.9,
        "duration": 4.98,
        "text": "it's it's one conceptual cluster but"
      },
      {
        "start": 1836.75,
        "duration": 4.62,
        "text": "replicating the data to a separate data"
      },
      {
        "start": 1838.88,
        "duration": 5.669,
        "text": "center and then I can pound the crap out"
      },
      {
        "start": 1841.37,
        "duration": 4.47,
        "text": "of that by co-locating spark on that"
      },
      {
        "start": 1844.549,
        "duration": 4.021,
        "text": "Cassandra cluster"
      },
      {
        "start": 1845.84,
        "duration": 4.29,
        "text": "and go to town and have operational"
      },
      {
        "start": 1848.57,
        "duration": 4.56,
        "text": "analytics and don't affect the"
      },
      {
        "start": 1850.13,
        "duration": 4.65,
        "text": "performance on my source cluster so like"
      },
      {
        "start": 1853.13,
        "duration": 3.179,
        "text": "what are the pros and cons of these"
      },
      {
        "start": 1854.78,
        "duration": 3.05,
        "text": "different things or you know how do I"
      },
      {
        "start": 1856.309,
        "duration": 4.681,
        "text": "know it's the right deployment model"
      },
      {
        "start": 1857.83,
        "duration": 5.98,
        "text": "yeah I mean this is a hard question it's"
      },
      {
        "start": 1860.99,
        "duration": 5.43,
        "text": "really about what is easiest"
      },
      {
        "start": 1863.81,
        "duration": 5.73,
        "text": "your organization because the different"
      },
      {
        "start": 1866.42,
        "duration": 5.75,
        "text": "models have severely different pros and"
      },
      {
        "start": 1869.54,
        "duration": 5.43,
        "text": "cons so obviously the DSC approach is"
      },
      {
        "start": 1872.17,
        "duration": 4.93,
        "text": "based around the idea of the simplest"
      },
      {
        "start": 1874.97,
        "duration": 5.16,
        "text": "possible deployment you have the same"
      },
      {
        "start": 1877.1,
        "duration": 5.43,
        "text": "software installed on every machine when"
      },
      {
        "start": 1880.13,
        "duration": 5.31,
        "text": "you start it up it's just very easy to"
      },
      {
        "start": 1882.53,
        "duration": 6.57,
        "text": "have spark there at the same time the"
      },
      {
        "start": 1885.44,
        "duration": 5.43,
        "text": "trade-offs are you aren't able to scale"
      },
      {
        "start": 1889.1,
        "duration": 3.99,
        "text": "the analytics portion as easily as you"
      },
      {
        "start": 1890.87,
        "duration": 3.99,
        "text": "might like and you can't have a"
      },
      {
        "start": 1893.09,
        "duration": 3.3,
        "text": "transient analytics because you have to"
      },
      {
        "start": 1894.86,
        "duration": 3.72,
        "text": "basically have this data center that's"
      },
      {
        "start": 1896.39,
        "duration": 3.66,
        "text": "up all the time yes but you also could"
      },
      {
        "start": 1898.58,
        "duration": 3.54,
        "text": "you get that you know that second"
      },
      {
        "start": 1900.05,
        "duration": 7.17,
        "text": "replication of your data so it's easier"
      },
      {
        "start": 1902.12,
        "duration": 7.23,
        "text": "to not affect your OLTP work we also"
      },
      {
        "start": 1907.22,
        "duration": 3.45,
        "text": "have locality built into the spark"
      },
      {
        "start": 1909.35,
        "duration": 3.93,
        "text": "estándar connector so it will take"
      },
      {
        "start": 1910.67,
        "duration": 5.1,
        "text": "advantage of code lake located spark and"
      },
      {
        "start": 1913.28,
        "duration": 4.41,
        "text": "Cassandra nodes but something I'm seeing"
      },
      {
        "start": 1915.77,
        "duration": 4.89,
        "text": "a lot more now especially with the rise"
      },
      {
        "start": 1917.69,
        "duration": 5.25,
        "text": "of more cloud-based spark providers is"
      },
      {
        "start": 1920.66,
        "duration": 4.92,
        "text": "that people want a more transient spark"
      },
      {
        "start": 1922.94,
        "duration": 3.9,
        "text": "environment or they're working with you"
      },
      {
        "start": 1925.58,
        "duration": 3.99,
        "text": "know within their organization their"
      },
      {
        "start": 1926.84,
        "duration": 4.47,
        "text": "organization has like a large spark"
      },
      {
        "start": 1929.57,
        "duration": 4.05,
        "text": "cluster somewhere and they want to use"
      },
      {
        "start": 1931.31,
        "duration": 4.53,
        "text": "resources from that so in that case I"
      },
      {
        "start": 1933.62,
        "duration": 4.98,
        "text": "think it's you know it makes sense to"
      },
      {
        "start": 1935.84,
        "duration": 4.35,
        "text": "just keep it you know not co-located"
      },
      {
        "start": 1938.6,
        "duration": 3.45,
        "text": "because you have someone else who's"
      },
      {
        "start": 1940.19,
        "duration": 3.51,
        "text": "taking care of the spark portion for you"
      },
      {
        "start": 1942.05,
        "duration": 3.87,
        "text": "you really only want to responsible for"
      },
      {
        "start": 1943.7,
        "duration": 4.05,
        "text": "the under part still probably make sense"
      },
      {
        "start": 1945.92,
        "duration": 4.23,
        "text": "to separate out a DC that you're gonna"
      },
      {
        "start": 1947.75,
        "duration": 5.25,
        "text": "read from and of course if you really"
      },
      {
        "start": 1950.15,
        "duration": 5.73,
        "text": "are trying to avoid constantly hitting"
      },
      {
        "start": 1953.0,
        "duration": 5.1,
        "text": "your Cassandra database do daily pulls"
      },
      {
        "start": 1955.88,
        "duration": 4.59,
        "text": "of all your data and move it into park'"
      },
      {
        "start": 1958.1,
        "duration": 5.01,
        "text": "files on s3 or something like that so"
      },
      {
        "start": 1960.47,
        "duration": 4.74,
        "text": "that you have a static piece of the data"
      },
      {
        "start": 1963.11,
        "duration": 4.76,
        "text": "that you can work on without impacting"
      },
      {
        "start": 1965.21,
        "duration": 5.04,
        "text": "Cassandra load at all"
      },
      {
        "start": 1967.87,
        "duration": 5.32,
        "text": "so those are like basically your two"
      },
      {
        "start": 1970.25,
        "duration": 6.96,
        "text": "ideas as are you as a team managing"
      },
      {
        "start": 1973.19,
        "duration": 5.64,
        "text": "spark and Cassandra if so CO deployment"
      },
      {
        "start": 1977.21,
        "duration": 3.03,
        "text": "probably makes most sense for you"
      },
      {
        "start": 1978.83,
        "duration": 4.41,
        "text": "because it's the least things you have"
      },
      {
        "start": 1980.24,
        "duration": 4.56,
        "text": "to take care of while if you're part of"
      },
      {
        "start": 1983.24,
        "duration": 3.09,
        "text": "an organization that already has a lot"
      },
      {
        "start": 1984.8,
        "duration": 3.36,
        "text": "of institutional spark knowledge is"
      },
      {
        "start": 1986.33,
        "duration": 4.62,
        "text": "already providing spark as a service"
      },
      {
        "start": 1988.16,
        "duration": 6.0,
        "text": "that it makes sense to use that instead"
      },
      {
        "start": 1990.95,
        "duration": 5.25,
        "text": "and not do colocation and I think for a"
      },
      {
        "start": 1994.16,
        "duration": 3.42,
        "text": "lot of a lot of groups starting up now"
      },
      {
        "start": 1996.2,
        "duration": 3.48,
        "text": "it makes a lot"
      },
      {
        "start": 1997.58,
        "duration": 4.08,
        "text": "sense to keep your spark in some kind of"
      },
      {
        "start": 1999.68,
        "duration": 3.9,
        "text": "cloud environment and keep your"
      },
      {
        "start": 2001.66,
        "duration": 3.33,
        "text": "Cassandra you know somewhere else well"
      },
      {
        "start": 2003.58,
        "duration": 4.08,
        "text": "you don't even have that option so like"
      },
      {
        "start": 2004.99,
        "duration": 5.22,
        "text": "if you're using Astra then you want to"
      },
      {
        "start": 2007.66,
        "duration": 5.64,
        "text": "use for spark you want to use like EMF"
      },
      {
        "start": 2010.21,
        "duration": 5.97,
        "text": "or Asher's I forget address brand name"
      },
      {
        "start": 2013.3,
        "duration": 5.07,
        "text": "for there right as a service or day to"
      },
      {
        "start": 2016.18,
        "duration": 3.96,
        "text": "Brick's cloud I actually maybe this day"
      },
      {
        "start": 2018.37,
        "duration": 3.84,
        "text": "to Brits that I think they branded day"
      },
      {
        "start": 2020.14,
        "duration": 3.63,
        "text": "to Brick's cloud as nature's spark as a"
      },
      {
        "start": 2022.21,
        "duration": 4.08,
        "text": "service but yeah that's basically what"
      },
      {
        "start": 2023.77,
        "duration": 4.8,
        "text": "I'm saying is don't lose any sleep"
      },
      {
        "start": 2026.29,
        "duration": 4.5,
        "text": "because you can't locate local okay I"
      },
      {
        "start": 2028.57,
        "duration": 4.53,
        "text": "think these days the majority of the"
      },
      {
        "start": 2030.79,
        "duration": 5.28,
        "text": "benefit is in operational simplicity for"
      },
      {
        "start": 2033.1,
        "duration": 6.24,
        "text": "people managing everything the gains you"
      },
      {
        "start": 2036.07,
        "duration": 6.51,
        "text": "get from the colocation are great but I"
      },
      {
        "start": 2039.34,
        "duration": 6.3,
        "text": "think compared to the operational gains"
      },
      {
        "start": 2042.58,
        "duration": 5.25,
        "text": "it's not not as valuable right no good"
      },
      {
        "start": 2045.64,
        "duration": 3.48,
        "text": "thing this is a really great explanation"
      },
      {
        "start": 2047.83,
        "duration": 3.45,
        "text": "of the trade-offs that you're making"
      },
      {
        "start": 2049.12,
        "duration": 4.41,
        "text": "here and and I would guess that because"
      },
      {
        "start": 2051.28,
        "duration": 4.92,
        "text": "of the improvements in the spark"
      },
      {
        "start": 2053.53,
        "duration": 6.21,
        "text": "Cassandra connector you know the the"
      },
      {
        "start": 2056.2,
        "duration": 5.76,
        "text": "performance of non collocated spark and"
      },
      {
        "start": 2059.74,
        "duration": 8.46,
        "text": "Cassandra is as good as it probably can"
      },
      {
        "start": 2061.96,
        "duration": 8.46,
        "text": "be given the reflash I mean we're we're"
      },
      {
        "start": 2068.2,
        "duration": 3.87,
        "text": "stuck right now really with Cassandra's"
      },
      {
        "start": 2070.42,
        "duration": 3.63,
        "text": "our bottleneck most of the time there"
      },
      {
        "start": 2072.07,
        "duration": 4.29,
        "text": "are very few instances we've seen where"
      },
      {
        "start": 2074.05,
        "duration": 3.42,
        "text": "the network is really the bottleneck for"
      },
      {
        "start": 2076.36,
        "duration": 3.0,
        "text": "this sort of thing of course your"
      },
      {
        "start": 2077.47,
        "duration": 4.71,
        "text": "mileage may vary depending on where your"
      },
      {
        "start": 2079.36,
        "duration": 5.76,
        "text": "egressing and you know moving data to"
      },
      {
        "start": 2082.18,
        "duration": 5.55,
        "text": "and from it may make sense to limit that"
      },
      {
        "start": 2085.12,
        "duration": 4.38,
        "text": "if at all possible I mean we all know"
      },
      {
        "start": 2087.73,
        "duration": 5.43,
        "text": "that if you've got a cluster that's not"
      },
      {
        "start": 2089.5,
        "duration": 5.52,
        "text": "on AWS moving data to AWS I mean it's"
      },
      {
        "start": 2093.16,
        "duration": 4.32,
        "text": "not the rental cost that's gonna cost"
      },
      {
        "start": 2095.02,
        "duration": 7.08,
        "text": "you it's that data interesting yet it's"
      },
      {
        "start": 2097.48,
        "duration": 6.66,
        "text": "pass where they get you so you know it's"
      },
      {
        "start": 2102.1,
        "duration": 4.68,
        "text": "there's trade-offs all over the place"
      },
      {
        "start": 2104.14,
        "duration": 4.41,
        "text": "and you really have to figure out what"
      },
      {
        "start": 2106.78,
        "duration": 4.77,
        "text": "are what are your optimization goals I"
      },
      {
        "start": 2108.55,
        "duration": 4.86,
        "text": "think I still think doing DSC as a"
      },
      {
        "start": 2111.55,
        "duration": 3.63,
        "text": "little package is probably the fastest"
      },
      {
        "start": 2113.41,
        "duration": 3.35,
        "text": "way to get everything set up and in an"
      },
      {
        "start": 2115.18,
        "duration": 5.16,
        "text": "environment where you can work on it all"
      },
      {
        "start": 2116.76,
        "duration": 5.26,
        "text": "easily but for a production environment"
      },
      {
        "start": 2120.34,
        "duration": 3.75,
        "text": "where I'm at a company where we've got a"
      },
      {
        "start": 2122.02,
        "duration": 4.02,
        "text": "thousand node smart cluster it doesn't"
      },
      {
        "start": 2124.09,
        "duration": 3.81,
        "text": "make sense to Cole oh right right of"
      },
      {
        "start": 2126.04,
        "duration": 3.11,
        "text": "course yeah there's always I mean larger"
      },
      {
        "start": 2127.9,
        "duration": 3.77,
        "text": "the enterprise the more of these"
      },
      {
        "start": 2129.15,
        "duration": 5.02,
        "text": "decisions have already been made"
      },
      {
        "start": 2131.67,
        "duration": 4.27,
        "text": "on your behalf and you just have an"
      },
      {
        "start": 2134.17,
        "duration": 4.23,
        "text": "environment a self-service environment"
      },
      {
        "start": 2135.94,
        "duration": 6.659,
        "text": "which is great to kind of play into you"
      },
      {
        "start": 2138.4,
        "duration": 5.31,
        "text": "so yes trade offs everywhere well thank"
      },
      {
        "start": 2142.599,
        "duration": 6.331,
        "text": "you very much"
      },
      {
        "start": 2143.71,
        "duration": 6.659,
        "text": "Russ Russell with two L's 102 else but"
      },
      {
        "start": 2148.93,
        "duration": 4.47,
        "text": "very much appreciate you coming back on"
      },
      {
        "start": 2150.369,
        "duration": 4.591,
        "text": "the show and hanging out with us and we"
      },
      {
        "start": 2153.4,
        "duration": 4.56,
        "text": "will see you next week on another"
      },
      {
        "start": 2154.96,
        "duration": 6.33,
        "text": "distributed data fill all right thanks"
      },
      {
        "start": 2157.96,
        "duration": 5.37,
        "text": "so much for having me thanks for"
      },
      {
        "start": 2161.29,
        "duration": 4.11,
        "text": "listening to the distributed data show"
      },
      {
        "start": 2163.33,
        "duration": 5.72,
        "text": "please subscribe with your favorite"
      },
      {
        "start": 2165.4,
        "duration": 3.65,
        "text": "podcast app give us a rating"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T03:31:13.255330+00:00"
}