{
  "video_id": "cDtmG-NtYEo",
  "title": "Day #2 Operations with DataStax Mission Control",
  "description": "In this video, Aaron Ploetz walks you through running a repair on a keyspace, expanding a cluster from 3 to 6 nodes, and running a \"cleanup\" operation. He'll show you how all of this is super-easy with Mission Control.\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastaxdevs?sub_confirmation=1 \nTwitter: https://twitter.com/datastaxdevs\nTwitch: https://www.twitch.tv/datastaxdevs\n\n\nAbout DataStax Developer:\nOn the DataStax Developers YouTube channel, you can find tutorials, workshops and much more to help you learn and stay updated with the latest information on Apache CassandraÂ©.  Visit https://datastax.com/dev for more free learning resources.",
  "published_at": "2024-05-31T21:15:12Z",
  "thumbnail": "https://i.ytimg.com/vi/cDtmG-NtYEo/hqdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "datastax",
    "apache_cassandra",
    "workshop"
  ],
  "url": "https://www.youtube.com/watch?v=cDtmG-NtYEo",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "hello everyone I'm Aaron pletz and I'm going to take you through day two operations using data Stacks Mission Control let's go as you can see here I am um using cql shell to get out to my cluster um and again if I use my stack Overflow keyspace you can see that I I do have tables out there and you know let's say we want to take look at like times yep there's there's data in there select star from and also have a look at the keyword table so let's have a look at our cluster here um one thing that has always been a recommendation with Cassandra and data Stacks Enterprise is in running repairs and making sure that um you know your uh your repairs run to keep your key spaces consistent and the the whole idea a there is that um you know your your repair should be run H about once a week you know some some use cases will require more maybe less but um you know once a week is usually the um usually the recommendation so if I go up here and I click on the repairs tab um you can see my you can see that my repair history is here um having run repairs on both the stack Overflow and Reaper DB key spaces um what I'm going to do is trigger a repair for the stack Overflow key space so let's go ahead and see this run repair button I'm going to click on that yes I want to run this in my big box code cluster I'm going to run it on the stack Overflow keyspace I am the owner um the rest of this looks pretty good although you know maybe we can maybe double the uh repair threads out there um I don't really care if it's data center aware or not because well I'm only deployed across one data center but if you have multiple you may want to consider you know some of the other options here um and that's really it and then I can click run and as you can see my new entry for uh repairs on the stack Overflow key space um is out there and this repair job is running now it's probably going to take a little bit to complete so I'm going to go ahead and pause and then come back once it's completed all right our repair job running on the stack Overflow key space is just about complete looks like it has just one more segment to go that's the last segment so the repair job should change its state in just a moment here oh and there we go our repair job is done so as you can see it's very very easy to trigger and monitor repairs inside of Mission Control so let's assume that we want to expand our three node cluster here um well the easiest way to do that is to go click the modify cluster button um and then as I scroll through all of my cluster settings here there's this setting of nodes per rra now you can see that for this particular cluster I have three racks named Fernbrook 1 2 and three um and at one node per rack that gets me a three node cluster well if I just bump this to two that should double the size of my cluster so that should be all I need to do let's go ahead and say modify cluster all right and if I scroll down here you can see see that uh my three new nodes um already have their name with um you know big box Co Maple Grove Fernbrook and then either one two or three and then nodes um you know STS one on the end of them as the as the second node coming in when counting it from zero um and you can see that they're starting all right uh two of them have IP Ed addresses so this is good yeah yeah there's an IP for the for the last one so now we ought to start to see the uh the new nodes um improving up to a status of running oh and there we go the uh first of our three new nodes is fully up and running and has joined the cluster now for those of you who may be new to Cassandra um behind the scenes each node is joining and is receiving its uh token range assignments and then is having its share of the data streamed to it you know if you recall from one of my uh my earlier videos on creating a cluster we created um our stack Overflow key space with a replication factor of three now in a three node scenario with rf3 each node has 100% of the data but in a six node cluster with an RF of three each node has 50% of the available token ranges the the ranges of data are being calculated out assigned to the new nodes one at a time and then the data is being streamed and once that streaming process completes it should join with a status of running well there we go um we have five nodes up in the cluster the the second of our three new nodes has joined so uh we are really moving along here all we have to do is wait for this one last node to finish its uh its bootstrapping and uh join the cluster all right our last node has come up and joined the cluster now if you remember what I said about how each new node uh gets its share of of data that it's responsible for is determined by its uh its token ranges and the nodes that were previously responsible for those token ranges um actually still have the data so we need to do one last thing that's part of a cluster expansion I'm sure those of you who have done this before are are familiar with it uh we're going to need to run a cleanup operation on all nodes so um I'm going to go ahead and do bulk actions and look at that the first one in the list is cleanup so we're going to pick that one and we're going to run it all right the cleanup operation is running in fact if I want to track that operation I can go ahead and click on the activities Tab and you can see that um you know my my cleanup operation is indeed pending all right that should do it for more information on data Stacks Mission Control just head on out to data.com products slm Mission Control there you can find all the information you need including downloads and documentation thank you and have a great day",
    "segments": [
      {
        "start": 3.36,
        "duration": 3.68,
        "text": "hello everyone I'm Aaron pletz and I'm"
      },
      {
        "start": 5.64,
        "duration": 4.16,
        "text": "going to take you through day two"
      },
      {
        "start": 7.04,
        "duration": 6.719,
        "text": "operations using data Stacks Mission"
      },
      {
        "start": 9.8,
        "duration": 8.0,
        "text": "Control let's go as you can see here I"
      },
      {
        "start": 13.759,
        "duration": 7.6,
        "text": "am um using cql shell to get out to my"
      },
      {
        "start": 17.8,
        "duration": 5.08,
        "text": "cluster um and again if I use my stack"
      },
      {
        "start": 21.359,
        "duration": 3.881,
        "text": "Overflow"
      },
      {
        "start": 22.88,
        "duration": 4.719,
        "text": "keyspace you can see that I I do have"
      },
      {
        "start": 25.24,
        "duration": 4.439,
        "text": "tables out"
      },
      {
        "start": 27.599,
        "duration": 5.441,
        "text": "there and you know let's say we want to"
      },
      {
        "start": 29.679,
        "duration": 6.241,
        "text": "take look at like times yep there's"
      },
      {
        "start": 33.04,
        "duration": 5.519,
        "text": "there's data in there select star from"
      },
      {
        "start": 35.92,
        "duration": 4.72,
        "text": "and also have a look at the keyword"
      },
      {
        "start": 38.559,
        "duration": 5.601,
        "text": "table so let's have a look at our"
      },
      {
        "start": 40.64,
        "duration": 5.559,
        "text": "cluster here um one thing that has"
      },
      {
        "start": 44.16,
        "duration": 5.44,
        "text": "always been a recommendation with"
      },
      {
        "start": 46.199,
        "duration": 7.52,
        "text": "Cassandra and data Stacks Enterprise is"
      },
      {
        "start": 49.6,
        "duration": 6.639,
        "text": "in running repairs and making sure that"
      },
      {
        "start": 53.719,
        "duration": 5.601,
        "text": "um you know your uh your repairs run to"
      },
      {
        "start": 56.239,
        "duration": 6.12,
        "text": "keep your key spaces consistent and the"
      },
      {
        "start": 59.32,
        "duration": 5.36,
        "text": "the whole idea a there is that um you"
      },
      {
        "start": 62.359,
        "duration": 4.521,
        "text": "know your your repair should be run H"
      },
      {
        "start": 64.68,
        "duration": 4.72,
        "text": "about once a week you know some some use"
      },
      {
        "start": 66.88,
        "duration": 5.4,
        "text": "cases will require more maybe less but"
      },
      {
        "start": 69.4,
        "duration": 5.44,
        "text": "um you know once a week is usually the"
      },
      {
        "start": 72.28,
        "duration": 5.72,
        "text": "um usually the recommendation so if I go"
      },
      {
        "start": 74.84,
        "duration": 6.04,
        "text": "up here and I click on the repairs tab"
      },
      {
        "start": 78.0,
        "duration": 6.2,
        "text": "um you can see"
      },
      {
        "start": 80.88,
        "duration": 6.239,
        "text": "my you can see that my repair history is"
      },
      {
        "start": 84.2,
        "duration": 6.36,
        "text": "here um having run repairs on both the"
      },
      {
        "start": 87.119,
        "duration": 6.441,
        "text": "stack Overflow and Reaper DB key"
      },
      {
        "start": 90.56,
        "duration": 3.0,
        "text": "spaces"
      },
      {
        "start": 95.439,
        "duration": 7.64,
        "text": "um what I'm going to do is trigger a"
      },
      {
        "start": 98.84,
        "duration": 7.04,
        "text": "repair for the stack Overflow key space"
      },
      {
        "start": 103.079,
        "duration": 5.601,
        "text": "so let's go ahead and see this run"
      },
      {
        "start": 105.88,
        "duration": 4.279,
        "text": "repair button I'm going to click on that"
      },
      {
        "start": 108.68,
        "duration": 4.36,
        "text": "yes I want to run this in my big box"
      },
      {
        "start": 110.159,
        "duration": 6.761,
        "text": "code cluster I'm going to run it on the"
      },
      {
        "start": 113.04,
        "duration": 5.679,
        "text": "stack Overflow keyspace I am the owner"
      },
      {
        "start": 116.92,
        "duration": 4.76,
        "text": "um the rest of this looks pretty good"
      },
      {
        "start": 118.719,
        "duration": 5.841,
        "text": "although you know maybe we can maybe"
      },
      {
        "start": 121.68,
        "duration": 4.24,
        "text": "double the uh repair threads out there"
      },
      {
        "start": 124.56,
        "duration": 3.0,
        "text": "um I don't really care if it's data"
      },
      {
        "start": 125.92,
        "duration": 3.52,
        "text": "center aware or not because well I'm"
      },
      {
        "start": 127.56,
        "duration": 3.36,
        "text": "only deployed across one data center but"
      },
      {
        "start": 129.44,
        "duration": 3.36,
        "text": "if you have multiple you may want to"
      },
      {
        "start": 130.92,
        "duration": 4.72,
        "text": "consider you know some of the other"
      },
      {
        "start": 132.8,
        "duration": 6.159,
        "text": "options here um and that's really it and"
      },
      {
        "start": 135.64,
        "duration": 3.319,
        "text": "then I can click"
      },
      {
        "start": 147.64,
        "duration": 7.2,
        "text": "run and as you can see my new entry for"
      },
      {
        "start": 152.28,
        "duration": 6.2,
        "text": "uh repairs on the stack Overflow key"
      },
      {
        "start": 154.84,
        "duration": 6.52,
        "text": "space um is out there and this repair"
      },
      {
        "start": 158.48,
        "duration": 6.839,
        "text": "job is running now it's probably going"
      },
      {
        "start": 161.36,
        "duration": 6.879,
        "text": "to take a little bit to complete so I'm"
      },
      {
        "start": 165.319,
        "duration": 5.92,
        "text": "going to go ahead and pause and then"
      },
      {
        "start": 168.239,
        "duration": 3.0,
        "text": "come back once it's"
      },
      {
        "start": 175.56,
        "duration": 5.2,
        "text": "completed all"
      },
      {
        "start": 177.48,
        "duration": 7.319,
        "text": "right our repair job running on the"
      },
      {
        "start": 180.76,
        "duration": 7.28,
        "text": "stack Overflow key space is just about"
      },
      {
        "start": 184.799,
        "duration": 5.841,
        "text": "complete looks like it has just one more"
      },
      {
        "start": 188.04,
        "duration": 2.6,
        "text": "segment to"
      },
      {
        "start": 191.92,
        "duration": 6.319,
        "text": "go that's the last"
      },
      {
        "start": 195.159,
        "duration": 7.72,
        "text": "segment so the repair"
      },
      {
        "start": 198.239,
        "duration": 6.961,
        "text": "job should change its state in just a"
      },
      {
        "start": 202.879,
        "duration": 5.961,
        "text": "moment"
      },
      {
        "start": 205.2,
        "duration": 7.84,
        "text": "here oh and there we go our repair job"
      },
      {
        "start": 208.84,
        "duration": 7.28,
        "text": "is done so as you can see it's very very"
      },
      {
        "start": 213.04,
        "duration": 5.6,
        "text": "easy to trigger and monitor repairs"
      },
      {
        "start": 216.12,
        "duration": 5.24,
        "text": "inside of Mission"
      },
      {
        "start": 218.64,
        "duration": 5.879,
        "text": "Control so let's assume that we want to"
      },
      {
        "start": 221.36,
        "duration": 6.959,
        "text": "expand our three node cluster here um"
      },
      {
        "start": 224.519,
        "duration": 6.92,
        "text": "well the easiest way to do that is to go"
      },
      {
        "start": 228.319,
        "duration": 6.2,
        "text": "click the modify cluster"
      },
      {
        "start": 231.439,
        "duration": 6.44,
        "text": "button um and then as I scroll through"
      },
      {
        "start": 234.519,
        "duration": 6.201,
        "text": "all of my cluster settings here there's"
      },
      {
        "start": 237.879,
        "duration": 4.521,
        "text": "this setting of nodes per rra"
      },
      {
        "start": 240.72,
        "duration": 3.359,
        "text": "now you can see that for this particular"
      },
      {
        "start": 242.4,
        "duration": 5.36,
        "text": "cluster I have three racks named"
      },
      {
        "start": 244.079,
        "duration": 6.121,
        "text": "Fernbrook 1 2 and three um and at one"
      },
      {
        "start": 247.76,
        "duration": 7.199,
        "text": "node per rack that gets me a three node"
      },
      {
        "start": 250.2,
        "duration": 6.759,
        "text": "cluster well if I just bump this to two"
      },
      {
        "start": 254.959,
        "duration": 4.721,
        "text": "that should double the size of my"
      },
      {
        "start": 256.959,
        "duration": 6.601,
        "text": "cluster so that should be all I need to"
      },
      {
        "start": 259.68,
        "duration": 6.84,
        "text": "do let's go ahead and say modify"
      },
      {
        "start": 263.56,
        "duration": 5.88,
        "text": "cluster all"
      },
      {
        "start": 266.52,
        "duration": 6.0,
        "text": "right and if I scroll down here you can"
      },
      {
        "start": 269.44,
        "duration": 8.319,
        "text": "see see that uh my three new"
      },
      {
        "start": 272.52,
        "duration": 7.76,
        "text": "nodes um already have their name with um"
      },
      {
        "start": 277.759,
        "duration": 5.641,
        "text": "you know big box Co Maple Grove"
      },
      {
        "start": 280.28,
        "duration": 6.759,
        "text": "Fernbrook and then either one two or"
      },
      {
        "start": 283.4,
        "duration": 6.84,
        "text": "three and then nodes um you know"
      },
      {
        "start": 287.039,
        "duration": 4.761,
        "text": "STS one on the end of them as the as the"
      },
      {
        "start": 290.24,
        "duration": 2.76,
        "text": "second node coming in when counting it"
      },
      {
        "start": 291.8,
        "duration": 3.88,
        "text": "from"
      },
      {
        "start": 293.0,
        "duration": 6.08,
        "text": "zero um and you can see that they're"
      },
      {
        "start": 295.68,
        "duration": 8.56,
        "text": "starting all right uh two of them have"
      },
      {
        "start": 299.08,
        "duration": 5.16,
        "text": "IP Ed addresses so this is good yeah"
      },
      {
        "start": 304.36,
        "duration": 8.24,
        "text": "yeah there's an IP for the for the last"
      },
      {
        "start": 308.56,
        "duration": 7.72,
        "text": "one so now we ought to start to see the"
      },
      {
        "start": 312.6,
        "duration": 6.52,
        "text": "uh the new nodes um improving up to a"
      },
      {
        "start": 316.28,
        "duration": 2.84,
        "text": "status of"
      },
      {
        "start": 320.6,
        "duration": 6.64,
        "text": "running oh and there we go the uh first"
      },
      {
        "start": 324.24,
        "duration": 6.88,
        "text": "of our three new nodes is fully up and"
      },
      {
        "start": 327.24,
        "duration": 7.2,
        "text": "running and has joined the cluster"
      },
      {
        "start": 331.12,
        "duration": 5.919,
        "text": "now for those of you who may be new to"
      },
      {
        "start": 334.44,
        "duration": 6.879,
        "text": "Cassandra um behind the"
      },
      {
        "start": 337.039,
        "duration": 7.6,
        "text": "scenes each node is joining and is"
      },
      {
        "start": 341.319,
        "duration": 6.241,
        "text": "receiving its uh token range assignments"
      },
      {
        "start": 344.639,
        "duration": 6.0,
        "text": "and then is having its share of the data"
      },
      {
        "start": 347.56,
        "duration": 5.359,
        "text": "streamed to it you know if you recall"
      },
      {
        "start": 350.639,
        "duration": 5.84,
        "text": "from one of my uh my earlier videos on"
      },
      {
        "start": 352.919,
        "duration": 6.161,
        "text": "creating a cluster we created um our"
      },
      {
        "start": 356.479,
        "duration": 5.521,
        "text": "stack Overflow key space with a"
      },
      {
        "start": 359.08,
        "duration": 5.28,
        "text": "replication factor of three now in a"
      },
      {
        "start": 362.0,
        "duration": 6.56,
        "text": "three node scenario with"
      },
      {
        "start": 364.36,
        "duration": 6.52,
        "text": "rf3 each node has 100% of the data but"
      },
      {
        "start": 368.56,
        "duration": 5.88,
        "text": "in a six node cluster with an RF of"
      },
      {
        "start": 370.88,
        "duration": 6.599,
        "text": "three each node has 50% of the available"
      },
      {
        "start": 374.44,
        "duration": 5.08,
        "text": "token ranges the the ranges of data are"
      },
      {
        "start": 377.479,
        "duration": 4.361,
        "text": "being calculated out assigned to the new"
      },
      {
        "start": 379.52,
        "duration": 4.399,
        "text": "nodes one at a time and then the data is"
      },
      {
        "start": 381.84,
        "duration": 4.919,
        "text": "being streamed and once that streaming"
      },
      {
        "start": 383.919,
        "duration": 6.0,
        "text": "process completes it should join with a"
      },
      {
        "start": 386.759,
        "duration": 6.16,
        "text": "status of running"
      },
      {
        "start": 389.919,
        "duration": 5.241,
        "text": "well there we go um we have five nodes"
      },
      {
        "start": 392.919,
        "duration": 5.4,
        "text": "up in the cluster the the second of our"
      },
      {
        "start": 395.16,
        "duration": 5.4,
        "text": "three new nodes has joined so uh we are"
      },
      {
        "start": 398.319,
        "duration": 5.28,
        "text": "really moving along"
      },
      {
        "start": 400.56,
        "duration": 6.88,
        "text": "here all we have to do is wait for this"
      },
      {
        "start": 403.599,
        "duration": 7.481,
        "text": "one last node to finish its uh its"
      },
      {
        "start": 407.44,
        "duration": 6.879,
        "text": "bootstrapping and uh join the"
      },
      {
        "start": 411.08,
        "duration": 5.88,
        "text": "cluster all right our last node has come"
      },
      {
        "start": 414.319,
        "duration": 4.72,
        "text": "up and joined the cluster now if you"
      },
      {
        "start": 416.96,
        "duration": 3.28,
        "text": "remember what I said about how each new"
      },
      {
        "start": 419.039,
        "duration": 5.041,
        "text": "node"
      },
      {
        "start": 420.24,
        "duration": 6.28,
        "text": "uh gets its share of of data that it's"
      },
      {
        "start": 424.08,
        "duration": 5.32,
        "text": "responsible for is determined by its uh"
      },
      {
        "start": 426.52,
        "duration": 5.119,
        "text": "its token ranges and the nodes that were"
      },
      {
        "start": 429.4,
        "duration": 7.16,
        "text": "previously responsible for those token"
      },
      {
        "start": 431.639,
        "duration": 7.0,
        "text": "ranges um actually still have the data"
      },
      {
        "start": 436.56,
        "duration": 3.919,
        "text": "so we need to do one last thing that's"
      },
      {
        "start": 438.639,
        "duration": 3.321,
        "text": "part of a cluster expansion I'm sure"
      },
      {
        "start": 440.479,
        "duration": 3.72,
        "text": "those of you who have done this before"
      },
      {
        "start": 441.96,
        "duration": 4.919,
        "text": "are are familiar with it uh we're going"
      },
      {
        "start": 444.199,
        "duration": 6.481,
        "text": "to need to run a cleanup operation on"
      },
      {
        "start": 446.879,
        "duration": 6.88,
        "text": "all nodes so um I'm going to go ahead"
      },
      {
        "start": 450.68,
        "duration": 5.32,
        "text": "and do bulk actions and look at that the"
      },
      {
        "start": 453.759,
        "duration": 4.241,
        "text": "first one in the list is cleanup so"
      },
      {
        "start": 456.0,
        "duration": 5.44,
        "text": "we're going to pick that one and we're"
      },
      {
        "start": 458.0,
        "duration": 5.96,
        "text": "going to run it all right the cleanup"
      },
      {
        "start": 461.44,
        "duration": 5.28,
        "text": "operation is"
      },
      {
        "start": 463.96,
        "duration": 5.4,
        "text": "running in fact if I want to track that"
      },
      {
        "start": 466.72,
        "duration": 6.599,
        "text": "operation I can go ahead and click on"
      },
      {
        "start": 469.36,
        "duration": 6.679,
        "text": "the activities Tab and you can see that"
      },
      {
        "start": 473.319,
        "duration": 5.801,
        "text": "um you know my my cleanup operation is"
      },
      {
        "start": 476.039,
        "duration": 4.801,
        "text": "indeed pending all right that should do"
      },
      {
        "start": 479.12,
        "duration": 4.039,
        "text": "it"
      },
      {
        "start": 480.84,
        "duration": 5.799,
        "text": "for more information on data Stacks"
      },
      {
        "start": 483.159,
        "duration": 7.0,
        "text": "Mission Control just head on out to"
      },
      {
        "start": 486.639,
        "duration": 5.24,
        "text": "data.com products slm Mission Control"
      },
      {
        "start": 490.159,
        "duration": 4.44,
        "text": "there you can find all the information"
      },
      {
        "start": 491.879,
        "duration": 4.801,
        "text": "you need including downloads and"
      },
      {
        "start": 494.599,
        "duration": 4.681,
        "text": "documentation thank you and have a great"
      },
      {
        "start": 496.68,
        "duration": 2.6,
        "text": "day"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T17:08:24.224009+00:00"
}