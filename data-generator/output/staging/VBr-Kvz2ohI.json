{
  "video_id": "VBr-Kvz2ohI",
  "title": "DS320.09 Spark Essentials: RDD Actions | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.09 Spark Essentials: RDD Actions\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:26:26Z",
  "thumbnail": "https://i.ytimg.com/vi/VBr-Kvz2ohI/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=VBr-Kvz2ohI",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] that brings us to the topic of rdd actions remember transformations apply some sort of operation to a source rdd and create a new rdd actions take an rdd and as it were summarize the results or run some kind of computation across all of the elements and produce an output the thing that's important to remember about actions is that they actually trigger work being done when you call transformations on one rdd to another it sure looks like you're writing code it looks like you're doing things spark is just remembering what you say until you get to an action when you take action then it can infer what sort of data it needs to take action on and run all of the transformations in its dependency graph that produce the rdd on which the action is to be executed again i'm going to show you just a few common actions so you get a taste of what the api is like then we'll look at some examples to put these things to use collect you've seen already what that does is that returns an array with all of the elements of the source rdd count is simply going to count the total number of the elements of the source rdd reduce now reduce creates an aggregate value by taking some function and repeatedly applying it to pairs of elements and intermediate results in the source rdd so a bit more intuitively if you had a bunch of numbers in an rdd and you wanted to add them or multiply them or something like that reduce would be the way to do it you'll call reduce you'll pass a function that says add one element to another and spark will handle organizing all of the elements of making sure all the elements get reduced and all of their intermediate results get reduced together to produce a single number as output now of course reduce can also produce custom data types doesn't need to be a scalar but the trivial examples and the things that we've looked at already use reduce to add numbers that's very easy to see when you're just getting started take returns an array with the first n elements of the rdd now collect returns an array with everything in it but if you know you've got a big rdd and you're only interested in the first 5 or 10 or 20 or 30 results or something like that then you can call take and just get that easier to deal with data set back first is like a special case of take it's just taking the first one we also have for each that executes a given function on every element of the source rdd we've already shown you some examples where we passed print line as that function so we'll collect the results and for each them or you can even just directly for each an rdd save as text file creates a side effect in the system it takes that rdd and writes it to a text file in the local file system and save to cassandra is one example of the way that we can output data to a cassandra table that api of course gets a little richer and we'll look at those details in a separate module let's look at an example again this is the data we're looking at we have seven elements they're movies simple strings with a title a comma and a release year all created from that list literal that's parallelized into the spark context and made into an rdd here's an example of filtering for all movies that have a release year of 2010 that's pretty simple we've seen how that works in the transformation world already but now we'll explicitly collect that that means we're going to get an array of two elements and that array is going to be returned to the spark shell and then we're calling the for each method on that array count trivially doesn't get a lot easier than this we'll take our rdd and count its elements and get the number as the result reduce here's an example of counting something up now in our data right now we don't have anything that really lends itself to counting or to aggregation so we're going to contrive something we're going to say how many characters are there in all of the movie titles we have in our substantial seven element rdd let's look at the code we're going to grab all of the the characters that's the substring call there except the comma the space and the release here that's assuming four digit release years and no trailing spaces but that works for us we'll take the length of that substring that's the map so we'll take the rdd and make a new rdd that contains numbers that's what that map call does the new rdd contains the lengths of the titles in the source rdd so that intermediate rdd which you don't even really think of you don't see we're not even diagramming it's just kind of there in the system we've created it and it's this sort of ephemeral thing that comes and goes in the surface of our computation we'll take that rdd and call the reduce method on it and reduce expects two parameters we're calling them x and y and what we're going to do is add them so those will be the length values of two separate elements of the input rdd or an element of the input rdd and an intermediate computation the framework handles that we don't know anything about it we're just saying apply this function over and over and we'll be okay and in fact we are we get the number 72 as a result now to cover for each we're going to use a feature called accumulators accumulators get their own treatment in a separate section but for now let me give you a quick overview so you know what to expect an accumulator is a distributed counter it's not data that we need to manage or think about where it exists it's just a thing that we give a name to we create it we give it an initial value and we increment it when we want looking at the code you see that we create two accumulators up front one called total count and one called total length we again map the movies rdd to extract just the title string and then our action is to iterate over each of those elements and increment the total count of elements that we've received and the length of the strings that we have seen so far so we have a total number of characters and a total number of elements then when we're done with that of course it becomes straightforward to calculate an average that gives you an idea of some of the basics of how to get started with spark run a simple hello world program and think about the fundamental data abstraction or these resilient distributed data sets or rdds we're going to work with rdds in every single example just about everything we do with spark rdds to be involved hopefully you've gotten a picture of what they are how they're put together how we operate on them their api and a few examples of how to use the actual running code",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 6.319,
        "duration": 4.24,
        "text": "that brings us to the topic of rdd"
      },
      {
        "start": 8.16,
        "duration": 4.96,
        "text": "actions remember transformations apply"
      },
      {
        "start": 10.559,
        "duration": 6.081,
        "text": "some sort of operation to a source rdd"
      },
      {
        "start": 13.12,
        "duration": 5.6,
        "text": "and create a new rdd actions take an rdd"
      },
      {
        "start": 16.64,
        "duration": 3.76,
        "text": "and as it were summarize the results or"
      },
      {
        "start": 18.72,
        "duration": 4.479,
        "text": "run some kind of computation"
      },
      {
        "start": 20.4,
        "duration": 3.52,
        "text": "across all of the elements and produce"
      },
      {
        "start": 23.199,
        "duration": 1.92,
        "text": "an output"
      },
      {
        "start": 23.92,
        "duration": 2.96,
        "text": "the thing that's important to remember"
      },
      {
        "start": 25.119,
        "duration": 3.121,
        "text": "about actions is that they actually"
      },
      {
        "start": 26.88,
        "duration": 3.6,
        "text": "trigger work being done"
      },
      {
        "start": 28.24,
        "duration": 3.68,
        "text": "when you call transformations on one rdd"
      },
      {
        "start": 30.48,
        "duration": 2.4,
        "text": "to another it sure looks like you're"
      },
      {
        "start": 31.92,
        "duration": 2.72,
        "text": "writing code"
      },
      {
        "start": 32.88,
        "duration": 3.76,
        "text": "it looks like you're doing things spark"
      },
      {
        "start": 34.64,
        "duration": 3.919,
        "text": "is just remembering what you say"
      },
      {
        "start": 36.64,
        "duration": 4.0,
        "text": "until you get to an action when you take"
      },
      {
        "start": 38.559,
        "duration": 3.761,
        "text": "action then it can infer"
      },
      {
        "start": 40.64,
        "duration": 3.52,
        "text": "what sort of data it needs to take"
      },
      {
        "start": 42.32,
        "duration": 2.96,
        "text": "action on and run all of the"
      },
      {
        "start": 44.16,
        "duration": 4.0,
        "text": "transformations"
      },
      {
        "start": 45.28,
        "duration": 4.48,
        "text": "in its dependency graph that produce the"
      },
      {
        "start": 48.16,
        "duration": 3.2,
        "text": "rdd on which the action"
      },
      {
        "start": 49.76,
        "duration": 3.6,
        "text": "is to be executed again i'm going to"
      },
      {
        "start": 51.36,
        "duration": 3.679,
        "text": "show you just a few common actions"
      },
      {
        "start": 53.36,
        "duration": 3.199,
        "text": "so you get a taste of what the api is"
      },
      {
        "start": 55.039,
        "duration": 2.961,
        "text": "like then we'll look at some examples to"
      },
      {
        "start": 56.559,
        "duration": 3.121,
        "text": "put these things to use"
      },
      {
        "start": 58.0,
        "duration": 3.44,
        "text": "collect you've seen already what that"
      },
      {
        "start": 59.68,
        "duration": 3.519,
        "text": "does is that returns an array"
      },
      {
        "start": 61.44,
        "duration": 3.2,
        "text": "with all of the elements of the source"
      },
      {
        "start": 63.199,
        "duration": 3.201,
        "text": "rdd count"
      },
      {
        "start": 64.64,
        "duration": 3.839,
        "text": "is simply going to count the total"
      },
      {
        "start": 66.4,
        "duration": 4.719,
        "text": "number of the elements of the source rdd"
      },
      {
        "start": 68.479,
        "duration": 3.601,
        "text": "reduce now reduce creates an aggregate"
      },
      {
        "start": 71.119,
        "duration": 3.441,
        "text": "value"
      },
      {
        "start": 72.08,
        "duration": 3.44,
        "text": "by taking some function and repeatedly"
      },
      {
        "start": 74.56,
        "duration": 3.919,
        "text": "applying it"
      },
      {
        "start": 75.52,
        "duration": 3.84,
        "text": "to pairs of elements and intermediate"
      },
      {
        "start": 78.479,
        "duration": 3.201,
        "text": "results"
      },
      {
        "start": 79.36,
        "duration": 3.2,
        "text": "in the source rdd so a bit more"
      },
      {
        "start": 81.68,
        "duration": 3.119,
        "text": "intuitively"
      },
      {
        "start": 82.56,
        "duration": 3.599,
        "text": "if you had a bunch of numbers in an rdd"
      },
      {
        "start": 84.799,
        "duration": 3.521,
        "text": "and you wanted to add them"
      },
      {
        "start": 86.159,
        "duration": 4.081,
        "text": "or multiply them or something like that"
      },
      {
        "start": 88.32,
        "duration": 3.6,
        "text": "reduce would be the way to do it"
      },
      {
        "start": 90.24,
        "duration": 3.199,
        "text": "you'll call reduce you'll pass a"
      },
      {
        "start": 91.92,
        "duration": 3.519,
        "text": "function that says add"
      },
      {
        "start": 93.439,
        "duration": 4.161,
        "text": "one element to another and spark will"
      },
      {
        "start": 95.439,
        "duration": 4.401,
        "text": "handle organizing all of the elements of"
      },
      {
        "start": 97.6,
        "duration": 4.0,
        "text": "making sure all the elements get reduced"
      },
      {
        "start": 99.84,
        "duration": 2.959,
        "text": "and all of their intermediate results"
      },
      {
        "start": 101.6,
        "duration": 4.4,
        "text": "get reduced together"
      },
      {
        "start": 102.799,
        "duration": 5.041,
        "text": "to produce a single number as output now"
      },
      {
        "start": 106.0,
        "duration": 3.439,
        "text": "of course reduce can also produce"
      },
      {
        "start": 107.84,
        "duration": 3.12,
        "text": "custom data types doesn't need to be a"
      },
      {
        "start": 109.439,
        "duration": 3.521,
        "text": "scalar but"
      },
      {
        "start": 110.96,
        "duration": 3.28,
        "text": "the trivial examples and the things that"
      },
      {
        "start": 112.96,
        "duration": 3.519,
        "text": "we've looked at already"
      },
      {
        "start": 114.24,
        "duration": 3.04,
        "text": "use reduce to add numbers that's very"
      },
      {
        "start": 116.479,
        "duration": 2.561,
        "text": "easy to see"
      },
      {
        "start": 117.28,
        "duration": 3.28,
        "text": "when you're just getting started take"
      },
      {
        "start": 119.04,
        "duration": 4.56,
        "text": "returns an array with the first"
      },
      {
        "start": 120.56,
        "duration": 4.879,
        "text": "n elements of the rdd now collect"
      },
      {
        "start": 123.6,
        "duration": 3.76,
        "text": "returns an array with everything in it"
      },
      {
        "start": 125.439,
        "duration": 3.6,
        "text": "but if you know you've got a big rdd"
      },
      {
        "start": 127.36,
        "duration": 4.4,
        "text": "and you're only interested in the first"
      },
      {
        "start": 129.039,
        "duration": 3.361,
        "text": "5 or 10 or 20 or 30 results or something"
      },
      {
        "start": 131.76,
        "duration": 3.44,
        "text": "like that"
      },
      {
        "start": 132.4,
        "duration": 4.64,
        "text": "then you can call take and just get that"
      },
      {
        "start": 135.2,
        "duration": 3.759,
        "text": "easier to deal with data set back"
      },
      {
        "start": 137.04,
        "duration": 3.44,
        "text": "first is like a special case of take"
      },
      {
        "start": 138.959,
        "duration": 4.64,
        "text": "it's just taking the first"
      },
      {
        "start": 140.48,
        "duration": 4.24,
        "text": "one we also have for each that executes"
      },
      {
        "start": 143.599,
        "duration": 3.681,
        "text": "a given function"
      },
      {
        "start": 144.72,
        "duration": 4.0,
        "text": "on every element of the source rdd we've"
      },
      {
        "start": 147.28,
        "duration": 1.84,
        "text": "already shown you some examples where we"
      },
      {
        "start": 148.72,
        "duration": 3.2,
        "text": "passed"
      },
      {
        "start": 149.12,
        "duration": 4.08,
        "text": "print line as that function so we'll"
      },
      {
        "start": 151.92,
        "duration": 2.8,
        "text": "collect the results and"
      },
      {
        "start": 153.2,
        "duration": 3.2,
        "text": "for each them or you can even just"
      },
      {
        "start": 154.72,
        "duration": 4.879,
        "text": "directly for each"
      },
      {
        "start": 156.4,
        "duration": 4.72,
        "text": "an rdd save as text file"
      },
      {
        "start": 159.599,
        "duration": 3.121,
        "text": "creates a side effect in the system it"
      },
      {
        "start": 161.12,
        "duration": 2.24,
        "text": "takes that rdd and writes it to a text"
      },
      {
        "start": 162.72,
        "duration": 3.36,
        "text": "file"
      },
      {
        "start": 163.36,
        "duration": 3.44,
        "text": "in the local file system and save to"
      },
      {
        "start": 166.08,
        "duration": 3.12,
        "text": "cassandra"
      },
      {
        "start": 166.8,
        "duration": 4.32,
        "text": "is one example of the way that we can"
      },
      {
        "start": 169.2,
        "duration": 4.08,
        "text": "output data to a cassandra table"
      },
      {
        "start": 171.12,
        "duration": 3.68,
        "text": "that api of course gets a little richer"
      },
      {
        "start": 173.28,
        "duration": 3.679,
        "text": "and we'll look at those details"
      },
      {
        "start": 174.8,
        "duration": 3.439,
        "text": "in a separate module let's look at an"
      },
      {
        "start": 176.959,
        "duration": 2.961,
        "text": "example"
      },
      {
        "start": 178.239,
        "duration": 3.601,
        "text": "again this is the data we're looking at"
      },
      {
        "start": 179.92,
        "duration": 3.76,
        "text": "we have seven elements they're movies"
      },
      {
        "start": 181.84,
        "duration": 4.0,
        "text": "simple strings with a title a comma and"
      },
      {
        "start": 183.68,
        "duration": 4.4,
        "text": "a release year all created"
      },
      {
        "start": 185.84,
        "duration": 3.44,
        "text": "from that list literal that's"
      },
      {
        "start": 188.08,
        "duration": 3.2,
        "text": "parallelized"
      },
      {
        "start": 189.28,
        "duration": 3.679,
        "text": "into the spark context and made into an"
      },
      {
        "start": 191.28,
        "duration": 4.319,
        "text": "rdd here's an example"
      },
      {
        "start": 192.959,
        "duration": 4.401,
        "text": "of filtering for all movies that have a"
      },
      {
        "start": 195.599,
        "duration": 3.601,
        "text": "release year of 2010"
      },
      {
        "start": 197.36,
        "duration": 3.36,
        "text": "that's pretty simple we've seen how that"
      },
      {
        "start": 199.2,
        "duration": 2.16,
        "text": "works in the transformation world"
      },
      {
        "start": 200.72,
        "duration": 2.64,
        "text": "already"
      },
      {
        "start": 201.36,
        "duration": 3.36,
        "text": "but now we'll explicitly collect that"
      },
      {
        "start": 203.36,
        "duration": 2.799,
        "text": "that means we're going to get an array"
      },
      {
        "start": 204.72,
        "duration": 2.96,
        "text": "of two elements"
      },
      {
        "start": 206.159,
        "duration": 2.881,
        "text": "and that array is going to be returned"
      },
      {
        "start": 207.68,
        "duration": 2.96,
        "text": "to the spark shell and then we're"
      },
      {
        "start": 209.04,
        "duration": 4.24,
        "text": "calling the for each method"
      },
      {
        "start": 210.64,
        "duration": 3.84,
        "text": "on that array count trivially doesn't"
      },
      {
        "start": 213.28,
        "duration": 3.519,
        "text": "get a lot easier than this"
      },
      {
        "start": 214.48,
        "duration": 3.839,
        "text": "we'll take our rdd and count its"
      },
      {
        "start": 216.799,
        "duration": 2.561,
        "text": "elements and get the number as the"
      },
      {
        "start": 218.319,
        "duration": 3.681,
        "text": "result"
      },
      {
        "start": 219.36,
        "duration": 3.519,
        "text": "reduce here's an example of counting"
      },
      {
        "start": 222.0,
        "duration": 2.72,
        "text": "something up"
      },
      {
        "start": 222.879,
        "duration": 4.08,
        "text": "now in our data right now we don't have"
      },
      {
        "start": 224.72,
        "duration": 5.36,
        "text": "anything that really lends itself"
      },
      {
        "start": 226.959,
        "duration": 4.801,
        "text": "to counting or to aggregation"
      },
      {
        "start": 230.08,
        "duration": 2.96,
        "text": "so we're going to contrive something"
      },
      {
        "start": 231.76,
        "duration": 2.8,
        "text": "we're going to say how many characters"
      },
      {
        "start": 233.04,
        "duration": 2.16,
        "text": "are there in all of the movie titles we"
      },
      {
        "start": 234.56,
        "duration": 3.36,
        "text": "have"
      },
      {
        "start": 235.2,
        "duration": 4.319,
        "text": "in our substantial seven element rdd"
      },
      {
        "start": 237.92,
        "duration": 3.28,
        "text": "let's look at the code"
      },
      {
        "start": 239.519,
        "duration": 3.36,
        "text": "we're going to grab all of the the"
      },
      {
        "start": 241.2,
        "duration": 4.88,
        "text": "characters that's the substring"
      },
      {
        "start": 242.879,
        "duration": 5.601,
        "text": "call there except the comma the space"
      },
      {
        "start": 246.08,
        "duration": 4.159,
        "text": "and the release here that's assuming"
      },
      {
        "start": 248.48,
        "duration": 2.64,
        "text": "four digit release years and no trailing"
      },
      {
        "start": 250.239,
        "duration": 2.56,
        "text": "spaces"
      },
      {
        "start": 251.12,
        "duration": 3.44,
        "text": "but that works for us we'll take the"
      },
      {
        "start": 252.799,
        "duration": 4.321,
        "text": "length of that substring"
      },
      {
        "start": 254.56,
        "duration": 4.239,
        "text": "that's the map so we'll take the rdd and"
      },
      {
        "start": 257.12,
        "duration": 4.0,
        "text": "make a new rdd"
      },
      {
        "start": 258.799,
        "duration": 3.601,
        "text": "that contains numbers that's what that"
      },
      {
        "start": 261.12,
        "duration": 4.16,
        "text": "map call does"
      },
      {
        "start": 262.4,
        "duration": 3.6,
        "text": "the new rdd contains the lengths of the"
      },
      {
        "start": 265.28,
        "duration": 3.12,
        "text": "titles"
      },
      {
        "start": 266.0,
        "duration": 4.0,
        "text": "in the source rdd so that intermediate"
      },
      {
        "start": 268.4,
        "duration": 2.32,
        "text": "rdd which you don't even really think of"
      },
      {
        "start": 270.0,
        "duration": 2.32,
        "text": "you don't see"
      },
      {
        "start": 270.72,
        "duration": 3.52,
        "text": "we're not even diagramming it's just"
      },
      {
        "start": 272.32,
        "duration": 2.64,
        "text": "kind of there in the system we've"
      },
      {
        "start": 274.24,
        "duration": 2.8,
        "text": "created it"
      },
      {
        "start": 274.96,
        "duration": 3.2,
        "text": "and it's this sort of ephemeral thing"
      },
      {
        "start": 277.04,
        "duration": 3.04,
        "text": "that comes and goes"
      },
      {
        "start": 278.16,
        "duration": 3.28,
        "text": "in the surface of our computation we'll"
      },
      {
        "start": 280.08,
        "duration": 4.32,
        "text": "take that rdd"
      },
      {
        "start": 281.44,
        "duration": 4.16,
        "text": "and call the reduce method on it and"
      },
      {
        "start": 284.4,
        "duration": 2.96,
        "text": "reduce expects"
      },
      {
        "start": 285.6,
        "duration": 3.84,
        "text": "two parameters we're calling them x and"
      },
      {
        "start": 287.36,
        "duration": 5.2,
        "text": "y and what we're going to do"
      },
      {
        "start": 289.44,
        "duration": 6.8,
        "text": "is add them so those will be the"
      },
      {
        "start": 292.56,
        "duration": 6.8,
        "text": "length values of two separate elements"
      },
      {
        "start": 296.24,
        "duration": 4.399,
        "text": "of the input rdd or an element of the"
      },
      {
        "start": 299.36,
        "duration": 3.92,
        "text": "input rdd"
      },
      {
        "start": 300.639,
        "duration": 4.081,
        "text": "and an intermediate computation the"
      },
      {
        "start": 303.28,
        "duration": 2.96,
        "text": "framework handles that we don't know"
      },
      {
        "start": 304.72,
        "duration": 3.28,
        "text": "anything about it we're just saying"
      },
      {
        "start": 306.24,
        "duration": 3.28,
        "text": "apply this function over and over and"
      },
      {
        "start": 308.0,
        "duration": 3.44,
        "text": "we'll be okay and in fact we are we get"
      },
      {
        "start": 309.52,
        "duration": 3.84,
        "text": "the number 72 as a result"
      },
      {
        "start": 311.44,
        "duration": 3.44,
        "text": "now to cover for each we're going to use"
      },
      {
        "start": 313.36,
        "duration": 3.119,
        "text": "a feature called accumulators"
      },
      {
        "start": 314.88,
        "duration": 2.56,
        "text": "accumulators get their own treatment in"
      },
      {
        "start": 316.479,
        "duration": 2.641,
        "text": "a separate section"
      },
      {
        "start": 317.44,
        "duration": 3.84,
        "text": "but for now let me give you a quick"
      },
      {
        "start": 319.12,
        "duration": 4.48,
        "text": "overview so you know what to expect"
      },
      {
        "start": 321.28,
        "duration": 4.32,
        "text": "an accumulator is a distributed counter"
      },
      {
        "start": 323.6,
        "duration": 3.599,
        "text": "it's not data that we need to manage or"
      },
      {
        "start": 325.6,
        "duration": 3.36,
        "text": "think about where it exists"
      },
      {
        "start": 327.199,
        "duration": 4.0,
        "text": "it's just a thing that we give a name to"
      },
      {
        "start": 328.96,
        "duration": 4.72,
        "text": "we create it we give it an initial value"
      },
      {
        "start": 331.199,
        "duration": 3.121,
        "text": "and we increment it when we want looking"
      },
      {
        "start": 333.68,
        "duration": 2.48,
        "text": "at the code"
      },
      {
        "start": 334.32,
        "duration": 3.28,
        "text": "you see that we create two accumulators"
      },
      {
        "start": 336.16,
        "duration": 4.4,
        "text": "up front one called total count and"
      },
      {
        "start": 337.6,
        "duration": 4.56,
        "text": "one called total length we again map the"
      },
      {
        "start": 340.56,
        "duration": 4.24,
        "text": "movies rdd to extract"
      },
      {
        "start": 342.16,
        "duration": 3.92,
        "text": "just the title string and then our"
      },
      {
        "start": 344.8,
        "duration": 4.16,
        "text": "action is to iterate over"
      },
      {
        "start": 346.08,
        "duration": 4.399,
        "text": "each of those elements and increment the"
      },
      {
        "start": 348.96,
        "duration": 2.32,
        "text": "total count of elements that we've"
      },
      {
        "start": 350.479,
        "duration": 4.081,
        "text": "received"
      },
      {
        "start": 351.28,
        "duration": 4.8,
        "text": "and the length of the strings that we"
      },
      {
        "start": 354.56,
        "duration": 2.479,
        "text": "have seen so far so we have a total"
      },
      {
        "start": 356.08,
        "duration": 2.959,
        "text": "number of characters"
      },
      {
        "start": 357.039,
        "duration": 2.961,
        "text": "and a total number of elements then when"
      },
      {
        "start": 359.039,
        "duration": 2.481,
        "text": "we're done with that of course it"
      },
      {
        "start": 360.0,
        "duration": 3.36,
        "text": "becomes straightforward to calculate"
      },
      {
        "start": 361.52,
        "duration": 3.6,
        "text": "an average that gives you an idea of"
      },
      {
        "start": 363.36,
        "duration": 3.52,
        "text": "some of the basics of how to get started"
      },
      {
        "start": 365.12,
        "duration": 2.4,
        "text": "with spark run a simple hello world"
      },
      {
        "start": 366.88,
        "duration": 2.319,
        "text": "program"
      },
      {
        "start": 367.52,
        "duration": 3.119,
        "text": "and think about the fundamental data"
      },
      {
        "start": 369.199,
        "duration": 3.521,
        "text": "abstraction or these resilient"
      },
      {
        "start": 370.639,
        "duration": 4.081,
        "text": "distributed data sets or rdds we're"
      },
      {
        "start": 372.72,
        "duration": 3.44,
        "text": "going to work with rdds in every single"
      },
      {
        "start": 374.72,
        "duration": 2.96,
        "text": "example just about everything we do with"
      },
      {
        "start": 376.16,
        "duration": 3.44,
        "text": "spark rdds to be involved"
      },
      {
        "start": 377.68,
        "duration": 3.44,
        "text": "hopefully you've gotten a picture of"
      },
      {
        "start": 379.6,
        "duration": 3.439,
        "text": "what they are how they're put together"
      },
      {
        "start": 381.12,
        "duration": 3.84,
        "text": "how we operate on them their api"
      },
      {
        "start": 383.039,
        "duration": 11.681,
        "text": "and a few examples of how to use the"
      },
      {
        "start": 384.96,
        "duration": 9.76,
        "text": "actual running code"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:49:36.735795+00:00"
}