{
  "video_id": "Fra1HDIFwg8",
  "title": "Distributed Data Show Episode 29: Entity Resolution with Jonathan Lacefield and Denise Gosnell",
  "description": "Denise Gosnell and Jonathan Lacefield describe the challenges of entity resolution in distributed applications and why a graph database is a great fit for helping unify multiple sources of identity.\n\nABOUT DATASTAX ENTERPRISE 5.1\nDataStax Enterprise 5.1, the database platform for cloud applications, includes Apache Cassandra 3.x with materialized views, tiered storage and advanced replication. Introduced in 5.1 is DataStax Enterprise Graph, the first graph database fast enough to power customer-facing applications, scale to massive datasets and integrate advanced tools to power deep analytical queries.\n\nLearn more at http://www.datastax.com/products/datastax-enterprise and https://academy.datastax.com/resources/whats-new-datastax-enterprise-50\n\nCONNECT WITH DATASTAX\nSubscribe: http://www.youtube.com/c/datastax?sub_confirmation=1 \nSite: http://datastax.com \nFacebook: https://facebook.com/datastax \nTwitter: https://twitter.com/datastax \nLinkedin: https://www.linkedin.com/company/datastax\nhttp://feeds.feedburner.com/datastax \nhttps://github.com/datastax \n\nABOUT DATASTAX ACADEMY\nOn the DataStax Academy YouTube channel, you can find tutorials, webinars and much more to help you learn and stay updated with the latest information on DataStax EnterpriseÂ©.  Create an account on https://academy.datastax.com to watch our free online courses, tutorials, and more.",
  "published_at": "2018-01-09T16:00:01Z",
  "thumbnail": "https://i.ytimg.com/vi/Fra1HDIFwg8/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "distributed",
    "cassandra",
    "database",
    "apache_cassandra",
    "tutorial",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=Fra1HDIFwg8",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "welcome to another episode of the distributed data show brought to you by data Stax Academy where we bring you the latest news and interview technical experts to help you succeed at building large-scale distributed systems well it is time for yet another episode of the distributed data show and I love it when we have returned guests on the show so we are welcoming back today dr. Denise guys know yes we well you did such a great job the first time that we knew we needed to have more and you have brought a friend with you today is that right yeah introduce your friend this is Jonathan Wakefield but I will let you introduce yourself so I don't miss that ah hey Jeff so as Denise said my name is Jonathan Lacefield I'm the director of customer experience and graph products here at Davis X awesome and so yes we definitely have more to talk about in this expansive world of graph technology that were very interested in right now there's a lot to explore today we want to drill in on one use case in particular that's really common for graph databases because not all of them are great we talked about that last time with Denise we're not every problem is a graph problem but there are some that are so we want to talk about entity resolution today and what is that when you guys said you wanted to talk about any resolution I had to look it up so where did this term come from sir you know Denise can probably give you a very technical and specific answer I can give you my view on this and today resolution is a concept that's actually been around for a little while on the technology world particularly in the master domain space or ID space as well and it's really essentially trying to understand if two things are truly the same thing and that's what it comes down to and it's really impactful and meaningful for graph solutions because as you're modeling a graph you want to connect different you know events different people to a single person and when you have different streams of information coming from different source systems knowing that that one person is you know knowing that different source systems are seeing a person maybe differently in being able to resolve that to an individual person that one singular person allows you to build an accurate graph that you can act on and provides a lot of value for your business I don't need anything to add there no that's a that's a that's a really great way to look at the problem and to dive just a little bit deeper I like to think of this coming from the data science background or with the data science background I like to think about entity resolution as the process of taking a really large set of data and inferring through confidence scores or other types of statistical method methods whether or not two points within that data set are the same and so problems within entity resolution start to talk about different ways you can go about bidding your data intelligently so that if you have n pieces of data you're not doing inspirate algorithms or different ways that you can look at comparing properties if we're going to talk about exact matching fuzzy matching those two pieces not being in more of the graph modeling spaces or what do you want to look at using graph relationships to do entity resolution you can look at how two different pieces of data are connected within your overall graphic system and seeing if those relationships are enough to correlate that they are the same person and there's a lot of really interesting applications of how different companies are solving this problem both with and without graphs so it's a really fun problem my main area of research from when I was doing my PhD and also some of the work I've been doing out there in industry so it's a fun problem to work on awesome so ok so I'm sure you're going to tell us some of the details of that but just to kind of start out with why do i why do I need this or like what are the benefits that I get if I have an entity resolution solution in place what does that allow me to do yeah and I think I think Lacey's to touch on some of the overarching benefits but at the core when I think about needing and wanting to model the relationships that are inherent amongst graph data in order to create a graph model you need to be able to infer and deduce the one version or the master identity of say a person that is within your dataset so as far as the benefit goes what that gives you what really is the foundation for starting a more customer oriented model or just really any graph model that can model relationships between people the underlying foundation amongst that is being able to solve entity resolution you've got different data feeds about your customers coming in from all different sources within your company and it's a really hard technical problem to be able to infer who's the same amongst all these different data feeds and it's it's at the core of being able to create a graph model and it sounds not to harp too much on this as only talking about users or people like it seems like it could be part of the solution to solving some PII issues that we that we may have in different systems but then also in itself is maybe an area that we need to exercise some care and protecting identity of people right yeah we're you know it's interesting particularly over in Europe right now with GDP our entity resolution is becoming very important because there's a you know regulation that says that a person has the right to remove all of their data well if you don't know it's their data because your systems can't figure that out then you're out of compliance and that's you know not an acceptable response so it you know it goes right to your point there you know it's a very you know very challenging thing to figure out okay so we've kind of outlined what the problem is what is what is the data stacks answer to this like do we have a solution for this yeah say what let me take this from a high level I'm a product manager guy and then Denise who gets to go out and work with our customers actually implement this and you know leveraging all of her expertise there can some of the details so we really recently wrote a blog on this specific point Jeff it's actually out on data stacks Academy and you know our view of data stacks though I on the graph PM you know Denise mentioned she's here representing the full platform we believe that entity resolution is really a platform solution and when it comes down to it there's three different ways people are using the solution to the solvent to the resolution the first is through pure search kind of matching and that can leverage fuzzy searching or exact searching and everything in between but it's essentially is this person who they say who they you know who we think they are based on pure sir to match the second is to infer data once you've loaded into the to graph one of the neat things about DSC is that you know we're an integrated platform so graph and analytics are are truly integrated once your data is inside of graph and these separate vertices separate nodes you can use DSC analytics to do some of the to do some that inference to understand if two different vertices truly are the same and if they are then you can do some linking there or some next steps so the first being pure you know search graph or pure search kind of weighted to match the second being and infer based on graph and analytics and then the third denise was kind of talking around earlier and that's really leveraging more of DSC analytics and I think this is a lot of her background but it's to do infer just on raw data sets and then once you find things that are the same coming up with a master ID loading that into graph or into the DSC core itself and then acting on the solution so that's that's how we see you know for my kind of higher overarching level really those three different ways yeah and and that's actually a really interesting thing to to kind of note knowing that I'm transitioning from more of a data science role who is solving these problems to getting to be a part of the platform like the data management platform it's really fascinating to me to look the space of entity resolution and the types of techniques that you would use to solve this problem and and just look at the tools that are available and and why that it's really important to understand to use the right tool to to solve the right aspect of the problem and so it was an interesting lead into to look at how some problems are graphed specific and some problems are not because within the entity resolution space of the different ways you need to do the fuzzy searching or matching like Lacey just mentioned or even relationship inference you want to make sure that you have the platform that's going to allow you to to do or enable you for fuzzy search enable you to do more deep analytics or even start to use different entity relationships to determine if you've got bad apples or illicit actors or other types of specific communities within your data and as far as far as the current solutions state every entity resolution problem and every aspect that I've been involved with has to find a unique way to combine those tools to solve the problem in your domain so you can't really go after an entity resolution problem without really fully understanding the type of data that you have distributions of strong identifiers or lack of strong identifiers within your different data sets and being able to see which keys and values or which types of relationships can create that match to be able to say data point a here is the same as data point B from this this data set and so it's it's very vital to make sure that you have the platform that gives you the ability to solve every mining problem that transcends or maybe not transcends maybe is the underpinnings of all of the details that go into solving this is a large solution good and I like that you don't not trying to coerce every problem into the same cookie cutter kind of solution there so that that makes a lot of sense but all the same are there a set of kind of common patterns that you see recurring again and again yeah the solutions for these yeah that's that's a really great problem and there's gonna there's a lot of different subproblems here but there's a few that I feel like are much much more frequently observed when you're looking at different approaches to solving that and and that comes down to more of the binning bidding techniques looking at different ways to do streaming and then different large bulk processes for doing comparisons so for each of those the subproblems you're gonna you're gonna find when you're wanting to do into the resolution the first one I mentioned was thinning and let's say for example that you you expect to have 100 million different data payloads coming into your system over some length of time and your goal is to be able to with some type of confidence be able to determine amongst these 100 million things which represent the same identity and I would never recommend that you would go down a system that would be reliant on doing some type of N squared or even worse algorithm across 100 million 100 million potential data points and even larger for some of the systems we're working with and that's where you can start to talk about different bidding strategies where but you know that they're only going to be close to maybe four or five or up to the twenty twenty-five a hundred other data points so let's make sure we understand our data we understand the types of identifiers that are important and we're siphoning off common-looking data points into a certain data model in this area and maybe you end up with a thousand ten thousand data points over here instead of looking at the whole 100 million and doing some type of N squared algorithm once you've intelligently bends your data you can take a smaller approach to do some type of comparisons within that smaller subset so whenever you're starting to look at entity resolution that's one of the first problems you need to make sure you understand it is how to pin your data intelligently so does binning that sort of sounds to me when you say that vaguely like a partition key is there any relationship there there can be and that even kind of starts to get into the world of materialized views and so a rule of thumb or a way again I would say at its core an approach to any of these techniques requires a fundamental understanding of your data and understanding the keys and values their unique distributions the types of strong identifiers working with any type of additional domain knowledge that can help assist in the creation of a match but with that said let's say in your domain it's perfectly acceptable to have a match of three out of five for example so you've got some type of like an email phone number or social security numbers those types of things are strongly connected to humans identity in the digital age what you can look at doing you know kind of using the santur tables and materialized views potentially don't know it depends on your application but you can look at creating materialized views and duplicating your data across these different ways and partition keys to create a large bin for doing a subset comparison when you have exact match whether on a specific ID but that already gets you down a specific hole and down a specific view of the data that you can act with and if that's unacceptable for your application you might want to come up a little bit and not make such a strong assertion there on a partition key level like like we're just talking about so it does depend and it requires a lot of exploration of your data and understanding what you can do and can't do as far as creating matches yeah probably a couple interesting things to highlight would the nice spoke about and I'd be interested to hear you know what our customers are doing but you know what what I'm seeing is this entity resolution has really two sides from data flow perspective there's certainly a batch there's this you know I need a seed whatever the source system is whatever my known good source records are and then there's this you know real-time flowing in maybe it's you know low latency high volume high throughput or maybe it's more you know kind of what we would expect to see from a web type of scale application but the real-time need of entity resolution as well and like I said the needs going to be curious about you know how we're solving this today and uh those two angles yeah that's that's not - a really good solid way to kind of frame some of the other two main trends and patterns I was seeing and I would frame them I think you know using the word streaming or bulk searching and things like that we can reframe that kind of what you mentioned from a relatable data set you can think about how you have your supervised problems your supervised machine learning component to the entity resolution problem and then the unsupervised component and at its core after you've really gained a solid understanding of the properties amongst your data I always recommend that you start with some type of gold standard labeled labeled data so that you understand from a supervised perspective how to train your model and how to train and learn which features are relevant to your entity resolution matching problem and when you turn on the streaming component and you're looking maybe it entities that you haven't seen before that's when you start to trend more into unsupervised matching so you you already have you already have this population of data stored in your database most likely if it's going to be a master identity model we're looking at a craft database and what we can do is as you're processing a new payload of immutable instance data just in servants an identity you can stream and you know connect it to an existing master identity or according to some confidence level you've previously determined you can say up I'm pretty sure this is the new identity I'm gonna insert it into into my database as a new person and so with with each of these feeding each other once you've had your streaming component build up it's it's necessary to go back and examine the types of matches you've created as you've had more of your unsupervised streaming matching happening and that's when you can go back take it into more of an offline batch and bulk analytical approach so you can measure and kind of observe the types of false positives they can potentially have connected together from your unsupervised model and so with those two those two components the you know using labelled data to train your supervisor algorithm to determine which features are relevant and then putting that out into production for real-time streaming solutions for more of an unsupervised approach those two sub components and actually likely senior officer are good descriptions to understand patterns and in other ways that people are solving this problem well I seized a PhD and I'm the product manager by the way yeah it's thank you that was very kind okay well I you know I love it when we actually get it to dig into some practical use cases and and not just always talk about technology in the in the abstract without reference to a particular problem so thank you guys for taking on this tour of entity resolution problems and I hope that we could do more episodes like this in the future so thanks for joining us today guys yeah thanks Jeff thanks Lisa all right guys all good day all right until next time thank you for joining us again for the distributed data show we love your feedback so go to the distributed data show page on data Stax Academy and tell us what you think you can also find us on the data stacks Academy YouTube channel or find our podcast on iTunes Google Play or wherever you get great podcast while you're there make sure and subscribe so you don't miss a single episode [Music]",
    "segments": [
      {
        "start": 0.03,
        "duration": 4.17,
        "text": "welcome to another episode of the"
      },
      {
        "start": 2.31,
        "duration": 4.29,
        "text": "distributed data show brought to you by"
      },
      {
        "start": 4.2,
        "duration": 4.26,
        "text": "data Stax Academy where we bring you the"
      },
      {
        "start": 6.6,
        "duration": 4.05,
        "text": "latest news and interview technical"
      },
      {
        "start": 8.46,
        "duration": 8.61,
        "text": "experts to help you succeed at building"
      },
      {
        "start": 10.65,
        "duration": 9.87,
        "text": "large-scale distributed systems well it"
      },
      {
        "start": 17.07,
        "duration": 6.57,
        "text": "is time for yet another episode of the"
      },
      {
        "start": 20.52,
        "duration": 5.91,
        "text": "distributed data show and I love it when"
      },
      {
        "start": 23.64,
        "duration": 5.67,
        "text": "we have returned guests on the show so"
      },
      {
        "start": 26.43,
        "duration": 7.59,
        "text": "we are welcoming back today dr. Denise"
      },
      {
        "start": 29.31,
        "duration": 6.9,
        "text": "guys know yes we well you did such a"
      },
      {
        "start": 34.02,
        "duration": 3.93,
        "text": "great job the first time that we knew we"
      },
      {
        "start": 36.21,
        "duration": 3.48,
        "text": "needed to have more and you have brought"
      },
      {
        "start": 37.95,
        "duration": 4.07,
        "text": "a friend with you today is that right"
      },
      {
        "start": 39.69,
        "duration": 6.27,
        "text": "yeah introduce your friend"
      },
      {
        "start": 42.02,
        "duration": 5.17,
        "text": "this is Jonathan Wakefield but I will"
      },
      {
        "start": 45.96,
        "duration": 3.689,
        "text": "let you introduce yourself so I don't"
      },
      {
        "start": 47.19,
        "duration": 3.959,
        "text": "miss that ah hey Jeff so as Denise said"
      },
      {
        "start": 49.649,
        "duration": 3.121,
        "text": "my name is Jonathan Lacefield I'm the"
      },
      {
        "start": 51.149,
        "duration": 5.281,
        "text": "director of customer experience and"
      },
      {
        "start": 52.77,
        "duration": 6.269,
        "text": "graph products here at Davis X awesome"
      },
      {
        "start": 56.43,
        "duration": 5.219,
        "text": "and so yes we definitely have more to"
      },
      {
        "start": 59.039,
        "duration": 5.881,
        "text": "talk about in this expansive world of"
      },
      {
        "start": 61.649,
        "duration": 4.771,
        "text": "graph technology that were very"
      },
      {
        "start": 64.92,
        "duration": 5.94,
        "text": "interested in right now there's a lot to"
      },
      {
        "start": 66.42,
        "duration": 6.54,
        "text": "explore today we want to drill in on one"
      },
      {
        "start": 70.86,
        "duration": 5.579,
        "text": "use case in particular that's really"
      },
      {
        "start": 72.96,
        "duration": 4.74,
        "text": "common for graph databases because not"
      },
      {
        "start": 76.439,
        "duration": 2.941,
        "text": "all of them are great we talked about"
      },
      {
        "start": 77.7,
        "duration": 4.02,
        "text": "that last time with Denise we're not"
      },
      {
        "start": 79.38,
        "duration": 4.95,
        "text": "every problem is a graph problem but"
      },
      {
        "start": 81.72,
        "duration": 6.77,
        "text": "there are some that are so we want to"
      },
      {
        "start": 84.33,
        "duration": 6.96,
        "text": "talk about entity resolution today and"
      },
      {
        "start": 88.49,
        "duration": 4.36,
        "text": "what is that when you guys said you"
      },
      {
        "start": 91.29,
        "duration": 4.95,
        "text": "wanted to talk about any resolution I"
      },
      {
        "start": 92.85,
        "duration": 8.43,
        "text": "had to look it up so where did this term"
      },
      {
        "start": 96.24,
        "duration": 7.019,
        "text": "come from sir you know Denise can"
      },
      {
        "start": 101.28,
        "duration": 4.17,
        "text": "probably give you a very technical and"
      },
      {
        "start": 103.259,
        "duration": 5.671,
        "text": "specific answer I can give you my view"
      },
      {
        "start": 105.45,
        "duration": 5.07,
        "text": "on this and today resolution is a"
      },
      {
        "start": 108.93,
        "duration": 2.969,
        "text": "concept that's actually been around for"
      },
      {
        "start": 110.52,
        "duration": 3.45,
        "text": "a little while on the technology world"
      },
      {
        "start": 111.899,
        "duration": 6.481,
        "text": "particularly in the master domain space"
      },
      {
        "start": 113.97,
        "duration": 7.95,
        "text": "or ID space as well and it's really"
      },
      {
        "start": 118.38,
        "duration": 5.519,
        "text": "essentially trying to understand if two"
      },
      {
        "start": 121.92,
        "duration": 4.769,
        "text": "things are truly the same thing and"
      },
      {
        "start": 123.899,
        "duration": 4.441,
        "text": "that's what it comes down to and it's"
      },
      {
        "start": 126.689,
        "duration": 3.721,
        "text": "really impactful and meaningful for"
      },
      {
        "start": 128.34,
        "duration": 6.09,
        "text": "graph solutions because as you're"
      },
      {
        "start": 130.41,
        "duration": 6.51,
        "text": "modeling a graph you want to connect"
      },
      {
        "start": 134.43,
        "duration": 5.52,
        "text": "different you know events different"
      },
      {
        "start": 136.92,
        "duration": 4.289,
        "text": "people to a single person and when you"
      },
      {
        "start": 139.95,
        "duration": 3.09,
        "text": "have different streams of information"
      },
      {
        "start": 141.209,
        "duration": 4.831,
        "text": "coming from different source systems"
      },
      {
        "start": 143.04,
        "duration": 6.149,
        "text": "knowing that that one person is you know"
      },
      {
        "start": 146.04,
        "duration": 5.099,
        "text": "knowing that different source systems"
      },
      {
        "start": 149.189,
        "duration": 3.3,
        "text": "are seeing a person maybe differently in"
      },
      {
        "start": 151.139,
        "duration": 3.541,
        "text": "being able to resolve that to an"
      },
      {
        "start": 152.489,
        "duration": 4.59,
        "text": "individual person that one singular"
      },
      {
        "start": 154.68,
        "duration": 5.459,
        "text": "person allows you to build an accurate"
      },
      {
        "start": 157.079,
        "duration": 5.88,
        "text": "graph that you can act on and provides a"
      },
      {
        "start": 160.139,
        "duration": 4.951,
        "text": "lot of value for your business I don't"
      },
      {
        "start": 162.959,
        "duration": 3.42,
        "text": "need anything to add there no that's a"
      },
      {
        "start": 165.09,
        "duration": 4.319,
        "text": "that's a that's a really great way to"
      },
      {
        "start": 166.379,
        "duration": 4.95,
        "text": "look at the problem and to dive just a"
      },
      {
        "start": 169.409,
        "duration": 3.51,
        "text": "little bit deeper I like to think of"
      },
      {
        "start": 171.329,
        "duration": 3.541,
        "text": "this coming from the data science"
      },
      {
        "start": 172.919,
        "duration": 3.96,
        "text": "background or with the data science"
      },
      {
        "start": 174.87,
        "duration": 4.949,
        "text": "background I like to think about entity"
      },
      {
        "start": 176.879,
        "duration": 7.14,
        "text": "resolution as the process of taking a"
      },
      {
        "start": 179.819,
        "duration": 5.941,
        "text": "really large set of data and inferring"
      },
      {
        "start": 184.019,
        "duration": 3.78,
        "text": "through confidence scores or other types"
      },
      {
        "start": 185.76,
        "duration": 4.619,
        "text": "of statistical method methods whether or"
      },
      {
        "start": 187.799,
        "duration": 5.491,
        "text": "not two points within that data set are"
      },
      {
        "start": 190.379,
        "duration": 6.121,
        "text": "the same and so problems within entity"
      },
      {
        "start": 193.29,
        "duration": 5.189,
        "text": "resolution start to talk about different"
      },
      {
        "start": 196.5,
        "duration": 4.669,
        "text": "ways you can go about bidding your data"
      },
      {
        "start": 198.479,
        "duration": 4.68,
        "text": "intelligently so that if you have n"
      },
      {
        "start": 201.169,
        "duration": 4.75,
        "text": "pieces of data you're not doing"
      },
      {
        "start": 203.159,
        "duration": 4.11,
        "text": "inspirate algorithms or different ways"
      },
      {
        "start": 205.919,
        "duration": 2.611,
        "text": "that you can look at comparing"
      },
      {
        "start": 207.269,
        "duration": 4.44,
        "text": "properties if we're going to talk about"
      },
      {
        "start": 208.53,
        "duration": 5.099,
        "text": "exact matching fuzzy matching those two"
      },
      {
        "start": 211.709,
        "duration": 4.23,
        "text": "pieces not being in more of the graph"
      },
      {
        "start": 213.629,
        "duration": 4.591,
        "text": "modeling spaces or what do you want to"
      },
      {
        "start": 215.939,
        "duration": 4.741,
        "text": "look at using graph relationships to do"
      },
      {
        "start": 218.22,
        "duration": 4.409,
        "text": "entity resolution you can look at how"
      },
      {
        "start": 220.68,
        "duration": 3.839,
        "text": "two different pieces of data are"
      },
      {
        "start": 222.629,
        "duration": 4.08,
        "text": "connected within your overall graphic"
      },
      {
        "start": 224.519,
        "duration": 3.93,
        "text": "system and seeing if those relationships"
      },
      {
        "start": 226.709,
        "duration": 4.56,
        "text": "are enough to correlate that they are"
      },
      {
        "start": 228.449,
        "duration": 4.551,
        "text": "the same person and there's a lot of"
      },
      {
        "start": 231.269,
        "duration": 3.84,
        "text": "really interesting applications of how"
      },
      {
        "start": 233.0,
        "duration": 5.259,
        "text": "different companies are solving this"
      },
      {
        "start": 235.109,
        "duration": 6.27,
        "text": "problem both with and without graphs so"
      },
      {
        "start": 238.259,
        "duration": 5.25,
        "text": "it's a really fun problem my main area"
      },
      {
        "start": 241.379,
        "duration": 3.39,
        "text": "of research from when I was doing my PhD"
      },
      {
        "start": 243.509,
        "duration": 2.43,
        "text": "and also some of the work I've been"
      },
      {
        "start": 244.769,
        "duration": 3.3,
        "text": "doing out there"
      },
      {
        "start": 245.939,
        "duration": 5.04,
        "text": "in industry so it's a fun problem to"
      },
      {
        "start": 248.069,
        "duration": 4.111,
        "text": "work on awesome so ok so I'm sure you're"
      },
      {
        "start": 250.979,
        "duration": 3.63,
        "text": "going to tell us some of the details of"
      },
      {
        "start": 252.18,
        "duration": 4.5,
        "text": "that but just to kind of start out with"
      },
      {
        "start": 254.609,
        "duration": 3.99,
        "text": "why do i why do I need this or like what"
      },
      {
        "start": 256.68,
        "duration": 4.829,
        "text": "are the benefits that I get if I have an"
      },
      {
        "start": 258.599,
        "duration": 5.221,
        "text": "entity resolution solution in place what"
      },
      {
        "start": 261.509,
        "duration": 3.401,
        "text": "does that allow me to do yeah and I"
      },
      {
        "start": 263.82,
        "duration": 3.97,
        "text": "think I think Lacey's"
      },
      {
        "start": 264.91,
        "duration": 5.88,
        "text": "to touch on some of the overarching"
      },
      {
        "start": 267.79,
        "duration": 5.49,
        "text": "benefits but at the core when I think"
      },
      {
        "start": 270.79,
        "duration": 4.53,
        "text": "about needing and wanting to model the"
      },
      {
        "start": 273.28,
        "duration": 5.58,
        "text": "relationships that are inherent amongst"
      },
      {
        "start": 275.32,
        "duration": 6.33,
        "text": "graph data in order to create a graph"
      },
      {
        "start": 278.86,
        "duration": 5.4,
        "text": "model you need to be able to infer and"
      },
      {
        "start": 281.65,
        "duration": 6.24,
        "text": "deduce the one version or the master"
      },
      {
        "start": 284.26,
        "duration": 5.94,
        "text": "identity of say a person that is within"
      },
      {
        "start": 287.89,
        "duration": 4.86,
        "text": "your dataset so as far as the benefit"
      },
      {
        "start": 290.2,
        "duration": 5.13,
        "text": "goes what that gives you what really is"
      },
      {
        "start": 292.75,
        "duration": 4.56,
        "text": "the foundation for starting a more"
      },
      {
        "start": 295.33,
        "duration": 3.84,
        "text": "customer oriented model or just really"
      },
      {
        "start": 297.31,
        "duration": 4.23,
        "text": "any graph model that can model"
      },
      {
        "start": 299.17,
        "duration": 4.23,
        "text": "relationships between people the"
      },
      {
        "start": 301.54,
        "duration": 3.68,
        "text": "underlying foundation amongst that is"
      },
      {
        "start": 303.4,
        "duration": 4.32,
        "text": "being able to solve entity resolution"
      },
      {
        "start": 305.22,
        "duration": 4.96,
        "text": "you've got different data feeds about"
      },
      {
        "start": 307.72,
        "duration": 4.53,
        "text": "your customers coming in from all"
      },
      {
        "start": 310.18,
        "duration": 4.62,
        "text": "different sources within your company"
      },
      {
        "start": 312.25,
        "duration": 4.71,
        "text": "and it's a really hard technical problem"
      },
      {
        "start": 314.8,
        "duration": 3.6,
        "text": "to be able to infer who's the same"
      },
      {
        "start": 316.96,
        "duration": 3.48,
        "text": "amongst all these different data feeds"
      },
      {
        "start": 318.4,
        "duration": 4.92,
        "text": "and it's it's at the core of being able"
      },
      {
        "start": 320.44,
        "duration": 6.12,
        "text": "to create a graph model and it sounds"
      },
      {
        "start": 323.32,
        "duration": 6.18,
        "text": "not to harp too much on this as only"
      },
      {
        "start": 326.56,
        "duration": 4.92,
        "text": "talking about users or people like it"
      },
      {
        "start": 329.5,
        "duration": 5.46,
        "text": "seems like it could be part of the"
      },
      {
        "start": 331.48,
        "duration": 4.8,
        "text": "solution to solving some PII issues that"
      },
      {
        "start": 334.96,
        "duration": 3.63,
        "text": "we that we may have in different systems"
      },
      {
        "start": 336.28,
        "duration": 4.08,
        "text": "but then also in itself is maybe an area"
      },
      {
        "start": 338.59,
        "duration": 5.4,
        "text": "that we need to exercise some care and"
      },
      {
        "start": 340.36,
        "duration": 5.12,
        "text": "protecting identity of people right yeah"
      },
      {
        "start": 343.99,
        "duration": 3.12,
        "text": "we're you know it's interesting"
      },
      {
        "start": 345.48,
        "duration": 4.21,
        "text": "particularly over in Europe right now"
      },
      {
        "start": 347.11,
        "duration": 4.53,
        "text": "with GDP our entity resolution is"
      },
      {
        "start": 349.69,
        "duration": 4.77,
        "text": "becoming very important because there's"
      },
      {
        "start": 351.64,
        "duration": 4.71,
        "text": "a you know regulation that says that a"
      },
      {
        "start": 354.46,
        "duration": 3.959,
        "text": "person has the right to remove all of"
      },
      {
        "start": 356.35,
        "duration": 3.99,
        "text": "their data well if you don't know it's"
      },
      {
        "start": 358.419,
        "duration": 3.361,
        "text": "their data because your systems can't"
      },
      {
        "start": 360.34,
        "duration": 3.84,
        "text": "figure that out then you're out of"
      },
      {
        "start": 361.78,
        "duration": 5.85,
        "text": "compliance and that's you know not an"
      },
      {
        "start": 364.18,
        "duration": 4.62,
        "text": "acceptable response so it you know it"
      },
      {
        "start": 367.63,
        "duration": 4.11,
        "text": "goes right to your point there you know"
      },
      {
        "start": 368.8,
        "duration": 7.92,
        "text": "it's a very you know very challenging"
      },
      {
        "start": 371.74,
        "duration": 7.32,
        "text": "thing to figure out okay so we've kind"
      },
      {
        "start": 376.72,
        "duration": 5.16,
        "text": "of outlined what the problem is what is"
      },
      {
        "start": 379.06,
        "duration": 6.87,
        "text": "what is the data stacks answer to this"
      },
      {
        "start": 381.88,
        "duration": 6.18,
        "text": "like do we have a solution for this yeah"
      },
      {
        "start": 385.93,
        "duration": 5.04,
        "text": "say what let me take this from a high"
      },
      {
        "start": 388.06,
        "duration": 5.43,
        "text": "level I'm a product manager guy and then"
      },
      {
        "start": 390.97,
        "duration": 4.02,
        "text": "Denise who gets to go out and work with"
      },
      {
        "start": 393.49,
        "duration": 3.78,
        "text": "our customers actually implement this"
      },
      {
        "start": 394.99,
        "duration": 3.57,
        "text": "and you know leveraging all of her"
      },
      {
        "start": 397.27,
        "duration": 3.57,
        "text": "expertise there can"
      },
      {
        "start": 398.56,
        "duration": 4.8,
        "text": "some of the details so we really"
      },
      {
        "start": 400.84,
        "duration": 4.35,
        "text": "recently wrote a blog on this specific"
      },
      {
        "start": 403.36,
        "duration": 4.5,
        "text": "point Jeff it's actually out on data"
      },
      {
        "start": 405.19,
        "duration": 5.76,
        "text": "stacks Academy and you know our view of"
      },
      {
        "start": 407.86,
        "duration": 4.35,
        "text": "data stacks though I on the graph PM you"
      },
      {
        "start": 410.95,
        "duration": 3.21,
        "text": "know Denise mentioned she's here"
      },
      {
        "start": 412.21,
        "duration": 3.66,
        "text": "representing the full platform we"
      },
      {
        "start": 414.16,
        "duration": 4.77,
        "text": "believe that entity resolution is really"
      },
      {
        "start": 415.87,
        "duration": 6.12,
        "text": "a platform solution and when it comes"
      },
      {
        "start": 418.93,
        "duration": 4.62,
        "text": "down to it there's three different ways"
      },
      {
        "start": 421.99,
        "duration": 5.06,
        "text": "people are using the solution to the"
      },
      {
        "start": 423.55,
        "duration": 6.95,
        "text": "solvent to the resolution the first is"
      },
      {
        "start": 427.05,
        "duration": 5.95,
        "text": "through pure search kind of matching and"
      },
      {
        "start": 430.5,
        "duration": 5.86,
        "text": "that can leverage fuzzy searching or"
      },
      {
        "start": 433.0,
        "duration": 5.37,
        "text": "exact searching and everything in"
      },
      {
        "start": 436.36,
        "duration": 4.92,
        "text": "between but it's essentially is this"
      },
      {
        "start": 438.37,
        "duration": 5.16,
        "text": "person who they say who they you know"
      },
      {
        "start": 441.28,
        "duration": 6.33,
        "text": "who we think they are based on pure sir"
      },
      {
        "start": 443.53,
        "duration": 6.78,
        "text": "to match the second is to infer data"
      },
      {
        "start": 447.61,
        "duration": 5.52,
        "text": "once you've loaded into the to graph one"
      },
      {
        "start": 450.31,
        "duration": 4.38,
        "text": "of the neat things about DSC is that you"
      },
      {
        "start": 453.13,
        "duration": 3.509,
        "text": "know we're an integrated platform so"
      },
      {
        "start": 454.69,
        "duration": 4.41,
        "text": "graph and analytics are are truly"
      },
      {
        "start": 456.639,
        "duration": 5.011,
        "text": "integrated once your data is inside of"
      },
      {
        "start": 459.1,
        "duration": 5.25,
        "text": "graph and these separate vertices"
      },
      {
        "start": 461.65,
        "duration": 5.579,
        "text": "separate nodes you can use DSC analytics"
      },
      {
        "start": 464.35,
        "duration": 4.92,
        "text": "to do some of the to do some that"
      },
      {
        "start": 467.229,
        "duration": 4.861,
        "text": "inference to understand if two different"
      },
      {
        "start": 469.27,
        "duration": 4.35,
        "text": "vertices truly are the same and if they"
      },
      {
        "start": 472.09,
        "duration": 4.65,
        "text": "are then you can do some linking there"
      },
      {
        "start": 473.62,
        "duration": 5.13,
        "text": "or some next steps so the first being"
      },
      {
        "start": 476.74,
        "duration": 4.29,
        "text": "pure you know search graph or pure"
      },
      {
        "start": 478.75,
        "duration": 4.38,
        "text": "search kind of weighted to match the"
      },
      {
        "start": 481.03,
        "duration": 4.889,
        "text": "second being and infer based on graph"
      },
      {
        "start": 483.13,
        "duration": 4.74,
        "text": "and analytics and then the third denise"
      },
      {
        "start": 485.919,
        "duration": 4.171,
        "text": "was kind of talking around earlier and"
      },
      {
        "start": 487.87,
        "duration": 3.78,
        "text": "that's really leveraging more of DSC"
      },
      {
        "start": 490.09,
        "duration": 4.44,
        "text": "analytics and I think this is a lot of"
      },
      {
        "start": 491.65,
        "duration": 5.07,
        "text": "her background but it's to do infer just"
      },
      {
        "start": 494.53,
        "duration": 4.199,
        "text": "on raw data sets and then once you find"
      },
      {
        "start": 496.72,
        "duration": 4.86,
        "text": "things that are the same coming up with"
      },
      {
        "start": 498.729,
        "duration": 5.821,
        "text": "a master ID loading that into graph or"
      },
      {
        "start": 501.58,
        "duration": 6.39,
        "text": "into the DSC core itself and then acting"
      },
      {
        "start": 504.55,
        "duration": 4.83,
        "text": "on the solution so that's that's how we"
      },
      {
        "start": 507.97,
        "duration": 2.97,
        "text": "see you know for my kind of higher"
      },
      {
        "start": 509.38,
        "duration": 4.98,
        "text": "overarching level really those three"
      },
      {
        "start": 510.94,
        "duration": 4.529,
        "text": "different ways yeah and and that's"
      },
      {
        "start": 514.36,
        "duration": 3.33,
        "text": "actually a really interesting thing to"
      },
      {
        "start": 515.469,
        "duration": 5.641,
        "text": "to kind of note knowing that I'm"
      },
      {
        "start": 517.69,
        "duration": 4.649,
        "text": "transitioning from more of a data"
      },
      {
        "start": 521.11,
        "duration": 5.01,
        "text": "science role who is solving these"
      },
      {
        "start": 522.339,
        "duration": 5.131,
        "text": "problems to getting to be a part of the"
      },
      {
        "start": 526.12,
        "duration": 4.86,
        "text": "platform like the data management"
      },
      {
        "start": 527.47,
        "duration": 4.679,
        "text": "platform it's really fascinating to me"
      },
      {
        "start": 530.98,
        "duration": 5.039,
        "text": "to look"
      },
      {
        "start": 532.149,
        "duration": 5.13,
        "text": "the space of entity resolution and the"
      },
      {
        "start": 536.019,
        "duration": 3.69,
        "text": "types of techniques that you would use"
      },
      {
        "start": 537.279,
        "duration": 4.5,
        "text": "to solve this problem and and just look"
      },
      {
        "start": 539.709,
        "duration": 4.081,
        "text": "at the tools that are available and and"
      },
      {
        "start": 541.779,
        "duration": 4.74,
        "text": "why that it's really important to"
      },
      {
        "start": 543.79,
        "duration": 4.169,
        "text": "understand to use the right tool to to"
      },
      {
        "start": 546.519,
        "duration": 3.87,
        "text": "solve the right aspect of the problem"
      },
      {
        "start": 547.959,
        "duration": 5.82,
        "text": "and so it was an interesting lead into"
      },
      {
        "start": 550.389,
        "duration": 4.651,
        "text": "to look at how some problems are graphed"
      },
      {
        "start": 553.779,
        "duration": 2.97,
        "text": "specific and some problems are not"
      },
      {
        "start": 555.04,
        "duration": 4.14,
        "text": "because within the entity resolution"
      },
      {
        "start": 556.749,
        "duration": 4.77,
        "text": "space of the different ways you need to"
      },
      {
        "start": 559.18,
        "duration": 4.259,
        "text": "do the fuzzy searching or matching like"
      },
      {
        "start": 561.519,
        "duration": 3.69,
        "text": "Lacey just mentioned or even"
      },
      {
        "start": 563.439,
        "duration": 3.69,
        "text": "relationship inference you want to make"
      },
      {
        "start": 565.209,
        "duration": 4.86,
        "text": "sure that you have the platform that's"
      },
      {
        "start": 567.129,
        "duration": 4.56,
        "text": "going to allow you to to do or enable"
      },
      {
        "start": 570.069,
        "duration": 4.911,
        "text": "you for fuzzy search enable you to do"
      },
      {
        "start": 571.689,
        "duration": 5.46,
        "text": "more deep analytics or even start to use"
      },
      {
        "start": 574.98,
        "duration": 4.06,
        "text": "different entity relationships to"
      },
      {
        "start": 577.149,
        "duration": 3.331,
        "text": "determine if you've got bad apples or"
      },
      {
        "start": 579.04,
        "duration": 3.45,
        "text": "illicit actors or other types of"
      },
      {
        "start": 580.48,
        "duration": 4.56,
        "text": "specific communities within your data"
      },
      {
        "start": 582.49,
        "duration": 5.88,
        "text": "and as far as far as the current"
      },
      {
        "start": 585.04,
        "duration": 5.849,
        "text": "solutions state every entity resolution"
      },
      {
        "start": 588.37,
        "duration": 4.829,
        "text": "problem and every aspect that I've been"
      },
      {
        "start": 590.889,
        "duration": 5.161,
        "text": "involved with has to find a unique way"
      },
      {
        "start": 593.199,
        "duration": 5.971,
        "text": "to combine those tools to solve the"
      },
      {
        "start": 596.05,
        "duration": 5.25,
        "text": "problem in your domain so you can't"
      },
      {
        "start": 599.17,
        "duration": 4.859,
        "text": "really go after an entity resolution"
      },
      {
        "start": 601.3,
        "duration": 4.409,
        "text": "problem without really fully"
      },
      {
        "start": 604.029,
        "duration": 4.74,
        "text": "understanding the type of data that you"
      },
      {
        "start": 605.709,
        "duration": 5.461,
        "text": "have distributions of strong identifiers"
      },
      {
        "start": 608.769,
        "duration": 4.5,
        "text": "or lack of strong identifiers within"
      },
      {
        "start": 611.17,
        "duration": 3.99,
        "text": "your different data sets and being able"
      },
      {
        "start": 613.269,
        "duration": 4.591,
        "text": "to see which keys and values or which"
      },
      {
        "start": 615.16,
        "duration": 4.799,
        "text": "types of relationships can create that"
      },
      {
        "start": 617.86,
        "duration": 4.11,
        "text": "match to be able to say data point a"
      },
      {
        "start": 619.959,
        "duration": 5.1,
        "text": "here is the same as data point B from"
      },
      {
        "start": 621.97,
        "duration": 5.07,
        "text": "this this data set and so it's it's very"
      },
      {
        "start": 625.059,
        "duration": 3.541,
        "text": "vital to make sure that you have the"
      },
      {
        "start": 627.04,
        "duration": 4.459,
        "text": "platform that gives you the ability to"
      },
      {
        "start": 628.6,
        "duration": 5.339,
        "text": "solve every mining problem that"
      },
      {
        "start": 631.499,
        "duration": 4.9,
        "text": "transcends or maybe not transcends maybe"
      },
      {
        "start": 633.939,
        "duration": 3.931,
        "text": "is the underpinnings of all of the"
      },
      {
        "start": 636.399,
        "duration": 4.23,
        "text": "details that go into solving this is a"
      },
      {
        "start": 637.87,
        "duration": 5.73,
        "text": "large solution good and I like that you"
      },
      {
        "start": 640.629,
        "duration": 5.25,
        "text": "don't not trying to coerce every problem"
      },
      {
        "start": 643.6,
        "duration": 4.349,
        "text": "into the same cookie cutter kind of"
      },
      {
        "start": 645.879,
        "duration": 4.531,
        "text": "solution there so that that makes a lot"
      },
      {
        "start": 647.949,
        "duration": 5.101,
        "text": "of sense but all the same are there a"
      },
      {
        "start": 650.41,
        "duration": 5.429,
        "text": "set of kind of common patterns that you"
      },
      {
        "start": 653.05,
        "duration": 5.82,
        "text": "see recurring again and again yeah the"
      },
      {
        "start": 655.839,
        "duration": 5.19,
        "text": "solutions for these yeah that's that's a"
      },
      {
        "start": 658.87,
        "duration": 4.38,
        "text": "really great problem and there's gonna"
      },
      {
        "start": 661.029,
        "duration": 3.961,
        "text": "there's a lot of different subproblems"
      },
      {
        "start": 663.25,
        "duration": 3.32,
        "text": "here but there's a few that I feel like"
      },
      {
        "start": 664.99,
        "duration": 3.72,
        "text": "are much"
      },
      {
        "start": 666.57,
        "duration": 3.55,
        "text": "much more frequently observed when"
      },
      {
        "start": 668.71,
        "duration": 3.09,
        "text": "you're looking at different approaches"
      },
      {
        "start": 670.12,
        "duration": 5.04,
        "text": "to solving that and and that comes down"
      },
      {
        "start": 671.8,
        "duration": 5.64,
        "text": "to more of the binning bidding"
      },
      {
        "start": 675.16,
        "duration": 4.7,
        "text": "techniques looking at different ways to"
      },
      {
        "start": 677.44,
        "duration": 6.57,
        "text": "do streaming and then different large"
      },
      {
        "start": 679.86,
        "duration": 6.28,
        "text": "bulk processes for doing comparisons so"
      },
      {
        "start": 684.01,
        "duration": 3.6,
        "text": "for each of those the subproblems you're"
      },
      {
        "start": 686.14,
        "duration": 2.79,
        "text": "gonna you're gonna find when you're"
      },
      {
        "start": 687.61,
        "duration": 2.78,
        "text": "wanting to do into the resolution the"
      },
      {
        "start": 688.93,
        "duration": 4.8,
        "text": "first one I mentioned was thinning and"
      },
      {
        "start": 690.39,
        "duration": 6.699,
        "text": "let's say for example that you you"
      },
      {
        "start": 693.73,
        "duration": 5.43,
        "text": "expect to have 100 million different"
      },
      {
        "start": 697.089,
        "duration": 5.011,
        "text": "data payloads coming into your system"
      },
      {
        "start": 699.16,
        "duration": 5.4,
        "text": "over some length of time and your goal"
      },
      {
        "start": 702.1,
        "duration": 5.52,
        "text": "is to be able to with some type of"
      },
      {
        "start": 704.56,
        "duration": 5.91,
        "text": "confidence be able to determine amongst"
      },
      {
        "start": 707.62,
        "duration": 7.62,
        "text": "these 100 million things which represent"
      },
      {
        "start": 710.47,
        "duration": 6.36,
        "text": "the same identity and I would never"
      },
      {
        "start": 715.24,
        "duration": 3.81,
        "text": "recommend that you would go down a"
      },
      {
        "start": 716.83,
        "duration": 4.29,
        "text": "system that would be reliant on doing"
      },
      {
        "start": 719.05,
        "duration": 5.67,
        "text": "some type of N squared or even worse"
      },
      {
        "start": 721.12,
        "duration": 5.219,
        "text": "algorithm across 100 million 100 million"
      },
      {
        "start": 724.72,
        "duration": 2.88,
        "text": "potential data points and even larger"
      },
      {
        "start": 726.339,
        "duration": 3.331,
        "text": "for some of the systems we're working"
      },
      {
        "start": 727.6,
        "duration": 4.23,
        "text": "with and that's where you can start to"
      },
      {
        "start": 729.67,
        "duration": 4.89,
        "text": "talk about different bidding strategies"
      },
      {
        "start": 731.83,
        "duration": 4.41,
        "text": "where but you know that they're only"
      },
      {
        "start": 734.56,
        "duration": 3.18,
        "text": "going to be close to maybe four or five"
      },
      {
        "start": 736.24,
        "duration": 3.57,
        "text": "or up to the twenty twenty-five a"
      },
      {
        "start": 737.74,
        "duration": 3.96,
        "text": "hundred other data points so let's make"
      },
      {
        "start": 739.81,
        "duration": 3.48,
        "text": "sure we understand our data we"
      },
      {
        "start": 741.7,
        "duration": 3.92,
        "text": "understand the types of identifiers that"
      },
      {
        "start": 743.29,
        "duration": 4.38,
        "text": "are important and we're siphoning off"
      },
      {
        "start": 745.62,
        "duration": 3.88,
        "text": "common-looking data points into a"
      },
      {
        "start": 747.67,
        "duration": 4.14,
        "text": "certain data model in this area and"
      },
      {
        "start": 749.5,
        "duration": 4.98,
        "text": "maybe you end up with a thousand ten"
      },
      {
        "start": 751.81,
        "duration": 4.469,
        "text": "thousand data points over here instead"
      },
      {
        "start": 754.48,
        "duration": 3.299,
        "text": "of looking at the whole 100 million and"
      },
      {
        "start": 756.279,
        "duration": 3.451,
        "text": "doing some type of N squared algorithm"
      },
      {
        "start": 757.779,
        "duration": 4.861,
        "text": "once you've intelligently bends your"
      },
      {
        "start": 759.73,
        "duration": 5.64,
        "text": "data you can take a smaller approach to"
      },
      {
        "start": 762.64,
        "duration": 4.889,
        "text": "do some type of comparisons within that"
      },
      {
        "start": 765.37,
        "duration": 3.18,
        "text": "smaller subset so whenever you're"
      },
      {
        "start": 767.529,
        "duration": 2.671,
        "text": "starting to look at entity resolution"
      },
      {
        "start": 768.55,
        "duration": 3.09,
        "text": "that's one of the first problems you"
      },
      {
        "start": 770.2,
        "duration": 3.78,
        "text": "need to make sure you understand it is"
      },
      {
        "start": 771.64,
        "duration": 4.889,
        "text": "how to pin your data intelligently so"
      },
      {
        "start": 773.98,
        "duration": 4.049,
        "text": "does binning that sort of sounds to me"
      },
      {
        "start": 776.529,
        "duration": 4.171,
        "text": "when you say that vaguely like a"
      },
      {
        "start": 778.029,
        "duration": 6.511,
        "text": "partition key is there any relationship"
      },
      {
        "start": 780.7,
        "duration": 4.86,
        "text": "there there can be and that even kind of"
      },
      {
        "start": 784.54,
        "duration": 3.72,
        "text": "starts to get into the world of"
      },
      {
        "start": 785.56,
        "duration": 7.14,
        "text": "materialized views and so a rule of"
      },
      {
        "start": 788.26,
        "duration": 7.079,
        "text": "thumb or a way again I would say at its"
      },
      {
        "start": 792.7,
        "duration": 5.129,
        "text": "core an approach to any of these"
      },
      {
        "start": 795.339,
        "duration": 4.141,
        "text": "techniques requires a fundamental"
      },
      {
        "start": 797.829,
        "duration": 3.781,
        "text": "understanding of your data"
      },
      {
        "start": 799.48,
        "duration": 4.68,
        "text": "and understanding the keys and values"
      },
      {
        "start": 801.61,
        "duration": 4.32,
        "text": "their unique distributions the types of"
      },
      {
        "start": 804.16,
        "duration": 4.47,
        "text": "strong identifiers working with any type"
      },
      {
        "start": 805.93,
        "duration": 5.73,
        "text": "of additional domain knowledge that can"
      },
      {
        "start": 808.63,
        "duration": 6.389,
        "text": "help assist in the creation of a match"
      },
      {
        "start": 811.66,
        "duration": 5.58,
        "text": "but with that said let's say in your"
      },
      {
        "start": 815.019,
        "duration": 5.851,
        "text": "domain it's perfectly acceptable to have"
      },
      {
        "start": 817.24,
        "duration": 6.39,
        "text": "a match of three out of five for example"
      },
      {
        "start": 820.87,
        "duration": 4.92,
        "text": "so you've got some type of like an email"
      },
      {
        "start": 823.63,
        "duration": 4.88,
        "text": "phone number or social security numbers"
      },
      {
        "start": 825.79,
        "duration": 5.76,
        "text": "those types of things are strongly"
      },
      {
        "start": 828.51,
        "duration": 5.62,
        "text": "connected to humans identity in the"
      },
      {
        "start": 831.55,
        "duration": 4.77,
        "text": "digital age what you can look at doing"
      },
      {
        "start": 834.13,
        "duration": 4.11,
        "text": "you know kind of using the santur tables"
      },
      {
        "start": 836.32,
        "duration": 4.019,
        "text": "and materialized views potentially don't"
      },
      {
        "start": 838.24,
        "duration": 3.39,
        "text": "know it depends on your application but"
      },
      {
        "start": 840.339,
        "duration": 3.12,
        "text": "you can look at creating materialized"
      },
      {
        "start": 841.63,
        "duration": 3.959,
        "text": "views and duplicating your data across"
      },
      {
        "start": 843.459,
        "duration": 5.37,
        "text": "these different ways and partition keys"
      },
      {
        "start": 845.589,
        "duration": 4.891,
        "text": "to create a large bin for doing a subset"
      },
      {
        "start": 848.829,
        "duration": 5.671,
        "text": "comparison when you have exact match"
      },
      {
        "start": 850.48,
        "duration": 5.669,
        "text": "whether on a specific ID but that"
      },
      {
        "start": 854.5,
        "duration": 3.839,
        "text": "already gets you down a specific hole"
      },
      {
        "start": 856.149,
        "duration": 3.69,
        "text": "and down a specific view of the data"
      },
      {
        "start": 858.339,
        "duration": 3.36,
        "text": "that you can act with and if that's"
      },
      {
        "start": 859.839,
        "duration": 3.151,
        "text": "unacceptable for your application you"
      },
      {
        "start": 861.699,
        "duration": 3.781,
        "text": "might want to come up a little bit and"
      },
      {
        "start": 862.99,
        "duration": 5.339,
        "text": "not make such a strong assertion there"
      },
      {
        "start": 865.48,
        "duration": 4.83,
        "text": "on a partition key level like like we're"
      },
      {
        "start": 868.329,
        "duration": 4.44,
        "text": "just talking about so it does depend and"
      },
      {
        "start": 870.31,
        "duration": 4.95,
        "text": "it requires a lot of exploration of your"
      },
      {
        "start": 872.769,
        "duration": 3.241,
        "text": "data and understanding what you can do"
      },
      {
        "start": 875.26,
        "duration": 3.48,
        "text": "and can't do"
      },
      {
        "start": 876.01,
        "duration": 5.129,
        "text": "as far as creating matches yeah probably"
      },
      {
        "start": 878.74,
        "duration": 4.95,
        "text": "a couple interesting things to highlight"
      },
      {
        "start": 881.139,
        "duration": 3.661,
        "text": "would the nice spoke about and I'd be"
      },
      {
        "start": 883.69,
        "duration": 3.629,
        "text": "interested to hear you know what our"
      },
      {
        "start": 884.8,
        "duration": 5.159,
        "text": "customers are doing but you know what"
      },
      {
        "start": 887.319,
        "duration": 4.231,
        "text": "what I'm seeing is this entity"
      },
      {
        "start": 889.959,
        "duration": 3.75,
        "text": "resolution has really two sides from"
      },
      {
        "start": 891.55,
        "duration": 4.409,
        "text": "data flow perspective there's certainly"
      },
      {
        "start": 893.709,
        "duration": 4.231,
        "text": "a batch there's this you know I need a"
      },
      {
        "start": 895.959,
        "duration": 4.021,
        "text": "seed whatever the source system is"
      },
      {
        "start": 897.94,
        "duration": 4.199,
        "text": "whatever my known good source records"
      },
      {
        "start": 899.98,
        "duration": 4.859,
        "text": "are and then there's this you know"
      },
      {
        "start": 902.139,
        "duration": 5.551,
        "text": "real-time flowing in maybe it's you know"
      },
      {
        "start": 904.839,
        "duration": 5.49,
        "text": "low latency high volume high throughput"
      },
      {
        "start": 907.69,
        "duration": 5.009,
        "text": "or maybe it's more you know kind of what"
      },
      {
        "start": 910.329,
        "duration": 5.281,
        "text": "we would expect to see from a web type"
      },
      {
        "start": 912.699,
        "duration": 6.271,
        "text": "of scale application but the real-time"
      },
      {
        "start": 915.61,
        "duration": 4.44,
        "text": "need of entity resolution as well and"
      },
      {
        "start": 918.97,
        "duration": 3.57,
        "text": "like I said the needs going to be"
      },
      {
        "start": 920.05,
        "duration": 5.7,
        "text": "curious about you know how we're solving"
      },
      {
        "start": 922.54,
        "duration": 5.609,
        "text": "this today and uh those two angles yeah"
      },
      {
        "start": 925.75,
        "duration": 3.839,
        "text": "that's that's not - a really good solid"
      },
      {
        "start": 928.149,
        "duration": 3.69,
        "text": "way to kind of frame some of the other"
      },
      {
        "start": 929.589,
        "duration": 3.181,
        "text": "two main trends and patterns I was"
      },
      {
        "start": 931.839,
        "duration": 2.791,
        "text": "seeing"
      },
      {
        "start": 932.77,
        "duration": 3.629,
        "text": "and I would frame them I think you know"
      },
      {
        "start": 934.63,
        "duration": 3.48,
        "text": "using the word streaming or bulk"
      },
      {
        "start": 936.399,
        "duration": 4.261,
        "text": "searching and things like that we can"
      },
      {
        "start": 938.11,
        "duration": 5.039,
        "text": "reframe that kind of what you mentioned"
      },
      {
        "start": 940.66,
        "duration": 5.07,
        "text": "from a relatable data set you can think"
      },
      {
        "start": 943.149,
        "duration": 4.141,
        "text": "about how you have your supervised"
      },
      {
        "start": 945.73,
        "duration": 3.12,
        "text": "problems your supervised machine"
      },
      {
        "start": 947.29,
        "duration": 2.609,
        "text": "learning component to the entity"
      },
      {
        "start": 948.85,
        "duration": 5.52,
        "text": "resolution problem and then the"
      },
      {
        "start": 949.899,
        "duration": 6.241,
        "text": "unsupervised component and at its core"
      },
      {
        "start": 954.37,
        "duration": 3.48,
        "text": "after you've really gained a solid"
      },
      {
        "start": 956.14,
        "duration": 4.11,
        "text": "understanding of the properties amongst"
      },
      {
        "start": 957.85,
        "duration": 4.109,
        "text": "your data I always recommend that you"
      },
      {
        "start": 960.25,
        "duration": 4.2,
        "text": "start with some type of gold standard"
      },
      {
        "start": 961.959,
        "duration": 5.311,
        "text": "labeled labeled data so that you"
      },
      {
        "start": 964.45,
        "duration": 5.34,
        "text": "understand from a supervised perspective"
      },
      {
        "start": 967.27,
        "duration": 4.59,
        "text": "how to train your model and how to train"
      },
      {
        "start": 969.79,
        "duration": 3.84,
        "text": "and learn which features are relevant to"
      },
      {
        "start": 971.86,
        "duration": 4.14,
        "text": "your entity resolution matching problem"
      },
      {
        "start": 973.63,
        "duration": 4.53,
        "text": "and when you turn on the streaming"
      },
      {
        "start": 976.0,
        "duration": 3.27,
        "text": "component and you're looking maybe it"
      },
      {
        "start": 978.16,
        "duration": 4.02,
        "text": "entities that you haven't seen before"
      },
      {
        "start": 979.27,
        "duration": 5.28,
        "text": "that's when you start to trend more into"
      },
      {
        "start": 982.18,
        "duration": 5.25,
        "text": "unsupervised matching so you you already"
      },
      {
        "start": 984.55,
        "duration": 6.24,
        "text": "have you already have this population of"
      },
      {
        "start": 987.43,
        "duration": 5.07,
        "text": "data stored in your database most likely"
      },
      {
        "start": 990.79,
        "duration": 3.45,
        "text": "if it's going to be a master identity"
      },
      {
        "start": 992.5,
        "duration": 3.75,
        "text": "model we're looking at a craft database"
      },
      {
        "start": 994.24,
        "duration": 3.99,
        "text": "and what we can do is as you're"
      },
      {
        "start": 996.25,
        "duration": 4.5,
        "text": "processing a new payload of immutable"
      },
      {
        "start": 998.23,
        "duration": 5.79,
        "text": "instance data just in servants an"
      },
      {
        "start": 1000.75,
        "duration": 4.829,
        "text": "identity you can stream and you know"
      },
      {
        "start": 1004.02,
        "duration": 4.59,
        "text": "connect it to an existing master"
      },
      {
        "start": 1005.579,
        "duration": 4.921,
        "text": "identity or according to some confidence"
      },
      {
        "start": 1008.61,
        "duration": 3.719,
        "text": "level you've previously determined you"
      },
      {
        "start": 1010.5,
        "duration": 3.77,
        "text": "can say up I'm pretty sure this is the"
      },
      {
        "start": 1012.329,
        "duration": 5.521,
        "text": "new identity I'm gonna insert it into"
      },
      {
        "start": 1014.27,
        "duration": 5.71,
        "text": "into my database as a new person and so"
      },
      {
        "start": 1017.85,
        "duration": 4.41,
        "text": "with with each of these feeding each"
      },
      {
        "start": 1019.98,
        "duration": 4.65,
        "text": "other once you've had your streaming"
      },
      {
        "start": 1022.26,
        "duration": 5.04,
        "text": "component build up it's it's necessary"
      },
      {
        "start": 1024.63,
        "duration": 4.17,
        "text": "to go back and examine the types of"
      },
      {
        "start": 1027.3,
        "duration": 3.06,
        "text": "matches you've created as you've had"
      },
      {
        "start": 1028.8,
        "duration": 3.33,
        "text": "more of your unsupervised streaming"
      },
      {
        "start": 1030.36,
        "duration": 3.329,
        "text": "matching happening and that's when you"
      },
      {
        "start": 1032.13,
        "duration": 4.41,
        "text": "can go back take it into more of an"
      },
      {
        "start": 1033.689,
        "duration": 5.52,
        "text": "offline batch and bulk analytical"
      },
      {
        "start": 1036.54,
        "duration": 5.269,
        "text": "approach so you can measure and kind of"
      },
      {
        "start": 1039.209,
        "duration": 4.74,
        "text": "observe the types of false positives"
      },
      {
        "start": 1041.809,
        "duration": 4.241,
        "text": "they can potentially have connected"
      },
      {
        "start": 1043.949,
        "duration": 4.321,
        "text": "together from your unsupervised model"
      },
      {
        "start": 1046.05,
        "duration": 4.53,
        "text": "and so with those two those two"
      },
      {
        "start": 1048.27,
        "duration": 5.039,
        "text": "components the you know using labelled"
      },
      {
        "start": 1050.58,
        "duration": 4.11,
        "text": "data to train your supervisor algorithm"
      },
      {
        "start": 1053.309,
        "duration": 3.601,
        "text": "to determine which features are relevant"
      },
      {
        "start": 1054.69,
        "duration": 3.69,
        "text": "and then putting that out into"
      },
      {
        "start": 1056.91,
        "duration": 3.09,
        "text": "production for real-time streaming"
      },
      {
        "start": 1058.38,
        "duration": 4.049,
        "text": "solutions for more of an unsupervised"
      },
      {
        "start": 1060.0,
        "duration": 4.11,
        "text": "approach those two sub components and"
      },
      {
        "start": 1062.429,
        "duration": 4.261,
        "text": "actually likely senior officer are good"
      },
      {
        "start": 1064.11,
        "duration": 3.929,
        "text": "descriptions to understand patterns and"
      },
      {
        "start": 1066.69,
        "duration": 3.539,
        "text": "in other ways that people are solving"
      },
      {
        "start": 1068.039,
        "duration": 7.051,
        "text": "this problem well I seized a PhD and I'm"
      },
      {
        "start": 1070.229,
        "duration": 9.481,
        "text": "the product manager by the way yeah it's"
      },
      {
        "start": 1075.09,
        "duration": 6.419,
        "text": "thank you that was very kind okay well I"
      },
      {
        "start": 1079.71,
        "duration": 4.5,
        "text": "you know I love it when we actually get"
      },
      {
        "start": 1081.509,
        "duration": 4.59,
        "text": "it to dig into some practical use cases"
      },
      {
        "start": 1084.21,
        "duration": 3.99,
        "text": "and and not just always talk about"
      },
      {
        "start": 1086.099,
        "duration": 3.3,
        "text": "technology in the in the abstract"
      },
      {
        "start": 1088.2,
        "duration": 3.359,
        "text": "without reference to a particular"
      },
      {
        "start": 1089.399,
        "duration": 5.13,
        "text": "problem so thank you guys for taking on"
      },
      {
        "start": 1091.559,
        "duration": 4.651,
        "text": "this tour of entity resolution problems"
      },
      {
        "start": 1094.529,
        "duration": 3.661,
        "text": "and I hope that we could do more"
      },
      {
        "start": 1096.21,
        "duration": 4.679,
        "text": "episodes like this in the future"
      },
      {
        "start": 1098.19,
        "duration": 3.78,
        "text": "so thanks for joining us today guys yeah"
      },
      {
        "start": 1100.889,
        "duration": 3.451,
        "text": "thanks Jeff thanks Lisa"
      },
      {
        "start": 1101.97,
        "duration": 6.089,
        "text": "all right guys all good day all right"
      },
      {
        "start": 1104.34,
        "duration": 5.789,
        "text": "until next time thank you for joining us"
      },
      {
        "start": 1108.059,
        "duration": 3.6,
        "text": "again for the distributed data show we"
      },
      {
        "start": 1110.129,
        "duration": 3.451,
        "text": "love your feedback so go to the"
      },
      {
        "start": 1111.659,
        "duration": 3.75,
        "text": "distributed data show page on data Stax"
      },
      {
        "start": 1113.58,
        "duration": 3.299,
        "text": "Academy and tell us what you think you"
      },
      {
        "start": 1115.409,
        "duration": 3.87,
        "text": "can also find us on the data stacks"
      },
      {
        "start": 1116.879,
        "duration": 4.321,
        "text": "Academy YouTube channel or find our"
      },
      {
        "start": 1119.279,
        "duration": 4.32,
        "text": "podcast on iTunes Google Play or"
      },
      {
        "start": 1121.2,
        "duration": 4.38,
        "text": "wherever you get great podcast while"
      },
      {
        "start": 1123.599,
        "duration": 4.851,
        "text": "you're there make sure and subscribe so"
      },
      {
        "start": 1125.58,
        "duration": 5.979,
        "text": "you don't miss a single episode"
      },
      {
        "start": 1128.45,
        "duration": 3.109,
        "text": "[Music]"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T06:57:31.146686+00:00"
}