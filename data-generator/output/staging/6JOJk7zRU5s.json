{
  "video_id": "6JOJk7zRU5s",
  "title": "DS210.22 CQL Copy | Operations with Apache Cassandra",
  "description": "#DataStaxAcademy #DS210\nDS210.22 CQL COPY\nCQL Copy is a built-in command in Apache Cassandra on the command line. It is used to move data in and out of a Apache Cassandra cluster. Learn more about CQL Copy in this unit.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-12T01:13:52Z",
  "thumbnail": "https://i.ytimg.com/vi/6JOJk7zRU5s/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cql",
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=6JOJk7zRU5s",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's talk about cql copy which is a built-in command in cassandra on the command line well what is it used for well if you can see from this example there's really two ways to go there's copy table from and copy table two what this is used to do is move data in and out of a cassandra cluster not just a node an entire cluster and it's specifically around a table so the copy from is take the data out copy two put the data in and there's lots of options and how it works so the copy from a delimited file so you take a delimited file it could be a csv by default or you can use tab my personal favorite whatever the delimiter you could specify what it is but we want to take a delimited file and put that data into a cassandra table in a cluster so often this is a good way to load test data pre-production that sort of thing it's not meant to move a lot of data at the same time it's a small amount of data in comparison to how much a cassandra cluster can hold this should not be confused with the insert command which is a different way to do it but this is similar to how it works it will insert data into the cluster so there are some rules to work with here for instance if you do not specify the rows that you're inserting then you need to have the same number of columns that are inside the table and it will try to match those up another thing is if there's an empty data set it's considered to be a null it's not a zero it's not a space it's a null and that's what it assumes whenever it finds no data the copy from is always about importing but small amounts of data don't think about this as a i'm going to reload my entire database there's another tool for that that we'll talk about later but this is a good way to get small amounts of data in and out of the system just follow that rule and you'll be fine for those really large data sets you want to use ds bulk it's been designed to run the full load of the data that could be stored in a cluster at that scale so a better tool for if you're moving large data sets so to run cql copy first thing you do is you look at your file what's the delimiter what options do i need to use now in this case it's a pipe delimiter not my choice but hey that's what it is so you need to specify that the limiter is a pipe symbol and then it has a header so that last name first name email createddate is the first row of the file so if that's the first row of the file and it's the delimiter of a pipe you specify that in your configuration when you run a copy from it will be happy all that data will go directly into the user table good to go so those options as i mentioned before like delimiter are something you need to specify the default of course is a comma because it's comma delimited delimiter is something though that's pretty universally different let's face it no one uses the same delimiter all the time it's good to have options the header is a true or false if you're including a header specify the fields that you have in the file when you import the data you'll look at that first row and say aha here's all the data in the columns i want it presented i will import this data for you you have to make sure those column names exist in the database but that's what it's looking for true or false the chunk size or how many records are passed to the processes to run to insert into the database now this could be a performance concern if you have a lot of chunk size then it can potentially load faster as it's getting larger bits of data and pushing it faster into the database it can also overwhelm your database a little bit let's say that you're running on a very small configuration say on your laptop well maybe you don't need as big of a chunk size use that as a tuning parameter skip rows is one of those things that's used say in a data loading scenario where you just want to load a little bit of data maybe a sample again probably use for your testing scenarios not something that you'd normally use in production copy 2 in this case is when you want to be able to move the data from a cassandra table into a file copy 2 pick the name of the table and then output it directly to the file system this is the default the default is it creates a csv file of all the data inside of that table some of the parameters that you can put into a copy too are just specifying the exact fields that you want to copy out so for instance if you have a lot of columns in a particular table maybe you only want four or five of those fields out specify them here if you don't it will just dump the entire table so you need to specify the file name when you do the copy to you can also use standard out and in the case of copy from standard in this is useful if you're using say a pipe or redirect where you're bringing in data from the command line and if you're one of those command line nerds you know who my people are that's a good thing right using pipes and delimiters well we have an option for you just keep in mind if using standard in the end of the delimited data needs to have a backslash period to note the end of the file so that's a pretty basic overview of how to use the cql copy command in and out again limited use case don't use it for all of your data but i think you can find some pretty useful places for this to be used you",
    "segments": [
      {
        "start": 1.43,
        "duration": 5.33,
        "text": "[Music]"
      },
      {
        "start": 7.279,
        "duration": 4.4,
        "text": "let's talk about cql copy"
      },
      {
        "start": 9.2,
        "duration": 3.92,
        "text": "which is a built-in command in cassandra"
      },
      {
        "start": 11.679,
        "duration": 3.521,
        "text": "on the command line"
      },
      {
        "start": 13.12,
        "duration": 3.44,
        "text": "well what is it used for well if you can"
      },
      {
        "start": 15.2,
        "duration": 3.68,
        "text": "see from this example"
      },
      {
        "start": 16.56,
        "duration": 4.16,
        "text": "there's really two ways to go there's"
      },
      {
        "start": 18.88,
        "duration": 4.639,
        "text": "copy table from"
      },
      {
        "start": 20.72,
        "duration": 3.12,
        "text": "and copy table two what this is used to"
      },
      {
        "start": 23.519,
        "duration": 2.881,
        "text": "do"
      },
      {
        "start": 23.84,
        "duration": 3.439,
        "text": "is move data in and out of a cassandra"
      },
      {
        "start": 26.4,
        "duration": 3.84,
        "text": "cluster"
      },
      {
        "start": 27.279,
        "duration": 5.44,
        "text": "not just a node an entire cluster and"
      },
      {
        "start": 30.24,
        "duration": 5.839,
        "text": "it's specifically around a table"
      },
      {
        "start": 32.719,
        "duration": 5.36,
        "text": "so the copy from is take the data out"
      },
      {
        "start": 36.079,
        "duration": 4.0,
        "text": "copy two put the data in and there's"
      },
      {
        "start": 38.079,
        "duration": 4.721,
        "text": "lots of options and how it works"
      },
      {
        "start": 40.079,
        "duration": 4.64,
        "text": "so the copy from a delimited file so you"
      },
      {
        "start": 42.8,
        "duration": 2.96,
        "text": "take a delimited file it could be a csv"
      },
      {
        "start": 44.719,
        "duration": 4.0,
        "text": "by default"
      },
      {
        "start": 45.76,
        "duration": 4.16,
        "text": "or you can use tab my personal favorite"
      },
      {
        "start": 48.719,
        "duration": 3.52,
        "text": "whatever the delimiter"
      },
      {
        "start": 49.92,
        "duration": 3.84,
        "text": "you could specify what it is but we want"
      },
      {
        "start": 52.239,
        "duration": 4.561,
        "text": "to take a delimited file"
      },
      {
        "start": 53.76,
        "duration": 6.479,
        "text": "and put that data into a cassandra table"
      },
      {
        "start": 56.8,
        "duration": 3.84,
        "text": "in a cluster so often this is a good way"
      },
      {
        "start": 60.239,
        "duration": 3.521,
        "text": "to"
      },
      {
        "start": 60.64,
        "duration": 4.799,
        "text": "load test data pre-production"
      },
      {
        "start": 63.76,
        "duration": 3.92,
        "text": "that sort of thing it's not meant to"
      },
      {
        "start": 65.439,
        "duration": 3.921,
        "text": "move a lot of data at the same time"
      },
      {
        "start": 67.68,
        "duration": 3.28,
        "text": "it's a small amount of data in"
      },
      {
        "start": 69.36,
        "duration": 2.799,
        "text": "comparison to how much a cassandra"
      },
      {
        "start": 70.96,
        "duration": 2.56,
        "text": "cluster can hold"
      },
      {
        "start": 72.159,
        "duration": 3.28,
        "text": "this should not be confused with the"
      },
      {
        "start": 73.52,
        "duration": 2.56,
        "text": "insert command which is a different way"
      },
      {
        "start": 75.439,
        "duration": 2.481,
        "text": "to do it"
      },
      {
        "start": 76.08,
        "duration": 3.6,
        "text": "but this is similar to how it works it"
      },
      {
        "start": 77.92,
        "duration": 3.36,
        "text": "will insert data into the cluster"
      },
      {
        "start": 79.68,
        "duration": 4.56,
        "text": "so there are some rules to work with"
      },
      {
        "start": 81.28,
        "duration": 4.8,
        "text": "here for instance if you do not specify"
      },
      {
        "start": 84.24,
        "duration": 3.919,
        "text": "the rows that you're inserting then you"
      },
      {
        "start": 86.08,
        "duration": 3.999,
        "text": "need to have the same number of columns"
      },
      {
        "start": 88.159,
        "duration": 3.361,
        "text": "that are inside the table and it will"
      },
      {
        "start": 90.079,
        "duration": 3.121,
        "text": "try to match those up"
      },
      {
        "start": 91.52,
        "duration": 3.68,
        "text": "another thing is if there's an empty"
      },
      {
        "start": 93.2,
        "duration": 4.879,
        "text": "data set it's considered to be a null"
      },
      {
        "start": 95.2,
        "duration": 3.44,
        "text": "it's not a zero it's not a space it's a"
      },
      {
        "start": 98.079,
        "duration": 2.161,
        "text": "null"
      },
      {
        "start": 98.64,
        "duration": 4.08,
        "text": "and that's what it assumes whenever it"
      },
      {
        "start": 100.24,
        "duration": 4.32,
        "text": "finds no data the copy from"
      },
      {
        "start": 102.72,
        "duration": 3.759,
        "text": "is always about importing but small"
      },
      {
        "start": 104.56,
        "duration": 2.48,
        "text": "amounts of data don't think about this"
      },
      {
        "start": 106.479,
        "duration": 2.801,
        "text": "as a"
      },
      {
        "start": 107.04,
        "duration": 3.68,
        "text": "i'm going to reload my entire database"
      },
      {
        "start": 109.28,
        "duration": 2.64,
        "text": "there's another tool for that that we'll"
      },
      {
        "start": 110.72,
        "duration": 3.039,
        "text": "talk about later"
      },
      {
        "start": 111.92,
        "duration": 3.6,
        "text": "but this is a good way to get small"
      },
      {
        "start": 113.759,
        "duration": 3.841,
        "text": "amounts of data in and out of the system"
      },
      {
        "start": 115.52,
        "duration": 4.16,
        "text": "just follow that rule and you'll be fine"
      },
      {
        "start": 117.6,
        "duration": 4.0,
        "text": "for those really large data sets"
      },
      {
        "start": 119.68,
        "duration": 4.16,
        "text": "you want to use ds bulk it's been"
      },
      {
        "start": 121.6,
        "duration": 3.76,
        "text": "designed to run the full load"
      },
      {
        "start": 123.84,
        "duration": 3.2,
        "text": "of the data that could be stored in a"
      },
      {
        "start": 125.36,
        "duration": 3.679,
        "text": "cluster at that scale"
      },
      {
        "start": 127.04,
        "duration": 3.199,
        "text": "so a better tool for if you're moving"
      },
      {
        "start": 129.039,
        "duration": 3.601,
        "text": "large data sets"
      },
      {
        "start": 130.239,
        "duration": 3.681,
        "text": "so to run cql copy first thing you do is"
      },
      {
        "start": 132.64,
        "duration": 3.2,
        "text": "you look at your file"
      },
      {
        "start": 133.92,
        "duration": 3.92,
        "text": "what's the delimiter what options do i"
      },
      {
        "start": 135.84,
        "duration": 4.32,
        "text": "need to use now in this case"
      },
      {
        "start": 137.84,
        "duration": 3.84,
        "text": "it's a pipe delimiter not my choice but"
      },
      {
        "start": 140.16,
        "duration": 4.0,
        "text": "hey that's what it is"
      },
      {
        "start": 141.68,
        "duration": 3.84,
        "text": "so you need to specify that the limiter"
      },
      {
        "start": 144.16,
        "duration": 3.28,
        "text": "is a pipe symbol"
      },
      {
        "start": 145.52,
        "duration": 3.76,
        "text": "and then it has a header so that last"
      },
      {
        "start": 147.44,
        "duration": 4.799,
        "text": "name first name email createddate"
      },
      {
        "start": 149.28,
        "duration": 4.64,
        "text": "is the first row of the file so if"
      },
      {
        "start": 152.239,
        "duration": 3.761,
        "text": "that's the first row of the file"
      },
      {
        "start": 153.92,
        "duration": 4.48,
        "text": "and it's the delimiter of a pipe you"
      },
      {
        "start": 156.0,
        "duration": 5.28,
        "text": "specify that in your configuration"
      },
      {
        "start": 158.4,
        "duration": 3.68,
        "text": "when you run a copy from it will be"
      },
      {
        "start": 161.28,
        "duration": 2.48,
        "text": "happy"
      },
      {
        "start": 162.08,
        "duration": 4.08,
        "text": "all that data will go directly into the"
      },
      {
        "start": 163.76,
        "duration": 4.4,
        "text": "user table good to go"
      },
      {
        "start": 166.16,
        "duration": 3.84,
        "text": "so those options as i mentioned before"
      },
      {
        "start": 168.16,
        "duration": 2.56,
        "text": "like delimiter are something you need to"
      },
      {
        "start": 170.0,
        "duration": 2.8,
        "text": "specify"
      },
      {
        "start": 170.72,
        "duration": 3.28,
        "text": "the default of course is a comma because"
      },
      {
        "start": 172.8,
        "duration": 2.64,
        "text": "it's comma delimited"
      },
      {
        "start": 174.0,
        "duration": 3.2,
        "text": "delimiter is something though that's"
      },
      {
        "start": 175.44,
        "duration": 4.0,
        "text": "pretty universally"
      },
      {
        "start": 177.2,
        "duration": 3.679,
        "text": "different let's face it no one uses the"
      },
      {
        "start": 179.44,
        "duration": 3.92,
        "text": "same delimiter all the time"
      },
      {
        "start": 180.879,
        "duration": 3.841,
        "text": "it's good to have options the header is"
      },
      {
        "start": 183.36,
        "duration": 3.76,
        "text": "a true or false"
      },
      {
        "start": 184.72,
        "duration": 3.599,
        "text": "if you're including a header specify the"
      },
      {
        "start": 187.12,
        "duration": 3.759,
        "text": "fields that you have"
      },
      {
        "start": 188.319,
        "duration": 4.081,
        "text": "in the file when you import the data"
      },
      {
        "start": 190.879,
        "duration": 2.08,
        "text": "you'll look at that first row and say"
      },
      {
        "start": 192.4,
        "duration": 2.16,
        "text": "aha"
      },
      {
        "start": 192.959,
        "duration": 3.36,
        "text": "here's all the data in the columns i"
      },
      {
        "start": 194.56,
        "duration": 2.56,
        "text": "want it presented i will import this"
      },
      {
        "start": 196.319,
        "duration": 2.481,
        "text": "data for you"
      },
      {
        "start": 197.12,
        "duration": 3.68,
        "text": "you have to make sure those column names"
      },
      {
        "start": 198.8,
        "duration": 4.0,
        "text": "exist in the database"
      },
      {
        "start": 200.8,
        "duration": 4.24,
        "text": "but that's what it's looking for true or"
      },
      {
        "start": 202.8,
        "duration": 4.24,
        "text": "false the chunk size or how many records"
      },
      {
        "start": 205.04,
        "duration": 3.52,
        "text": "are passed to the processes to run to"
      },
      {
        "start": 207.04,
        "duration": 3.279,
        "text": "insert into the database"
      },
      {
        "start": 208.56,
        "duration": 4.16,
        "text": "now this could be a performance concern"
      },
      {
        "start": 210.319,
        "duration": 5.361,
        "text": "if you have a lot of chunk size"
      },
      {
        "start": 212.72,
        "duration": 3.599,
        "text": "then it can potentially load faster as"
      },
      {
        "start": 215.68,
        "duration": 2.4,
        "text": "it's getting"
      },
      {
        "start": 216.319,
        "duration": 3.2,
        "text": "larger bits of data and pushing it"
      },
      {
        "start": 218.08,
        "duration": 3.04,
        "text": "faster into the database"
      },
      {
        "start": 219.519,
        "duration": 3.041,
        "text": "it can also overwhelm your database a"
      },
      {
        "start": 221.12,
        "duration": 3.119,
        "text": "little bit let's say that you're running"
      },
      {
        "start": 222.56,
        "duration": 2.72,
        "text": "on a very small configuration say on"
      },
      {
        "start": 224.239,
        "duration": 3.041,
        "text": "your laptop"
      },
      {
        "start": 225.28,
        "duration": 3.679,
        "text": "well maybe you don't need as big of a"
      },
      {
        "start": 227.28,
        "duration": 2.8,
        "text": "chunk size use that as a tuning"
      },
      {
        "start": 228.959,
        "duration": 3.2,
        "text": "parameter"
      },
      {
        "start": 230.08,
        "duration": 3.92,
        "text": "skip rows is one of those things that's"
      },
      {
        "start": 232.159,
        "duration": 3.121,
        "text": "used say in a data loading scenario"
      },
      {
        "start": 234.0,
        "duration": 2.879,
        "text": "where you just want to load a little bit"
      },
      {
        "start": 235.28,
        "duration": 3.44,
        "text": "of data maybe a sample"
      },
      {
        "start": 236.879,
        "duration": 3.761,
        "text": "again probably use for your testing"
      },
      {
        "start": 238.72,
        "duration": 3.68,
        "text": "scenarios not something that you'd"
      },
      {
        "start": 240.64,
        "duration": 4.159,
        "text": "normally use in production"
      },
      {
        "start": 242.4,
        "duration": 3.68,
        "text": "copy 2 in this case is when you want to"
      },
      {
        "start": 244.799,
        "duration": 4.321,
        "text": "be able to move the data"
      },
      {
        "start": 246.08,
        "duration": 5.92,
        "text": "from a cassandra table into a file"
      },
      {
        "start": 249.12,
        "duration": 3.199,
        "text": "copy 2 pick the name of the table and"
      },
      {
        "start": 252.0,
        "duration": 2.239,
        "text": "then"
      },
      {
        "start": 252.319,
        "duration": 3.681,
        "text": "output it directly to the file system"
      },
      {
        "start": 254.239,
        "duration": 2.801,
        "text": "this is the default the default is it"
      },
      {
        "start": 256.0,
        "duration": 3.04,
        "text": "creates a csv"
      },
      {
        "start": 257.04,
        "duration": 4.08,
        "text": "file of all the data inside of that"
      },
      {
        "start": 259.04,
        "duration": 3.599,
        "text": "table some of the parameters that you"
      },
      {
        "start": 261.12,
        "duration": 3.6,
        "text": "can put into a copy too"
      },
      {
        "start": 262.639,
        "duration": 3.681,
        "text": "are just specifying the exact fields"
      },
      {
        "start": 264.72,
        "duration": 3.68,
        "text": "that you want to copy out"
      },
      {
        "start": 266.32,
        "duration": 3.92,
        "text": "so for instance if you have a lot of"
      },
      {
        "start": 268.4,
        "duration": 4.0,
        "text": "columns in a particular table"
      },
      {
        "start": 270.24,
        "duration": 3.519,
        "text": "maybe you only want four or five of"
      },
      {
        "start": 272.4,
        "duration": 3.68,
        "text": "those fields out"
      },
      {
        "start": 273.759,
        "duration": 4.081,
        "text": "specify them here if you don't it will"
      },
      {
        "start": 276.08,
        "duration": 4.08,
        "text": "just dump the entire table"
      },
      {
        "start": 277.84,
        "duration": 5.28,
        "text": "so you need to specify the file name"
      },
      {
        "start": 280.16,
        "duration": 4.96,
        "text": "when you do the copy to you can also use"
      },
      {
        "start": 283.12,
        "duration": 3.2,
        "text": "standard out and in the case of copy"
      },
      {
        "start": 285.12,
        "duration": 2.799,
        "text": "from standard in"
      },
      {
        "start": 286.32,
        "duration": 3.439,
        "text": "this is useful if you're using say a"
      },
      {
        "start": 287.919,
        "duration": 3.441,
        "text": "pipe or redirect where you're bringing"
      },
      {
        "start": 289.759,
        "duration": 3.121,
        "text": "in data from the command line"
      },
      {
        "start": 291.36,
        "duration": 3.119,
        "text": "and if you're one of those command line"
      },
      {
        "start": 292.88,
        "duration": 3.52,
        "text": "nerds you know who my people"
      },
      {
        "start": 294.479,
        "duration": 3.361,
        "text": "are that's a good thing right using"
      },
      {
        "start": 296.4,
        "duration": 2.48,
        "text": "pipes and delimiters well we have an"
      },
      {
        "start": 297.84,
        "duration": 3.04,
        "text": "option for you"
      },
      {
        "start": 298.88,
        "duration": 3.68,
        "text": "just keep in mind if using standard in"
      },
      {
        "start": 300.88,
        "duration": 2.48,
        "text": "the end of the delimited data needs to"
      },
      {
        "start": 302.56,
        "duration": 3.12,
        "text": "have a"
      },
      {
        "start": 303.36,
        "duration": 4.64,
        "text": "backslash period to note the end of the"
      },
      {
        "start": 305.68,
        "duration": 2.32,
        "text": "file"
      },
      {
        "start": 309.36,
        "duration": 4.24,
        "text": "so that's a pretty basic overview of how"
      },
      {
        "start": 311.44,
        "duration": 5.28,
        "text": "to use the cql copy command"
      },
      {
        "start": 313.6,
        "duration": 4.72,
        "text": "in and out again limited use case don't"
      },
      {
        "start": 316.72,
        "duration": 3.039,
        "text": "use it for all of your data"
      },
      {
        "start": 318.32,
        "duration": 6.4,
        "text": "but i think you can find some pretty"
      },
      {
        "start": 319.759,
        "duration": 7.041,
        "text": "useful places for this to be used"
      },
      {
        "start": 324.72,
        "duration": 2.08,
        "text": "you"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T01:23:24.464526+00:00"
}