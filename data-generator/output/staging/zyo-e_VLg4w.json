{
  "video_id": "zyo-e_VLg4w",
  "title": "DS210.24 sstableloader | Operations with Apache Cassandra",
  "description": "#DataStaxAcademy #DS210\nDS210.24 SSTABLELOADER\nGet a a lot data into an Apache Cassandra node quickly by using sstableloader. It was really built to take old existing sstables and load them into a cluster. Learn more about sstableloader in this unit.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-12T01:21:53Z",
  "thumbnail": "https://i.ytimg.com/vi/zyo-e_VLg4w/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "datastax",
    "tutorial",
    "apache_cassandra"
  ],
  "url": "https://www.youtube.com/watch?v=zyo-e_VLg4w",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] let's talk about ss table loader which is an old tool but a goodie this is how we used to get a lot of data into a cassandra node quickly how does it work so ss table loader was an old command from the original version of cassandra what it meant is you took a representation of an ss table and used that to load it into a cluster and it was a little onerous it was hard to make but it was really built to take old existing ss tables and load those into a cluster say if you were moving data from one production cluster to a test cluster if you're really fancy you could make your own ss tables and use that to load that data into your cluster so how it works is it takes a file and streams that data back into the cluster so no matter what the partition information is inside of that ss table it will figure out a home for all that data in the cluster the setup is that it has to run on an active cluster so it will then transform it into whatever that cluster is set up for so for instance if you have a different replication factor or a different node count it will all work because it is streaming just the data back into the cluster this isn't like copying an ss table directly into a data file and expecting it to work very different in that regard it's also really fast it used to be the fastest way to load a lot of data into your cluster granted it probably wasn't the easiest way to do it but when high speed was the key this is what got the job done so look at this command and you can see how it works and there's some very interesting little tidbits in here essence table requires you to specify a running node and then specify the file location that running node that we're specifying with the dash d has to be aware of the cluster or one of the coordinators on the cluster this can put a tremendous amount of load on that coordinator when it's loading that data now again this is what was supposed to do because it's a bulk loader but just keep that in mind so one of the things that this was really useful for though was the non-light clusters say moving from a 10 node cluster to a 5 node cluster there's really not a lot of ways to do that this is one of the things that it really excels at probably the hardest part is this is a prerequisites you need to have a cassandra yama file set up somewhere that has basically the same information of the cluster you're dumping the data into and that yaml file needs to be configured just the same with the cluster name the address the storage port all of that it makes it like pretty much setting up a node without actually running it in the cluster that was a hard part of running this but if you need to load a lot of data from existing ss tables this is probably a process that you have set up already and this is something you need to set up as a methodology so as you'll see there's different ways to load data we have the cql copy we have ss table loader and we also have another tool it's called ds bulk which we'll be talking about but this tool will get the job done if you have an ss table that you need to load into system",
    "segments": [
      {
        "start": 1.43,
        "duration": 5.33,
        "text": "[Music]"
      },
      {
        "start": 7.12,
        "duration": 4.399,
        "text": "let's talk about"
      },
      {
        "start": 8.08,
        "duration": 5.439,
        "text": "ss table loader which is an old tool"
      },
      {
        "start": 11.519,
        "duration": 3.761,
        "text": "but a goodie this is how we used to get"
      },
      {
        "start": 13.519,
        "duration": 3.121,
        "text": "a lot of data into a cassandra node"
      },
      {
        "start": 15.28,
        "duration": 3.839,
        "text": "quickly how does it work"
      },
      {
        "start": 16.64,
        "duration": 4.559,
        "text": "so ss table loader was an old command"
      },
      {
        "start": 19.119,
        "duration": 3.121,
        "text": "from the original version of cassandra"
      },
      {
        "start": 21.199,
        "duration": 3.281,
        "text": "what it meant is you took a"
      },
      {
        "start": 22.24,
        "duration": 5.359,
        "text": "representation of an ss table"
      },
      {
        "start": 24.48,
        "duration": 5.6,
        "text": "and used that to load it into a cluster"
      },
      {
        "start": 27.599,
        "duration": 3.121,
        "text": "and it was a little onerous it was hard"
      },
      {
        "start": 30.08,
        "duration": 2.8,
        "text": "to make"
      },
      {
        "start": 30.72,
        "duration": 4.4,
        "text": "but it was really built to take old"
      },
      {
        "start": 32.88,
        "duration": 2.72,
        "text": "existing ss tables and load those into a"
      },
      {
        "start": 35.12,
        "duration": 2.72,
        "text": "cluster"
      },
      {
        "start": 35.6,
        "duration": 4.16,
        "text": "say if you were moving data from one"
      },
      {
        "start": 37.84,
        "duration": 3.68,
        "text": "production cluster to a test cluster"
      },
      {
        "start": 39.76,
        "duration": 3.36,
        "text": "if you're really fancy you could make"
      },
      {
        "start": 41.52,
        "duration": 3.519,
        "text": "your own ss tables"
      },
      {
        "start": 43.12,
        "duration": 4.4,
        "text": "and use that to load that data into your"
      },
      {
        "start": 45.039,
        "duration": 4.721,
        "text": "cluster so how it works is it takes a"
      },
      {
        "start": 47.52,
        "duration": 5.199,
        "text": "file and streams that data back"
      },
      {
        "start": 49.76,
        "duration": 4.4,
        "text": "into the cluster so no matter what the"
      },
      {
        "start": 52.719,
        "duration": 3.68,
        "text": "partition information is"
      },
      {
        "start": 54.16,
        "duration": 3.76,
        "text": "inside of that ss table it will figure"
      },
      {
        "start": 56.399,
        "duration": 2.241,
        "text": "out a home for all that data in the"
      },
      {
        "start": 57.92,
        "duration": 3.04,
        "text": "cluster"
      },
      {
        "start": 58.64,
        "duration": 3.68,
        "text": "the setup is that it has to run on an"
      },
      {
        "start": 60.96,
        "duration": 3.36,
        "text": "active cluster"
      },
      {
        "start": 62.32,
        "duration": 3.92,
        "text": "so it will then transform it into"
      },
      {
        "start": 64.32,
        "duration": 2.96,
        "text": "whatever that cluster is set up for so"
      },
      {
        "start": 66.24,
        "duration": 2.48,
        "text": "for instance if you have a different"
      },
      {
        "start": 67.28,
        "duration": 3.44,
        "text": "replication factor"
      },
      {
        "start": 68.72,
        "duration": 3.439,
        "text": "or a different node count it will all"
      },
      {
        "start": 70.72,
        "duration": 3.6,
        "text": "work because it is streaming"
      },
      {
        "start": 72.159,
        "duration": 4.241,
        "text": "just the data back into the cluster this"
      },
      {
        "start": 74.32,
        "duration": 4.0,
        "text": "isn't like copying an ss table"
      },
      {
        "start": 76.4,
        "duration": 4.24,
        "text": "directly into a data file and expecting"
      },
      {
        "start": 78.32,
        "duration": 4.799,
        "text": "it to work very different in that regard"
      },
      {
        "start": 80.64,
        "duration": 4.64,
        "text": "it's also really fast it used to be the"
      },
      {
        "start": 83.119,
        "duration": 3.201,
        "text": "fastest way to load a lot of data into"
      },
      {
        "start": 85.28,
        "duration": 2.64,
        "text": "your cluster"
      },
      {
        "start": 86.32,
        "duration": 3.759,
        "text": "granted it probably wasn't the easiest"
      },
      {
        "start": 87.92,
        "duration": 2.72,
        "text": "way to do it but when high speed was the"
      },
      {
        "start": 90.079,
        "duration": 3.201,
        "text": "key"
      },
      {
        "start": 90.64,
        "duration": 4.32,
        "text": "this is what got the job done so look at"
      },
      {
        "start": 93.28,
        "duration": 3.519,
        "text": "this command and you can see how it"
      },
      {
        "start": 94.96,
        "duration": 2.24,
        "text": "works and there's some very interesting"
      },
      {
        "start": 96.799,
        "duration": 2.721,
        "text": "little"
      },
      {
        "start": 97.2,
        "duration": 3.52,
        "text": "tidbits in here essence table requires"
      },
      {
        "start": 99.52,
        "duration": 4.239,
        "text": "you to specify"
      },
      {
        "start": 100.72,
        "duration": 6.24,
        "text": "a running node and then specify"
      },
      {
        "start": 103.759,
        "duration": 4.561,
        "text": "the file location that running node that"
      },
      {
        "start": 106.96,
        "duration": 4.0,
        "text": "we're specifying with the dash"
      },
      {
        "start": 108.32,
        "duration": 4.159,
        "text": "d has to be aware of the cluster or one"
      },
      {
        "start": 110.96,
        "duration": 3.199,
        "text": "of the coordinators on the cluster"
      },
      {
        "start": 112.479,
        "duration": 3.6,
        "text": "this can put a tremendous amount of load"
      },
      {
        "start": 114.159,
        "duration": 2.96,
        "text": "on that coordinator when it's loading"
      },
      {
        "start": 116.079,
        "duration": 3.121,
        "text": "that data"
      },
      {
        "start": 117.119,
        "duration": 4.0,
        "text": "now again this is what was supposed to"
      },
      {
        "start": 119.2,
        "duration": 3.919,
        "text": "do because it's a bulk loader"
      },
      {
        "start": 121.119,
        "duration": 3.521,
        "text": "but just keep that in mind so one of the"
      },
      {
        "start": 123.119,
        "duration": 2.081,
        "text": "things that this was really useful for"
      },
      {
        "start": 124.64,
        "duration": 3.679,
        "text": "though"
      },
      {
        "start": 125.2,
        "duration": 5.119,
        "text": "was the non-light clusters say moving"
      },
      {
        "start": 128.319,
        "duration": 2.64,
        "text": "from a 10 node cluster to a 5 node"
      },
      {
        "start": 130.319,
        "duration": 2.161,
        "text": "cluster"
      },
      {
        "start": 130.959,
        "duration": 3.201,
        "text": "there's really not a lot of ways to do"
      },
      {
        "start": 132.48,
        "duration": 3.119,
        "text": "that this is one of the things that it"
      },
      {
        "start": 134.16,
        "duration": 4.159,
        "text": "really excels at"
      },
      {
        "start": 135.599,
        "duration": 3.921,
        "text": "probably the hardest part is this is a"
      },
      {
        "start": 138.319,
        "duration": 3.201,
        "text": "prerequisites"
      },
      {
        "start": 139.52,
        "duration": 4.16,
        "text": "you need to have a cassandra yama file"
      },
      {
        "start": 141.52,
        "duration": 3.76,
        "text": "set up somewhere that has"
      },
      {
        "start": 143.68,
        "duration": 3.76,
        "text": "basically the same information of the"
      },
      {
        "start": 145.28,
        "duration": 3.52,
        "text": "cluster you're dumping the data into"
      },
      {
        "start": 147.44,
        "duration": 2.64,
        "text": "and that yaml file needs to be"
      },
      {
        "start": 148.8,
        "duration": 2.24,
        "text": "configured just the same with the"
      },
      {
        "start": 150.08,
        "duration": 3.84,
        "text": "cluster name"
      },
      {
        "start": 151.04,
        "duration": 4.32,
        "text": "the address the storage port all of that"
      },
      {
        "start": 153.92,
        "duration": 2.16,
        "text": "it makes it like pretty much setting up"
      },
      {
        "start": 155.36,
        "duration": 2.08,
        "text": "a node"
      },
      {
        "start": 156.08,
        "duration": 3.439,
        "text": "without actually running it in the"
      },
      {
        "start": 157.44,
        "duration": 2.56,
        "text": "cluster that was a hard part of running"
      },
      {
        "start": 159.519,
        "duration": 2.72,
        "text": "this"
      },
      {
        "start": 160.0,
        "duration": 4.0,
        "text": "but if you need to load a lot of data"
      },
      {
        "start": 162.239,
        "duration": 3.521,
        "text": "from existing ss tables"
      },
      {
        "start": 164.0,
        "duration": 3.599,
        "text": "this is probably a process that you have"
      },
      {
        "start": 165.76,
        "duration": 2.64,
        "text": "set up already and this is something you"
      },
      {
        "start": 167.599,
        "duration": 3.041,
        "text": "need to set up"
      },
      {
        "start": 168.4,
        "duration": 3.919,
        "text": "as a methodology so as you'll see"
      },
      {
        "start": 170.64,
        "duration": 2.239,
        "text": "there's different ways to load data we"
      },
      {
        "start": 172.319,
        "duration": 4.0,
        "text": "have"
      },
      {
        "start": 172.879,
        "duration": 4.64,
        "text": "the cql copy we have ss table loader"
      },
      {
        "start": 176.319,
        "duration": 2.721,
        "text": "and we also have another tool it's"
      },
      {
        "start": 177.519,
        "duration": 2.321,
        "text": "called ds bulk which we'll be talking"
      },
      {
        "start": 179.04,
        "duration": 3.68,
        "text": "about"
      },
      {
        "start": 179.84,
        "duration": 4.399,
        "text": "but this tool will get the job done if"
      },
      {
        "start": 182.72,
        "duration": 7.599,
        "text": "you have an ss table that you need to"
      },
      {
        "start": 184.239,
        "duration": 6.08,
        "text": "load into system"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T01:21:22.321475+00:00"
}