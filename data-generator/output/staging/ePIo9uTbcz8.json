{
  "video_id": "ePIo9uTbcz8",
  "title": "Distributed Data Show Episode 41: Graph-based Genealogy with Dave Bechberger",
  "description": "Migrating from a Relational application to a Graph based application is an undertaking that takes forethought, planning and the right use case.  The challenges with taking a team used to working in a relational world and transitioning them to a distributed, eventually consistent system based on a graph are many.  In this episode we talk to Dave Bechberger who is the Chief Software for Gene By Gene which is a Bioinformatics company specializing in Genetic Genealogy. Dave will share his experience and learnings from migrating their relational application to a graph based one.\n\nHighlights!\n0:15 - We welcome Dave Bechberger to the show and learn how he got into big data technologies like Apache Cassandra and DSE Graph\n1:38 - Dave introduces his current work with Gene by Gene applying graph technology to genealogy applications\n3:43 - The performance of legacy systems was database bound, so they began to look at using non-relational databases, especially graph, starting with a family tree application. \n6:19 - Dave describes how his team identified a graph datastore as the best approach for the family tree functionality. Family tree queries can be very recursive - for example: how are these two people related?\n7:43 - The biggest challenge in adopting graph technology was training - Gremlin queries require a different way of thinking. At the same time, they were also migrating to a microservice architecture style based on asynchronous event passing.\n10:04 - The benefits of all this change outweighed cost. The biggest “do different” they identified was to Invest in upfront training on pragmatic approaches to graph.\n11:24 - Why specialization can be beneficial to a team learning multiple new technologies.\n13:06 - The graph schema evolved over time - while their initial schema was based on an industry standard, they ended up adding additional vertex and edge types, as well as indexes to help optimize queries for both analytical and transactional use cases.\n15:31 - Dave’s team ",
  "published_at": "2018-04-03T15:00:03Z",
  "thumbnail": "https://i.ytimg.com/vi/ePIo9uTbcz8/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "distributed",
    "cassandra",
    "database",
    "apache_cassandra",
    "performance",
    "talk",
    "architecture",
    "dse"
  ],
  "url": "https://www.youtube.com/watch?v=ePIo9uTbcz8",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "welcome to another episode of the distributed data show brought to you by data Stax Academy where we bring you the latest news and interview technical experts to help you succeed at building large-scale distributed systems welcome to another episode of the distributed data show I'm David Gilardi this is Jeff carpenter and today our guest is Dave Beck Berger and today we're gonna talk about migrating an application using a legacy database to one using a craft based solution so Dave I like that name by the way it's a good name you know it feels very familiar some reason I don't know what it is but you know it's great so why don't you tell us a little bit about your background and you know how how'd you even get into graph databases yeah so I've been doing full stack software development for about 18 years now pretty actually mainly focusing on the.net stack for about or for most of that time about four or five years ago I kind of made the migration over to dealing with big data technologies in which point I kind of switched over to more of the java scala side i started out just kind of dealing with a variety of different uh you know different Big Data technologies including Cassandra and from there I was working at a consulting company and we were one of the initial we used to work on graph databases such as Titan and we kind of migrated across to working on DSC graph from the beta phase on so I've actually been kind of involved in pretty much exclusively using graph databases for about three or - two and a half three years now okay yeah what what kind of graph database problems are you working on right now so right now I'm chief Software Architect for a company called gene by gene what we do is we do genetic genealogy so we're a competitor with companies probably a little more familiar with if someone like 23andme or ancestry DNA basically you know you send us a DNA you basically you buy a kit from us we but you basically get it you swab your cheek you send that back to us and then we sequence your data we sequence your DNA and provide you out with part of the data that we provide to you is your genetic genealogy so we they are able to compare your DNA against the other people in our database to find out how you are related to them so you know you is it your third cousin three times removed or is it your aunt and uncle and we're able to basically run that against our data - or run your DNA against our data to be able to basically say are you related or and how roughly how closely are you related one of the specific parts that we're working with right now is Bill is basically rebuilding out our family tree portion of our application being genealogy being genealogists building out your family tree and have that function well is a really crucial part to our website so that is one of the things that we're kind of focusing on right now is getting that part of our business you know working very consistently performant and solid you know those things everybody wants in every web application right it's got to be fast it's got to be reliable it's got to be always on this is like the table stakes right you know it's funny too cuz as he's as he's talking about this this sounds like it sounds like a good use case for graph I feel like we should have Denice guys know on here right now you know determining whether or not oh just to provide like the this is a graph use case stamp of approval I think we need to give her like a judge wig and have her like in the background yeah and just have her rate whether or not this is a use case but I think she would approve I think she would have proved pretty sure I know everything would be so so like what made you you know decide to transition your your relational solution to graph I mean honestly I think you started just touching on it from from what I was hearing but now I'm gonna hear some of the details like how'd that happen well and that wasn't all you - I just want to interject like obviously they hired you as a graph expert knowing I think like foreshadow that they wanted to be making this transition right but obviously there was some some thought process that went into this so yeah so I mean our current application is it's the so the company has been around for 18 years and our application is been around for 18 years so it's you know it says you would probably expect it's a big it's a big monolithic application built on top of sequel server we're actually a.net shot mainly so we would spill on top of Microsoft sequel server so it's as I said it's a you know dotnet application build on top of sequel server and one of the big problems they were having as you do with pretty much any monolithic application is it was database bound you know the performance of our website was completely dependent on the performance of our database so you know since that database had grown organically over 18 years as more and more features were added as we all know it was not exactly the optimal data model either so what we kind of decided to do was that they were going to start basically taking that monolithic database and breaking it out into kind of vertical feature silos and Bakey and building those feed capture silos using you know you know different data stores for different problems and different in micro services to basically allow you to do all those great things that micro services do is you know I can change them as I need to I can you know scale them up scale them down use my resources more efficiently you know those sorts of problems so that was kind of how they came to the decision of they wanted to do some architectural changes the first one they chose to do was actually one of the core features of our system which is that family tree aspect you know it's a genealogy site having your family tree yeah a little important so they basically a couple of the other senior devs and one of the other senior architects kind of looked looked at the problem space and decided that I looked at the problem they were trying to solve and the different tools that were out there and decided that they wanted to go with a graph database in that point that's when I came on to basically help them implement that out because as well get into talking about here you know making that switch is that there's there's definitely uh definite gotchas know in the in the whole switch admit to making it work gotcha okay so this is I get the architectural context here you're kind of transitioning from a monolith carving off pieces of it maybe a little bit at a time and moving to what sounds like a little bit more of a polyglot persistence kind of approach where different services might have different backing data source behind them am I on the right track yep no absolutely you're you're absolutely correct that's kind of where it's definitely going towards more of a poly sort of persistence where you know we're trying to choose the best best data store technology out there for the different problems that exists okay cool so so for at least some significant portions of the data that right solution is a graph database yeah yeah we're working through as I said we're kind of working through building out your family tree I mean what's the family tree but it's just you know it's a very recursive problem just by the true nature of it it's a recursive problem where you want to then be able to query it to answer very graph sort of problems you know right problems like what's the you know how is this person related to this person well if you want to find how two nodes in a graph are related to each other it's very easy if you want to do that in something like a relational database it's very time consuming and difficult and I don't want to write that CTE to make that happen right where's something like graph it's a simplest path query we probably all seen examples of the multi-page SQL query to do some of these kind of equivalent to what a graph traversal would look like right so okay that makes perfect sense to me but we all know in doing migrations like this like just because you have a use case that's a good fit for a graph that doesn't mean that everything is easy-peasy and you know that like these are obviously like this is the big money-making system of the company you're working on the crown jewels your I've destroyed a monolith myself before it's it's not easy so what were some of the challenges that you ran into in doing this migration oh I mean there was variety different challenges the first and probably most significant one is the fact that you know the team I was working with as well as pretty much I would guess 99% of you know development teams out there are very familiar with relational databases and not as familiar with graph databases so being able to kind of make that you know be able to able to teach people how to actually think think is it's thinking a graph mannerism versus a relational manner and work through the problems that a graph presents you that you don't have whether relational and things like that because you know in a relational this you know it's you know people are very used to how sequel works you know pretty much any developer that's been around for any amount of can write a reasonably good sequel query gremlin queries are not quite as easy to learn and they're very it's a very different way to think about the same sort of problem you know you're thinking about how you're traversing through your graph versus how I'm joining across tables and keys so there was definitely some challenges and basically getting the team up to speed on how how to think and work in a graph you know if anyone that's worked with a graph firm you know any amount of time finds that definitely has that there's definitely a learning curve that you have to go through with that sort of thing right I mean the other thing was one of one of the the key things we ended up kind of getting tripped up on was we were not only switching from a you know relational model to a graph model the same time we were kind of switching from a you know a response request acid transactions sort of system to one that is eventually consistent in a web application well it's dealing with eventual consistency in a web application you have to treat the problem a little bit different I can't you know create a family tree and then read it back immediately like you're used to being able to do so we had to work through those sorts of problems as well cool okay okay well like so given those particular challenges right the ones you were just talking about would you do it again the same way uh yeah I mean actually I would probably do it well the exact same way no let's dig deeper yeah would be some additional learnings I would take from it but yeah I mean I still think based on I mean what we're working on right now is really just kind of the foundation for a lot of the next-level things we want to be able to do that a graph that the graph will enable us to do as far as being able to match things and and and you know do all the great analytics that we can get with having all our information in a graph that are very difficult to do inside of a sequel server server or any other relational database so I think the benefits outweigh the the cost but I think you know the one thing I would definitely do a little bit differently the second time around versus the first time around is invest a little more in that upfront training on those kind of two key topics I talked about one being you know graph you know the nut that Kraft theory so much but basically I call it like pragmatic graph you know there's the fear your pal graft work and then you have to lay that layer of pragmatism on top of it somebody in a writing book pragmatic graph someone should write that book nice but what but seriously like what resources did you did you use to get your team up to speed your development team up to speed on graph how did you cuz you bet you got there eventually at least I hope you did well it's you know there was you know we probably started out with four or five people that were really learning graph and we ended up ready to two and a half or three of them that really kind of got it in the end okay you know we were trying to do this as well as getting everything else done at the same time same great there's a distinct time investment involved in getting this out you know I've been working no that makes sense I've seen a parallel experience in a development team where I was working with and they were you know they were coming up to speed on Cassandra and it ended up that not every person ended up being you know the full on Cassandra DBA with that level of data modeling skills right it's okay to have that specialization with your team to be sure yeah well it's not only okay to have that specialization with your team I mean my biggest concern was at least that the majority of the team understood the difference between the two and when one was right versus the other because you don't have to know one's an expert in everything but you it's really nice to know whether this is the right tool for the job or you know I always think about it you know the relational database is the hammer if all you have is a relational database everything looks like a nail well it's nice to be able to you know expand it out and they're like well this is a screw and this is a hammer and you know this is a but I can do different things with different pieces of it especially as we're moving from that you know monolithic sequel server application to a more polyglot persistence layer it's very good you know we wanted to make sure that people would be able to have the the knowledge and freedom to choose the right tools to do the right job right so just a follow up to you to dig in there a little bit more dude I was curious about how your schema evolved over time did you like really have like you're so good with graph databases and you know the problem domain so that you just nailed the scheme of the first time or was it you know migration overtime it what did that look like well so with the specific problem we solving with dealing with family trees there was actually there's a standard out there it's called gedcom GED Co em okay last updated in like the 1995 it's kind of somewhere between a really bad XML and a really bad linked list sort of now it's yeah it's interesting so we actually you know we wanted to be standards compliant because we wanted to be able to import and export these because that's the you know the industry standard in genealogy so we were able to at least use that as some of the basis of it okay but uh so the getting the basic data model correct was relatively easy because we could use that as our basis of how it works we had our domain model already set for us but it was when we started layering all the other you know value-added features that we do with our genetic aspects of it that started become a little more interesting about how how we wanted to do that because wasn't as well defined and that definitely changed over time as we started going through the implementation phases so does that look like adding are adding new vertex and edge types or just is it more about like optimizing and deciding where you need to put search indexes and that kind of thing or is it all of the above it was you know adding new vertexes and edges or properties on vertexes or property on edges in order to you know optimize well not only you know be able to optimize our queries but out the you know optimize our search indexes and since we kind of are using the same graph to do some analytical sort of queries and some transactional queries we needed to be able to support both use cases at least for some some of our simplistic example are simplistic analytics so being able to have all you can able to have that data in a way that you could query it both as a transactional workload in an analytical workload and it be reasonably performant was a bit of a an ongoing iterative process I should say it is an ongoing iterative process yeah as a matter of fact as far as the the performance aspect I was actually gonna ask a little bit about that so okay right you've got the the model part and and kind of the you know the iterations you have to go through to kind of figure that part out but then from the performance standpoint like was it was it a you know a home run the first hit or did you find that you still also had to iterate through how you are you know modeling your graph and stuff to get get it to perform well or how that actually work out I mean we definitely had to interact through you know you started out with what you think your best best guest data model yes but it definitely took some iterations to not only get the data model in a place that we could traverse it quickly but also get the true you know optimize the traversals and indices on those rehearsals to basically make it so we could handle a couple of different you know handle our different queries in different ways and you know in a couple places we ended up having to denormalize data down into you know different vertices from you know you know from other vertices in order to be able to get a performing query because you know you're trying to reverse out and have to hit you know 5,000 vertices it's gonna be a lot slower than if I only have to hit one facet that's true yeah nice okay so but yeah and it's David was hinting at you definitely had to figure out how to kind of scale up and in order to see what the performance was gonna be do you have any advice for somebody who's just starting out how do they know or maybe it's a testing question maybe it's a load testing kind of question more specifically like how do you when you have a schema how do you know that it's gonna perform how do you build up the scale and really see if you have the right schema I mean kind of you know the approach we took was we looked at what are realistic like what our realistic load on our customer from our customers were and what the realistic amount of data sizes we were pulling back were and then we kind of worked towards optimizing towards those cuz I mean I can make you a data model that you can pull back 10,000 vertices really quick yeah but if you're never if you're only ever gonna pull back a hundred why do I want to go through that headache of me it's good point you know so it was really about first you know first off its knowing your data knowing what knowing your data and knowing how you're gonna query it my graphs is much like a sander in that way you know is you know how you you need to know how you're gonna be able to query it out in order to be able to optimize for that's what I mean the same is true with the relational model as well you know if I know that I need to be able to pull these two things out at the same time maybe I want to denormalize them down into the same vertices so I don't have to make an extra you know extra traversal or extra traversal hops out in order to pull that baited that data back and then you know we mix it back in the end gotcha okay good so I know that you've already shared with us about how you would definitely make sure to train up your team before you get started on a big graph project do you have any other advice for somebody out there who is ready to embark on a graph database effort to incorporate that into you in existing or a new application yeah I mean I guess one of the other pieces of advice I would say is you know come at it with an open mind in as much as if you're used to working in a relational mindset don't try to directly translate your relational mindset and your relational expectations to a graph you know you can put a relational database directly nua graph but it's not going to be optimized I mean at the same time there's definitely some downsides you want to be aware of of using the graph you know the one that we there was probably the largest complaint from my team was basically the the lack of is robust a tooling as you're used to with graph with relational databases you know relational databases have been around for forty years they have tools around it's just right you know we did you know graph databases has maybe been around for ten at most you know they're still yeah at least some Sun of the modern form that to be sure right yeah I mean I always kind of think of graph databases as they're like teenagers you can eventually get the information out of you out of them that you want but it takes a little bit more work and you really want to put into it right well it does seem like there's a lot of effort going into the tooling and I think that that's like like a teenager eating a lot and growing rapidly that's the same for the maturity that I think is that we're really starting to see emerge for from the graph databases and the tooling around them absolutely I think that's actually one of the the biggest areas I see graph growing where you right now is is the tooling and making the tooling better and easier to use and easier to use for people that aren't necessarily grasp experts yeah you know the ability even just like what data stacks to do to be able to visualize the results of your query was a huge step forward from a green spring console you know being that those sorts of tooling and those sorts of improvements are are tremendously beneficial to actually trying to really get a graph up for production of a product is aja mm-hmm well thanks Dave I really appreciate it that was awesome and thanks everybody for watching another episode of the distributed data show see ya thank you for joining us again for the distributed data show we love your feedback so go to the distributed data show page on data Stax Academy and tell us what you think you can also find us on the data Stax Academy YouTube channel or find our podcast on iTunes Google Play or wherever you get great podcast while you're there make sure and subscribe so you don't miss a single episode [Music]",
    "segments": [
      {
        "start": 0.03,
        "duration": 4.17,
        "text": "welcome to another episode of the"
      },
      {
        "start": 2.37,
        "duration": 4.23,
        "text": "distributed data show brought to you by"
      },
      {
        "start": 4.2,
        "duration": 4.26,
        "text": "data Stax Academy where we bring you the"
      },
      {
        "start": 6.6,
        "duration": 4.05,
        "text": "latest news and interview technical"
      },
      {
        "start": 8.46,
        "duration": 8.55,
        "text": "experts to help you succeed at building"
      },
      {
        "start": 10.65,
        "duration": 8.43,
        "text": "large-scale distributed systems welcome"
      },
      {
        "start": 17.01,
        "duration": 4.47,
        "text": "to another episode of the distributed"
      },
      {
        "start": 19.08,
        "duration": 5.58,
        "text": "data show I'm David Gilardi this is Jeff"
      },
      {
        "start": 21.48,
        "duration": 5.879,
        "text": "carpenter and today our guest is Dave"
      },
      {
        "start": 24.66,
        "duration": 4.65,
        "text": "Beck Berger and today we're gonna talk"
      },
      {
        "start": 27.359,
        "duration": 4.111,
        "text": "about migrating an application using a"
      },
      {
        "start": 29.31,
        "duration": 4.53,
        "text": "legacy database to one using a craft"
      },
      {
        "start": 31.47,
        "duration": 3.63,
        "text": "based solution so Dave I like that name"
      },
      {
        "start": 33.84,
        "duration": 3.27,
        "text": "by the way it's a good name you know it"
      },
      {
        "start": 35.1,
        "duration": 5.01,
        "text": "feels very familiar some reason I don't"
      },
      {
        "start": 37.11,
        "duration": 4.5,
        "text": "know what it is but you know it's great"
      },
      {
        "start": 40.11,
        "duration": 3.51,
        "text": "so why don't you tell us a little bit"
      },
      {
        "start": 41.61,
        "duration": 4.07,
        "text": "about your background and you know how"
      },
      {
        "start": 43.62,
        "duration": 5.13,
        "text": "how'd you even get into graph databases"
      },
      {
        "start": 45.68,
        "duration": 5.649,
        "text": "yeah so I've been doing full stack"
      },
      {
        "start": 48.75,
        "duration": 5.789,
        "text": "software development for about 18 years"
      },
      {
        "start": 51.329,
        "duration": 5.491,
        "text": "now pretty actually mainly focusing on"
      },
      {
        "start": 54.539,
        "duration": 5.581,
        "text": "the.net stack for about or for most of"
      },
      {
        "start": 56.82,
        "duration": 4.77,
        "text": "that time about four or five years ago I"
      },
      {
        "start": 60.12,
        "duration": 3.899,
        "text": "kind of made the migration over to"
      },
      {
        "start": 61.59,
        "duration": 3.48,
        "text": "dealing with big data technologies in"
      },
      {
        "start": 64.019,
        "duration": 4.14,
        "text": "which point I kind of switched over to"
      },
      {
        "start": 65.07,
        "duration": 4.71,
        "text": "more of the java scala side i started"
      },
      {
        "start": 68.159,
        "duration": 3.871,
        "text": "out just kind of dealing with a variety"
      },
      {
        "start": 69.78,
        "duration": 3.99,
        "text": "of different uh you know different Big"
      },
      {
        "start": 72.03,
        "duration": 4.229,
        "text": "Data technologies including Cassandra"
      },
      {
        "start": 73.77,
        "duration": 5.94,
        "text": "and from there I was working at a"
      },
      {
        "start": 76.259,
        "duration": 7.11,
        "text": "consulting company and we were one of"
      },
      {
        "start": 79.71,
        "duration": 5.729,
        "text": "the initial we used to work on graph"
      },
      {
        "start": 83.369,
        "duration": 5.07,
        "text": "databases such as Titan and we kind of"
      },
      {
        "start": 85.439,
        "duration": 5.341,
        "text": "migrated across to working on DSC graph"
      },
      {
        "start": 88.439,
        "duration": 4.441,
        "text": "from the beta phase on so I've actually"
      },
      {
        "start": 90.78,
        "duration": 4.26,
        "text": "been kind of involved in pretty much"
      },
      {
        "start": 92.88,
        "duration": 4.77,
        "text": "exclusively using graph databases for"
      },
      {
        "start": 95.04,
        "duration": 5.16,
        "text": "about three or - two and a half three"
      },
      {
        "start": 97.65,
        "duration": 4.649,
        "text": "years now okay yeah what what kind of"
      },
      {
        "start": 100.2,
        "duration": 5.73,
        "text": "graph database problems are you working"
      },
      {
        "start": 102.299,
        "duration": 5.491,
        "text": "on right now so right now I'm chief"
      },
      {
        "start": 105.93,
        "duration": 5.43,
        "text": "Software Architect for a company called"
      },
      {
        "start": 107.79,
        "duration": 6.45,
        "text": "gene by gene what we do is we do genetic"
      },
      {
        "start": 111.36,
        "duration": 4.259,
        "text": "genealogy so we're a competitor with"
      },
      {
        "start": 114.24,
        "duration": 3.659,
        "text": "companies probably a little more"
      },
      {
        "start": 115.619,
        "duration": 5.161,
        "text": "familiar with if someone like 23andme or"
      },
      {
        "start": 117.899,
        "duration": 5.851,
        "text": "ancestry DNA basically you know you send"
      },
      {
        "start": 120.78,
        "duration": 6.15,
        "text": "us a DNA you basically you buy a kit"
      },
      {
        "start": 123.75,
        "duration": 5.25,
        "text": "from us we but you basically get it you"
      },
      {
        "start": 126.93,
        "duration": 4.86,
        "text": "swab your cheek you send that back to us"
      },
      {
        "start": 129.0,
        "duration": 4.5,
        "text": "and then we sequence your data we"
      },
      {
        "start": 131.79,
        "duration": 2.62,
        "text": "sequence your DNA and provide you out"
      },
      {
        "start": 133.5,
        "duration": 2.8,
        "text": "with"
      },
      {
        "start": 134.41,
        "duration": 5.4,
        "text": "part of the data that we provide to you"
      },
      {
        "start": 136.3,
        "duration": 5.58,
        "text": "is your genetic genealogy so we they are"
      },
      {
        "start": 139.81,
        "duration": 4.14,
        "text": "able to compare your DNA against the"
      },
      {
        "start": 141.88,
        "duration": 4.26,
        "text": "other people in our database to find out"
      },
      {
        "start": 143.95,
        "duration": 2.73,
        "text": "how you are related to them so you know"
      },
      {
        "start": 146.14,
        "duration": 2.13,
        "text": "you"
      },
      {
        "start": 146.68,
        "duration": 4.47,
        "text": "is it your third cousin three times"
      },
      {
        "start": 148.27,
        "duration": 5.1,
        "text": "removed or is it your aunt and uncle and"
      },
      {
        "start": 151.15,
        "duration": 4.71,
        "text": "we're able to basically run that against"
      },
      {
        "start": 153.37,
        "duration": 5.369,
        "text": "our data - or run your DNA against our"
      },
      {
        "start": 155.86,
        "duration": 4.379,
        "text": "data to be able to basically say are you"
      },
      {
        "start": 158.739,
        "duration": 4.621,
        "text": "related or and how roughly how closely"
      },
      {
        "start": 160.239,
        "duration": 4.86,
        "text": "are you related one of the specific"
      },
      {
        "start": 163.36,
        "duration": 6.24,
        "text": "parts that we're working with right now"
      },
      {
        "start": 165.099,
        "duration": 6.151,
        "text": "is Bill is basically rebuilding out our"
      },
      {
        "start": 169.6,
        "duration": 4.199,
        "text": "family tree portion of our application"
      },
      {
        "start": 171.25,
        "duration": 4.29,
        "text": "being genealogy being genealogists"
      },
      {
        "start": 173.799,
        "duration": 3.75,
        "text": "building out your family tree and have"
      },
      {
        "start": 175.54,
        "duration": 5.58,
        "text": "that function well is a really crucial"
      },
      {
        "start": 177.549,
        "duration": 4.921,
        "text": "part to our website so that is one of"
      },
      {
        "start": 181.12,
        "duration": 3.179,
        "text": "the things that we're kind of focusing"
      },
      {
        "start": 182.47,
        "duration": 4.049,
        "text": "on right now is getting that part of our"
      },
      {
        "start": 184.299,
        "duration": 4.621,
        "text": "business you know working very"
      },
      {
        "start": 186.519,
        "duration": 3.69,
        "text": "consistently performant and solid you"
      },
      {
        "start": 188.92,
        "duration": 3.539,
        "text": "know those things everybody wants in"
      },
      {
        "start": 190.209,
        "duration": 4.261,
        "text": "every web application right it's got to"
      },
      {
        "start": 192.459,
        "duration": 4.081,
        "text": "be fast it's got to be reliable it's got"
      },
      {
        "start": 194.47,
        "duration": 4.71,
        "text": "to be always on this is like the table"
      },
      {
        "start": 196.54,
        "duration": 4.169,
        "text": "stakes right you know it's funny too cuz"
      },
      {
        "start": 199.18,
        "duration": 4.74,
        "text": "as he's as he's talking about this this"
      },
      {
        "start": 200.709,
        "duration": 4.471,
        "text": "sounds like it sounds like a good use"
      },
      {
        "start": 203.92,
        "duration": 3.33,
        "text": "case for graph I feel like we should"
      },
      {
        "start": 205.18,
        "duration": 3.39,
        "text": "have Denice guys know on here right now"
      },
      {
        "start": 207.25,
        "duration": 4.019,
        "text": "you know determining whether or not oh"
      },
      {
        "start": 208.57,
        "duration": 4.8,
        "text": "just to provide like the this is a graph"
      },
      {
        "start": 211.269,
        "duration": 3.81,
        "text": "use case stamp of approval I think we"
      },
      {
        "start": 213.37,
        "duration": 3.63,
        "text": "need to give her like a judge wig and"
      },
      {
        "start": 215.079,
        "duration": 3.63,
        "text": "have her like in the background yeah and"
      },
      {
        "start": 217.0,
        "duration": 2.88,
        "text": "just have her rate whether or not this"
      },
      {
        "start": 218.709,
        "duration": 2.28,
        "text": "is a use case but I think she would"
      },
      {
        "start": 219.88,
        "duration": 3.439,
        "text": "approve I think she would have proved"
      },
      {
        "start": 220.989,
        "duration": 5.491,
        "text": "pretty sure I know everything would be"
      },
      {
        "start": 223.319,
        "duration": 5.591,
        "text": "so so like what made you you know decide"
      },
      {
        "start": 226.48,
        "duration": 3.87,
        "text": "to transition your your relational"
      },
      {
        "start": 228.91,
        "duration": 2.909,
        "text": "solution to graph I mean honestly I"
      },
      {
        "start": 230.35,
        "duration": 3.0,
        "text": "think you started just touching on it"
      },
      {
        "start": 231.819,
        "duration": 2.67,
        "text": "from from what I was hearing but now I'm"
      },
      {
        "start": 233.35,
        "duration": 2.94,
        "text": "gonna hear some of the details like"
      },
      {
        "start": 234.489,
        "duration": 3.87,
        "text": "how'd that happen well and that wasn't"
      },
      {
        "start": 236.29,
        "duration": 3.75,
        "text": "all you - I just want to interject like"
      },
      {
        "start": 238.359,
        "duration": 4.621,
        "text": "obviously they hired you as a graph"
      },
      {
        "start": 240.04,
        "duration": 4.71,
        "text": "expert knowing I think like foreshadow"
      },
      {
        "start": 242.98,
        "duration": 4.02,
        "text": "that they wanted to be making this"
      },
      {
        "start": 244.75,
        "duration": 4.59,
        "text": "transition right but obviously there was"
      },
      {
        "start": 247.0,
        "duration": 5.489,
        "text": "some some thought process that went into"
      },
      {
        "start": 249.34,
        "duration": 5.85,
        "text": "this so yeah so I mean our current"
      },
      {
        "start": 252.489,
        "duration": 4.59,
        "text": "application is it's the so the company"
      },
      {
        "start": 255.19,
        "duration": 4.019,
        "text": "has been around for 18 years and our"
      },
      {
        "start": 257.079,
        "duration": 5.4,
        "text": "application is been around for 18 years"
      },
      {
        "start": 259.209,
        "duration": 4.921,
        "text": "so it's you know it says you would"
      },
      {
        "start": 262.479,
        "duration": 3.75,
        "text": "probably expect it's a big it's a big"
      },
      {
        "start": 264.13,
        "duration": 3.37,
        "text": "monolithic application built on top of"
      },
      {
        "start": 266.229,
        "duration": 3.431,
        "text": "sequel server"
      },
      {
        "start": 267.5,
        "duration": 3.63,
        "text": "we're actually a.net shot mainly so we"
      },
      {
        "start": 269.66,
        "duration": 3.78,
        "text": "would spill on top of Microsoft sequel"
      },
      {
        "start": 271.13,
        "duration": 3.78,
        "text": "server so it's as I said it's a you know"
      },
      {
        "start": 273.44,
        "duration": 3.84,
        "text": "dotnet application build on top of"
      },
      {
        "start": 274.91,
        "duration": 3.99,
        "text": "sequel server and one of the big"
      },
      {
        "start": 277.28,
        "duration": 3.39,
        "text": "problems they were having as you do with"
      },
      {
        "start": 278.9,
        "duration": 4.71,
        "text": "pretty much any monolithic application"
      },
      {
        "start": 280.67,
        "duration": 4.11,
        "text": "is it was database bound you know the"
      },
      {
        "start": 283.61,
        "duration": 3.39,
        "text": "performance of our website was"
      },
      {
        "start": 284.78,
        "duration": 5.7,
        "text": "completely dependent on the performance"
      },
      {
        "start": 287.0,
        "duration": 5.49,
        "text": "of our database so you know since that"
      },
      {
        "start": 290.48,
        "duration": 3.69,
        "text": "database had grown organically over 18"
      },
      {
        "start": 292.49,
        "duration": 3.9,
        "text": "years as more and more features were"
      },
      {
        "start": 294.17,
        "duration": 5.16,
        "text": "added as we all know it was not exactly"
      },
      {
        "start": 296.39,
        "duration": 4.56,
        "text": "the optimal data model either so what we"
      },
      {
        "start": 299.33,
        "duration": 3.42,
        "text": "kind of decided to do was that they were"
      },
      {
        "start": 300.95,
        "duration": 3.719,
        "text": "going to start basically taking that"
      },
      {
        "start": 302.75,
        "duration": 4.76,
        "text": "monolithic database and breaking it out"
      },
      {
        "start": 304.669,
        "duration": 5.041,
        "text": "into kind of vertical feature silos and"
      },
      {
        "start": 307.51,
        "duration": 5.17,
        "text": "Bakey and building those feed capture"
      },
      {
        "start": 309.71,
        "duration": 4.88,
        "text": "silos using you know you know different"
      },
      {
        "start": 312.68,
        "duration": 4.89,
        "text": "data stores for different problems and"
      },
      {
        "start": 314.59,
        "duration": 4.45,
        "text": "different in micro services to basically"
      },
      {
        "start": 317.57,
        "duration": 3.54,
        "text": "allow you to do all those great things"
      },
      {
        "start": 319.04,
        "duration": 3.629,
        "text": "that micro services do is you know I can"
      },
      {
        "start": 321.11,
        "duration": 3.3,
        "text": "change them as I need to I can you know"
      },
      {
        "start": 322.669,
        "duration": 3.931,
        "text": "scale them up scale them down use my"
      },
      {
        "start": 324.41,
        "duration": 3.81,
        "text": "resources more efficiently you know"
      },
      {
        "start": 326.6,
        "duration": 4.41,
        "text": "those sorts of problems so that was kind"
      },
      {
        "start": 328.22,
        "duration": 5.09,
        "text": "of how they came to the decision of they"
      },
      {
        "start": 331.01,
        "duration": 4.35,
        "text": "wanted to do some architectural changes"
      },
      {
        "start": 333.31,
        "duration": 4.0,
        "text": "the first one they chose to do was"
      },
      {
        "start": 335.36,
        "duration": 3.96,
        "text": "actually one of the core features of our"
      },
      {
        "start": 337.31,
        "duration": 4.44,
        "text": "system which is that family tree aspect"
      },
      {
        "start": 339.32,
        "duration": 7.31,
        "text": "you know it's a genealogy site having"
      },
      {
        "start": 341.75,
        "duration": 9.0,
        "text": "your family tree yeah a little important"
      },
      {
        "start": 346.63,
        "duration": 7.18,
        "text": "so they basically a couple of the other"
      },
      {
        "start": 350.75,
        "duration": 5.58,
        "text": "senior devs and one of the other senior"
      },
      {
        "start": 353.81,
        "duration": 5.609,
        "text": "architects kind of looked looked at the"
      },
      {
        "start": 356.33,
        "duration": 4.11,
        "text": "problem space and decided that I looked"
      },
      {
        "start": 359.419,
        "duration": 2.071,
        "text": "at the problem they were trying to solve"
      },
      {
        "start": 360.44,
        "duration": 2.34,
        "text": "and the different tools that were out"
      },
      {
        "start": 361.49,
        "duration": 2.91,
        "text": "there and decided that they wanted to go"
      },
      {
        "start": 362.78,
        "duration": 3.33,
        "text": "with a graph database in that point"
      },
      {
        "start": 364.4,
        "duration": 5.76,
        "text": "that's when I came on to basically help"
      },
      {
        "start": 366.11,
        "duration": 5.16,
        "text": "them implement that out because as well"
      },
      {
        "start": 370.16,
        "duration": 3.27,
        "text": "get into talking about here you know"
      },
      {
        "start": 371.27,
        "duration": 4.2,
        "text": "making that switch is that there's"
      },
      {
        "start": 373.43,
        "duration": 4.5,
        "text": "there's definitely uh definite gotchas"
      },
      {
        "start": 375.47,
        "duration": 4.65,
        "text": "know in the in the whole switch admit to"
      },
      {
        "start": 377.93,
        "duration": 4.14,
        "text": "making it work gotcha okay so this is I"
      },
      {
        "start": 380.12,
        "duration": 3.33,
        "text": "get the architectural context here"
      },
      {
        "start": 382.07,
        "duration": 3.15,
        "text": "you're kind of transitioning from a"
      },
      {
        "start": 383.45,
        "duration": 2.88,
        "text": "monolith carving off pieces of it maybe"
      },
      {
        "start": 385.22,
        "duration": 3.9,
        "text": "a little bit at a time"
      },
      {
        "start": 386.33,
        "duration": 4.62,
        "text": "and moving to what sounds like a little"
      },
      {
        "start": 389.12,
        "duration": 4.26,
        "text": "bit more of a polyglot persistence kind"
      },
      {
        "start": 390.95,
        "duration": 4.05,
        "text": "of approach where different services"
      },
      {
        "start": 393.38,
        "duration": 3.78,
        "text": "might have different backing data source"
      },
      {
        "start": 395.0,
        "duration": 3.33,
        "text": "behind them am I on the right track yep"
      },
      {
        "start": 397.16,
        "duration": 2.46,
        "text": "no absolutely you're you're absolutely"
      },
      {
        "start": 398.33,
        "duration": 2.76,
        "text": "correct that's kind of where it's"
      },
      {
        "start": 399.62,
        "duration": 4.23,
        "text": "definitely going towards more of a poly"
      },
      {
        "start": 401.09,
        "duration": 4.95,
        "text": "sort of persistence where you know we're"
      },
      {
        "start": 403.85,
        "duration": 3.48,
        "text": "trying to choose the best best data"
      },
      {
        "start": 406.04,
        "duration": 3.599,
        "text": "store technology out there for the"
      },
      {
        "start": 407.33,
        "duration": 4.41,
        "text": "different problems that exists okay cool"
      },
      {
        "start": 409.639,
        "duration": 4.321,
        "text": "so so for at least some significant"
      },
      {
        "start": 411.74,
        "duration": 5.7,
        "text": "portions of the data that right solution"
      },
      {
        "start": 413.96,
        "duration": 4.769,
        "text": "is a graph database yeah yeah we're"
      },
      {
        "start": 417.44,
        "duration": 3.03,
        "text": "working through as I said we're kind of"
      },
      {
        "start": 418.729,
        "duration": 3.391,
        "text": "working through building out your family"
      },
      {
        "start": 420.47,
        "duration": 3.87,
        "text": "tree I mean what's the family tree but"
      },
      {
        "start": 422.12,
        "duration": 4.07,
        "text": "it's just you know it's a very recursive"
      },
      {
        "start": 424.34,
        "duration": 4.139,
        "text": "problem just by the true nature of it"
      },
      {
        "start": 426.19,
        "duration": 4.03,
        "text": "it's a recursive problem where you want"
      },
      {
        "start": 428.479,
        "duration": 4.681,
        "text": "to then be able to query it to answer"
      },
      {
        "start": 430.22,
        "duration": 5.34,
        "text": "very graph sort of problems you know"
      },
      {
        "start": 433.16,
        "duration": 3.75,
        "text": "right problems like what's the you know"
      },
      {
        "start": 435.56,
        "duration": 3.57,
        "text": "how is this person related to this"
      },
      {
        "start": 436.91,
        "duration": 4.29,
        "text": "person well if you want to find how two"
      },
      {
        "start": 439.13,
        "duration": 3.69,
        "text": "nodes in a graph are related to each"
      },
      {
        "start": 441.2,
        "duration": 2.73,
        "text": "other it's very easy if you want to do"
      },
      {
        "start": 442.82,
        "duration": 3.21,
        "text": "that in something like a relational"
      },
      {
        "start": 443.93,
        "duration": 4.2,
        "text": "database it's very time consuming and"
      },
      {
        "start": 446.03,
        "duration": 3.75,
        "text": "difficult and I don't want to write that"
      },
      {
        "start": 448.13,
        "duration": 3.78,
        "text": "CTE to make that happen"
      },
      {
        "start": 449.78,
        "duration": 4.44,
        "text": "right where's something like graph it's"
      },
      {
        "start": 451.91,
        "duration": 5.91,
        "text": "a simplest path query we probably all"
      },
      {
        "start": 454.22,
        "duration": 5.61,
        "text": "seen examples of the multi-page SQL"
      },
      {
        "start": 457.82,
        "duration": 3.45,
        "text": "query to do some of these kind of"
      },
      {
        "start": 459.83,
        "duration": 3.78,
        "text": "equivalent to what a graph traversal"
      },
      {
        "start": 461.27,
        "duration": 3.38,
        "text": "would look like right so okay that makes"
      },
      {
        "start": 463.61,
        "duration": 4.08,
        "text": "perfect sense to me"
      },
      {
        "start": 464.65,
        "duration": 5.68,
        "text": "but we all know in doing migrations like"
      },
      {
        "start": 467.69,
        "duration": 4.71,
        "text": "this like just because you have a use"
      },
      {
        "start": 470.33,
        "duration": 3.93,
        "text": "case that's a good fit for a graph that"
      },
      {
        "start": 472.4,
        "duration": 4.079,
        "text": "doesn't mean that everything is"
      },
      {
        "start": 474.26,
        "duration": 4.5,
        "text": "easy-peasy and you know that like these"
      },
      {
        "start": 476.479,
        "duration": 3.81,
        "text": "are obviously like this is the big"
      },
      {
        "start": 478.76,
        "duration": 3.779,
        "text": "money-making system of the company"
      },
      {
        "start": 480.289,
        "duration": 5.521,
        "text": "you're working on the crown jewels your"
      },
      {
        "start": 482.539,
        "duration": 6.271,
        "text": "I've destroyed a monolith myself before"
      },
      {
        "start": 485.81,
        "duration": 4.62,
        "text": "it's it's not easy so what were some of"
      },
      {
        "start": 488.81,
        "duration": 3.9,
        "text": "the challenges that you ran into in"
      },
      {
        "start": 490.43,
        "duration": 4.739,
        "text": "doing this migration oh I mean there was"
      },
      {
        "start": 492.71,
        "duration": 6.06,
        "text": "variety different challenges the first"
      },
      {
        "start": 495.169,
        "duration": 6.271,
        "text": "and probably most significant one is the"
      },
      {
        "start": 498.77,
        "duration": 4.41,
        "text": "fact that you know the team I was"
      },
      {
        "start": 501.44,
        "duration": 4.8,
        "text": "working with as well as pretty much I"
      },
      {
        "start": 503.18,
        "duration": 4.56,
        "text": "would guess 99% of you know development"
      },
      {
        "start": 506.24,
        "duration": 3.63,
        "text": "teams out there are very familiar with"
      },
      {
        "start": 507.74,
        "duration": 4.59,
        "text": "relational databases and not as familiar"
      },
      {
        "start": 509.87,
        "duration": 4.799,
        "text": "with graph databases so being able to"
      },
      {
        "start": 512.33,
        "duration": 4.199,
        "text": "kind of make that you know be able to"
      },
      {
        "start": 514.669,
        "duration": 4.05,
        "text": "able to teach people how to actually"
      },
      {
        "start": 516.529,
        "duration": 4.651,
        "text": "think think is it's thinking a graph"
      },
      {
        "start": 518.719,
        "duration": 5.161,
        "text": "mannerism versus a relational manner and"
      },
      {
        "start": 521.18,
        "duration": 3.96,
        "text": "work through the problems that a graph"
      },
      {
        "start": 523.88,
        "duration": 3.36,
        "text": "presents you that you don't have whether"
      },
      {
        "start": 525.14,
        "duration": 3.449,
        "text": "relational and things like that because"
      },
      {
        "start": 527.24,
        "duration": 3.57,
        "text": "you know in a relational this you know"
      },
      {
        "start": 528.589,
        "duration": 4.171,
        "text": "it's you know people are very used to"
      },
      {
        "start": 530.81,
        "duration": 3.57,
        "text": "how sequel works you know pretty much"
      },
      {
        "start": 532.76,
        "duration": 2.07,
        "text": "any developer that's been around for any"
      },
      {
        "start": 534.38,
        "duration": 4.04,
        "text": "amount of"
      },
      {
        "start": 534.83,
        "duration": 6.12,
        "text": "can write a reasonably good sequel query"
      },
      {
        "start": 538.42,
        "duration": 4.81,
        "text": "gremlin queries are not quite as easy to"
      },
      {
        "start": 540.95,
        "duration": 4.59,
        "text": "learn and they're very it's a very"
      },
      {
        "start": 543.23,
        "duration": 3.99,
        "text": "different way to think about the same"
      },
      {
        "start": 545.54,
        "duration": 3.12,
        "text": "sort of problem you know you're thinking"
      },
      {
        "start": 547.22,
        "duration": 3.39,
        "text": "about how you're traversing through your"
      },
      {
        "start": 548.66,
        "duration": 4.56,
        "text": "graph versus how I'm joining across"
      },
      {
        "start": 550.61,
        "duration": 4.53,
        "text": "tables and keys so there was definitely"
      },
      {
        "start": 553.22,
        "duration": 4.83,
        "text": "some challenges and basically getting"
      },
      {
        "start": 555.14,
        "duration": 5.16,
        "text": "the team up to speed on how how to think"
      },
      {
        "start": 558.05,
        "duration": 3.659,
        "text": "and work in a graph you know if anyone"
      },
      {
        "start": 560.3,
        "duration": 4.289,
        "text": "that's worked with a graph firm you know"
      },
      {
        "start": 561.709,
        "duration": 4.051,
        "text": "any amount of time finds that definitely"
      },
      {
        "start": 564.589,
        "duration": 2.731,
        "text": "has that there's definitely a learning"
      },
      {
        "start": 565.76,
        "duration": 3.75,
        "text": "curve that you have to go through with"
      },
      {
        "start": 567.32,
        "duration": 4.32,
        "text": "that sort of thing right I mean the"
      },
      {
        "start": 569.51,
        "duration": 4.29,
        "text": "other thing was one of one of the the"
      },
      {
        "start": 571.64,
        "duration": 3.66,
        "text": "key things we ended up kind of getting"
      },
      {
        "start": 573.8,
        "duration": 4.89,
        "text": "tripped up on was we were not only"
      },
      {
        "start": 575.3,
        "duration": 5.34,
        "text": "switching from a you know relational"
      },
      {
        "start": 578.69,
        "duration": 4.44,
        "text": "model to a graph model the same time we"
      },
      {
        "start": 580.64,
        "duration": 5.069,
        "text": "were kind of switching from a you know a"
      },
      {
        "start": 583.13,
        "duration": 5.31,
        "text": "response request acid transactions sort"
      },
      {
        "start": 585.709,
        "duration": 5.041,
        "text": "of system to one that is eventually"
      },
      {
        "start": 588.44,
        "duration": 3.75,
        "text": "consistent in a web application well"
      },
      {
        "start": 590.75,
        "duration": 3.72,
        "text": "it's dealing with eventual consistency"
      },
      {
        "start": 592.19,
        "duration": 3.45,
        "text": "in a web application you have to treat"
      },
      {
        "start": 594.47,
        "duration": 3.09,
        "text": "the problem a little bit different I"
      },
      {
        "start": 595.64,
        "duration": 3.15,
        "text": "can't you know create a family tree and"
      },
      {
        "start": 597.56,
        "duration": 2.76,
        "text": "then read it back immediately like"
      },
      {
        "start": 598.79,
        "duration": 2.64,
        "text": "you're used to being able to do so we"
      },
      {
        "start": 600.32,
        "duration": 3.69,
        "text": "had to work through those sorts of"
      },
      {
        "start": 601.43,
        "duration": 3.93,
        "text": "problems as well cool okay okay well"
      },
      {
        "start": 604.01,
        "duration": 3.269,
        "text": "like so given those particular"
      },
      {
        "start": 605.36,
        "duration": 3.69,
        "text": "challenges right the ones you were just"
      },
      {
        "start": 607.279,
        "duration": 5.011,
        "text": "talking about would you do it again the"
      },
      {
        "start": 609.05,
        "duration": 5.37,
        "text": "same way uh yeah I mean actually I would"
      },
      {
        "start": 612.29,
        "duration": 8.43,
        "text": "probably do it well the exact same way"
      },
      {
        "start": 614.42,
        "duration": 8.039,
        "text": "no let's dig deeper yeah would be some"
      },
      {
        "start": 620.72,
        "duration": 4.05,
        "text": "additional learnings I would take from"
      },
      {
        "start": 622.459,
        "duration": 4.531,
        "text": "it but yeah I mean I still think based"
      },
      {
        "start": 624.77,
        "duration": 3.39,
        "text": "on I mean what we're working on right"
      },
      {
        "start": 626.99,
        "duration": 3.089,
        "text": "now is really just kind of the"
      },
      {
        "start": 628.16,
        "duration": 3.45,
        "text": "foundation for a lot of the next-level"
      },
      {
        "start": 630.079,
        "duration": 3.0,
        "text": "things we want to be able to do that a"
      },
      {
        "start": 631.61,
        "duration": 4.469,
        "text": "graph that the graph will enable us to"
      },
      {
        "start": 633.079,
        "duration": 6.031,
        "text": "do as far as being able to match things"
      },
      {
        "start": 636.079,
        "duration": 4.95,
        "text": "and and and you know do all the great"
      },
      {
        "start": 639.11,
        "duration": 3.24,
        "text": "analytics that we can get with having"
      },
      {
        "start": 641.029,
        "duration": 3.0,
        "text": "all our information in a graph that are"
      },
      {
        "start": 642.35,
        "duration": 4.349,
        "text": "very difficult to do inside of a sequel"
      },
      {
        "start": 644.029,
        "duration": 5.37,
        "text": "server server or any other relational"
      },
      {
        "start": 646.699,
        "duration": 6.061,
        "text": "database so I think the benefits"
      },
      {
        "start": 649.399,
        "duration": 4.531,
        "text": "outweigh the the cost but I think you"
      },
      {
        "start": 652.76,
        "duration": 3.389,
        "text": "know the one thing I would definitely do"
      },
      {
        "start": 653.93,
        "duration": 4.55,
        "text": "a little bit differently the second time"
      },
      {
        "start": 656.149,
        "duration": 5.31,
        "text": "around versus the first time around is"
      },
      {
        "start": 658.48,
        "duration": 5.29,
        "text": "invest a little more in that upfront"
      },
      {
        "start": 661.459,
        "duration": 3.75,
        "text": "training on those kind of two key topics"
      },
      {
        "start": 663.77,
        "duration": 4.11,
        "text": "I talked about one being you know graph"
      },
      {
        "start": 665.209,
        "duration": 3.571,
        "text": "you know the nut that Kraft theory so"
      },
      {
        "start": 667.88,
        "duration": 3.11,
        "text": "much but basically"
      },
      {
        "start": 668.78,
        "duration": 4.5,
        "text": "I call it like pragmatic graph you know"
      },
      {
        "start": 670.99,
        "duration": 3.31,
        "text": "there's the fear your pal graft work and"
      },
      {
        "start": 673.28,
        "duration": 2.88,
        "text": "then you have to lay that layer of"
      },
      {
        "start": 674.3,
        "duration": 7.17,
        "text": "pragmatism on top of it somebody in a"
      },
      {
        "start": 676.16,
        "duration": 9.45,
        "text": "writing book pragmatic graph someone"
      },
      {
        "start": 681.47,
        "duration": 5.73,
        "text": "should write that book nice but what but"
      },
      {
        "start": 685.61,
        "duration": 4.44,
        "text": "seriously like what resources did you"
      },
      {
        "start": 687.2,
        "duration": 4.44,
        "text": "did you use to get your team up to speed"
      },
      {
        "start": 690.05,
        "duration": 3.39,
        "text": "your development team up to speed on"
      },
      {
        "start": 691.64,
        "duration": 4.05,
        "text": "graph how did you cuz you bet you got"
      },
      {
        "start": 693.44,
        "duration": 6.39,
        "text": "there eventually at least I hope you did"
      },
      {
        "start": 695.69,
        "duration": 5.85,
        "text": "well it's you know there was you know we"
      },
      {
        "start": 699.83,
        "duration": 3.33,
        "text": "probably started out with four or five"
      },
      {
        "start": 701.54,
        "duration": 4.65,
        "text": "people that were really learning graph"
      },
      {
        "start": 703.16,
        "duration": 4.95,
        "text": "and we ended up ready to two and a half"
      },
      {
        "start": 706.19,
        "duration": 2.79,
        "text": "or three of them that really kind of got"
      },
      {
        "start": 708.11,
        "duration": 3.66,
        "text": "it in the end"
      },
      {
        "start": 708.98,
        "duration": 4.17,
        "text": "okay you know we were trying to do this"
      },
      {
        "start": 711.77,
        "duration": 3.63,
        "text": "as well as getting everything else done"
      },
      {
        "start": 713.15,
        "duration": 3.9,
        "text": "at the same time same great there's a"
      },
      {
        "start": 715.4,
        "duration": 2.67,
        "text": "distinct time investment involved in"
      },
      {
        "start": 717.05,
        "duration": 3.75,
        "text": "getting this out you know I've been"
      },
      {
        "start": 718.07,
        "duration": 4.59,
        "text": "working no that makes sense I've seen a"
      },
      {
        "start": 720.8,
        "duration": 3.06,
        "text": "parallel experience in a development"
      },
      {
        "start": 722.66,
        "duration": 2.31,
        "text": "team where I was working with and they"
      },
      {
        "start": 723.86,
        "duration": 3.51,
        "text": "were you know they were coming up to"
      },
      {
        "start": 724.97,
        "duration": 5.16,
        "text": "speed on Cassandra and it ended up that"
      },
      {
        "start": 727.37,
        "duration": 5.76,
        "text": "not every person ended up being you know"
      },
      {
        "start": 730.13,
        "duration": 5.19,
        "text": "the full on Cassandra DBA with that"
      },
      {
        "start": 733.13,
        "duration": 3.78,
        "text": "level of data modeling skills right it's"
      },
      {
        "start": 735.32,
        "duration": 4.38,
        "text": "okay to have that specialization with"
      },
      {
        "start": 736.91,
        "duration": 4.44,
        "text": "your team to be sure yeah well it's not"
      },
      {
        "start": 739.7,
        "duration": 3.66,
        "text": "only okay to have that specialization"
      },
      {
        "start": 741.35,
        "duration": 4.44,
        "text": "with your team I mean my biggest concern"
      },
      {
        "start": 743.36,
        "duration": 4.62,
        "text": "was at least that the majority of the"
      },
      {
        "start": 745.79,
        "duration": 4.29,
        "text": "team understood the difference between"
      },
      {
        "start": 747.98,
        "duration": 4.11,
        "text": "the two and when one was right versus"
      },
      {
        "start": 750.08,
        "duration": 4.05,
        "text": "the other because you don't have to know"
      },
      {
        "start": 752.09,
        "duration": 3.63,
        "text": "one's an expert in everything but you"
      },
      {
        "start": 754.13,
        "duration": 2.73,
        "text": "it's really nice to know whether this is"
      },
      {
        "start": 755.72,
        "duration": 3.75,
        "text": "the right tool for the job"
      },
      {
        "start": 756.86,
        "duration": 3.75,
        "text": "or you know I always think about it you"
      },
      {
        "start": 759.47,
        "duration": 2.94,
        "text": "know the relational database is the"
      },
      {
        "start": 760.61,
        "duration": 2.94,
        "text": "hammer if all you have is a relational"
      },
      {
        "start": 762.41,
        "duration": 3.03,
        "text": "database everything looks like a nail"
      },
      {
        "start": 763.55,
        "duration": 3.15,
        "text": "well it's nice to be able to you know"
      },
      {
        "start": 765.44,
        "duration": 3.39,
        "text": "expand it out and they're like well this"
      },
      {
        "start": 766.7,
        "duration": 3.72,
        "text": "is a screw and this is a hammer and you"
      },
      {
        "start": 768.83,
        "duration": 3.15,
        "text": "know this is a but I can do different"
      },
      {
        "start": 770.42,
        "duration": 2.94,
        "text": "things with different pieces of it"
      },
      {
        "start": 771.98,
        "duration": 2.64,
        "text": "especially as we're moving from that you"
      },
      {
        "start": 773.36,
        "duration": 3.15,
        "text": "know monolithic sequel server"
      },
      {
        "start": 774.62,
        "duration": 5.1,
        "text": "application to a more polyglot"
      },
      {
        "start": 776.51,
        "duration": 4.47,
        "text": "persistence layer it's very good you"
      },
      {
        "start": 779.72,
        "duration": 3.51,
        "text": "know we wanted to make sure that people"
      },
      {
        "start": 780.98,
        "duration": 3.93,
        "text": "would be able to have the the knowledge"
      },
      {
        "start": 783.23,
        "duration": 4.53,
        "text": "and freedom to choose the right tools to"
      },
      {
        "start": 784.91,
        "duration": 4.23,
        "text": "do the right job right so just a follow"
      },
      {
        "start": 787.76,
        "duration": 3.45,
        "text": "up to you to dig in there a little bit"
      },
      {
        "start": 789.14,
        "duration": 5.25,
        "text": "more dude I was curious about how your"
      },
      {
        "start": 791.21,
        "duration": 5.43,
        "text": "schema evolved over time did you like"
      },
      {
        "start": 794.39,
        "duration": 4.32,
        "text": "really have like you're so good with"
      },
      {
        "start": 796.64,
        "duration": 3.57,
        "text": "graph databases and you know the problem"
      },
      {
        "start": 798.71,
        "duration": 3.03,
        "text": "domain so that you just nailed the"
      },
      {
        "start": 800.21,
        "duration": 4.8,
        "text": "scheme of the first time"
      },
      {
        "start": 801.74,
        "duration": 6.6,
        "text": "or was it you know migration overtime it"
      },
      {
        "start": 805.01,
        "duration": 5.31,
        "text": "what did that look like well so with the"
      },
      {
        "start": 808.34,
        "duration": 3.9,
        "text": "specific problem we solving with dealing"
      },
      {
        "start": 810.32,
        "duration": 3.84,
        "text": "with family trees there was actually"
      },
      {
        "start": 812.24,
        "duration": 6.33,
        "text": "there's a standard out there it's called"
      },
      {
        "start": 814.16,
        "duration": 8.79,
        "text": "gedcom GED Co em okay last updated in"
      },
      {
        "start": 818.57,
        "duration": 6.66,
        "text": "like the 1995 it's kind of somewhere"
      },
      {
        "start": 822.95,
        "duration": 8.34,
        "text": "between a really bad XML and a really"
      },
      {
        "start": 825.23,
        "duration": 8.4,
        "text": "bad linked list sort of now it's yeah"
      },
      {
        "start": 831.29,
        "duration": 3.81,
        "text": "it's interesting so we actually you know"
      },
      {
        "start": 833.63,
        "duration": 2.73,
        "text": "we wanted to be standards compliant"
      },
      {
        "start": 835.1,
        "duration": 2.7,
        "text": "because we wanted to be able to import"
      },
      {
        "start": 836.36,
        "duration": 2.76,
        "text": "and export these because that's the you"
      },
      {
        "start": 837.8,
        "duration": 3.6,
        "text": "know the industry standard in genealogy"
      },
      {
        "start": 839.12,
        "duration": 5.49,
        "text": "so we were able to at least use that as"
      },
      {
        "start": 841.4,
        "duration": 5.46,
        "text": "some of the basis of it okay but uh so"
      },
      {
        "start": 844.61,
        "duration": 4.05,
        "text": "the getting the basic data model correct"
      },
      {
        "start": 846.86,
        "duration": 3.66,
        "text": "was relatively easy because we could use"
      },
      {
        "start": 848.66,
        "duration": 5.1,
        "text": "that as our basis of how it works we had"
      },
      {
        "start": 850.52,
        "duration": 5.37,
        "text": "our domain model already set for us but"
      },
      {
        "start": 853.76,
        "duration": 4.02,
        "text": "it was when we started layering all the"
      },
      {
        "start": 855.89,
        "duration": 3.72,
        "text": "other you know value-added features that"
      },
      {
        "start": 857.78,
        "duration": 3.42,
        "text": "we do with our genetic aspects of it"
      },
      {
        "start": 859.61,
        "duration": 3.63,
        "text": "that started become a little more"
      },
      {
        "start": 861.2,
        "duration": 3.75,
        "text": "interesting about how how we wanted to"
      },
      {
        "start": 863.24,
        "duration": 4.8,
        "text": "do that because wasn't as well defined"
      },
      {
        "start": 864.95,
        "duration": 3.84,
        "text": "and that definitely changed over time as"
      },
      {
        "start": 868.04,
        "duration": 2.76,
        "text": "we started going through the"
      },
      {
        "start": 868.79,
        "duration": 5.04,
        "text": "implementation phases so does that look"
      },
      {
        "start": 870.8,
        "duration": 6.93,
        "text": "like adding are adding new vertex and"
      },
      {
        "start": 873.83,
        "duration": 5.91,
        "text": "edge types or just is it more about like"
      },
      {
        "start": 877.73,
        "duration": 4.26,
        "text": "optimizing and deciding where you need"
      },
      {
        "start": 879.74,
        "duration": 5.76,
        "text": "to put search indexes and that kind of"
      },
      {
        "start": 881.99,
        "duration": 5.67,
        "text": "thing or is it all of the above it was"
      },
      {
        "start": 885.5,
        "duration": 4.89,
        "text": "you know adding new vertexes and edges"
      },
      {
        "start": 887.66,
        "duration": 6.12,
        "text": "or properties on vertexes or property on"
      },
      {
        "start": 890.39,
        "duration": 4.65,
        "text": "edges in order to you know optimize well"
      },
      {
        "start": 893.78,
        "duration": 2.91,
        "text": "not only you know be able to optimize"
      },
      {
        "start": 895.04,
        "duration": 4.41,
        "text": "our queries but out the you know"
      },
      {
        "start": 896.69,
        "duration": 5.28,
        "text": "optimize our search indexes and since we"
      },
      {
        "start": 899.45,
        "duration": 4.53,
        "text": "kind of are using the same graph to do"
      },
      {
        "start": 901.97,
        "duration": 3.3,
        "text": "some analytical sort of queries and some"
      },
      {
        "start": 903.98,
        "duration": 3.51,
        "text": "transactional queries we needed to be"
      },
      {
        "start": 905.27,
        "duration": 3.93,
        "text": "able to support both use cases at least"
      },
      {
        "start": 907.49,
        "duration": 4.95,
        "text": "for some some of our simplistic example"
      },
      {
        "start": 909.2,
        "duration": 5.88,
        "text": "are simplistic analytics so being able"
      },
      {
        "start": 912.44,
        "duration": 4.53,
        "text": "to have all you can able to have that"
      },
      {
        "start": 915.08,
        "duration": 3.6,
        "text": "data in a way that you could query it"
      },
      {
        "start": 916.97,
        "duration": 4.14,
        "text": "both as a transactional workload in an"
      },
      {
        "start": 918.68,
        "duration": 6.12,
        "text": "analytical workload and it be reasonably"
      },
      {
        "start": 921.11,
        "duration": 7.11,
        "text": "performant was a bit of a an ongoing"
      },
      {
        "start": 924.8,
        "duration": 5.89,
        "text": "iterative process I should say it is an"
      },
      {
        "start": 928.22,
        "duration": 4.87,
        "text": "ongoing iterative process"
      },
      {
        "start": 930.69,
        "duration": 3.99,
        "text": "yeah as a matter of fact as far as the"
      },
      {
        "start": 933.09,
        "duration": 3.18,
        "text": "the performance aspect I was actually"
      },
      {
        "start": 934.68,
        "duration": 3.93,
        "text": "gonna ask a little bit about that so"
      },
      {
        "start": 936.27,
        "duration": 4.05,
        "text": "okay right you've got the the model part"
      },
      {
        "start": 938.61,
        "duration": 3.0,
        "text": "and and kind of the you know the"
      },
      {
        "start": 940.32,
        "duration": 2.52,
        "text": "iterations you have to go through to"
      },
      {
        "start": 941.61,
        "duration": 3.36,
        "text": "kind of figure that part out but then"
      },
      {
        "start": 942.84,
        "duration": 4.59,
        "text": "from the performance standpoint like was"
      },
      {
        "start": 944.97,
        "duration": 4.35,
        "text": "it was it a you know a home run the"
      },
      {
        "start": 947.43,
        "duration": 4.83,
        "text": "first hit or did you find that you still"
      },
      {
        "start": 949.32,
        "duration": 4.83,
        "text": "also had to iterate through how you are"
      },
      {
        "start": 952.26,
        "duration": 4.32,
        "text": "you know modeling your graph and stuff"
      },
      {
        "start": 954.15,
        "duration": 4.44,
        "text": "to get get it to perform well or how"
      },
      {
        "start": 956.58,
        "duration": 3.06,
        "text": "that actually work out I mean we"
      },
      {
        "start": 958.59,
        "duration": 2.61,
        "text": "definitely had to interact through you"
      },
      {
        "start": 959.64,
        "duration": 4.98,
        "text": "know you started out with what you think"
      },
      {
        "start": 961.2,
        "duration": 5.1,
        "text": "your best best guest data model yes but"
      },
      {
        "start": 964.62,
        "duration": 4.26,
        "text": "it definitely took some iterations to"
      },
      {
        "start": 966.3,
        "duration": 4.95,
        "text": "not only get the data model in a place"
      },
      {
        "start": 968.88,
        "duration": 3.51,
        "text": "that we could traverse it quickly but"
      },
      {
        "start": 971.25,
        "duration": 2.79,
        "text": "also get the true you know"
      },
      {
        "start": 972.39,
        "duration": 5.34,
        "text": "optimize the traversals and indices on"
      },
      {
        "start": 974.04,
        "duration": 5.43,
        "text": "those rehearsals to basically make it so"
      },
      {
        "start": 977.73,
        "duration": 3.66,
        "text": "we could handle a couple of different"
      },
      {
        "start": 979.47,
        "duration": 3.96,
        "text": "you know handle our different queries in"
      },
      {
        "start": 981.39,
        "duration": 3.36,
        "text": "different ways and you know in a couple"
      },
      {
        "start": 983.43,
        "duration": 3.72,
        "text": "places we ended up having to denormalize"
      },
      {
        "start": 984.75,
        "duration": 4.83,
        "text": "data down into you know different"
      },
      {
        "start": 987.15,
        "duration": 4.17,
        "text": "vertices from you know you know from"
      },
      {
        "start": 989.58,
        "duration": 3.3,
        "text": "other vertices in order to be able to"
      },
      {
        "start": 991.32,
        "duration": 3.36,
        "text": "get a performing query because you know"
      },
      {
        "start": 992.88,
        "duration": 4.44,
        "text": "you're trying to reverse out and have to"
      },
      {
        "start": 994.68,
        "duration": 3.96,
        "text": "hit you know 5,000 vertices it's gonna"
      },
      {
        "start": 997.32,
        "duration": 5.52,
        "text": "be a lot slower than if I only have to"
      },
      {
        "start": 998.64,
        "duration": 6.3,
        "text": "hit one facet that's true yeah nice okay"
      },
      {
        "start": 1002.84,
        "duration": 3.84,
        "text": "so but yeah and it's David was hinting"
      },
      {
        "start": 1004.94,
        "duration": 5.37,
        "text": "at you definitely had to figure out how"
      },
      {
        "start": 1006.68,
        "duration": 5.61,
        "text": "to kind of scale up and in order to see"
      },
      {
        "start": 1010.31,
        "duration": 4.53,
        "text": "what the performance was gonna be do you"
      },
      {
        "start": 1012.29,
        "duration": 4.98,
        "text": "have any advice for somebody who's just"
      },
      {
        "start": 1014.84,
        "duration": 5.25,
        "text": "starting out how do they know or maybe"
      },
      {
        "start": 1017.27,
        "duration": 4.92,
        "text": "it's a testing question maybe it's a"
      },
      {
        "start": 1020.09,
        "duration": 4.56,
        "text": "load testing kind of question more"
      },
      {
        "start": 1022.19,
        "duration": 4.769,
        "text": "specifically like how do you when you"
      },
      {
        "start": 1024.65,
        "duration": 4.26,
        "text": "have a schema how do you know that it's"
      },
      {
        "start": 1026.959,
        "duration": 4.051,
        "text": "gonna perform how do you build up the"
      },
      {
        "start": 1028.91,
        "duration": 6.6,
        "text": "scale and really see if you have the"
      },
      {
        "start": 1031.01,
        "duration": 7.17,
        "text": "right schema I mean kind of you know the"
      },
      {
        "start": 1035.51,
        "duration": 5.04,
        "text": "approach we took was we looked at what"
      },
      {
        "start": 1038.18,
        "duration": 3.779,
        "text": "are realistic like what our realistic"
      },
      {
        "start": 1040.55,
        "duration": 3.24,
        "text": "load on our customer from our customers"
      },
      {
        "start": 1041.959,
        "duration": 3.541,
        "text": "were and what the realistic amount of"
      },
      {
        "start": 1043.79,
        "duration": 2.88,
        "text": "data sizes we were pulling back were and"
      },
      {
        "start": 1045.5,
        "duration": 3.21,
        "text": "then we kind of worked towards"
      },
      {
        "start": 1046.67,
        "duration": 3.54,
        "text": "optimizing towards those cuz I mean I"
      },
      {
        "start": 1048.71,
        "duration": 4.26,
        "text": "can make you a data model that you can"
      },
      {
        "start": 1050.21,
        "duration": 4.89,
        "text": "pull back 10,000 vertices really quick"
      },
      {
        "start": 1052.97,
        "duration": 3.51,
        "text": "yeah but if you're never if you're only"
      },
      {
        "start": 1055.1,
        "duration": 3.36,
        "text": "ever gonna pull back a hundred why do I"
      },
      {
        "start": 1056.48,
        "duration": 4.5,
        "text": "want to go through that headache of me"
      },
      {
        "start": 1058.46,
        "duration": 4.17,
        "text": "it's good point you know so it was"
      },
      {
        "start": 1060.98,
        "duration": 3.21,
        "text": "really about first you know first off"
      },
      {
        "start": 1062.63,
        "duration": 2.76,
        "text": "its knowing your data knowing what"
      },
      {
        "start": 1064.19,
        "duration": 3.36,
        "text": "knowing your data and knowing how you're"
      },
      {
        "start": 1065.39,
        "duration": 3.3,
        "text": "gonna query it my graphs is much like a"
      },
      {
        "start": 1067.55,
        "duration": 2.76,
        "text": "sander in that way you know is you know"
      },
      {
        "start": 1068.69,
        "duration": 2.91,
        "text": "how you you need to know how you're"
      },
      {
        "start": 1070.31,
        "duration": 2.85,
        "text": "gonna be able to query it out in order"
      },
      {
        "start": 1071.6,
        "duration": 2.37,
        "text": "to be able to optimize for that's what I"
      },
      {
        "start": 1073.16,
        "duration": 3.33,
        "text": "mean the same is true with the"
      },
      {
        "start": 1073.97,
        "duration": 3.96,
        "text": "relational model as well you know if I"
      },
      {
        "start": 1076.49,
        "duration": 2.97,
        "text": "know that I need to be able to pull"
      },
      {
        "start": 1077.93,
        "duration": 2.97,
        "text": "these two things out at the same time"
      },
      {
        "start": 1079.46,
        "duration": 3.12,
        "text": "maybe I want to denormalize them down"
      },
      {
        "start": 1080.9,
        "duration": 3.45,
        "text": "into the same vertices so I don't have"
      },
      {
        "start": 1082.58,
        "duration": 3.9,
        "text": "to make an extra you know extra"
      },
      {
        "start": 1084.35,
        "duration": 3.72,
        "text": "traversal or extra traversal hops out in"
      },
      {
        "start": 1086.48,
        "duration": 4.02,
        "text": "order to pull that baited that data back"
      },
      {
        "start": 1088.07,
        "duration": 6.99,
        "text": "and then you know we mix it back in the"
      },
      {
        "start": 1090.5,
        "duration": 6.24,
        "text": "end gotcha okay good so I know that"
      },
      {
        "start": 1095.06,
        "duration": 3.57,
        "text": "you've already shared with us about how"
      },
      {
        "start": 1096.74,
        "duration": 4.35,
        "text": "you would definitely make sure to train"
      },
      {
        "start": 1098.63,
        "duration": 4.59,
        "text": "up your team before you get started on a"
      },
      {
        "start": 1101.09,
        "duration": 4.16,
        "text": "big graph project do you have any other"
      },
      {
        "start": 1103.22,
        "duration": 4.11,
        "text": "advice for somebody out there who is"
      },
      {
        "start": 1105.25,
        "duration": 4.12,
        "text": "ready to embark on a graph database"
      },
      {
        "start": 1107.33,
        "duration": 4.83,
        "text": "effort to incorporate that into you in"
      },
      {
        "start": 1109.37,
        "duration": 4.11,
        "text": "existing or a new application yeah I"
      },
      {
        "start": 1112.16,
        "duration": 4.56,
        "text": "mean I guess one of the other pieces of"
      },
      {
        "start": 1113.48,
        "duration": 6.0,
        "text": "advice I would say is you know come at"
      },
      {
        "start": 1116.72,
        "duration": 4.94,
        "text": "it with an open mind in as much as if"
      },
      {
        "start": 1119.48,
        "duration": 5.1,
        "text": "you're used to working in a relational"
      },
      {
        "start": 1121.66,
        "duration": 4.18,
        "text": "mindset don't try to directly translate"
      },
      {
        "start": 1124.58,
        "duration": 4.38,
        "text": "your relational mindset and your"
      },
      {
        "start": 1125.84,
        "duration": 4.5,
        "text": "relational expectations to a graph you"
      },
      {
        "start": 1128.96,
        "duration": 3.0,
        "text": "know you can put a relational database"
      },
      {
        "start": 1130.34,
        "duration": 3.84,
        "text": "directly nua graph but it's not going to"
      },
      {
        "start": 1131.96,
        "duration": 4.71,
        "text": "be optimized I mean at the same time"
      },
      {
        "start": 1134.18,
        "duration": 3.78,
        "text": "there's definitely some downsides you"
      },
      {
        "start": 1136.67,
        "duration": 2.79,
        "text": "want to be aware of of using the graph"
      },
      {
        "start": 1137.96,
        "duration": 3.78,
        "text": "you know the one that we there was"
      },
      {
        "start": 1139.46,
        "duration": 6.45,
        "text": "probably the largest complaint from my"
      },
      {
        "start": 1141.74,
        "duration": 6.06,
        "text": "team was basically the the lack of is"
      },
      {
        "start": 1145.91,
        "duration": 3.6,
        "text": "robust a tooling as you're used to with"
      },
      {
        "start": 1147.8,
        "duration": 2.91,
        "text": "graph with relational databases"
      },
      {
        "start": 1149.51,
        "duration": 4.17,
        "text": "you know relational databases have been"
      },
      {
        "start": 1150.71,
        "duration": 5.49,
        "text": "around for forty years they have tools"
      },
      {
        "start": 1153.68,
        "duration": 4.71,
        "text": "around it's just right you know we did"
      },
      {
        "start": 1156.2,
        "duration": 5.49,
        "text": "you know graph databases has maybe been"
      },
      {
        "start": 1158.39,
        "duration": 4.71,
        "text": "around for ten at most you know they're"
      },
      {
        "start": 1161.69,
        "duration": 4.11,
        "text": "still yeah at least some Sun of the"
      },
      {
        "start": 1163.1,
        "duration": 4.53,
        "text": "modern form that to be sure right yeah I"
      },
      {
        "start": 1165.8,
        "duration": 4.02,
        "text": "mean I always kind of think of graph"
      },
      {
        "start": 1167.63,
        "duration": 3.69,
        "text": "databases as they're like teenagers you"
      },
      {
        "start": 1169.82,
        "duration": 3.36,
        "text": "can eventually get the information out"
      },
      {
        "start": 1171.32,
        "duration": 3.3,
        "text": "of you out of them that you want but it"
      },
      {
        "start": 1173.18,
        "duration": 6.99,
        "text": "takes a little bit more work and you"
      },
      {
        "start": 1174.62,
        "duration": 6.9,
        "text": "really want to put into it right well it"
      },
      {
        "start": 1180.17,
        "duration": 2.88,
        "text": "does seem like there's a lot of effort"
      },
      {
        "start": 1181.52,
        "duration": 5.28,
        "text": "going into the tooling and I think that"
      },
      {
        "start": 1183.05,
        "duration": 7.2,
        "text": "that's like like a teenager eating a lot"
      },
      {
        "start": 1186.8,
        "duration": 5.34,
        "text": "and growing rapidly that's the same for"
      },
      {
        "start": 1190.25,
        "duration": 3.48,
        "text": "the maturity that I think is"
      },
      {
        "start": 1192.14,
        "duration": 3.93,
        "text": "that we're really starting to see emerge"
      },
      {
        "start": 1193.73,
        "duration": 4.38,
        "text": "for from the graph databases and the"
      },
      {
        "start": 1196.07,
        "duration": 3.39,
        "text": "tooling around them absolutely I think"
      },
      {
        "start": 1198.11,
        "duration": 2.7,
        "text": "that's actually one of the the biggest"
      },
      {
        "start": 1199.46,
        "duration": 2.97,
        "text": "areas I see graph growing where you"
      },
      {
        "start": 1200.81,
        "duration": 3.24,
        "text": "right now is is the tooling and making"
      },
      {
        "start": 1202.43,
        "duration": 2.73,
        "text": "the tooling better and easier to use and"
      },
      {
        "start": 1204.05,
        "duration": 3.54,
        "text": "easier to use for people that aren't"
      },
      {
        "start": 1205.16,
        "duration": 4.41,
        "text": "necessarily grasp experts yeah you know"
      },
      {
        "start": 1207.59,
        "duration": 3.57,
        "text": "the ability even just like what data"
      },
      {
        "start": 1209.57,
        "duration": 3.9,
        "text": "stacks to do to be able to visualize the"
      },
      {
        "start": 1211.16,
        "duration": 5.91,
        "text": "results of your query was a huge step"
      },
      {
        "start": 1213.47,
        "duration": 5.1,
        "text": "forward from a green spring console you"
      },
      {
        "start": 1217.07,
        "duration": 4.23,
        "text": "know being that those sorts of tooling"
      },
      {
        "start": 1218.57,
        "duration": 4.98,
        "text": "and those sorts of improvements are are"
      },
      {
        "start": 1221.3,
        "duration": 4.35,
        "text": "tremendously beneficial to actually"
      },
      {
        "start": 1223.55,
        "duration": 4.59,
        "text": "trying to really get a graph up for"
      },
      {
        "start": 1225.65,
        "duration": 4.62,
        "text": "production of a product is aja mm-hmm"
      },
      {
        "start": 1228.14,
        "duration": 4.92,
        "text": "well thanks Dave I really appreciate it"
      },
      {
        "start": 1230.27,
        "duration": 4.77,
        "text": "that was awesome and thanks everybody"
      },
      {
        "start": 1233.06,
        "duration": 5.18,
        "text": "for watching another episode of the"
      },
      {
        "start": 1235.04,
        "duration": 5.49,
        "text": "distributed data show see ya"
      },
      {
        "start": 1238.24,
        "duration": 4.27,
        "text": "thank you for joining us again for the"
      },
      {
        "start": 1240.53,
        "duration": 3.69,
        "text": "distributed data show we love your"
      },
      {
        "start": 1242.51,
        "duration": 3.72,
        "text": "feedback so go to the distributed data"
      },
      {
        "start": 1244.22,
        "duration": 3.81,
        "text": "show page on data Stax Academy and tell"
      },
      {
        "start": 1246.23,
        "duration": 3.66,
        "text": "us what you think you can also find us"
      },
      {
        "start": 1248.03,
        "duration": 4.35,
        "text": "on the data Stax Academy YouTube channel"
      },
      {
        "start": 1249.89,
        "duration": 4.86,
        "text": "or find our podcast on iTunes Google"
      },
      {
        "start": 1252.38,
        "duration": 4.35,
        "text": "Play or wherever you get great podcast"
      },
      {
        "start": 1254.75,
        "duration": 3.42,
        "text": "while you're there make sure and"
      },
      {
        "start": 1256.73,
        "duration": 3.56,
        "text": "subscribe so you don't miss a single"
      },
      {
        "start": 1258.17,
        "duration": 6.0,
        "text": "episode"
      },
      {
        "start": 1260.29,
        "duration": 3.88,
        "text": "[Music]"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-16T06:43:25.763645+00:00"
}