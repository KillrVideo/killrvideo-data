{
  "video_id": "o2Z3XwsTLZ4",
  "title": "DS320.45 Spark SQL: Saving DataFrames to Cassandra | DataStax Enterprise Analytics",
  "description": "#DataStaxAcademy #DS320\nDS320.45 Spark SQL: Saving DataFrames to Cassandra\nIn this course, you will learn how to effectively and efficiently solve analytical problems with Apache Spark™, Apache Cassandra™, and DataStax Enterprise. You will learn about the Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques.  You will also learn the basics of the productive and robust Scala programming language for data analysis and processing in Apache Spark™.\n\nLEARN FOR FREE at https://academy.datastax.com -- access all the FREE complete courses, tutorials, and hands-on exercises.\n\nASK QUESTIONS at https://community.datastax.com -- where experts from DataStax & the Apache Cassandra community share their expertise everyday.",
  "published_at": "2020-08-16T00:34:20Z",
  "thumbnail": "https://i.ytimg.com/vi/o2Z3XwsTLZ4/maxresdefault.jpg",
  "channel_title": "DataStax Developers",
  "channel_id": "UCAIQY251avaMv7bBv5PCo-A",
  "tags": [
    "cassandra",
    "tutorial",
    "apache_cassandra",
    "performance",
    "datastax"
  ],
  "url": "https://www.youtube.com/watch?v=o2Z3XwsTLZ4",
  "transcript": {
    "available": true,
    "language": "English (auto-generated)",
    "language_code": "en",
    "is_generated": true,
    "text": "[Music] well once we've done some computation with our data frames we're going to want to write them back into cassandra the whole idea with analytics is we'll take some large set of data do interesting analysis on it and write the aggregated results back to the database where they can be easily queried at low latency it's actually pretty easy to get this done with spark sql so let's look at a quick example imagine we're doing some super simple kind of etl process we've got our movies table and we want to extract and transform that data and load it into a table called family movies and when i say transform i mean we want to filter it to only movies containing the genre family pretty simple we don't want to muddy the middle with a bunch of complex computations that's your job to do in your own spark sql applications we just want to show you the basic process of getting the data out transforming it and finally writing it which is the new thing in this section back to cassandra so here we are we'll do our read using the read method on the cassandra sql context we're going to get all of the rows from the movies table into our initial data frame here's a little twist on filtering we haven't shown you precisely this before but we're wanting to filter on a collection column that filter method you can pass in a string expression like actor equals johnny depp or release here equals 2008. in this case we're going to use the column api now that call method is getting imported from that spark sql functions package on the import line there what it does is it creates an instance of a class called column and that class check it out in the api docs has got various ways through a fluent api of creating conditions in this case we're asserting that that collection genres contains the string families we pass that in to the filter method and away we go we could show that real quickly as you see there on the slide we've got a few rows you can't see the whole genres collection for each one but if you look at the titles those look like family movies and finally we save back to cassandra this is easy on the data frame we use the write method we said we want this to be cassandra data that we're writing out the table name is family movies the key spaces killer video save it and away we go now this is a simple example because the schemas of the two tables are the same of course we have complete power over what the schema of this resulting data frame is going to be we're able to do projections and do the things that we need to manipulate the schema so the saving here really is pretty simple we're able to write the data back into cassandra this would be the case with any data frame we create in any spark sql application we're probably going to want to write that back into a cassandra table so we can do nice low latency reads on those results in some other part of our application",
    "segments": [
      {
        "start": 0.06,
        "duration": 3.45,
        "text": "[Music]"
      },
      {
        "start": 7.04,
        "duration": 3.679,
        "text": "well once we've done some computation"
      },
      {
        "start": 9.12,
        "duration": 3.2,
        "text": "with our data frames we're going to want"
      },
      {
        "start": 10.719,
        "duration": 2.241,
        "text": "to write them back into cassandra the"
      },
      {
        "start": 12.32,
        "duration": 2.319,
        "text": "whole idea"
      },
      {
        "start": 12.96,
        "duration": 3.84,
        "text": "with analytics is we'll take some large"
      },
      {
        "start": 14.639,
        "duration": 2.48,
        "text": "set of data do interesting analysis on"
      },
      {
        "start": 16.8,
        "duration": 2.399,
        "text": "it"
      },
      {
        "start": 17.119,
        "duration": 3.521,
        "text": "and write the aggregated results back to"
      },
      {
        "start": 19.199,
        "duration": 3.361,
        "text": "the database where they can be easily"
      },
      {
        "start": 20.64,
        "duration": 3.44,
        "text": "queried at low latency it's actually"
      },
      {
        "start": 22.56,
        "duration": 2.719,
        "text": "pretty easy to get this done with spark"
      },
      {
        "start": 24.08,
        "duration": 3.039,
        "text": "sql so let's look at a quick"
      },
      {
        "start": 25.279,
        "duration": 3.84,
        "text": "example imagine we're doing some super"
      },
      {
        "start": 27.119,
        "duration": 2.881,
        "text": "simple kind of etl process we've got our"
      },
      {
        "start": 29.119,
        "duration": 2.721,
        "text": "movies table"
      },
      {
        "start": 30.0,
        "duration": 3.28,
        "text": "and we want to extract and transform"
      },
      {
        "start": 31.84,
        "duration": 2.64,
        "text": "that data and load it into a table"
      },
      {
        "start": 33.28,
        "duration": 3.119,
        "text": "called family movies"
      },
      {
        "start": 34.48,
        "duration": 3.04,
        "text": "and when i say transform i mean we want"
      },
      {
        "start": 36.399,
        "duration": 4.16,
        "text": "to filter it to"
      },
      {
        "start": 37.52,
        "duration": 3.68,
        "text": "only movies containing the genre family"
      },
      {
        "start": 40.559,
        "duration": 2.481,
        "text": "pretty simple"
      },
      {
        "start": 41.2,
        "duration": 3.679,
        "text": "we don't want to muddy the middle with a"
      },
      {
        "start": 43.04,
        "duration": 3.679,
        "text": "bunch of complex computations that's"
      },
      {
        "start": 44.879,
        "duration": 3.121,
        "text": "your job to do in your own spark sql"
      },
      {
        "start": 46.719,
        "duration": 2.881,
        "text": "applications we just want to show you"
      },
      {
        "start": 48.0,
        "duration": 1.92,
        "text": "the basic process of getting the data"
      },
      {
        "start": 49.6,
        "duration": 2.32,
        "text": "out"
      },
      {
        "start": 49.92,
        "duration": 3.36,
        "text": "transforming it and finally writing it"
      },
      {
        "start": 51.92,
        "duration": 3.44,
        "text": "which is the new thing"
      },
      {
        "start": 53.28,
        "duration": 3.68,
        "text": "in this section back to cassandra so"
      },
      {
        "start": 55.36,
        "duration": 3.28,
        "text": "here we are we'll do our read"
      },
      {
        "start": 56.96,
        "duration": 3.279,
        "text": "using the read method on the cassandra"
      },
      {
        "start": 58.64,
        "duration": 4.239,
        "text": "sql context we're going to get"
      },
      {
        "start": 60.239,
        "duration": 4.56,
        "text": "all of the rows from the movies table"
      },
      {
        "start": 62.879,
        "duration": 2.881,
        "text": "into our initial data frame here's a"
      },
      {
        "start": 64.799,
        "duration": 3.121,
        "text": "little twist on"
      },
      {
        "start": 65.76,
        "duration": 4.48,
        "text": "filtering we haven't shown you precisely"
      },
      {
        "start": 67.92,
        "duration": 4.64,
        "text": "this before but we're wanting to filter"
      },
      {
        "start": 70.24,
        "duration": 4.16,
        "text": "on a collection column that filter"
      },
      {
        "start": 72.56,
        "duration": 4.0,
        "text": "method you can pass in a string"
      },
      {
        "start": 74.4,
        "duration": 4.0,
        "text": "expression like actor equals johnny depp"
      },
      {
        "start": 76.56,
        "duration": 3.199,
        "text": "or release here equals 2008."
      },
      {
        "start": 78.4,
        "duration": 3.039,
        "text": "in this case we're going to use the"
      },
      {
        "start": 79.759,
        "duration": 4.161,
        "text": "column api now that"
      },
      {
        "start": 81.439,
        "duration": 4.241,
        "text": "call method is getting imported from"
      },
      {
        "start": 83.92,
        "duration": 3.6,
        "text": "that spark sql functions"
      },
      {
        "start": 85.68,
        "duration": 3.2,
        "text": "package on the import line there what it"
      },
      {
        "start": 87.52,
        "duration": 2.08,
        "text": "does is it creates an instance of a"
      },
      {
        "start": 88.88,
        "duration": 2.64,
        "text": "class called"
      },
      {
        "start": 89.6,
        "duration": 3.6,
        "text": "column and that class check it out in"
      },
      {
        "start": 91.52,
        "duration": 4.0,
        "text": "the api docs has got"
      },
      {
        "start": 93.2,
        "duration": 3.76,
        "text": "various ways through a fluent api of"
      },
      {
        "start": 95.52,
        "duration": 3.36,
        "text": "creating conditions"
      },
      {
        "start": 96.96,
        "duration": 3.44,
        "text": "in this case we're asserting that that"
      },
      {
        "start": 98.88,
        "duration": 4.0,
        "text": "collection genres"
      },
      {
        "start": 100.4,
        "duration": 3.28,
        "text": "contains the string families we pass"
      },
      {
        "start": 102.88,
        "duration": 3.44,
        "text": "that in"
      },
      {
        "start": 103.68,
        "duration": 3.92,
        "text": "to the filter method and away we go we"
      },
      {
        "start": 106.32,
        "duration": 2.88,
        "text": "could show that real quickly"
      },
      {
        "start": 107.6,
        "duration": 3.12,
        "text": "as you see there on the slide we've got"
      },
      {
        "start": 109.2,
        "duration": 3.04,
        "text": "a few rows you can't see the whole"
      },
      {
        "start": 110.72,
        "duration": 2.8,
        "text": "genres collection for each one"
      },
      {
        "start": 112.24,
        "duration": 3.28,
        "text": "but if you look at the titles those look"
      },
      {
        "start": 113.52,
        "duration": 3.12,
        "text": "like family movies and finally we save"
      },
      {
        "start": 115.52,
        "duration": 3.919,
        "text": "back to cassandra"
      },
      {
        "start": 116.64,
        "duration": 3.759,
        "text": "this is easy on the data frame we use"
      },
      {
        "start": 119.439,
        "duration": 3.201,
        "text": "the write method"
      },
      {
        "start": 120.399,
        "duration": 3.601,
        "text": "we said we want this to be cassandra"
      },
      {
        "start": 122.64,
        "duration": 3.2,
        "text": "data that we're writing out"
      },
      {
        "start": 124.0,
        "duration": 3.2,
        "text": "the table name is family movies the key"
      },
      {
        "start": 125.84,
        "duration": 3.839,
        "text": "spaces killer video"
      },
      {
        "start": 127.2,
        "duration": 4.24,
        "text": "save it and away we go now this is a"
      },
      {
        "start": 129.679,
        "duration": 2.64,
        "text": "simple example because the schemas of"
      },
      {
        "start": 131.44,
        "duration": 3.2,
        "text": "the two tables"
      },
      {
        "start": 132.319,
        "duration": 4.56,
        "text": "are the same of course we have complete"
      },
      {
        "start": 134.64,
        "duration": 4.0,
        "text": "power over what the schema of this"
      },
      {
        "start": 136.879,
        "duration": 3.521,
        "text": "resulting data frame is going to be"
      },
      {
        "start": 138.64,
        "duration": 2.72,
        "text": "we're able to do projections and do the"
      },
      {
        "start": 140.4,
        "duration": 3.04,
        "text": "things that we need"
      },
      {
        "start": 141.36,
        "duration": 3.76,
        "text": "to manipulate the schema so the saving"
      },
      {
        "start": 143.44,
        "duration": 3.519,
        "text": "here really is"
      },
      {
        "start": 145.12,
        "duration": 3.839,
        "text": "pretty simple we're able to write the"
      },
      {
        "start": 146.959,
        "duration": 3.92,
        "text": "data back into cassandra this would be"
      },
      {
        "start": 148.959,
        "duration": 4.0,
        "text": "the case with any data frame we create"
      },
      {
        "start": 150.879,
        "duration": 3.44,
        "text": "in any spark sql application we're"
      },
      {
        "start": 152.959,
        "duration": 1.92,
        "text": "probably going to want to write that"
      },
      {
        "start": 154.319,
        "duration": 2.801,
        "text": "back"
      },
      {
        "start": 154.879,
        "duration": 3.601,
        "text": "into a cassandra table so we can do nice"
      },
      {
        "start": 157.12,
        "duration": 3.759,
        "text": "low latency reads"
      },
      {
        "start": 158.48,
        "duration": 12.08,
        "text": "on those results in some other part of"
      },
      {
        "start": 160.879,
        "duration": 9.681,
        "text": "our application"
      }
    ],
    "error": null,
    "error_type": null
  },
  "collected_at": "2025-12-15T23:09:19.841578+00:00"
}